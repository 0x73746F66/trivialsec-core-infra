{"meta":{"exported_on":1617200449006,"version":"4.1.2"},"data":{"migrations_lock":[{"lock_key":"km01","locked":0,"acquired_at":"2021-03-31 13:09:14","released_at":"2021-03-31 13:09:17"}],"migrations":[{"id":1,"name":"1-create-tables.js","version":"init","currentVersion":"4.1"},{"id":2,"name":"2-create-fixtures.js","version":"init","currentVersion":"4.1"},{"id":3,"name":"1-post-excerpt.js","version":"1.3","currentVersion":"4.1"},{"id":4,"name":"1-codeinjection-post.js","version":"1.4","currentVersion":"4.1"},{"id":5,"name":"1-og-twitter-post.js","version":"1.5","currentVersion":"4.1"},{"id":6,"name":"1-add-backup-client.js","version":"1.7","currentVersion":"4.1"},{"id":7,"name":"1-add-permissions-redirect.js","version":"1.9","currentVersion":"4.1"},{"id":8,"name":"1-custom-template-post.js","version":"1.13","currentVersion":"4.1"},{"id":9,"name":"2-theme-permissions.js","version":"1.13","currentVersion":"4.1"},{"id":10,"name":"1-add-webhooks-table.js","version":"1.18","currentVersion":"4.1"},{"id":11,"name":"1-webhook-permissions.js","version":"1.19","currentVersion":"4.1"},{"id":12,"name":"1-remove-settings-keys.js","version":"1.20","currentVersion":"4.1"},{"id":13,"name":"1-add-contributor-role.js","version":"1.21","currentVersion":"4.1"},{"id":14,"name":"1-multiple-authors-DDL.js","version":"1.22","currentVersion":"4.1"},{"id":15,"name":"1-multiple-authors-DML.js","version":"1.22","currentVersion":"4.1"},{"id":16,"name":"1-update-koenig-beta-html.js","version":"1.25","currentVersion":"4.1"},{"id":17,"name":"2-demo-post.js","version":"1.25","currentVersion":"4.1"},{"id":18,"name":"1-rename-amp-column.js","version":"2.0","currentVersion":"4.1"},{"id":19,"name":"2-update-posts.js","version":"2.0","currentVersion":"4.1"},{"id":20,"name":"3-remove-koenig-labs.js","version":"2.0","currentVersion":"4.1"},{"id":21,"name":"4-permalink-setting.js","version":"2.0","currentVersion":"4.1"},{"id":22,"name":"5-remove-demo-post.js","version":"2.0","currentVersion":"4.1"},{"id":23,"name":"6-replace-fixture-posts.js","version":"2.0","currentVersion":"4.1"},{"id":24,"name":"1-add-sessions-table.js","version":"2.2","currentVersion":"4.1"},{"id":25,"name":"2-add-integrations-and-api-key-tables.js","version":"2.2","currentVersion":"4.1"},{"id":26,"name":"3-insert-admin-integration-role.js","version":"2.2","currentVersion":"4.1"},{"id":27,"name":"4-insert-integration-and-api-key-permissions.js","version":"2.2","currentVersion":"4.1"},{"id":28,"name":"5-add-mobiledoc-revisions-table.js","version":"2.2","currentVersion":"4.1"},{"id":29,"name":"1-add-webhook-columns.js","version":"2.3","currentVersion":"4.1"},{"id":30,"name":"2-add-webhook-edit-permission.js","version":"2.3","currentVersion":"4.1"},{"id":31,"name":"1-add-webhook-permission-roles.js","version":"2.6","currentVersion":"4.1"},{"id":32,"name":"1-add-members-table.js","version":"2.8","currentVersion":"4.1"},{"id":33,"name":"1-remove-empty-strings.js","version":"2.13","currentVersion":"4.1"},{"id":34,"name":"1-add-actions-table.js","version":"2.14","currentVersion":"4.1"},{"id":35,"name":"2-add-actions-permissions.js","version":"2.14","currentVersion":"4.1"},{"id":36,"name":"1-add-type-column-to-integrations.js","version":"2.15","currentVersion":"4.1"},{"id":37,"name":"2-insert-zapier-integration.js","version":"2.15","currentVersion":"4.1"},{"id":38,"name":"1-add-members-perrmissions.js","version":"2.16","currentVersion":"4.1"},{"id":39,"name":"1-normalize-settings.js","version":"2.17","currentVersion":"4.1"},{"id":40,"name":"2-posts-add-canonical-url.js","version":"2.17","currentVersion":"4.1"},{"id":41,"name":"1-restore-settings-from-backup.js","version":"2.18","currentVersion":"4.1"},{"id":42,"name":"1-update-editor-permissions.js","version":"2.21","currentVersion":"4.1"},{"id":43,"name":"1-add-member-permissions-to-roles.js","version":"2.22","currentVersion":"4.1"},{"id":44,"name":"1-insert-ghost-db-backup-role.js","version":"2.27","currentVersion":"4.1"},{"id":45,"name":"2-insert-db-backup-integration.js","version":"2.27","currentVersion":"4.1"},{"id":46,"name":"3-add-subdirectory-to-relative-canonical-urls.js","version":"2.27","currentVersion":"4.1"},{"id":47,"name":"1-add-db-backup-content-permission.js","version":"2.28","currentVersion":"4.1"},{"id":48,"name":"2-add-db-backup-content-permission-to-roles.js","version":"2.28","currentVersion":"4.1"},{"id":49,"name":"3-insert-ghost-scheduler-role.js","version":"2.28","currentVersion":"4.1"},{"id":50,"name":"4-insert-scheduler-integration.js","version":"2.28","currentVersion":"4.1"},{"id":51,"name":"5-add-scheduler-permission-to-roles.js","version":"2.28","currentVersion":"4.1"},{"id":52,"name":"6-add-type-column.js","version":"2.28","currentVersion":"4.1"},{"id":53,"name":"7-populate-type-column.js","version":"2.28","currentVersion":"4.1"},{"id":54,"name":"8-remove-page-column.js","version":"2.28","currentVersion":"4.1"},{"id":55,"name":"1-add-post-page-column.js","version":"2.29","currentVersion":"4.1"},{"id":56,"name":"2-populate-post-page-column.js","version":"2.29","currentVersion":"4.1"},{"id":57,"name":"3-remove-page-type-column.js","version":"2.29","currentVersion":"4.1"},{"id":58,"name":"1-remove-name-and-password-from-members-table.js","version":"2.31","currentVersion":"4.1"},{"id":59,"name":"01-add-members-stripe-customers-table.js","version":"2.32","currentVersion":"4.1"},{"id":60,"name":"02-add-name-to-members-table.js","version":"2.32","currentVersion":"4.1"},{"id":61,"name":"01-correct-members-stripe-customers-table.js","version":"2.33","currentVersion":"4.1"},{"id":62,"name":"01-add-stripe-customers-subscriptions-table.js","version":"2.34","currentVersion":"4.1"},{"id":63,"name":"02-add-email-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"4.1"},{"id":64,"name":"03-add-name-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"4.1"},{"id":65,"name":"01-add-note-to-members-table.js","version":"2.35","currentVersion":"4.1"},{"id":66,"name":"01-add-self-signup-and-from address-to-members-settings.js","version":"2.37","currentVersion":"4.1"},{"id":67,"name":"01-remove-user-ghost-auth-columns.js","version":"3.0","currentVersion":"4.1"},{"id":68,"name":"02-drop-token-auth-tables.js","version":"3.0","currentVersion":"4.1"},{"id":69,"name":"03-drop-client-auth-tables.js","version":"3.0","currentVersion":"4.1"},{"id":70,"name":"04-add-posts-meta-table.js","version":"3.0","currentVersion":"4.1"},{"id":71,"name":"05-populate-posts-meta-table.js","version":"3.0","currentVersion":"4.1"},{"id":72,"name":"06-remove-posts-meta-columns.js","version":"3.0","currentVersion":"4.1"},{"id":73,"name":"07-add-posts-type-column.js","version":"3.0","currentVersion":"4.1"},{"id":74,"name":"08-populate-posts-type-column.js","version":"3.0","currentVersion":"4.1"},{"id":75,"name":"09-remove-posts-page-column.js","version":"3.0","currentVersion":"4.1"},{"id":76,"name":"10-remove-empty-strings.js","version":"3.0","currentVersion":"4.1"},{"id":77,"name":"11-update-posts-html.js","version":"3.0","currentVersion":"4.1"},{"id":78,"name":"12-populate-members-table-from-subscribers.js","version":"3.0","currentVersion":"4.1"},{"id":79,"name":"13-drop-subscribers-table.js","version":"3.0","currentVersion":"4.1"},{"id":80,"name":"14-remove-subscribers-flag.js","version":"3.0","currentVersion":"4.1"},{"id":81,"name":"01-add-send-email-when-published-to-posts.js","version":"3.1","currentVersion":"4.1"},{"id":82,"name":"02-add-email-subject-to-posts-meta.js","version":"3.1","currentVersion":"4.1"},{"id":83,"name":"03-add-email-preview-permissions.js","version":"3.1","currentVersion":"4.1"},{"id":84,"name":"04-add-subscribed-flag-to-members.js","version":"3.1","currentVersion":"4.1"},{"id":85,"name":"05-add-emails-table.js","version":"3.1","currentVersion":"4.1"},{"id":86,"name":"06-add-email-permissions.js","version":"3.1","currentVersion":"4.1"},{"id":87,"name":"07-add-uuid-field-to-members.js","version":"3.1","currentVersion":"4.1"},{"id":88,"name":"08-add-uuid-values-to-members.js","version":"3.1","currentVersion":"4.1"},{"id":89,"name":"09-add-further-email-permissions.js","version":"3.1","currentVersion":"4.1"},{"id":90,"name":"10-add-email-error-data-column.js","version":"3.1","currentVersion":"4.1"},{"id":91,"name":"01-add-cancel-at-period-end-to-subscriptions.js","version":"3.2","currentVersion":"4.1"},{"id":92,"name":"1-add-labels-table.js","version":"3.6","currentVersion":"4.1"},{"id":93,"name":"2-add-members-labels-table.js","version":"3.6","currentVersion":"4.1"},{"id":94,"name":"3-add-labels-permissions.js","version":"3.6","currentVersion":"4.1"},{"id":95,"name":"01-fix-incorrect-member-labels-foreign-keys.js","version":"3.7","currentVersion":"4.1"},{"id":96,"name":"01-add-geolocation-to-members.js","version":"3.8","currentVersion":"4.1"},{"id":97,"name":"01-add-member-sigin-url-permissions.js","version":"3.9","currentVersion":"4.1"},{"id":98,"name":"01-remove-broken-complimentary-plan-from-members-settings.js","version":"3.11","currentVersion":"4.1"},{"id":99,"name":"01-add-identity-permission.js","version":"3.12","currentVersion":"4.1"},{"id":100,"name":"02-remove-legacy-is-paid-flag-from-settings.js","version":"3.12","currentVersion":"4.1"},{"id":101,"name":"01-add-email-preview-permissions-to-roles.js","version":"3.18","currentVersion":"4.1"},{"id":102,"name":"02-add-members_stripe_connect-auth-permissions.js","version":"3.18","currentVersion":"4.1"},{"id":103,"name":"01-update-member-from-email-address.js","version":"3.19","currentVersion":"4.1"},{"id":104,"name":"01-removed-legacy-values-from-settings-table.js","version":"3.22","currentVersion":"4.1"},{"id":105,"name":"02-settings-key-renames.js","version":"3.22","currentVersion":"4.1"},{"id":106,"name":"03-add-group-and-flags-to-settings.js","version":"3.22","currentVersion":"4.1"},{"id":107,"name":"04-populate-settings-groups-and-flags.js","version":"3.22","currentVersion":"4.1"},{"id":108,"name":"05-migrate-members-subscription-settings.js","version":"3.22","currentVersion":"4.1"},{"id":109,"name":"06-migrate-stripe-connect-settings.js","version":"3.22","currentVersion":"4.1"},{"id":110,"name":"07-update-type-for-settings.js","version":"3.22","currentVersion":"4.1"},{"id":111,"name":"01-migrate-bulk-email-settings.js","version":"3.23","currentVersion":"4.1"},{"id":112,"name":"02-remove-bulk-email-settings.js","version":"3.23","currentVersion":"4.1"},{"id":113,"name":"03-update-portal-button-setting.js","version":"3.23","currentVersion":"4.1"},{"id":114,"name":"04-add-meta-columns-to-tags-table.js","version":"3.23","currentVersion":"4.1"},{"id":115,"name":"01-populate-group-for-new-portal-settings.js","version":"3.24","currentVersion":"4.1"},{"id":116,"name":"01-add-members-stripe-webhook-settings.js","version":"3.25","currentVersion":"4.1"},{"id":117,"name":"01-add-amp-gtag-id-setting.js","version":"3.26","currentVersion":"4.1"},{"id":118,"name":"01-remove-duplicate-subscriptions.js","version":"3.29","currentVersion":"4.1"},{"id":119,"name":"02-remove-duplicate-customers.js","version":"3.29","currentVersion":"4.1"},{"id":120,"name":"03-remove-orphaned-customers.js","version":"3.29","currentVersion":"4.1"},{"id":121,"name":"04-remove-orphaned-subscriptions.js","version":"3.29","currentVersion":"4.1"},{"id":122,"name":"05-add-member-constraints.js","version":"3.29","currentVersion":"4.1"},{"id":123,"name":"01-add-member-signin-url-permission-roles.js","version":"3.30","currentVersion":"4.1"},{"id":124,"name":"01-add-member-support-address-setting.js","version":"3.32","currentVersion":"4.1"},{"id":125,"name":"02-add-member-reply-address-setting.js","version":"3.32","currentVersion":"4.1"},{"id":126,"name":"03-add-routes-hash-setting.js","version":"3.32","currentVersion":"4.1"},{"id":127,"name":"01-add-email-recipients-tables.js","version":"3.33","currentVersion":"4.1"},{"id":128,"name":"01-add-tokens-table.js","version":"3.34","currentVersion":"4.1"},{"id":129,"name":"01-add-address-columns-to-emails-table.js","version":"3.35","currentVersion":"4.1"},{"id":130,"name":"01-add-snippets-table.js","version":"3.36","currentVersion":"4.1"},{"id":131,"name":"02-add-snippets-permissions.js","version":"3.36","currentVersion":"4.1"},{"id":132,"name":"01-update-portal-button-setting.js","version":"3.37","currentVersion":"4.1"},{"id":133,"name":"01-add-email-recipient-filter-column.js","version":"3.38","currentVersion":"4.1"},{"id":134,"name":"02-populate-email-recipient-filter-column.js","version":"3.38","currentVersion":"4.1"},{"id":135,"name":"03-add-recipient-filter-column.js","version":"3.38","currentVersion":"4.1"},{"id":136,"name":"04-populate-recipient-filter-column.js","version":"3.38","currentVersion":"4.1"},{"id":137,"name":"05-add-emails-track-opens-column.js","version":"3.38","currentVersion":"4.1"},{"id":138,"name":"06-add-newsletter-settings.js","version":"3.38","currentVersion":"4.1"},{"id":139,"name":"07-migrate-newsletter-settings-from-config.js","version":"3.38","currentVersion":"4.1"},{"id":140,"name":"08-repopulate-send-email-when-published-down-migration.js","version":"3.38","currentVersion":"4.1"},{"id":141,"name":"09-remove-send-email-when-published-column.js","version":"3.38","currentVersion":"4.1"},{"id":142,"name":"01-add-members-signup-redirect-settings.js","version":"3.39","currentVersion":"4.1"},{"id":143,"name":"02-add-user-id-to-api-keys-table.js","version":"3.39","currentVersion":"4.1"},{"id":144,"name":"03-add-email-track-opens-setting.js","version":"3.39","currentVersion":"4.1"},{"id":145,"name":"04-add-cancellation-reason-column.js","version":"3.39","currentVersion":"4.1"},{"id":146,"name":"05-remove-unused-columns-on-emails.js","version":"3.39","currentVersion":"4.1"},{"id":147,"name":"06-add-email-recipient-index.js","version":"3.39","currentVersion":"4.1"},{"id":148,"name":"07-add-email-recipients-event-timestamps.js","version":"3.39","currentVersion":"4.1"},{"id":149,"name":"08-add-email-stats-columns.js","version":"3.39","currentVersion":"4.1"},{"id":150,"name":"01-add-members-email-open-rate-column.js","version":"3.40","currentVersion":"4.1"},{"id":151,"name":"02-add members-email-aggregation-columns.js","version":"3.40","currentVersion":"4.1"},{"id":152,"name":"03-populate-members-email-counts.js","version":"3.40","currentVersion":"4.1"},{"id":153,"name":"01-add-firstpromoter-settings.js","version":"3.41","currentVersion":"4.1"},{"id":154,"name":"01-update-mobiledoc.js","version":"4.0","currentVersion":"4.1"},{"id":155,"name":"02-add-status-column-to-members.js","version":"4.0","currentVersion":"4.1"},{"id":156,"name":"03-populate-status-column-for-members.js","version":"4.0","currentVersion":"4.1"},{"id":157,"name":"04-drop-apps-related-tables.js","version":"4.0","currentVersion":"4.1"},{"id":158,"name":"05-add-members-subscribe-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":159,"name":"06-populate-members-subscribe-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":160,"name":"07-alter-unique-constraint-for-posts-slug.js","version":"4.0","currentVersion":"4.1"},{"id":161,"name":"08-add-members-login-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":162,"name":"09-add-members-email-change-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":163,"name":"10-add-members-status-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":164,"name":"11-add-members-paid-subscription-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":165,"name":"12-delete-apps-related-settings-keys.js","version":"4.0","currentVersion":"4.1"},{"id":166,"name":"13-add-members-payment-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":167,"name":"14-remove-orphaned-stripe-records.js","version":"4.0","currentVersion":"4.1"},{"id":168,"name":"15-add-frontmatter-column-to-meta.js","version":"4.0","currentVersion":"4.1"},{"id":169,"name":"16-refactor-slack-setting.js","version":"4.0","currentVersion":"4.1"},{"id":170,"name":"17-populate-members-status-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":171,"name":"18-transform-urls-absolute-to-transform-ready.js","version":"4.0","currentVersion":"4.1"},{"id":172,"name":"19-remove-labs-members-setting.js","version":"4.0","currentVersion":"4.1"},{"id":173,"name":"20-refactor-unsplash-setting.js","version":"4.0","currentVersion":"4.1"},{"id":174,"name":"21-sanitize-email-batches-provider-id.js","version":"4.0","currentVersion":"4.1"},{"id":175,"name":"22-solve-orphaned-webhooks.js","version":"4.0","currentVersion":"4.1"},{"id":176,"name":"23-regenerate-posts-html.js","version":"4.0","currentVersion":"4.1"},{"id":177,"name":"24-add-missing-email-permissions.js","version":"4.0","currentVersion":"4.1"},{"id":178,"name":"25-populate-members-paid-subscription-events-table.js","version":"4.0","currentVersion":"4.1"},{"id":179,"name":"26-add-cascade-on-delete.js","version":"4.0","currentVersion":"4.1"},{"id":180,"name":"27-add-primary-key-brute-migrations-lock.js","version":"4.0","currentVersion":"4.1"},{"id":181,"name":"28-add-webhook-intergrations-foreign-key.js","version":"4.0","currentVersion":"4.1"},{"id":182,"name":"29-fix-foreign-key-for-members-stripe-customers-subscriptions.js","version":"4.0","currentVersion":"4.1"},{"id":183,"name":"30-set-default-accent-color.js","version":"4.0","currentVersion":"4.1"},{"id":184,"name":"01-fix-backup-content-permission-typo.js","version":"4.1","currentVersion":"4.1"},{"id":185,"name":"02-add-unique-constraint-for-member-stripe-tables.js","version":"4.1","currentVersion":"4.1"}],"posts":[{"id":"6064789220c4c500017fbc8b","uuid":"391afc4b-8ca9-4f7c-80fd-f495d05d970a","title":"Prototypes and Inheritance","slug":"prototypes-and-inheritance-in-javascript","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Recently I had the unique pleasure of presenting at General Assembly Melbourne, I was asked to share what I knew about Prototypal Inheritance and would like to now share that with you and dive a bit deeper into some examples.\\n\\n## What is Prototypal Inheritance\\n\\nPrototypal Inheritance can be succinctly described as objects without classes, and prototype delegation, also known to some of us as OLOO if you've been keeping up with the latest articles about JavaScript.\\n\\n> OLOO | Objects Linking to Other Objects\\n\\n## Isn't that just OOP?\\n\\nYou might ask this question after someone starts to describe Prototypes the first time, and you wouldn't be wrong in your assumption either.\\n\\n> OOP | Object Oriented Programming\\n\\nBut in JavaScript we call OOP *Classical* in terms of Inheritance.\\n\\n## How is Classical OOP different?\\n\\nClassical and Prototypal Inheritance are fundamentally and semantically distinct. In Prototypal Instances inherit from other instances through concatenative inheritance which is literally OLOO and the purest form of OOP. But in Classical this is simply not the case.\\n\\n## Objects in JavaScript\\n\\nObjects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. \\n\\nWe don't need classes to make lots of similar objects.\\n\\nYou can introduce new functionality to native Objects in the same manner as your own Object definitions, without the need of creating a cloned instance of the native object or even extending a new object instance from the native one. In JavaScript all Objects are the same native or otherwise.\\n\\nAll variables, `function`, and data types in JavaScript excluding `undefined` `null` `NaN` `Infinity` which are more keywords anyway expose their prototypes.\\n\\n## Definitions and Instances of Objects\\n\\nIn classical OOP languages you will note that an object is first defined but until it has been instantiated the only functionality it provides is through static declarations which in itself still requires defining before access to its functionality is possible. \\n\\nIn JavaScript we still have both definitions and Instances yet all functionality is available directly within the definition without it ever being instantiated.\\nIn fact, the idea of instantiating an Object instance is strictly not Prototypal at all in terms of the paradigm.\\n\\n## Constructors\\n\\nThere are no constructors in JavaScript, and before you leave a comment correcting this statement let me extend on that.\\nThe internals implements a process called a constructor and that method can be inspected but it cannot be accessed, overridden or extended at all. It is completely not possible to write javascript that will execute in the native constructor.\\n\\nOne would also argue or link to articles on the internet that demonstrate JavaScript constructor examples, but these are not constructors in anything but a label given to the code they emulate. They are in fact nothing more than the humble function being executed when an object definition is invoked with the `new` keyword.\\n\\n> This is not a JavaScript constructor, it is merely coding like a constructor in JavaScript\\n\\nThe key to writing good Prototypal Inheritance is to avoid using the `new` keyword entirely as this process does not demonstrate the use of prototypes in any way and is essentially just following classical OOP paradigm.\\n\\n> If you’re creating constructor functions and inheriting from them, you haven’t learned JavaScript.\\n\\n## How pseudo constructors work\\n\\nConsider\\n\\n```language-javascript\\nfunction Func() {\\n  // code stuff\\n}\\nvar func = new Func();\\n```\\n\\nProduces a new object that inherits from `Func.prototype` which can be accessed from the `func` variable.\\n\\nThe `new` keyword is used to invoke a constructor-like function.\\n\\nWhat it actually does is;\\n\\n* Create a new instance\\n* Bind `this` to the new instance\\n* Allows `instanceof` to check whether or not an object’s prototype reference is the same object referenced by the `.prototype`\\n* Names the object type after the constructor, which you’ll notice mostly in the debugging console. You’ll see `[Object Foo]`, for example, instead of `[Object object]`\\n\\nExtending this example to see how it is problematic\\n\\n```language-javascript\\nfunction Obj() {\\n    /* do your constructor things here */\\n    return this;\\n}\\nObj.prototype.foo = function() {\\n    return '\\\"said foo\\\"'\\n}\\nObj.prototype.bar = function() {return '\\\"said bar\\\"'}\\nvar obj = new object();\\nobj.foo();\\n// > \\\"said foo\\\"\\nobj.bar();\\n// > \\\"said bar\\\"\\nObj.foo()\\n// > Uncaught TypeError: Obj.foo is not a function\\n```\\n\\nNotice the `TypeError` ?\\nTry without using `new` ;\\n\\n```language-javascript\\nvar obj = Obj();\\n// > Uncaught TypeError: obj.set is not a function\\n```\\n\\nso, is obj = undefined  ???\\n\\n```language-javascript\\n obj\\n// > window\\n```\\nobj = window !?!\\n\\nWHERE DID WINDOW COME FROM\\n\\nWe attempted to simulate a constructor but in our simulated constructor we referenced `this` in an attempt to return the object properties, this would allow the variable holding the instance the ability to access said properties. Using `new` is the flaw. Objects in JavaScript are already accessible, using `new` changes that as described earlier.\\n\\n> Using `new` is the flaw.\\n\\nBut we can still handle it if we wanted to;\\n\\n```language-javascript\\nvar Obj = function() {\\n  if (this === window)\\n    return new Obj(arguments);\\n  return this;\\n};\\n```\\n\\nNow you have a constructor in JavaScript that will work as intended whether you invoke its properties directly or via an instance invoked using `new` no matter what.\\n\\nThis is getting complicated, and we haven't even yet added functionality, and any additional methods are now constrained.\\n\\nWant to repeat this for all object definitions? \\n\\n> DRY | Don't Repeat Yourself\\n\\n## Namespace\\n\\nNamespacing is a technique employed to avoid collisions with other objects or variables in the global scope.\\n\\nJavaScript doesn't have built-in support for namespaces like other languages provides, it does have closures which can be used to achieve a similar effect to namespacing.\\n\\nA closure is a function which remembers its environment in which it was created.\\nSo, we'll use a self-executing anonymous function that exports an object definition to its namespace.\\n\\n```language-javascript\\n(function(){ // private area, away from the global scope\\n  var privateProp = function(){\\n    if (this === window)\\n      return new privateProp(arguments);\\n    return this;\\n  };\\n  privateProp.prototype.get = function(key){\\n    return this[key];\\n  };\\n  privateProp.prototype.set = function(key, value){\\n    this[key] = value;\\n    return this;\\n  };\\n  window.mynamespace = privateProp;\\n})();\\n```\\n\\nIf you run this you will notice that you cannot access `privateProp` in your console, it is now only accessible via `window.mynamespace` or simply `mynamespace` when in the global scope.\\n\\n## Prototypes In Practice\\n\\nI mentioned before that you can introduce new functionality to native Objects in the same manner as your own Object definitions, here is a simple example.\\n\\n```language-javascript\\nNumber.prototype.multiplyBy = function (number) {\\n  return this * number;\\n}\\n```\\nTest it;\\n```language-javascript\\n(123).multiplyBy(2)\\n// > 246\\n```\\n\\nStoring object instances without the `new` keyword\\n\\n```language-javascript\\nif (typeof Object.create !== 'function') {\\n    Object.create = function(o) {\\n        function F() {}\\n        F.prototype = o;\\n        return new F();\\n    };\\n}\\nmyQuery = Object.create(jQuery);\\n```\\n\\nThe `Object.create` function untangles JavaScript's constructor pattern.\\n\\nIt takes an existing object as a parameter and returns an empty new object that inherits from the old one. \\n\\nIf we attempt to obtain a member from the new object, and it lacks that key, then the old object will supply the member.\\n\\nNow add functionality through Prototypal Inheritance to your own Object definitions.\\n\\n```language-javascript\\nvar definition = {\\n  init: function(){},\\n  get: function(){},\\n  set: function(){}\\n};\\nmyObject = Object.create(definition);\\n```\\n\\nObjects inherit from objects. I love Prototypal JavaScript.\\n\\n## Conclusions\\n\\n> Avoid `new` - use .prototype\\n\\nBoth constructors and namespaces are paradigms of classical inheritance *not* Prototypal Inheritance.\\n\\n> Always use Prototypal Inheritance over Classical OOP\\n\\nDouglas Crockford; the most experienced JavaScript programmer renown for being the first to publish methods of Classical OOP in JavaScript has himself stated [he was wrong](http://javascript.crockford.com/inheritance.html) to to do so all along.\\n\\n> You will have to make mistakes to learn how to fix them.\\n\\nIf you’re creating constructor functions and inheriting from them, you haven’t learned JavaScript.\\n\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Recently I had the unique pleasure of presenting at General Assembly Melbourne, I was asked to share what I knew about Prototypal Inheritance and would like to now share that with you and dive a bit deeper into some examples.</p>\n<h2 id=\"whatisprototypalinheritance\">What is Prototypal Inheritance</h2>\n<p>Prototypal Inheritance can be succinctly described as objects without classes, and prototype delegation, also known to some of us as OLOO if you've been keeping up with the latest articles about JavaScript.</p>\n<blockquote>\n<p>OLOO | Objects Linking to Other Objects</p>\n</blockquote>\n<h2 id=\"isntthatjustoop\">Isn't that just OOP?</h2>\n<p>You might ask this question after someone starts to describe Prototypes the first time, and you wouldn't be wrong in your assumption either.</p>\n<blockquote>\n<p>OOP | Object Oriented Programming</p>\n</blockquote>\n<p>But in JavaScript we call OOP <em>Classical</em> in terms of Inheritance.</p>\n<h2 id=\"howisclassicaloopdifferent\">How is Classical OOP different?</h2>\n<p>Classical and Prototypal Inheritance are fundamentally and semantically distinct. In Prototypal Instances inherit from other instances through concatenative inheritance which is literally OLOO and the purest form of OOP. But in Classical this is simply not the case.</p>\n<h2 id=\"objectsinjavascript\">Objects in JavaScript</h2>\n<p>Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects.</p>\n<p>We don't need classes to make lots of similar objects.</p>\n<p>You can introduce new functionality to native Objects in the same manner as your own Object definitions, without the need of creating a cloned instance of the native object or even extending a new object instance from the native one. In JavaScript all Objects are the same native or otherwise.</p>\n<p>All variables, <code>function</code>, and data types in JavaScript excluding <code>undefined</code> <code>null</code> <code>NaN</code> <code>Infinity</code> which are more keywords anyway expose their prototypes.</p>\n<h2 id=\"definitionsandinstancesofobjects\">Definitions and Instances of Objects</h2>\n<p>In classical OOP languages you will note that an object is first defined but until it has been instantiated the only functionality it provides is through static declarations which in itself still requires defining before access to its functionality is possible.</p>\n<p>In JavaScript we still have both definitions and Instances yet all functionality is available directly within the definition without it ever being instantiated.<br>\nIn fact, the idea of instantiating an Object instance is strictly not Prototypal at all in terms of the paradigm.</p>\n<h2 id=\"constructors\">Constructors</h2>\n<p>There are no constructors in JavaScript, and before you leave a comment correcting this statement let me extend on that.<br>\nThe internals implements a process called a constructor and that method can be inspected but it cannot be accessed, overridden or extended at all. It is completely not possible to write javascript that will execute in the native constructor.</p>\n<p>One would also argue or link to articles on the internet that demonstrate JavaScript constructor examples, but these are not constructors in anything but a label given to the code they emulate. They are in fact nothing more than the humble function being executed when an object definition is invoked with the <code>new</code> keyword.</p>\n<blockquote>\n<p>This is not a JavaScript constructor, it is merely coding like a constructor in JavaScript</p>\n</blockquote>\n<p>The key to writing good Prototypal Inheritance is to avoid using the <code>new</code> keyword entirely as this process does not demonstrate the use of prototypes in any way and is essentially just following classical OOP paradigm.</p>\n<blockquote>\n<p>If you’re creating constructor functions and inheriting from them, you haven’t learned JavaScript.</p>\n</blockquote>\n<h2 id=\"howpseudoconstructorswork\">How pseudo constructors work</h2>\n<p>Consider</p>\n<pre><code class=\"language-language-javascript\">function Func() {\n  // code stuff\n}\nvar func = new Func();\n</code></pre>\n<p>Produces a new object that inherits from <code>Func.prototype</code> which can be accessed from the <code>func</code> variable.</p>\n<p>The <code>new</code> keyword is used to invoke a constructor-like function.</p>\n<p>What it actually does is;</p>\n<ul>\n<li>Create a new instance</li>\n<li>Bind <code>this</code> to the new instance</li>\n<li>Allows <code>instanceof</code> to check whether or not an object’s prototype reference is the same object referenced by the <code>.prototype</code></li>\n<li>Names the object type after the constructor, which you’ll notice mostly in the debugging console. You’ll see <code>[Object Foo]</code>, for example, instead of <code>[Object object]</code></li>\n</ul>\n<p>Extending this example to see how it is problematic</p>\n<pre><code class=\"language-language-javascript\">function Obj() {\n    /* do your constructor things here */\n    return this;\n}\nObj.prototype.foo = function() {\n    return '&quot;said foo&quot;'\n}\nObj.prototype.bar = function() {return '&quot;said bar&quot;'}\nvar obj = new object();\nobj.foo();\n// &gt; &quot;said foo&quot;\nobj.bar();\n// &gt; &quot;said bar&quot;\nObj.foo()\n// &gt; Uncaught TypeError: Obj.foo is not a function\n</code></pre>\n<p>Notice the <code>TypeError</code> ?<br>\nTry without using <code>new</code> ;</p>\n<pre><code class=\"language-language-javascript\">var obj = Obj();\n// &gt; Uncaught TypeError: obj.set is not a function\n</code></pre>\n<p>so, is obj = undefined  ???</p>\n<pre><code class=\"language-language-javascript\"> obj\n// &gt; window\n</code></pre>\n<p>obj = window !?!</p>\n<p>WHERE DID WINDOW COME FROM</p>\n<p>We attempted to simulate a constructor but in our simulated constructor we referenced <code>this</code> in an attempt to return the object properties, this would allow the variable holding the instance the ability to access said properties. Using <code>new</code> is the flaw. Objects in JavaScript are already accessible, using <code>new</code> changes that as described earlier.</p>\n<blockquote>\n<p>Using <code>new</code> is the flaw.</p>\n</blockquote>\n<p>But we can still handle it if we wanted to;</p>\n<pre><code class=\"language-language-javascript\">var Obj = function() {\n  if (this === window)\n    return new Obj(arguments);\n  return this;\n};\n</code></pre>\n<p>Now you have a constructor in JavaScript that will work as intended whether you invoke its properties directly or via an instance invoked using <code>new</code> no matter what.</p>\n<p>This is getting complicated, and we haven't even yet added functionality, and any additional methods are now constrained.</p>\n<p>Want to repeat this for all object definitions?</p>\n<blockquote>\n<p>DRY | Don't Repeat Yourself</p>\n</blockquote>\n<h2 id=\"namespace\">Namespace</h2>\n<p>Namespacing is a technique employed to avoid collisions with other objects or variables in the global scope.</p>\n<p>JavaScript doesn't have built-in support for namespaces like other languages provides, it does have closures which can be used to achieve a similar effect to namespacing.</p>\n<p>A closure is a function which remembers its environment in which it was created.<br>\nSo, we'll use a self-executing anonymous function that exports an object definition to its namespace.</p>\n<pre><code class=\"language-language-javascript\">(function(){ // private area, away from the global scope\n  var privateProp = function(){\n    if (this === window)\n      return new privateProp(arguments);\n    return this;\n  };\n  privateProp.prototype.get = function(key){\n    return this[key];\n  };\n  privateProp.prototype.set = function(key, value){\n    this[key] = value;\n    return this;\n  };\n  window.mynamespace = privateProp;\n})();\n</code></pre>\n<p>If you run this you will notice that you cannot access <code>privateProp</code> in your console, it is now only accessible via <code>window.mynamespace</code> or simply <code>mynamespace</code> when in the global scope.</p>\n<h2 id=\"prototypesinpractice\">Prototypes In Practice</h2>\n<p>I mentioned before that you can introduce new functionality to native Objects in the same manner as your own Object definitions, here is a simple example.</p>\n<pre><code class=\"language-language-javascript\">Number.prototype.multiplyBy = function (number) {\n  return this * number;\n}\n</code></pre>\n<p>Test it;</p>\n<pre><code class=\"language-language-javascript\">(123).multiplyBy(2)\n// &gt; 246\n</code></pre>\n<p>Storing object instances without the <code>new</code> keyword</p>\n<pre><code class=\"language-language-javascript\">if (typeof Object.create !== 'function') {\n    Object.create = function(o) {\n        function F() {}\n        F.prototype = o;\n        return new F();\n    };\n}\nmyQuery = Object.create(jQuery);\n</code></pre>\n<p>The <code>Object.create</code> function untangles JavaScript's constructor pattern.</p>\n<p>It takes an existing object as a parameter and returns an empty new object that inherits from the old one.</p>\n<p>If we attempt to obtain a member from the new object, and it lacks that key, then the old object will supply the member.</p>\n<p>Now add functionality through Prototypal Inheritance to your own Object definitions.</p>\n<pre><code class=\"language-language-javascript\">var definition = {\n  init: function(){},\n  get: function(){},\n  set: function(){}\n};\nmyObject = Object.create(definition);\n</code></pre>\n<p>Objects inherit from objects. I love Prototypal JavaScript.</p>\n<h2 id=\"conclusions\">Conclusions</h2>\n<blockquote>\n<p>Avoid <code>new</code> - use .prototype</p>\n</blockquote>\n<p>Both constructors and namespaces are paradigms of classical inheritance <em>not</em> Prototypal Inheritance.</p>\n<blockquote>\n<p>Always use Prototypal Inheritance over Classical OOP</p>\n</blockquote>\n<p>Douglas Crockford; the most experienced JavaScript programmer renown for being the first to publish methods of Classical OOP in JavaScript has himself stated <a href=\"http://javascript.crockford.com/inheritance.html\">he was wrong</a> to to do so all along.</p>\n<blockquote>\n<p>You will have to make mistakes to learn how to fix them.</p>\n</blockquote>\n<p>If you’re creating constructor functions and inheriting from them, you haven’t learned JavaScript.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"3","plaintext":"Recently I had the unique pleasure of presenting at General Assembly Melbourne,\nI was asked to share what I knew about Prototypal Inheritance and would like to\nnow share that with you and dive a bit deeper into some examples.\n\nWhat is Prototypal Inheritance\nPrototypal Inheritance can be succinctly described as objects without classes,\nand prototype delegation, also known to some of us as OLOO if you've been\nkeeping up with the latest articles about JavaScript.\n\n> OLOO | Objects Linking to Other Objects\n\n\nIsn't that just OOP?\nYou might ask this question after someone starts to describe Prototypes the\nfirst time, and you wouldn't be wrong in your assumption either.\n\n> OOP | Object Oriented Programming\n\n\nBut in JavaScript we call OOP Classical in terms of Inheritance.\n\nHow is Classical OOP different?\nClassical and Prototypal Inheritance are fundamentally and semantically\ndistinct. In Prototypal Instances inherit from other instances through\nconcatenative inheritance which is literally OLOO and the purest form of OOP.\nBut in Classical this is simply not the case.\n\nObjects in JavaScript\nObjects are mutable in JavaScript, so we can augment the new instances, giving\nthem new fields and methods. These can then act as prototypes for even newer\nobjects.\n\nWe don't need classes to make lots of similar objects.\n\nYou can introduce new functionality to native Objects in the same manner as your\nown Object definitions, without the need of creating a cloned instance of the\nnative object or even extending a new object instance from the native one. In\nJavaScript all Objects are the same native or otherwise.\n\nAll variables, function, and data types in JavaScript excluding undefined null \nNaN Infinity which are more keywords anyway expose their prototypes.\n\nDefinitions and Instances of Objects\nIn classical OOP languages you will note that an object is first defined but\nuntil it has been instantiated the only functionality it provides is through\nstatic declarations which in itself still requires defining before access to its\nfunctionality is possible.\n\nIn JavaScript we still have both definitions and Instances yet all functionality\nis available directly within the definition without it ever being instantiated.\nIn fact, the idea of instantiating an Object instance is strictly not Prototypal\nat all in terms of the paradigm.\n\nConstructors\nThere are no constructors in JavaScript, and before you leave a comment\ncorrecting this statement let me extend on that.\nThe internals implements a process called a constructor and that method can be\ninspected but it cannot be accessed, overridden or extended at all. It is\ncompletely not possible to write javascript that will execute in the native\nconstructor.\n\nOne would also argue or link to articles on the internet that demonstrate\nJavaScript constructor examples, but these are not constructors in anything but\na label given to the code they emulate. They are in fact nothing more than the\nhumble function being executed when an object definition is invoked with the new \nkeyword.\n\n> This is not a JavaScript constructor, it is merely coding like a constructor in\nJavaScript\n\n\nThe key to writing good Prototypal Inheritance is to avoid using the new keyword\nentirely as this process does not demonstrate the use of prototypes in any way\nand is essentially just following classical OOP paradigm.\n\n> If you’re creating constructor functions and inheriting from them, you haven’t\nlearned JavaScript.\n\n\nHow pseudo constructors work\nConsider\n\nfunction Func() {\n  // code stuff\n}\nvar func = new Func();\n\n\nProduces a new object that inherits from Func.prototype which can be accessed\nfrom the func variable.\n\nThe new keyword is used to invoke a constructor-like function.\n\nWhat it actually does is;\n\n * Create a new instance\n * Bind this to the new instance\n * Allows instanceof to check whether or not an object’s prototype reference is\n   the same object referenced by the .prototype\n * Names the object type after the constructor, which you’ll notice mostly in\n   the debugging console. You’ll see [Object Foo], for example, instead of \n   [Object object]\n\nExtending this example to see how it is problematic\n\nfunction Obj() {\n    /* do your constructor things here */\n    return this;\n}\nObj.prototype.foo = function() {\n    return '\"said foo\"'\n}\nObj.prototype.bar = function() {return '\"said bar\"'}\nvar obj = new object();\nobj.foo();\n// > \"said foo\"\nobj.bar();\n// > \"said bar\"\nObj.foo()\n// > Uncaught TypeError: Obj.foo is not a function\n\n\nNotice the TypeError ?\nTry without using new ;\n\nvar obj = Obj();\n// > Uncaught TypeError: obj.set is not a function\n\n\nso, is obj = undefined ???\n\n obj\n// > window\n\n\nobj = window !?!\n\nWHERE DID WINDOW COME FROM\n\nWe attempted to simulate a constructor but in our simulated constructor we\nreferenced this in an attempt to return the object properties, this would allow\nthe variable holding the instance the ability to access said properties. Using \nnew is the flaw. Objects in JavaScript are already accessible, using new changes\nthat as described earlier.\n\n> Using new is the flaw.\n\n\nBut we can still handle it if we wanted to;\n\nvar Obj = function() {\n  if (this === window)\n    return new Obj(arguments);\n  return this;\n};\n\n\nNow you have a constructor in JavaScript that will work as intended whether you\ninvoke its properties directly or via an instance invoked using new no matter\nwhat.\n\nThis is getting complicated, and we haven't even yet added functionality, and\nany additional methods are now constrained.\n\nWant to repeat this for all object definitions?\n\n> DRY | Don't Repeat Yourself\n\n\nNamespace\nNamespacing is a technique employed to avoid collisions with other objects or\nvariables in the global scope.\n\nJavaScript doesn't have built-in support for namespaces like other languages\nprovides, it does have closures which can be used to achieve a similar effect to\nnamespacing.\n\nA closure is a function which remembers its environment in which it was created.\nSo, we'll use a self-executing anonymous function that exports an object\ndefinition to its namespace.\n\n(function(){ // private area, away from the global scope\n  var privateProp = function(){\n    if (this === window)\n      return new privateProp(arguments);\n    return this;\n  };\n  privateProp.prototype.get = function(key){\n    return this[key];\n  };\n  privateProp.prototype.set = function(key, value){\n    this[key] = value;\n    return this;\n  };\n  window.mynamespace = privateProp;\n})();\n\n\nIf you run this you will notice that you cannot access privateProp in your\nconsole, it is now only accessible via window.mynamespace or simply mynamespace \nwhen in the global scope.\n\nPrototypes In Practice\nI mentioned before that you can introduce new functionality to native Objects in\nthe same manner as your own Object definitions, here is a simple example.\n\nNumber.prototype.multiplyBy = function (number) {\n  return this * number;\n}\n\n\nTest it;\n\n(123).multiplyBy(2)\n// > 246\n\n\nStoring object instances without the new keyword\n\nif (typeof Object.create !== 'function') {\n    Object.create = function(o) {\n        function F() {}\n        F.prototype = o;\n        return new F();\n    };\n}\nmyQuery = Object.create(jQuery);\n\n\nThe Object.create function untangles JavaScript's constructor pattern.\n\nIt takes an existing object as a parameter and returns an empty new object that\ninherits from the old one.\n\nIf we attempt to obtain a member from the new object, and it lacks that key,\nthen the old object will supply the member.\n\nNow add functionality through Prototypal Inheritance to your own Object\ndefinitions.\n\nvar definition = {\n  init: function(){},\n  get: function(){},\n  set: function(){}\n};\nmyObject = Object.create(definition);\n\n\nObjects inherit from objects. I love Prototypal JavaScript.\n\nConclusions\n> Avoid new - use .prototype\n\n\nBoth constructors and namespaces are paradigms of classical inheritance not \nPrototypal Inheritance.\n\n> Always use Prototypal Inheritance over Classical OOP\n\n\nDouglas Crockford; the most experienced JavaScript programmer renown for being\nthe first to publish methods of Classical OOP in JavaScript has himself stated \nhe was wrong [http://javascript.crockford.com/inheritance.html] to to do so all\nalong.\n\n> You will have to make mistakes to learn how to fix them.\n\n\nIf you’re creating constructor functions and inheriting from them, you haven’t\nlearned JavaScript.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-18.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 05:50:06","created_by":"1","updated_at":"2021-03-31 14:19:02","updated_by":"1","published_at":"2016-03-19 07:42:54","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc8c","uuid":"5f7c6e6b-532a-4499-bbd4-4d868c0135b8","title":"DevOps Explained","slug":"devops-explained","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\nDevOps is a movement, a state of mind, way of thinking. It will solve some problems and it can even save you money, but DevOps will not be the one change that fixes everything.\\n\\n## What is DevOps\\n\\nBefore we can understand some of the DevOps concepts and implementations we should first identify some key motivations for why we are interested in the subject, what it could mean for us and our work.\\n\\n* Rapid service delivery through automation and simplicity.\\n* Better communication and cooperation.\\n* Reduced failure rate and faster recovery.\\n* Less time troubleshooting, more time innovating or learning.\\n* Clear proven processes.\\n\\nIn your study of DevOps there will be many explanations as to how, where, and why Automation, Testing, CI and CD make up various parts of a DevOps implementation.\\n\\nThe success of DevOps adoption into any business will be greatly increased if the right culture is set and people are motivated to work in cooperation to achieve succinct DevOps.\\n\\n> Culture and people are more important than process for DevOps success\\n\\nDevOps has been highly debated and for all the benefits it can yield there are many themes and techniques to consider. In almost all cases of DevOps implementations there 6 main focus points consistent with the \\nmain methodology of the movement.\\n\\n1. Collaboration, and Communication.\\n2. Automation, as well as autonomous developers.\\n3. Continuous integration.\\n4. Continuous testing.\\n5. Continuous delivery.\\n6. Continuous monitoring.\\n\\n## History\\n\\nDevOps, the combination of the 2 words Development and Operations, can be traced back to debates in a Google Group called “Agile Systems Administrators”. The group was created by Patrick Debois and Andrew Clay Shafer in 2008 after their meeting at a meetup for Agile Infrastructure for which Shafer had organised and Debois was the sole attendee. \\n\\nAmazing what passionate and driven people can accomplish. Shafer; a self-proclaimed hacker, and a modest freelancer Debois, were brought together by their vision and went on to start a movement we know as DevOps.\\n\\nOne of the earliest events celebrating the name DevOps was in 2009 when John Allspaw (Yahoo, Flickr) presented “ 10 deploys per day” at the Velocity conference, and also at velocity Paul Hammond (formerly of Flickr) talked about scaling Typekit.\\n\\nBetween the time of Velocity and what we now refer to as “The perfect storm” of late 2010, there were several talks around the world, and software released all in the name of DevOps. What started to become common where the myths about DevOps, for which many debates arose. One certainty was that DevOps is not a job, a skill, or spec - but rather a set of values and methodologies to achieve success.\\n\\nToday we can look at Amazon founder Jeff Bezos, whom many believe as the greatest example of a tech influencer of narrow-minded sheer determination to deliver services and products in such a way that the process for which it is achieved is without exception pure DevOps.\\n\\n## The People\\n\\nAs a team leader, manager, or team member within a business adopting DevOps, there are some key \\npersonal developments to consider for each and every person involved.\\n\\n* For successful adoption the right culture is required.\\n* All employees and implementers alike are required to be motivated for change.\\n* Individual teams are expected, and absolutely required to cooperate and communicate well. This is core.\\n* There should be one single minded goal; Succinct rapid service delivery.\\n\\nSome things to expect when a business migrates to, or restructures towards DevOps methodologies;\\n\\n* Developers understand the full impact of their code hitting production and are accountable.\\n* Operations are comfortable and prepared for the developer’s need to deploy rapidly.\\n* Infrastructure and development processes will change to remove all central points of failure.\\n* Work that is considered “on the tools” will be reduced for all team members and replaced with process. \\n* Everyone will have his/her own approach or solution, plan to have a clear decision making process in advance.\\n\\n## Business Motivations\\n\\nWhether you’re planning to move hardware offsite, to the cloud, or use existing infrastructure there will need to \\nbe business considerations in terms of security, IP, data, regulatory, and costs.\\n\\nWhen planning for DevOps adoption;\\n\\nInvestigate similar business case studies or published materials outlining their experiences.\\nThese will identify any caveats that may apply and give incentives that may be argued for.\\n\\nQuantify your current risk by comparing current process to DevOps methodologies. Any of your KPIs may help here, as they were defined with deliverables that may be disrupted.\\n\\nAll single points of failure are a risk. Persistent data, replication, backup, networking, ect. Devise the components of your software, service, and products so that they are modular. Decouple data and expose it openly through external APIs.\\n\\nDevelopers write code to consume APIs, never allow business logic to directly access data or services.\\nDesign all environments, deployments, ect to be terminated as an expectation, nothing lost is ever lost.\\n\\nNo asset or configuration should exist in source code, question everything! Few exceptions exist.\\n\\n> question everything\\n\\nDo all you can to enable you to move as many if not all of the components to cloud services. Cloud services are designed and priced for specific processes. They are configurable and modular, allowing new services/projects to slot in with little more than a config. Complexity is reduced though reliability, scalability, and compatibility are all improved for you.\\n\\nChoose monitoring tools that will identify risks in real-time. If you use services such as Pingdom already you can use them for much more than just identifying availability, you can use services like this for testing APIs at no extra cost.\\n\\nCover all identified risks and anything that will affect KPIs. Test all automated services, not just the expected outcomes of their purpose.\\n\\nDevOps encourages quality code to be checked in to VCS, the developer will be able to immediately identify any faults with their new code which ensures that these faults are eliminated with a fresh understanding and rapid turn around. In waterfall these faults could have been left unknown for a time as the integrations and deployments would have previously been done by operations usually days or weeks later. That process left the operations with accountability and the developers were ignorant to their own accountability. \\n\\nOperations now need to open the infrastructure to developers via APIs or configurations so that the perspective is that no hardware or networking exists and it is all just represented in “code”. Open monitoring, and open communication channels allows these teams to work autonomously with the business and each other but also without the need of conflict/crisis management.\\n\\n## Further reading\\n\\nAn interesting visualisation of DevOps tools;\\nhttps://xebialabs.com/periodic-table-of-devops-tools\\n\\nAlternatively they are all listed here;\\nhttps://xebialabs.com/the-ultimate-devops-tool-chest/\\n\\nA deep and detailed glossary of DevOps;\\nhttps://xebialabs.com/glossary/\\n\\nOther Case studies;\\nhttp://aws.amazon.com/solutions/case-studies/artsy/ \\nhttp://newrelic.com/resources/case-studies \\n\\n#### Notable people in DevOps\\n\\n[@allspaw](https://twitter.com/allspaw)\\nJohn Allspaw\\nSVP Tech Operations at Etsy\\n\\n[@jezhumble](https://twitter.com/jezhumble)\\nJez Humble\\nPrincipal at Thoughtworks\\n\\n[@patrickdebois](https://twitter.com/patrickdebois)\\nPatrick Debois\\nFounder of DevOps, amongst other achievements\\n\\n[@xamat](https://twitter.com/xamat)\\nXavier Amatriain \\nDirector at Netflix\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>DevOps is a movement, a state of mind, way of thinking. It will solve some problems and it can even save you money, but DevOps will not be the one change that fixes everything.</p>\n<h2 id=\"whatisdevops\">What is DevOps</h2>\n<p>Before we can understand some of the DevOps concepts and implementations we should first identify some key motivations for why we are interested in the subject, what it could mean for us and our work.</p>\n<ul>\n<li>Rapid service delivery through automation and simplicity.</li>\n<li>Better communication and cooperation.</li>\n<li>Reduced failure rate and faster recovery.</li>\n<li>Less time troubleshooting, more time innovating or learning.</li>\n<li>Clear proven processes.</li>\n</ul>\n<p>In your study of DevOps there will be many explanations as to how, where, and why Automation, Testing, CI and CD make up various parts of a DevOps implementation.</p>\n<p>The success of DevOps adoption into any business will be greatly increased if the right culture is set and people are motivated to work in cooperation to achieve succinct DevOps.</p>\n<blockquote>\n<p>Culture and people are more important than process for DevOps success</p>\n</blockquote>\n<p>DevOps has been highly debated and for all the benefits it can yield there are many themes and techniques to consider. In almost all cases of DevOps implementations there 6 main focus points consistent with the<br>\nmain methodology of the movement.</p>\n<ol>\n<li>Collaboration, and Communication.</li>\n<li>Automation, as well as autonomous developers.</li>\n<li>Continuous integration.</li>\n<li>Continuous testing.</li>\n<li>Continuous delivery.</li>\n<li>Continuous monitoring.</li>\n</ol>\n<h2 id=\"history\">History</h2>\n<p>DevOps, the combination of the 2 words Development and Operations, can be traced back to debates in a Google Group called “Agile Systems Administrators”. The group was created by Patrick Debois and Andrew Clay Shafer in 2008 after their meeting at a meetup for Agile Infrastructure for which Shafer had organised and Debois was the sole attendee.</p>\n<p>Amazing what passionate and driven people can accomplish. Shafer; a self-proclaimed hacker, and a modest freelancer Debois, were brought together by their vision and went on to start a movement we know as DevOps.</p>\n<p>One of the earliest events celebrating the name DevOps was in 2009 when John Allspaw (Yahoo, Flickr) presented “ 10 deploys per day” at the Velocity conference, and also at velocity Paul Hammond (formerly of Flickr) talked about scaling Typekit.</p>\n<p>Between the time of Velocity and what we now refer to as “The perfect storm” of late 2010, there were several talks around the world, and software released all in the name of DevOps. What started to become common where the myths about DevOps, for which many debates arose. One certainty was that DevOps is not a job, a skill, or spec - but rather a set of values and methodologies to achieve success.</p>\n<p>Today we can look at Amazon founder Jeff Bezos, whom many believe as the greatest example of a tech influencer of narrow-minded sheer determination to deliver services and products in such a way that the process for which it is achieved is without exception pure DevOps.</p>\n<h2 id=\"thepeople\">The People</h2>\n<p>As a team leader, manager, or team member within a business adopting DevOps, there are some key<br>\npersonal developments to consider for each and every person involved.</p>\n<ul>\n<li>For successful adoption the right culture is required.</li>\n<li>All employees and implementers alike are required to be motivated for change.</li>\n<li>Individual teams are expected, and absolutely required to cooperate and communicate well. This is core.</li>\n<li>There should be one single minded goal; Succinct rapid service delivery.</li>\n</ul>\n<p>Some things to expect when a business migrates to, or restructures towards DevOps methodologies;</p>\n<ul>\n<li>Developers understand the full impact of their code hitting production and are accountable.</li>\n<li>Operations are comfortable and prepared for the developer’s need to deploy rapidly.</li>\n<li>Infrastructure and development processes will change to remove all central points of failure.</li>\n<li>Work that is considered “on the tools” will be reduced for all team members and replaced with process.</li>\n<li>Everyone will have his/her own approach or solution, plan to have a clear decision making process in advance.</li>\n</ul>\n<h2 id=\"businessmotivations\">Business Motivations</h2>\n<p>Whether you’re planning to move hardware offsite, to the cloud, or use existing infrastructure there will need to<br>\nbe business considerations in terms of security, IP, data, regulatory, and costs.</p>\n<p>When planning for DevOps adoption;</p>\n<p>Investigate similar business case studies or published materials outlining their experiences.<br>\nThese will identify any caveats that may apply and give incentives that may be argued for.</p>\n<p>Quantify your current risk by comparing current process to DevOps methodologies. Any of your KPIs may help here, as they were defined with deliverables that may be disrupted.</p>\n<p>All single points of failure are a risk. Persistent data, replication, backup, networking, ect. Devise the components of your software, service, and products so that they are modular. Decouple data and expose it openly through external APIs.</p>\n<p>Developers write code to consume APIs, never allow business logic to directly access data or services.<br>\nDesign all environments, deployments, ect to be terminated as an expectation, nothing lost is ever lost.</p>\n<p>No asset or configuration should exist in source code, question everything! Few exceptions exist.</p>\n<blockquote>\n<p>question everything</p>\n</blockquote>\n<p>Do all you can to enable you to move as many if not all of the components to cloud services. Cloud services are designed and priced for specific processes. They are configurable and modular, allowing new services/projects to slot in with little more than a config. Complexity is reduced though reliability, scalability, and compatibility are all improved for you.</p>\n<p>Choose monitoring tools that will identify risks in real-time. If you use services such as Pingdom already you can use them for much more than just identifying availability, you can use services like this for testing APIs at no extra cost.</p>\n<p>Cover all identified risks and anything that will affect KPIs. Test all automated services, not just the expected outcomes of their purpose.</p>\n<p>DevOps encourages quality code to be checked in to VCS, the developer will be able to immediately identify any faults with their new code which ensures that these faults are eliminated with a fresh understanding and rapid turn around. In waterfall these faults could have been left unknown for a time as the integrations and deployments would have previously been done by operations usually days or weeks later. That process left the operations with accountability and the developers were ignorant to their own accountability.</p>\n<p>Operations now need to open the infrastructure to developers via APIs or configurations so that the perspective is that no hardware or networking exists and it is all just represented in “code”. Open monitoring, and open communication channels allows these teams to work autonomously with the business and each other but also without the need of conflict/crisis management.</p>\n<h2 id=\"furtherreading\">Further reading</h2>\n<p>An interesting visualisation of DevOps tools;<br>\n<a href=\"https://xebialabs.com/periodic-table-of-devops-tools\">https://xebialabs.com/periodic-table-of-devops-tools</a></p>\n<p>Alternatively they are all listed here;<br>\n<a href=\"https://xebialabs.com/the-ultimate-devops-tool-chest/\">https://xebialabs.com/the-ultimate-devops-tool-chest/</a></p>\n<p>A deep and detailed glossary of DevOps;<br>\n<a href=\"https://xebialabs.com/glossary/\">https://xebialabs.com/glossary/</a></p>\n<p>Other Case studies;<br>\n<a href=\"http://aws.amazon.com/solutions/case-studies/artsy/\">http://aws.amazon.com/solutions/case-studies/artsy/</a><br>\n<a href=\"http://newrelic.com/resources/case-studies\">http://newrelic.com/resources/case-studies</a></p>\n<h4 id=\"notablepeopleindevops\">Notable people in DevOps</h4>\n<p><a href=\"https://twitter.com/allspaw\">@allspaw</a><br>\nJohn Allspaw<br>\nSVP Tech Operations at Etsy</p>\n<p><a href=\"https://twitter.com/jezhumble\">@jezhumble</a><br>\nJez Humble<br>\nPrincipal at Thoughtworks</p>\n<p><a href=\"https://twitter.com/patrickdebois\">@patrickdebois</a><br>\nPatrick Debois<br>\nFounder of DevOps, amongst other achievements</p>\n<p><a href=\"https://twitter.com/xamat\">@xamat</a><br>\nXavier Amatriain<br>\nDirector at Netflix</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"4","plaintext":"DevOps is a movement, a state of mind, way of thinking. It will solve some\nproblems and it can even save you money, but DevOps will not be the one change\nthat fixes everything.\n\nWhat is DevOps\nBefore we can understand some of the DevOps concepts and implementations we\nshould first identify some key motivations for why we are interested in the\nsubject, what it could mean for us and our work.\n\n * Rapid service delivery through automation and simplicity.\n * Better communication and cooperation.\n * Reduced failure rate and faster recovery.\n * Less time troubleshooting, more time innovating or learning.\n * Clear proven processes.\n\nIn your study of DevOps there will be many explanations as to how, where, and\nwhy Automation, Testing, CI and CD make up various parts of a DevOps\nimplementation.\n\nThe success of DevOps adoption into any business will be greatly increased if\nthe right culture is set and people are motivated to work in cooperation to\nachieve succinct DevOps.\n\n> Culture and people are more important than process for DevOps success\n\n\nDevOps has been highly debated and for all the benefits it can yield there are\nmany themes and techniques to consider. In almost all cases of DevOps\nimplementations there 6 main focus points consistent with the\nmain methodology of the movement.\n\n 1. Collaboration, and Communication.\n 2. Automation, as well as autonomous developers.\n 3. Continuous integration.\n 4. Continuous testing.\n 5. Continuous delivery.\n 6. Continuous monitoring.\n\nHistory\nDevOps, the combination of the 2 words Development and Operations, can be traced\nback to debates in a Google Group called “Agile Systems Administrators”. The\ngroup was created by Patrick Debois and Andrew Clay Shafer in 2008 after their\nmeeting at a meetup for Agile Infrastructure for which Shafer had organised and\nDebois was the sole attendee.\n\nAmazing what passionate and driven people can accomplish. Shafer; a\nself-proclaimed hacker, and a modest freelancer Debois, were brought together by\ntheir vision and went on to start a movement we know as DevOps.\n\nOne of the earliest events celebrating the name DevOps was in 2009 when John\nAllspaw (Yahoo, Flickr) presented “ 10 deploys per day” at the Velocity\nconference, and also at velocity Paul Hammond (formerly of Flickr) talked about\nscaling Typekit.\n\nBetween the time of Velocity and what we now refer to as “The perfect storm” of\nlate 2010, there were several talks around the world, and software released all\nin the name of DevOps. What started to become common where the myths about\nDevOps, for which many debates arose. One certainty was that DevOps is not a\njob, a skill, or spec - but rather a set of values and methodologies to achieve\nsuccess.\n\nToday we can look at Amazon founder Jeff Bezos, whom many believe as the\ngreatest example of a tech influencer of narrow-minded sheer determination to\ndeliver services and products in such a way that the process for which it is\nachieved is without exception pure DevOps.\n\nThe People\nAs a team leader, manager, or team member within a business adopting DevOps,\nthere are some key\npersonal developments to consider for each and every person involved.\n\n * For successful adoption the right culture is required.\n * All employees and implementers alike are required to be motivated for change.\n * Individual teams are expected, and absolutely required to cooperate and\n   communicate well. This is core.\n * There should be one single minded goal; Succinct rapid service delivery.\n\nSome things to expect when a business migrates to, or restructures towards\nDevOps methodologies;\n\n * Developers understand the full impact of their code hitting production and\n   are accountable.\n * Operations are comfortable and prepared for the developer’s need to deploy\n   rapidly.\n * Infrastructure and development processes will change to remove all central\n   points of failure.\n * Work that is considered “on the tools” will be reduced for all team members\n   and replaced with process.\n * Everyone will have his/her own approach or solution, plan to have a clear\n   decision making process in advance.\n\nBusiness Motivations\nWhether you’re planning to move hardware offsite, to the cloud, or use existing\ninfrastructure there will need to\nbe business considerations in terms of security, IP, data, regulatory, and\ncosts.\n\nWhen planning for DevOps adoption;\n\nInvestigate similar business case studies or published materials outlining their\nexperiences.\nThese will identify any caveats that may apply and give incentives that may be\nargued for.\n\nQuantify your current risk by comparing current process to DevOps methodologies.\nAny of your KPIs may help here, as they were defined with deliverables that may\nbe disrupted.\n\nAll single points of failure are a risk. Persistent data, replication, backup,\nnetworking, ect. Devise the components of your software, service, and products\nso that they are modular. Decouple data and expose it openly through external\nAPIs.\n\nDevelopers write code to consume APIs, never allow business logic to directly\naccess data or services.\nDesign all environments, deployments, ect to be terminated as an expectation,\nnothing lost is ever lost.\n\nNo asset or configuration should exist in source code, question everything! Few\nexceptions exist.\n\n> question everything\n\n\nDo all you can to enable you to move as many if not all of the components to\ncloud services. Cloud services are designed and priced for specific processes.\nThey are configurable and modular, allowing new services/projects to slot in\nwith little more than a config. Complexity is reduced though reliability,\nscalability, and compatibility are all improved for you.\n\nChoose monitoring tools that will identify risks in real-time. If you use\nservices such as Pingdom already you can use them for much more than just\nidentifying availability, you can use services like this for testing APIs at no\nextra cost.\n\nCover all identified risks and anything that will affect KPIs. Test all\nautomated services, not just the expected outcomes of their purpose.\n\nDevOps encourages quality code to be checked in to VCS, the developer will be\nable to immediately identify any faults with their new code which ensures that\nthese faults are eliminated with a fresh understanding and rapid turn around. In\nwaterfall these faults could have been left unknown for a time as the\nintegrations and deployments would have previously been done by operations\nusually days or weeks later. That process left the operations with\naccountability and the developers were ignorant to their own accountability.\n\nOperations now need to open the infrastructure to developers via APIs or\nconfigurations so that the perspective is that no hardware or networking exists\nand it is all just represented in “code”. Open monitoring, and open\ncommunication channels allows these teams to work autonomously with the business\nand each other but also without the need of conflict/crisis management.\n\nFurther reading\nAn interesting visualisation of DevOps tools;\nhttps://xebialabs.com/periodic-table-of-devops-tools\n\nAlternatively they are all listed here;\nhttps://xebialabs.com/the-ultimate-devops-tool-chest/\n\nA deep and detailed glossary of DevOps;\nhttps://xebialabs.com/glossary/\n\nOther Case studies;\nhttp://aws.amazon.com/solutions/case-studies/artsy/\nhttp://newrelic.com/resources/case-studies\n\nNotable people in DevOps\n@allspaw [https://twitter.com/allspaw]\nJohn Allspaw\nSVP Tech Operations at Etsy\n\n@jezhumble [https://twitter.com/jezhumble]\nJez Humble\nPrincipal at Thoughtworks\n\n@patrickdebois [https://twitter.com/patrickdebois]\nPatrick Debois\nFounder of DevOps, amongst other achievements\n\n@xamat [https://twitter.com/xamat]\nXavier Amatriain\nDirector at Netflix\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-1.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 10:10:36","created_by":"1","updated_at":"2021-03-31 14:19:18","updated_by":"1","published_at":"2016-03-15 10:10:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc8d","uuid":"b4ea1cec-d92d-430b-be4a-68ec06d55645","title":"What has been made PHP7 ready?","slug":"what-has-been-made-php7-ready","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Let's take a look at the PHP based software that may be important to you before to get ready for your products PHP7 support migration.\\n\\nSo what has support for PHP7 at this early stage?\\n\\n### IDEs\\n* Netbeans Official\\n* Jetbrains PHPStorm 10 Official\\n* Cloud9 - Unofficial via custom workspace template\\n* Eclipse PDT 3.6 Official\\n* Komodo Unsupported\\n* PHP Intellisense (Visual Studio) Unsupported\\n* DraftCode Unsupported\\n* Selenium IDE Unsupported\\n\\n### Frameworks\\n* Symphony Official 100% compatible - all branches\\n* Laravel 4.2 Unofficial\\n* Laravel 5.1 Official\\n* Zend Framework 2 Unofficial\\n* Magento 2.0 Official\\n* Cakephp 3.0 Official\\n* Yii 2.0 Official\\n* Codeigniter 2.2.6 and 3 Unofficial\\n* Phalcon unsupported\\n* Kohana unsupported\\n* Fuelpho unsupported\\n\\n### Software\\n* Backfire.io Offical\\n* PEAR 1.10 Official\\n* memcached Official\\n* Pecl memcache unsupported\\n* Elasticache Official\\n* Drupal 8 Oficial\\n* Drupal 7 Unofficial\\n* WordPress 4.3.1 Unofficial\\n* Joomla! 3.5 Official\\n* OctoberCMS Official\\n* PyroCMS 3 Official\\n* Moodle 3.0.1 Official\\n* Joomla 3.5 Official\\n\\n### Hosting\\n* AWS EC2 via A community AMI I released\\n* Zend Server Official\\n* cPanel Official\\n* Laravel Homestead Official\\n* Digital Ocean Unofficial\\n* Openshift Unofficial\\n* Heroku Official\\n* ownCloud Official\\n* Google Cloud via managed VMs\\n* Kualo Official\\n* Inclusive Host Official\\n* LayerShift Official\\n* TSOHOST Official\\n* AccuWebHosting Official\\n* Fastcomet Official\\n* SiteGround Official\\n\\n## Further reading\\n\\nhttp://php.net/ChangeLog-7.php\\nhttp://php.net/manual/en/migration70.deprecated.php\\nhttps://twitter.com/hashtag/php7\\n\\n##### Found this useful or I missed something?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Let's take a look at the PHP based software that may be important to you before to get ready for your products PHP7 support migration.</p>\n<p>So what has support for PHP7 at this early stage?</p>\n<h3 id=\"ides\">IDEs</h3>\n<ul>\n<li>Netbeans Official</li>\n<li>Jetbrains PHPStorm 10 Official</li>\n<li>Cloud9 - Unofficial via custom workspace template</li>\n<li>Eclipse PDT 3.6 Official</li>\n<li>Komodo Unsupported</li>\n<li>PHP Intellisense (Visual Studio) Unsupported</li>\n<li>DraftCode Unsupported</li>\n<li>Selenium IDE Unsupported</li>\n</ul>\n<h3 id=\"frameworks\">Frameworks</h3>\n<ul>\n<li>Symphony Official 100% compatible - all branches</li>\n<li>Laravel 4.2 Unofficial</li>\n<li>Laravel 5.1 Official</li>\n<li>Zend Framework 2 Unofficial</li>\n<li>Magento 2.0 Official</li>\n<li>Cakephp 3.0 Official</li>\n<li>Yii 2.0 Official</li>\n<li>Codeigniter 2.2.6 and 3 Unofficial</li>\n<li>Phalcon unsupported</li>\n<li>Kohana unsupported</li>\n<li>Fuelpho unsupported</li>\n</ul>\n<h3 id=\"software\">Software</h3>\n<ul>\n<li>Backfire.io Offical</li>\n<li>PEAR 1.10 Official</li>\n<li>memcached Official</li>\n<li>Pecl memcache unsupported</li>\n<li>Elasticache Official</li>\n<li>Drupal 8 Oficial</li>\n<li>Drupal 7 Unofficial</li>\n<li>WordPress 4.3.1 Unofficial</li>\n<li>Joomla! 3.5 Official</li>\n<li>OctoberCMS Official</li>\n<li>PyroCMS 3 Official</li>\n<li>Moodle 3.0.1 Official</li>\n<li>Joomla 3.5 Official</li>\n</ul>\n<h3 id=\"hosting\">Hosting</h3>\n<ul>\n<li>AWS EC2 via A community AMI I released</li>\n<li>Zend Server Official</li>\n<li>cPanel Official</li>\n<li>Laravel Homestead Official</li>\n<li>Digital Ocean Unofficial</li>\n<li>Openshift Unofficial</li>\n<li>Heroku Official</li>\n<li>ownCloud Official</li>\n<li>Google Cloud via managed VMs</li>\n<li>Kualo Official</li>\n<li>Inclusive Host Official</li>\n<li>LayerShift Official</li>\n<li>TSOHOST Official</li>\n<li>AccuWebHosting Official</li>\n<li>Fastcomet Official</li>\n<li>SiteGround Official</li>\n</ul>\n<h2 id=\"furtherreading\">Further reading</h2>\n<p><a href=\"http://php.net/ChangeLog-7.php\">http://php.net/ChangeLog-7.php</a><br>\n<a href=\"http://php.net/manual/en/migration70.deprecated.php\">http://php.net/manual/en/migration70.deprecated.php</a><br>\n<a href=\"https://twitter.com/hashtag/php7\">https://twitter.com/hashtag/php7</a></p>\n<h5 id=\"foundthisusefulorimissedsomething\">Found this useful or I missed something?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"5","plaintext":"Let's take a look at the PHP based software that may be important to you before\nto get ready for your products PHP7 support migration.\n\nSo what has support for PHP7 at this early stage?\n\nIDEs\n * Netbeans Official\n * Jetbrains PHPStorm 10 Official\n * Cloud9 - Unofficial via custom workspace template\n * Eclipse PDT 3.6 Official\n * Komodo Unsupported\n * PHP Intellisense (Visual Studio) Unsupported\n * DraftCode Unsupported\n * Selenium IDE Unsupported\n\nFrameworks\n * Symphony Official 100% compatible - all branches\n * Laravel 4.2 Unofficial\n * Laravel 5.1 Official\n * Zend Framework 2 Unofficial\n * Magento 2.0 Official\n * Cakephp 3.0 Official\n * Yii 2.0 Official\n * Codeigniter 2.2.6 and 3 Unofficial\n * Phalcon unsupported\n * Kohana unsupported\n * Fuelpho unsupported\n\nSoftware\n * Backfire.io Offical\n * PEAR 1.10 Official\n * memcached Official\n * Pecl memcache unsupported\n * Elasticache Official\n * Drupal 8 Oficial\n * Drupal 7 Unofficial\n * WordPress 4.3.1 Unofficial\n * Joomla! 3.5 Official\n * OctoberCMS Official\n * PyroCMS 3 Official\n * Moodle 3.0.1 Official\n * Joomla 3.5 Official\n\nHosting\n * AWS EC2 via A community AMI I released\n * Zend Server Official\n * cPanel Official\n * Laravel Homestead Official\n * Digital Ocean Unofficial\n * Openshift Unofficial\n * Heroku Official\n * ownCloud Official\n * Google Cloud via managed VMs\n * Kualo Official\n * Inclusive Host Official\n * LayerShift Official\n * TSOHOST Official\n * AccuWebHosting Official\n * Fastcomet Official\n * SiteGround Official\n\nFurther reading\nhttp://php.net/ChangeLog-7.php\nhttp://php.net/manual/en/migration70.deprecated.php\nhttps://twitter.com/hashtag/php7\n\nFound this useful or I missed something?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-12.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 10:53:33","created_by":"1","updated_at":"2021-03-31 14:19:27","updated_by":"1","published_at":"2016-03-13 10:56:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc8e","uuid":"879d4620-0c32-4b94-be4a-dc1c738a76ec","title":"Callback Functions Double Executing? Dont Forget To return","slug":"callback-functions-double-executing","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\n### Callback Functions Double Executing?\\n\\nA quick example\\n\\n```language-javascript\\nfunction doSomething(err, result, final){\\n if(err){\\n   final(err);\\n }\\n final(null,result);\\n}\\n```\\n\\nTake the above as a simple way to express how you would have your final execute twice.\\n\\nIt may not be immediately apparent why the callback executes twice, consider the scenario below;\\n\\n```language-javascript\\nvar match = 3;\\n(function(id, callback){\\n  var interval = setInterval(function(){\\n    var random = (Math.floor(Math.random() * 5) + 1);\\n    if (random === id) {\\n      var err = new Error(\\\"process terminated\\\");\\n      callback(err,{},final);\\n      return false;\\n\\t}\\n    callback(null, { some: \\\"data\\\", id: id }, final);\\n  }, 5000 );\\n})(match, doSomething);\\n```\\n\\nHere we are matching a random number between 1 and 5 with a number stored in `id` which was defined by `var match` as 3. Our `doSomething` callback is also provided as a callback in argument callback.\\n\\nEvery 5 sec we do this with a new random number (always the interval will call `callback`) and for each iteration the `callback` function is called either with an exception or some data to be processed.\\n\\nNotice the return false after we call callback, consider the random number to be a killswitch.\\n\\n> Note: [Previously](__GHOST_URL__/2013/12/05/nodejs-error-handling-pattern/) I wrote about Node.js Error Handling Patterns, that the first argument for callbacks should always be the error.\\n\\nThe scenario ensures the callback is not executed twice when we encounter the killswitch (match the random number) because we only want to pass an instance of Error to the callback in-line with best node.js practice, not provide the callback `doSomething` data to process when we match the killswitch.\\n\\nWhen we have no match, we send some results to the callback as well as our final callback function which is expected to be executed once after each callback regardless if there was an Error (final can handle the Error).\\n\\nAs we saw in the begining, `final` executes twice in `doSomething` when there is an Error. Fixing the `doSomething` is easy, just add a return false when there is an Error, just like our scenario.\\n\\n```language-javascript\\nfunction doSomething(err, result, final){\\n if(err){\\n   final(err);\\n   return false;\\n }\\n final(null,result);\\n return result;\\n}\\n```\\n\\nAnd add a return to the callback also, to return the result.\\n\\n## Conclusion\\n\\nA common mistake made by all early node developers is forgetting to return after a callback. While sometimes this has no implications, there are many times where you will run into odd issues or worse, debugging nightmares because your callback is being called twice.\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"callbackfunctionsdoubleexecuting\">Callback Functions Double Executing?</h3>\n<p>A quick example</p>\n<pre><code class=\"language-language-javascript\">function doSomething(err, result, final){\n if(err){\n   final(err);\n }\n final(null,result);\n}\n</code></pre>\n<p>Take the above as a simple way to express how you would have your final execute twice.</p>\n<p>It may not be immediately apparent why the callback executes twice, consider the scenario below;</p>\n<pre><code class=\"language-language-javascript\">var match = 3;\n(function(id, callback){\n  var interval = setInterval(function(){\n    var random = (Math.floor(Math.random() * 5) + 1);\n    if (random === id) {\n      var err = new Error(&quot;process terminated&quot;);\n      callback(err,{},final);\n      return false;\n\t}\n    callback(null, { some: &quot;data&quot;, id: id }, final);\n  }, 5000 );\n})(match, doSomething);\n</code></pre>\n<p>Here we are matching a random number between 1 and 5 with a number stored in <code>id</code> which was defined by <code>var match</code> as 3. Our <code>doSomething</code> callback is also provided as a callback in argument callback.</p>\n<p>Every 5 sec we do this with a new random number (always the interval will call <code>callback</code>) and for each iteration the <code>callback</code> function is called either with an exception or some data to be processed.</p>\n<p>Notice the return false after we call callback, consider the random number to be a killswitch.</p>\n<blockquote>\n<p>Note: <a href=\"__GHOST_URL__/2013/12/05/nodejs-error-handling-pattern/\">Previously</a> I wrote about Node.js Error Handling Patterns, that the first argument for callbacks should always be the error.</p>\n</blockquote>\n<p>The scenario ensures the callback is not executed twice when we encounter the killswitch (match the random number) because we only want to pass an instance of Error to the callback in-line with best node.js practice, not provide the callback <code>doSomething</code> data to process when we match the killswitch.</p>\n<p>When we have no match, we send some results to the callback as well as our final callback function which is expected to be executed once after each callback regardless if there was an Error (final can handle the Error).</p>\n<p>As we saw in the begining, <code>final</code> executes twice in <code>doSomething</code> when there is an Error. Fixing the <code>doSomething</code> is easy, just add a return false when there is an Error, just like our scenario.</p>\n<pre><code class=\"language-language-javascript\">function doSomething(err, result, final){\n if(err){\n   final(err);\n   return false;\n }\n final(null,result);\n return result;\n}\n</code></pre>\n<p>And add a return to the callback also, to return the result.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>A common mistake made by all early node developers is forgetting to return after a callback. While sometimes this has no implications, there are many times where you will run into odd issues or worse, debugging nightmares because your callback is being called twice.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"6","plaintext":"Callback Functions Double Executing?\nA quick example\n\nfunction doSomething(err, result, final){\n if(err){\n   final(err);\n }\n final(null,result);\n}\n\n\nTake the above as a simple way to express how you would have your final execute\ntwice.\n\nIt may not be immediately apparent why the callback executes twice, consider the\nscenario below;\n\nvar match = 3;\n(function(id, callback){\n  var interval = setInterval(function(){\n    var random = (Math.floor(Math.random() * 5) + 1);\n    if (random === id) {\n      var err = new Error(\"process terminated\");\n      callback(err,{},final);\n      return false;\n\t}\n    callback(null, { some: \"data\", id: id }, final);\n  }, 5000 );\n})(match, doSomething);\n\n\nHere we are matching a random number between 1 and 5 with a number stored in id \nwhich was defined by var match as 3. Our doSomething callback is also provided\nas a callback in argument callback.\n\nEvery 5 sec we do this with a new random number (always the interval will call \ncallback) and for each iteration the callback function is called either with an\nexception or some data to be processed.\n\nNotice the return false after we call callback, consider the random number to be\na killswitch.\n\n> Note: Previously [__GHOST_URL__/2013/12/05/nodejs-error-handling-pattern/] I wrote about\nNode.js Error Handling Patterns, that the first argument for callbacks should\nalways be the error.\n\n\nThe scenario ensures the callback is not executed twice when we encounter the\nkillswitch (match the random number) because we only want to pass an instance of\nError to the callback in-line with best node.js practice, not provide the\ncallback doSomething data to process when we match the killswitch.\n\nWhen we have no match, we send some results to the callback as well as our final\ncallback function which is expected to be executed once after each callback\nregardless if there was an Error (final can handle the Error).\n\nAs we saw in the begining, final executes twice in doSomething when there is an\nError. Fixing the doSomething is easy, just add a return false when there is an\nError, just like our scenario.\n\nfunction doSomething(err, result, final){\n if(err){\n   final(err);\n   return false;\n }\n final(null,result);\n return result;\n}\n\n\nAnd add a return to the callback also, to return the result.\n\nConclusion\nA common mistake made by all early node developers is forgetting to return after\na callback. While sometimes this has no implications, there are many times where\nyou will run into odd issues or worse, debugging nightmares because your\ncallback is being called twice.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-21.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 12:57:54","created_by":"1","updated_at":"2021-03-31 14:19:45","updated_by":"1","published_at":"2014-01-03 03:48:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc8f","uuid":"a9bc7082-29ec-4a8e-925a-1759e7b5d0e2","title":"Node.js Error Handling Patterns","slug":"nodejs-error-handling-pattern","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"![node.js exception](__GHOST_URL__/content/images/2016/03/nodeErr-1.png)\\n\\n## The WRONG Way\\n\\nThere is always a exceptionally bad way of doing something in NodeJS (or any language) so I figured covering this will put perspective on the alternatives.\\n\\n## Listening for uncaughtException\\n\\nListening for events on the global `process` variable is easy, and this is likely why we see this so often in Node.js projects.\\n\\nTake the following example if you are unfamiliar;\\n\\n```language-javascript\\nprocess.on('uncaughtException', function(err) {\\n\\tconsole.log('Threw Exception: ', err);\\n});\\n```\\n\\nSo what exactly is wrong with this you ask? Simply put, after this event is emitted Node.js would usually crash and need to be restarted which keeps your software state as expected, whereas if you were to implement this `On Error Resume Next` equivalent your software will enter an unknown state and bad things WILL happen.\\n\\n>  Restart your application after every uncaughtException!\\n\\nNode.js [officially warn](http://nodejs.org/api/process.html#process_event_uncaughtexception) of the harm when listening for `uncaughtException` on the global `process` variable and advise to use use _domains_ which is covered further in this article.\\n\\n## Using Modules\\n\\nA graceful shutdown is the best you can hope for when you encounter `uncaughtException`, graceful meaning you save all in-memory data.\\n\\nNPM modules such as `monit`, `forever`, or `upstart` can be used to restart node process gracefully when `uncaughtException` is emitted.\\n\\n## Using Node.js Domain\\n\\nThis is the recommended pattern by Node.js. \\n\\nWrap a section of code in a [node.js domain](http://nodejs.org/api/domain.html) like this;\\n\\n```language-javascript\\nvar domain = require('domain').create();\\ndomain.on('error', function(err){\\n\\tconsole.log(err);\\n});\\n\\ndomain.run(function(){\\n\\tthrow new Error('thwump');\\n});\\n```\\n\\nThis new Node.js feature is currently unstable so use with caution if deploying to production.\\n\\nDesigned to work for asynchronous or synchronous code blocks, you are able to handle expections in context as opposed to `uncaughtException` which looses the exception context.\\n\\n> Old Node.js version? domain feature not available? Try/Catch\\n\\nNow the really interesting thing about using domains is it is closely tied to the cluster module, meaning it is actually possible to treat each connection (user) _state_ individually, therefore allowing Node.js to operate uneffected connections until(if) they themselves encounter the exception the first user encountered. Of course you will have to refuse new connections until you have restarted the Node.js process, as well as handle that user(s) who triggered the error.\\n\\n## Try/Catch\\n\\nThe most common exception handling in Node.js (most languages) is Try/Catch.\\n\\nBut I [read](http://jsperf.com/try-catch-performance-overhead) JavaScript Try/Catch performance is bad I hear you say, well you read correct (sort of)\\n\\nThis performance problem is mainly in context to JavaScript running in the browser not in Node.js which is v8, unfortunately there are [gotchas with v8](https://github.com/joyent/node/wiki/Best-practices-and-gotchas-with-v8) also.\\n\\nTo shorten this, use Try/Catch _outside_ your functions for best results.\\n\\n```language-javascript\\ntry {\\n    throw new Error('thwump');\\n} catch (e) {\\n    console.log(e);\\n}\\n```\\n\\n**Finally:** only use Try/Catch for synchronous code.\\n\\n## Functional Approach\\n\\nAn example using the Express.js framework, define a function that returns an error handler:\\n\\n```language-javascript\\nfunction error(res,next) {\\n  return function( err, result ) {\\n    if( err ) {\\n      res.writeHead(500); res.end(); console.log(err);\\n    }\\n    else {\\n      next(result)\\n    }\\n  }\\n}\\n```\\n\\nWhich allows you to do this;\\n\\n```language-javascript\\napp.get('/foo', function(req,res) {\\n  db.query('sql', error(res, function(result) {\\n    // use result\\n  }));\\n});\\n```\\n\\nCovering this method only as an example, _do not_ use this method for production code, unless you actually _handle_ the error properly.\\n\\n## Chain Of Responsibility - Your Error Handler\\n\\nRule of thumb: Never, ever!, deal with the error you don't know, pass it to next callback, or throw it.\\n\\nOtherwise according to the type of error which you know, design your error handler to correct the problem so the process may continue to the succeeding callback.\\n\\n## Safely _throwing_ Errors\\n\\nInstead of literally throwing the error, you design your error handler in such a way to allow for continuance if the error thrown can be corrected.\\n\\n### For Synchronous Code\\n\\nIf an error happens, return the error:\\n\\n```language-javascript\\nvar match = function match(foo,bar) {\\n    if ( foo !== bar ) {\\n        return new Error(\\\"no match\\\");\\n    } else {\\n        return true;\\n    }\\n};\\nvar result = match(1,2);\\nif ( result instanceof Error ) {\\n    console.log('Error: ', result);\\n} else {\\n    console.log('Result: ', result);\\n}\\n```\\n\\nRealistically this function would return false rather than an instance of Error, but you get the point.\\n\\n### For Callback Based (Asynchronous) Code\\n\\nThere is a common and well known pattern for handling errors in Node.js.\\n\\n```language-javascript\\nvar match = function match(foo,bar,next) {\\n    if ( foo !== bar ) {\\n        next( new Error(\\\"no match\\\") );\\n    } else {\\n    \\tvar result = true;\\n        next(null, result);\\n    }\\n};\\nmatch(1, 2, function handleMatch(err,result){\\n  if ( err ) {\\n  \\tconsole.log('Error: ', err);\\n  } else {\\n  \\tconsole.log('Result: ', result);\\n  }\\n});\\n```\\n\\nThe first argument of the callback is always err.\\nSo you know always that if err is Error, handle it, or if err is null you can expect any other arguments to follow the err argument.\\n\\n### For _eventful_ Code\\n\\nWith the event coding pattern the error may happen anywhere, so instead of throwing the error one would emit the error event instead:\\n\\n```language-javascript\\nvar events = require('events');\\nvar Matcher = function Matcher(){\\n    events.EventEmitter.call(this);\\n};\\nrequire('util').inherits(Matcher, events.EventEmitter);\\n\\nMatcher.prototype.match = function match(foo,bar) {\\n  if ( foo !== bar ) {\\n  \\tthis.emit('error', new Error(\\\"no match\\\"));\\n  } else {\\n  \\tthis.emit('match', foo, bar, true);\\n  }\\n  return this;\\n};\\nvar matcher = new Matcher();\\nmatcher.on('error', function(err){\\n    console.log(err);\\n});\\nmatcher.on('match', function(foo, bar, result){\\n    console.log('Match: ', result);\\n});\\nmatcher.match(1,2).match().match(1,1).match(1);\\n```\\n\\nHere I defined the Matcher Event Emitter, a method called match with the functionality, emitting an error or match event depending ont he outcome, and finally return itself for chaining.\\n\\n## Conclusion\\n\\nDiscovering the cause of a crash in Node.js doesn't have to be terrible, if you have a good sense of error correction and reporting.\\nThe end result is a stable program and excellent exposure if problems do arise.\\n\\nRemember to refer to [Node.js](http://nodejs.org/) frequently and always restart your application after every uncaught, unhandled, or unknown exceptions! Once the cause is ascertained and corrected of course.\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><img src=\"__GHOST_URL__/content/images/2016/03/nodeErr-1.png\" alt=\"node.js exception\" loading=\"lazy\"></p>\n<h2 id=\"thewrongway\">The WRONG Way</h2>\n<p>There is always a exceptionally bad way of doing something in NodeJS (or any language) so I figured covering this will put perspective on the alternatives.</p>\n<h2 id=\"listeningforuncaughtexception\">Listening for uncaughtException</h2>\n<p>Listening for events on the global <code>process</code> variable is easy, and this is likely why we see this so often in Node.js projects.</p>\n<p>Take the following example if you are unfamiliar;</p>\n<pre><code class=\"language-language-javascript\">process.on('uncaughtException', function(err) {\n\tconsole.log('Threw Exception: ', err);\n});\n</code></pre>\n<p>So what exactly is wrong with this you ask? Simply put, after this event is emitted Node.js would usually crash and need to be restarted which keeps your software state as expected, whereas if you were to implement this <code>On Error Resume Next</code> equivalent your software will enter an unknown state and bad things WILL happen.</p>\n<blockquote>\n<p>Restart your application after every uncaughtException!</p>\n</blockquote>\n<p>Node.js <a href=\"http://nodejs.org/api/process.html#process_event_uncaughtexception\">officially warn</a> of the harm when listening for <code>uncaughtException</code> on the global <code>process</code> variable and advise to use use <em>domains</em> which is covered further in this article.</p>\n<h2 id=\"usingmodules\">Using Modules</h2>\n<p>A graceful shutdown is the best you can hope for when you encounter <code>uncaughtException</code>, graceful meaning you save all in-memory data.</p>\n<p>NPM modules such as <code>monit</code>, <code>forever</code>, or <code>upstart</code> can be used to restart node process gracefully when <code>uncaughtException</code> is emitted.</p>\n<h2 id=\"usingnodejsdomain\">Using Node.js Domain</h2>\n<p>This is the recommended pattern by Node.js.</p>\n<p>Wrap a section of code in a <a href=\"http://nodejs.org/api/domain.html\">node.js domain</a> like this;</p>\n<pre><code class=\"language-language-javascript\">var domain = require('domain').create();\ndomain.on('error', function(err){\n\tconsole.log(err);\n});\n\ndomain.run(function(){\n\tthrow new Error('thwump');\n});\n</code></pre>\n<p>This new Node.js feature is currently unstable so use with caution if deploying to production.</p>\n<p>Designed to work for asynchronous or synchronous code blocks, you are able to handle expections in context as opposed to <code>uncaughtException</code> which looses the exception context.</p>\n<blockquote>\n<p>Old Node.js version? domain feature not available? Try/Catch</p>\n</blockquote>\n<p>Now the really interesting thing about using domains is it is closely tied to the cluster module, meaning it is actually possible to treat each connection (user) <em>state</em> individually, therefore allowing Node.js to operate uneffected connections until(if) they themselves encounter the exception the first user encountered. Of course you will have to refuse new connections until you have restarted the Node.js process, as well as handle that user(s) who triggered the error.</p>\n<h2 id=\"trycatch\">Try/Catch</h2>\n<p>The most common exception handling in Node.js (most languages) is Try/Catch.</p>\n<p>But I <a href=\"http://jsperf.com/try-catch-performance-overhead\">read</a> JavaScript Try/Catch performance is bad I hear you say, well you read correct (sort of)</p>\n<p>This performance problem is mainly in context to JavaScript running in the browser not in Node.js which is v8, unfortunately there are <a href=\"https://github.com/joyent/node/wiki/Best-practices-and-gotchas-with-v8\">gotchas with v8</a> also.</p>\n<p>To shorten this, use Try/Catch <em>outside</em> your functions for best results.</p>\n<pre><code class=\"language-language-javascript\">try {\n    throw new Error('thwump');\n} catch (e) {\n    console.log(e);\n}\n</code></pre>\n<p><strong>Finally:</strong> only use Try/Catch for synchronous code.</p>\n<h2 id=\"functionalapproach\">Functional Approach</h2>\n<p>An example using the Express.js framework, define a function that returns an error handler:</p>\n<pre><code class=\"language-language-javascript\">function error(res,next) {\n  return function( err, result ) {\n    if( err ) {\n      res.writeHead(500); res.end(); console.log(err);\n    }\n    else {\n      next(result)\n    }\n  }\n}\n</code></pre>\n<p>Which allows you to do this;</p>\n<pre><code class=\"language-language-javascript\">app.get('/foo', function(req,res) {\n  db.query('sql', error(res, function(result) {\n    // use result\n  }));\n});\n</code></pre>\n<p>Covering this method only as an example, <em>do not</em> use this method for production code, unless you actually <em>handle</em> the error properly.</p>\n<h2 id=\"chainofresponsibilityyourerrorhandler\">Chain Of Responsibility - Your Error Handler</h2>\n<p>Rule of thumb: Never, ever!, deal with the error you don't know, pass it to next callback, or throw it.</p>\n<p>Otherwise according to the type of error which you know, design your error handler to correct the problem so the process may continue to the succeeding callback.</p>\n<h2 id=\"safelythrowingerrors\">Safely <em>throwing</em> Errors</h2>\n<p>Instead of literally throwing the error, you design your error handler in such a way to allow for continuance if the error thrown can be corrected.</p>\n<h3 id=\"forsynchronouscode\">For Synchronous Code</h3>\n<p>If an error happens, return the error:</p>\n<pre><code class=\"language-language-javascript\">var match = function match(foo,bar) {\n    if ( foo !== bar ) {\n        return new Error(&quot;no match&quot;);\n    } else {\n        return true;\n    }\n};\nvar result = match(1,2);\nif ( result instanceof Error ) {\n    console.log('Error: ', result);\n} else {\n    console.log('Result: ', result);\n}\n</code></pre>\n<p>Realistically this function would return false rather than an instance of Error, but you get the point.</p>\n<h3 id=\"forcallbackbasedasynchronouscode\">For Callback Based (Asynchronous) Code</h3>\n<p>There is a common and well known pattern for handling errors in Node.js.</p>\n<pre><code class=\"language-language-javascript\">var match = function match(foo,bar,next) {\n    if ( foo !== bar ) {\n        next( new Error(&quot;no match&quot;) );\n    } else {\n    \tvar result = true;\n        next(null, result);\n    }\n};\nmatch(1, 2, function handleMatch(err,result){\n  if ( err ) {\n  \tconsole.log('Error: ', err);\n  } else {\n  \tconsole.log('Result: ', result);\n  }\n});\n</code></pre>\n<p>The first argument of the callback is always err.<br>\nSo you know always that if err is Error, handle it, or if err is null you can expect any other arguments to follow the err argument.</p>\n<h3 id=\"foreventfulcode\">For <em>eventful</em> Code</h3>\n<p>With the event coding pattern the error may happen anywhere, so instead of throwing the error one would emit the error event instead:</p>\n<pre><code class=\"language-language-javascript\">var events = require('events');\nvar Matcher = function Matcher(){\n    events.EventEmitter.call(this);\n};\nrequire('util').inherits(Matcher, events.EventEmitter);\n\nMatcher.prototype.match = function match(foo,bar) {\n  if ( foo !== bar ) {\n  \tthis.emit('error', new Error(&quot;no match&quot;));\n  } else {\n  \tthis.emit('match', foo, bar, true);\n  }\n  return this;\n};\nvar matcher = new Matcher();\nmatcher.on('error', function(err){\n    console.log(err);\n});\nmatcher.on('match', function(foo, bar, result){\n    console.log('Match: ', result);\n});\nmatcher.match(1,2).match().match(1,1).match(1);\n</code></pre>\n<p>Here I defined the Matcher Event Emitter, a method called match with the functionality, emitting an error or match event depending ont he outcome, and finally return itself for chaining.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Discovering the cause of a crash in Node.js doesn't have to be terrible, if you have a good sense of error correction and reporting.<br>\nThe end result is a stable program and excellent exposure if problems do arise.</p>\n<p>Remember to refer to <a href=\"http://nodejs.org/\">Node.js</a> frequently and always restart your application after every uncaught, unhandled, or unknown exceptions! Once the cause is ascertained and corrected of course.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"8","plaintext":"\n\nThe WRONG Way\nThere is always a exceptionally bad way of doing something in NodeJS (or any\nlanguage) so I figured covering this will put perspective on the alternatives.\n\nListening for uncaughtException\nListening for events on the global process variable is easy, and this is likely\nwhy we see this so often in Node.js projects.\n\nTake the following example if you are unfamiliar;\n\nprocess.on('uncaughtException', function(err) {\n\tconsole.log('Threw Exception: ', err);\n});\n\n\nSo what exactly is wrong with this you ask? Simply put, after this event is\nemitted Node.js would usually crash and need to be restarted which keeps your\nsoftware state as expected, whereas if you were to implement this On Error\nResume Next equivalent your software will enter an unknown state and bad things\nWILL happen.\n\n> Restart your application after every uncaughtException!\n\n\nNode.js officially warn\n[http://nodejs.org/api/process.html#process_event_uncaughtexception] of the harm\nwhen listening for uncaughtException on the global process variable and advise\nto use use domains which is covered further in this article.\n\nUsing Modules\nA graceful shutdown is the best you can hope for when you encounter \nuncaughtException, graceful meaning you save all in-memory data.\n\nNPM modules such as monit, forever, or upstart can be used to restart node\nprocess gracefully when uncaughtException is emitted.\n\nUsing Node.js Domain\nThis is the recommended pattern by Node.js.\n\nWrap a section of code in a node.js domain [http://nodejs.org/api/domain.html] \nlike this;\n\nvar domain = require('domain').create();\ndomain.on('error', function(err){\n\tconsole.log(err);\n});\n\ndomain.run(function(){\n\tthrow new Error('thwump');\n});\n\n\nThis new Node.js feature is currently unstable so use with caution if deploying\nto production.\n\nDesigned to work for asynchronous or synchronous code blocks, you are able to\nhandle expections in context as opposed to uncaughtException which looses the\nexception context.\n\n> Old Node.js version? domain feature not available? Try/Catch\n\n\nNow the really interesting thing about using domains is it is closely tied to\nthe cluster module, meaning it is actually possible to treat each connection\n(user) state individually, therefore allowing Node.js to operate uneffected\nconnections until(if) they themselves encounter the exception the first user\nencountered. Of course you will have to refuse new connections until you have\nrestarted the Node.js process, as well as handle that user(s) who triggered the\nerror.\n\nTry/Catch\nThe most common exception handling in Node.js (most languages) is Try/Catch.\n\nBut I read [http://jsperf.com/try-catch-performance-overhead] JavaScript\nTry/Catch performance is bad I hear you say, well you read correct (sort of)\n\nThis performance problem is mainly in context to JavaScript running in the\nbrowser not in Node.js which is v8, unfortunately there are gotchas with v8\n[https://github.com/joyent/node/wiki/Best-practices-and-gotchas-with-v8] also.\n\nTo shorten this, use Try/Catch outside your functions for best results.\n\ntry {\n    throw new Error('thwump');\n} catch (e) {\n    console.log(e);\n}\n\n\nFinally: only use Try/Catch for synchronous code.\n\nFunctional Approach\nAn example using the Express.js framework, define a function that returns an\nerror handler:\n\nfunction error(res,next) {\n  return function( err, result ) {\n    if( err ) {\n      res.writeHead(500); res.end(); console.log(err);\n    }\n    else {\n      next(result)\n    }\n  }\n}\n\n\nWhich allows you to do this;\n\napp.get('/foo', function(req,res) {\n  db.query('sql', error(res, function(result) {\n    // use result\n  }));\n});\n\n\nCovering this method only as an example, do not use this method for production\ncode, unless you actually handle the error properly.\n\nChain Of Responsibility - Your Error Handler\nRule of thumb: Never, ever!, deal with the error you don't know, pass it to next\ncallback, or throw it.\n\nOtherwise according to the type of error which you know, design your error\nhandler to correct the problem so the process may continue to the succeeding\ncallback.\n\nSafely throwing Errors\nInstead of literally throwing the error, you design your error handler in such a\nway to allow for continuance if the error thrown can be corrected.\n\nFor Synchronous Code\nIf an error happens, return the error:\n\nvar match = function match(foo,bar) {\n    if ( foo !== bar ) {\n        return new Error(\"no match\");\n    } else {\n        return true;\n    }\n};\nvar result = match(1,2);\nif ( result instanceof Error ) {\n    console.log('Error: ', result);\n} else {\n    console.log('Result: ', result);\n}\n\n\nRealistically this function would return false rather than an instance of Error,\nbut you get the point.\n\nFor Callback Based (Asynchronous) Code\nThere is a common and well known pattern for handling errors in Node.js.\n\nvar match = function match(foo,bar,next) {\n    if ( foo !== bar ) {\n        next( new Error(\"no match\") );\n    } else {\n    \tvar result = true;\n        next(null, result);\n    }\n};\nmatch(1, 2, function handleMatch(err,result){\n  if ( err ) {\n  \tconsole.log('Error: ', err);\n  } else {\n  \tconsole.log('Result: ', result);\n  }\n});\n\n\nThe first argument of the callback is always err.\nSo you know always that if err is Error, handle it, or if err is null you can\nexpect any other arguments to follow the err argument.\n\nFor eventful Code\nWith the event coding pattern the error may happen anywhere, so instead of\nthrowing the error one would emit the error event instead:\n\nvar events = require('events');\nvar Matcher = function Matcher(){\n    events.EventEmitter.call(this);\n};\nrequire('util').inherits(Matcher, events.EventEmitter);\n\nMatcher.prototype.match = function match(foo,bar) {\n  if ( foo !== bar ) {\n  \tthis.emit('error', new Error(\"no match\"));\n  } else {\n  \tthis.emit('match', foo, bar, true);\n  }\n  return this;\n};\nvar matcher = new Matcher();\nmatcher.on('error', function(err){\n    console.log(err);\n});\nmatcher.on('match', function(foo, bar, result){\n    console.log('Match: ', result);\n});\nmatcher.match(1,2).match().match(1,1).match(1);\n\n\nHere I defined the Matcher Event Emitter, a method called match with the\nfunctionality, emitting an error or match event depending ont he outcome, and\nfinally return itself for chaining.\n\nConclusion\nDiscovering the cause of a crash in Node.js doesn't have to be terrible, if you\nhave a good sense of error correction and reporting.\nThe end result is a stable program and excellent exposure if problems do arise.\n\nRemember to refer to Node.js [http://nodejs.org/] frequently and always restart\nyour application after every uncaught, unhandled, or unknown exceptions! Once\nthe cause is ascertained and corrected of course.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-9.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 13:18:52","created_by":"1","updated_at":"2021-03-31 14:16:00","updated_by":"1","published_at":"2013-12-05 04:18:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc90","uuid":"18e83fec-369f-4e81-9ea8-889edb4cfaae","title":"Process Functions Asynchronously with a Final Callback","slug":"process-functions-asynchronously-with-a-final-callback","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Purpose and use case\\n\\nGenerally your functions are executed asynchronously with some exceptions, and the challenge for some is when you have a piece of code that can not be executed until previous functions are complete or data is returned.\\n\\n> So then why exactly would you want to process functions asynchronously with a final callback function, give me examples.\\n\\n1. Let's say you have some database calls that are then combined into a single object which is then passed onto some functionality or client side component, your logic to do this would be the final callback where as all of the logic to get the different dependant data can freely execute asynchronously!\\n\\n2. Perhaps your app will aggregate data from various API's, or the same API but needs to call it multiple times. Perhaps a good example is to gather recent tweets from 10 random accounts you are following and aggregating these into a single thread to be passed to a client side component to display the results.\\n\\n## Node Module async\\n\\nThis solution is actually so light and generic that it can work both in NodeJS and in the browser, doing exactly _what it says on the box_ so to speak.\\n\\n## The code\\n\\n#### Vanilla JavaScript\\nLets take a look at the familiar vanilla JavaScript first;\\n\\n```language-javascript\\nfunction asyncProcess(array, callback) {\\n    var completed = 0;\\n    if (array.length === 0) {\\n        callback(); // done immediately\\n    }\\n    for (var i = 0, len = array.length; i < len; i++) {\\n        array[i](function () {\\n            completed++;\\n            if (completed === array.length) {\\n                callback();\\n            }\\n        });\\n    }\\n};\\n```\\n\\nSee, simple and functional.\\n\\n#### Using It\\n\\nAt first you might think this _problem_ would be hard to solve, trust me, it is not scary at all.\\n\\n```language-javascript\\nvar funcArray = [\\n    \\tfunction first(callback){\\n        \\t//do stuff\\n            callback();\\n        },\\n    \\tfunction second(callback){\\n        \\t//do stuff\\n            callback();\\n        },\\n    \\tfunction third(callback){\\n        \\t//do stuff\\n            callback();\\n        }\\n    ],\\n    Callback = function Callback(){\\n    \\t//final function\\n    };\\nasyncProcess(funcArray,Callback);\\n```\\n\\nSurprised? It is that simple!\\n\\n#### NodeJS\\n\\nHow does it look in node? lets use a simple `export` so that we can easily `require` it as a dependency;\\n\\nIn a library/module script, let's call it `async.js`;\\n\\n```language-javascript\\nexports.process = function process(array, callback) {\\n    var completed = 0;\\n    if (array.length === 0) {\\n        callback(); // done immediately\\n    }\\n    for (var i = 0, len = array.length; i < len; i++) {\\n        array[i](function () {\\n            completed++;\\n            if (completed === array.length) {\\n                callback();\\n            }\\n        });\\n    }\\n};\\n```\\n\\n#### Using It\\n\\nIn a logic script, lets use `server.js`;\\n\\n```language-javascript\\nvar async = require('async'),\\n\\tfuncArray = [\\n    \\tfunction first(callback){\\n        \\t//do stuff\\n            callback();\\n        },\\n    \\tfunction second(callback){\\n        \\t//do stuff\\n            callback();\\n        },\\n    \\tfunction third(callback){\\n        \\t//do stuff\\n            callback();\\n        }\\n    ],\\n    Callback = function Callback(){\\n    \\t//final function\\n    };\\nasync.process(funcArray,Callback);\\n```\\n\\n### Tip\\n\\nNext time you're facing a problem relating to async and data, do what I do, make a written list of the process flow, then try to write a simple function to handle it. This way you remove application logic from the business process and your code stays clean.\\n\\nLastly, avoid nested Callbacks! Name your callbacks so they are traceable and pass the name as an argument instead of nesting your callbacks.\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h2 id=\"purposeandusecase\">Purpose and use case</h2>\n<p>Generally your functions are executed asynchronously with some exceptions, and the challenge for some is when you have a piece of code that can not be executed until previous functions are complete or data is returned.</p>\n<blockquote>\n<p>So then why exactly would you want to process functions asynchronously with a final callback function, give me examples.</p>\n</blockquote>\n<ol>\n<li>\n<p>Let's say you have some database calls that are then combined into a single object which is then passed onto some functionality or client side component, your logic to do this would be the final callback where as all of the logic to get the different dependant data can freely execute asynchronously!</p>\n</li>\n<li>\n<p>Perhaps your app will aggregate data from various API's, or the same API but needs to call it multiple times. Perhaps a good example is to gather recent tweets from 10 random accounts you are following and aggregating these into a single thread to be passed to a client side component to display the results.</p>\n</li>\n</ol>\n<h2 id=\"nodemoduleasync\">Node Module async</h2>\n<p>This solution is actually so light and generic that it can work both in NodeJS and in the browser, doing exactly <em>what it says on the box</em> so to speak.</p>\n<h2 id=\"thecode\">The code</h2>\n<h4 id=\"vanillajavascript\">Vanilla JavaScript</h4>\n<p>Lets take a look at the familiar vanilla JavaScript first;</p>\n<pre><code class=\"language-language-javascript\">function asyncProcess(array, callback) {\n    var completed = 0;\n    if (array.length === 0) {\n        callback(); // done immediately\n    }\n    for (var i = 0, len = array.length; i &lt; len; i++) {\n        array[i](function () {\n            completed++;\n            if (completed === array.length) {\n                callback();\n            }\n        });\n    }\n};\n</code></pre>\n<p>See, simple and functional.</p>\n<h4 id=\"usingit\">Using It</h4>\n<p>At first you might think this <em>problem</em> would be hard to solve, trust me, it is not scary at all.</p>\n<pre><code class=\"language-language-javascript\">var funcArray = [\n    \tfunction first(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction second(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction third(callback){\n        \t//do stuff\n            callback();\n        }\n    ],\n    Callback = function Callback(){\n    \t//final function\n    };\nasyncProcess(funcArray,Callback);\n</code></pre>\n<p>Surprised? It is that simple!</p>\n<h4 id=\"nodejs\">NodeJS</h4>\n<p>How does it look in node? lets use a simple <code>export</code> so that we can easily <code>require</code> it as a dependency;</p>\n<p>In a library/module script, let's call it <code>async.js</code>;</p>\n<pre><code class=\"language-language-javascript\">exports.process = function process(array, callback) {\n    var completed = 0;\n    if (array.length === 0) {\n        callback(); // done immediately\n    }\n    for (var i = 0, len = array.length; i &lt; len; i++) {\n        array[i](function () {\n            completed++;\n            if (completed === array.length) {\n                callback();\n            }\n        });\n    }\n};\n</code></pre>\n<h4 id=\"usingit\">Using It</h4>\n<p>In a logic script, lets use <code>server.js</code>;</p>\n<pre><code class=\"language-language-javascript\">var async = require('async'),\n\tfuncArray = [\n    \tfunction first(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction second(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction third(callback){\n        \t//do stuff\n            callback();\n        }\n    ],\n    Callback = function Callback(){\n    \t//final function\n    };\nasync.process(funcArray,Callback);\n</code></pre>\n<h3 id=\"tip\">Tip</h3>\n<p>Next time you're facing a problem relating to async and data, do what I do, make a written list of the process flow, then try to write a simple function to handle it. This way you remove application logic from the business process and your code stays clean.</p>\n<p>Lastly, avoid nested Callbacks! Name your callbacks so they are traceable and pass the name as an argument instead of nesting your callbacks.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"9","plaintext":"Purpose and use case\nGenerally your functions are executed asynchronously with some exceptions, and\nthe challenge for some is when you have a piece of code that can not be executed\nuntil previous functions are complete or data is returned.\n\n> So then why exactly would you want to process functions asynchronously with a\nfinal callback function, give me examples.\n\n\n 1. Let's say you have some database calls that are then combined into a single\n    object which is then passed onto some functionality or client side\n    component, your logic to do this would be the final callback where as all of\n    the logic to get the different dependant data can freely execute\n    asynchronously!\n    \n    \n 2. Perhaps your app will aggregate data from various API's, or the same API but\n    needs to call it multiple times. Perhaps a good example is to gather recent\n    tweets from 10 random accounts you are following and aggregating these into\n    a single thread to be passed to a client side component to display the\n    results.\n    \n    \n\nNode Module async\nThis solution is actually so light and generic that it can work both in NodeJS\nand in the browser, doing exactly what it says on the box so to speak.\n\nThe code\nVanilla JavaScript\nLets take a look at the familiar vanilla JavaScript first;\n\nfunction asyncProcess(array, callback) {\n    var completed = 0;\n    if (array.length === 0) {\n        callback(); // done immediately\n    }\n    for (var i = 0, len = array.length; i < len; i++) {\n        array[i](function () {\n            completed++;\n            if (completed === array.length) {\n                callback();\n            }\n        });\n    }\n};\n\n\nSee, simple and functional.\n\nUsing It\nAt first you might think this problem would be hard to solve, trust me, it is\nnot scary at all.\n\nvar funcArray = [\n    \tfunction first(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction second(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction third(callback){\n        \t//do stuff\n            callback();\n        }\n    ],\n    Callback = function Callback(){\n    \t//final function\n    };\nasyncProcess(funcArray,Callback);\n\n\nSurprised? It is that simple!\n\nNodeJS\nHow does it look in node? lets use a simple export so that we can easily require \nit as a dependency;\n\nIn a library/module script, let's call it async.js;\n\nexports.process = function process(array, callback) {\n    var completed = 0;\n    if (array.length === 0) {\n        callback(); // done immediately\n    }\n    for (var i = 0, len = array.length; i < len; i++) {\n        array[i](function () {\n            completed++;\n            if (completed === array.length) {\n                callback();\n            }\n        });\n    }\n};\n\n\nUsing It\nIn a logic script, lets use server.js;\n\nvar async = require('async'),\n\tfuncArray = [\n    \tfunction first(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction second(callback){\n        \t//do stuff\n            callback();\n        },\n    \tfunction third(callback){\n        \t//do stuff\n            callback();\n        }\n    ],\n    Callback = function Callback(){\n    \t//final function\n    };\nasync.process(funcArray,Callback);\n\n\nTip\nNext time you're facing a problem relating to async and data, do what I do, make\na written list of the process flow, then try to write a simple function to\nhandle it. This way you remove application logic from the business process and\nyour code stays clean.\n\nLastly, avoid nested Callbacks! Name your callbacks so they are traceable and\npass the name as an argument instead of nesting your callbacks.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-20.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 13:24:13","created_by":"1","updated_at":"2021-03-31 14:11:40","updated_by":"1","published_at":"2013-11-28 00:17:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc91","uuid":"999987aa-58ac-4544-830b-746703b96955","title":"Page Loads in JavaScript and Performance","slug":"page-loads-in-javascript-and-performance","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This is a quick post to list the methods of initializing your JavaScript the right way in terms of Page Load Speed and in turn SEO rankings improvements and faster mobile UX.\\n\\n## using a jQuery helper\\n\\nWhat it looks like;\\n\\n```language-javascript\\n$(document).ready(function(){\\n\\n\\tconsole.log(\\\"DOM loaded but not fully parsed\\\");\\n\\n});\\n```\\n\\n#### Notes:\\n\\n* This jQuery helper is essentially an alias too ```document.readyState == \\\"interactive\\\"```\\n* All style, async=false script, and img src resources have been loaded.\\n* The execution order of multiple uses of this helper is based on load order or placement in the DOM\\n* Any other synchronous JavaScript has executed \\n\\n## window load method\\n\\nWhat it looks like;\\n\\n```language-javascript\\n$(window).load(function() {\\n                \\n\\tconsole.log(\\\"DOM and all frames are loaded and fully parsed\\\");\\n\\n});\\n```\\n\\n#### Notes: \\n\\n* This is an alternative to ```document.readyState == \\\"complete\\\"```\\n* All style, script, img src resources, and frames have been loaded, including all of these inside the frames, also includes asynchronous scripts\\n* This is also asynchronous\\n\\n## Native browser states\\n\\nWhat it looks like;\\n\\n```language-javascript\\ndocument.onreadystatechange = function () {\\n  if (document.readyState == \\\"complete\\\") {\\n\\n\\tconsole.log(\\\"DOM fully loaded and parsed\\\");\\n\\n  }\\n}\\n```\\n\\n#### Notes: \\n\\n* This is an alternative to ```window.load()```\\n* DOM fully loaded and parsed - including, ```jQuery.ready```\\n* This is synchronous, using this method will lock your browser so don't do any intensive tasks.\\n\\n## Another native browser state\\n\\n```language-javascript\\ndocument.onreadystatechange = function () {\\n  if (document.readyState == \\\"interactive\\\") {\\n\\n\\tconsole.log(\\\"DOM loaded but not fully parsed\\\");\\n\\n  }\\n}\\n```\\n\\n#### Notes: \\n\\n* This is an alternative to ```DOMContentLoaded```\\n* DOM fully loaded and not fully parsed - so, ```jQuery.ready``` has not yet completed but may be asynchronously executing code\\n* This is also synchronous\\n\\n## Native browser event listener\\n\\n```language-javascript\\ndocument.addEventListener(\\\"DOMContentLoaded\\\", function(event) {\\n\\t\\n    console.log(\\\"DOM fully loaded and parsed\\\");\\n    \\n});\\n```\\n\\n#### Notes: \\n\\n* This is an alternative to ```document.readyState == \\\"interactive\\\"``` so those notes apply\\n* Bubbling for this event is supported by at least Gecko 1.9.2, Chrome 6, and Safari 4\\n* Earlier than Internet Explorer 8 requires the ```.doScroll()``` hack\\n\\n\\n## Self-executing anonymous functions\\n\\nWhat it looks like;\\n\\n```language-javascript\\n(function(){\\n\\n\\tconsole.log(\\\"DOM loaded to the comment >here< and not yet fully parsed\\\");\\n\\n})(); // >here<\\n\\n```\\n#### Notes: \\n\\n* This wont execute your code within until it reaches the bottom of its declaration (comment as >here<)\\n* DOM has only partially loaded to this point\\n* The code within executes asynchronously as the DOM continues to load\\n\\n#### Why would we do this?\\n\\n* Easier to remember than all of the above and has no browser compatibility concerns as it is original native JavaScript.\\n* Lazy devs testing before production release will \\\"get lucky\\\" thinking that all their resources are loaded and available in here because they haven't needed to go through the internet to load them all.\\n* The only acceptable use of this technique is to do declarations asynchronously for later use, do not execute any code that rely on resources elsewhere.\\n\\n## jQuery anonymous function aka Deferred\\n\\n```language-javascript\\n$(function() {\\n\\n\\tconsole.log(\\\"DOM loaded to the comment >here< and not yet fully parsed\\\");\\n   \\n}); // >here<\\n```\\n#### Notes: \\n\\n* Similar in appearance to Self executing anonymous functions but very different.\\n* This is an alternative to ```document.readyState == \\\"interactive\\\"``` so those notes apply.\\n* A great technique if you need to declare something that multiple ```jQuery.ready``` may use - but no other acceptable reason to use this.\\n\\n## Lazy load\\n\\nSay you have an element on page that uses a date picker plugin, instead of loading in the resources during page load, learn the best time in your UX to load the content after the page has loaded.\\n\\n```language-javascript\\n$(document).on('mouseover', '.date-container',function(){\\n\\t$.getScript(\\\"ajax/test.js\\\",function(){\\n\\n\\t\\tconsole.log( \\\"Load was performed. Init it if needed\\\" );\\n        \\n   });\\n});\\n```\\n\\n#### Benefits?\\n\\n* Less downloading, faster page speeds\\n* You're browser won't load in unnecessary scripts unless the user actually needs to use them.\\n* UX is unchanged from other usages but will be easier to debug and identify UX dependencies.\\n\\n> ### TL;DR\\n\\n1) Avoid using the native Javascript implementations as they are harder to remember, too verbose and clumsy. Instead opt to use a library (jQuery not the only option here) which should simplify things and encourage best practice\\n\\n2) Do not execute code until the whole DOM has loaded and parsed, but feel free to declare as much as you like as these can be easily referenced later with shared code.\\n\\n3) Being fancy and using self-executing anonymous functions because they look cool and someone said they work like jQuery.ready is actually wrong and plain lazy. Use proven or library helpers because they work.\\n\\n## Conclusion\\n\\nLazy load everything you possibly can, this is not only the best in terms of page load speed and therefore mobile and SEO benefits, it is also logical and makes debugging UX extremely easy to follow.\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>This is a quick post to list the methods of initializing your JavaScript the right way in terms of Page Load Speed and in turn SEO rankings improvements and faster mobile UX.</p>\n<h2 id=\"usingajqueryhelper\">using a jQuery helper</h2>\n<p>What it looks like;</p>\n<pre><code class=\"language-language-javascript\">$(document).ready(function(){\n\n\tconsole.log(&quot;DOM loaded but not fully parsed&quot;);\n\n});\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This jQuery helper is essentially an alias too <code>document.readyState == &quot;interactive&quot;</code></li>\n<li>All style, async=false script, and img src resources have been loaded.</li>\n<li>The execution order of multiple uses of this helper is based on load order or placement in the DOM</li>\n<li>Any other synchronous JavaScript has executed</li>\n</ul>\n<h2 id=\"windowloadmethod\">window load method</h2>\n<p>What it looks like;</p>\n<pre><code class=\"language-language-javascript\">$(window).load(function() {\n                \n\tconsole.log(&quot;DOM and all frames are loaded and fully parsed&quot;);\n\n});\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This is an alternative to <code>document.readyState == &quot;complete&quot;</code></li>\n<li>All style, script, img src resources, and frames have been loaded, including all of these inside the frames, also includes asynchronous scripts</li>\n<li>This is also asynchronous</li>\n</ul>\n<h2 id=\"nativebrowserstates\">Native browser states</h2>\n<p>What it looks like;</p>\n<pre><code class=\"language-language-javascript\">document.onreadystatechange = function () {\n  if (document.readyState == &quot;complete&quot;) {\n\n\tconsole.log(&quot;DOM fully loaded and parsed&quot;);\n\n  }\n}\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This is an alternative to <code>window.load()</code></li>\n<li>DOM fully loaded and parsed - including, <code>jQuery.ready</code></li>\n<li>This is synchronous, using this method will lock your browser so don't do any intensive tasks.</li>\n</ul>\n<h2 id=\"anothernativebrowserstate\">Another native browser state</h2>\n<pre><code class=\"language-language-javascript\">document.onreadystatechange = function () {\n  if (document.readyState == &quot;interactive&quot;) {\n\n\tconsole.log(&quot;DOM loaded but not fully parsed&quot;);\n\n  }\n}\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This is an alternative to <code>DOMContentLoaded</code></li>\n<li>DOM fully loaded and not fully parsed - so, <code>jQuery.ready</code> has not yet completed but may be asynchronously executing code</li>\n<li>This is also synchronous</li>\n</ul>\n<h2 id=\"nativebrowsereventlistener\">Native browser event listener</h2>\n<pre><code class=\"language-language-javascript\">document.addEventListener(&quot;DOMContentLoaded&quot;, function(event) {\n\t\n    console.log(&quot;DOM fully loaded and parsed&quot;);\n    \n});\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This is an alternative to <code>document.readyState == &quot;interactive&quot;</code> so those notes apply</li>\n<li>Bubbling for this event is supported by at least Gecko 1.9.2, Chrome 6, and Safari 4</li>\n<li>Earlier than Internet Explorer 8 requires the <code>.doScroll()</code> hack</li>\n</ul>\n<h2 id=\"selfexecutinganonymousfunctions\">Self-executing anonymous functions</h2>\n<p>What it looks like;</p>\n<pre><code class=\"language-language-javascript\">(function(){\n\n\tconsole.log(&quot;DOM loaded to the comment &gt;here&lt; and not yet fully parsed&quot;);\n\n})(); // &gt;here&lt;\n\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>This wont execute your code within until it reaches the bottom of its declaration (comment as &gt;here&lt;)</li>\n<li>DOM has only partially loaded to this point</li>\n<li>The code within executes asynchronously as the DOM continues to load</li>\n</ul>\n<h4 id=\"whywouldwedothis\">Why would we do this?</h4>\n<ul>\n<li>Easier to remember than all of the above and has no browser compatibility concerns as it is original native JavaScript.</li>\n<li>Lazy devs testing before production release will &quot;get lucky&quot; thinking that all their resources are loaded and available in here because they haven't needed to go through the internet to load them all.</li>\n<li>The only acceptable use of this technique is to do declarations asynchronously for later use, do not execute any code that rely on resources elsewhere.</li>\n</ul>\n<h2 id=\"jqueryanonymousfunctionakadeferred\">jQuery anonymous function aka Deferred</h2>\n<pre><code class=\"language-language-javascript\">$(function() {\n\n\tconsole.log(&quot;DOM loaded to the comment &gt;here&lt; and not yet fully parsed&quot;);\n   \n}); // &gt;here&lt;\n</code></pre>\n<h4 id=\"notes\">Notes:</h4>\n<ul>\n<li>Similar in appearance to Self executing anonymous functions but very different.</li>\n<li>This is an alternative to <code>document.readyState == &quot;interactive&quot;</code> so those notes apply.</li>\n<li>A great technique if you need to declare something that multiple <code>jQuery.ready</code> may use - but no other acceptable reason to use this.</li>\n</ul>\n<h2 id=\"lazyload\">Lazy load</h2>\n<p>Say you have an element on page that uses a date picker plugin, instead of loading in the resources during page load, learn the best time in your UX to load the content after the page has loaded.</p>\n<pre><code class=\"language-language-javascript\">$(document).on('mouseover', '.date-container',function(){\n\t$.getScript(&quot;ajax/test.js&quot;,function(){\n\n\t\tconsole.log( &quot;Load was performed. Init it if needed&quot; );\n        \n   });\n});\n</code></pre>\n<h4 id=\"benefits\">Benefits?</h4>\n<ul>\n<li>Less downloading, faster page speeds</li>\n<li>You're browser won't load in unnecessary scripts unless the user actually needs to use them.</li>\n<li>UX is unchanged from other usages but will be easier to debug and identify UX dependencies.</li>\n</ul>\n<blockquote>\n<h3 id=\"tldr\">TL;DR</h3>\n</blockquote>\n<ol>\n<li>\n<p>Avoid using the native Javascript implementations as they are harder to remember, too verbose and clumsy. Instead opt to use a library (jQuery not the only option here) which should simplify things and encourage best practice</p>\n</li>\n<li>\n<p>Do not execute code until the whole DOM has loaded and parsed, but feel free to declare as much as you like as these can be easily referenced later with shared code.</p>\n</li>\n<li>\n<p>Being fancy and using self-executing anonymous functions because they look cool and someone said they work like jQuery.ready is actually wrong and plain lazy. Use proven or library helpers because they work.</p>\n</li>\n</ol>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Lazy load everything you possibly can, this is not only the best in terms of page load speed and therefore mobile and SEO benefits, it is also logical and makes debugging UX extremely easy to follow.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"10","plaintext":"This is a quick post to list the methods of initializing your JavaScript the\nright way in terms of Page Load Speed and in turn SEO rankings improvements and\nfaster mobile UX.\n\nusing a jQuery helper\nWhat it looks like;\n\n$(document).ready(function(){\n\n\tconsole.log(\"DOM loaded but not fully parsed\");\n\n});\n\n\nNotes:\n * This jQuery helper is essentially an alias too document.readyState ==\n   \"interactive\"\n * All style, async=false script, and img src resources have been loaded.\n * The execution order of multiple uses of this helper is based on load order or\n   placement in the DOM\n * Any other synchronous JavaScript has executed\n\nwindow load method\nWhat it looks like;\n\n$(window).load(function() {\n                \n\tconsole.log(\"DOM and all frames are loaded and fully parsed\");\n\n});\n\n\nNotes:\n * This is an alternative to document.readyState == \"complete\"\n * All style, script, img src resources, and frames have been loaded, including\n   all of these inside the frames, also includes asynchronous scripts\n * This is also asynchronous\n\nNative browser states\nWhat it looks like;\n\ndocument.onreadystatechange = function () {\n  if (document.readyState == \"complete\") {\n\n\tconsole.log(\"DOM fully loaded and parsed\");\n\n  }\n}\n\n\nNotes:\n * This is an alternative to window.load()\n * DOM fully loaded and parsed - including, jQuery.ready\n * This is synchronous, using this method will lock your browser so don't do any\n   intensive tasks.\n\nAnother native browser state\ndocument.onreadystatechange = function () {\n  if (document.readyState == \"interactive\") {\n\n\tconsole.log(\"DOM loaded but not fully parsed\");\n\n  }\n}\n\n\nNotes:\n * This is an alternative to DOMContentLoaded\n * DOM fully loaded and not fully parsed - so, jQuery.ready has not yet\n   completed but may be asynchronously executing code\n * This is also synchronous\n\nNative browser event listener\ndocument.addEventListener(\"DOMContentLoaded\", function(event) {\n\t\n    console.log(\"DOM fully loaded and parsed\");\n    \n});\n\n\nNotes:\n * This is an alternative to document.readyState == \"interactive\" so those notes\n   apply\n * Bubbling for this event is supported by at least Gecko 1.9.2, Chrome 6, and\n   Safari 4\n * Earlier than Internet Explorer 8 requires the .doScroll() hack\n\nSelf-executing anonymous functions\nWhat it looks like;\n\n(function(){\n\n\tconsole.log(\"DOM loaded to the comment >here< and not yet fully parsed\");\n\n})(); // >here<\n\n\n\nNotes:\n * This wont execute your code within until it reaches the bottom of its\n   declaration (comment as >here<)\n * DOM has only partially loaded to this point\n * The code within executes asynchronously as the DOM continues to load\n\nWhy would we do this?\n * Easier to remember than all of the above and has no browser compatibility\n   concerns as it is original native JavaScript.\n * Lazy devs testing before production release will \"get lucky\" thinking that\n   all their resources are loaded and available in here because they haven't\n   needed to go through the internet to load them all.\n * The only acceptable use of this technique is to do declarations\n   asynchronously for later use, do not execute any code that rely on resources\n   elsewhere.\n\njQuery anonymous function aka Deferred\n$(function() {\n\n\tconsole.log(\"DOM loaded to the comment >here< and not yet fully parsed\");\n   \n}); // >here<\n\n\nNotes:\n * Similar in appearance to Self executing anonymous functions but very\n   different.\n * This is an alternative to document.readyState == \"interactive\" so those notes\n   apply.\n * A great technique if you need to declare something that multiple jQuery.ready \n   may use - but no other acceptable reason to use this.\n\nLazy load\nSay you have an element on page that uses a date picker plugin, instead of\nloading in the resources during page load, learn the best time in your UX to\nload the content after the page has loaded.\n\n$(document).on('mouseover', '.date-container',function(){\n\t$.getScript(\"ajax/test.js\",function(){\n\n\t\tconsole.log( \"Load was performed. Init it if needed\" );\n        \n   });\n});\n\n\nBenefits?\n * Less downloading, faster page speeds\n * You're browser won't load in unnecessary scripts unless the user actually\n   needs to use them.\n * UX is unchanged from other usages but will be easier to debug and identify UX\n   dependencies.\n\n> TL;DR\n\n 1. Avoid using the native Javascript implementations as they are harder to\n    remember, too verbose and clumsy. Instead opt to use a library (jQuery not\n    the only option here) which should simplify things and encourage best\n    practice\n    \n    \n 2. Do not execute code until the whole DOM has loaded and parsed, but feel free\n    to declare as much as you like as these can be easily referenced later with\n    shared code.\n    \n    \n 3. Being fancy and using self-executing anonymous functions because they look\n    cool and someone said they work like jQuery.ready is actually wrong and\n    plain lazy. Use proven or library helpers because they work.\n    \n    \n\nConclusion\nLazy load everything you possibly can, this is not only the best in terms of\npage load speed and therefore mobile and SEO benefits, it is also logical and\nmakes debugging UX extremely easy to follow.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-22.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-19 13:33:28","created_by":"1","updated_at":"2021-03-31 14:19:36","updated_by":"1","published_at":"2015-10-30 06:38:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc92","uuid":"d48e1ba8-af06-418c-9237-b85df4e53a25","title":"CSS Selector Performance","slug":"css-selector-performance","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Back in July 2014 I had the privilege to talk at the first MelbCSS meetup at 99Designs in front of a respectable crowd of front-end developers from beginners to the very experienced in the community.\\nAt the end of my talk there was a common theme in the questions, almost all came back to the disbelief on how browsers interpret your CSS rules.\\n\\n##### A common rule example\\n\\nConsider\\n\\n```language-javascript\\nsection#layout-rhs .module div {\\n  /* styles */\\n}\\n```\\n\\nI put a very similar CSS selector to the audience and asked for someone to guess what is the first interpretation of this rule, specifically, which elements in my DOM would be selected in the dataset of the browsers initial interpretation while parsing this rule.\\n\\nHere are some of the answers;\\n\\n* the child `dvi` element of element with id of `layout-rhs`\\n* just the element with id of `layout-rhs`\\n* All `section` elements\\n* element with id of `layout-rhs` but only if it is a `section` element\\n\\nUnfortunately I had the expectation that the second or third answer would be correct when i asked this question so i was astonished to get 4 incorrect answers.\\n\\nMy talk was never originally meant to educate on how the browser interprets the CSS selector, as to me that seemed the most basic fundamentals of CSS. I intended to simply engage the audience in my talk with a simple question.\\n\\nSo being confused by the answers I presented a slide with the following phrase.\\n\\n> All CSS rules are read _right_ to _left_\\n\\nOne person yelled out \\\"no way\\\", while another was giggling. A young woman in the front row had heard this before and as she stated it to the person beside I continued.\\n\\n##### What was the first elements selected?\\n\\n> All of the `div` elements in the entire DOM\\n\\nThat may surprise some of you even today.\\n\\n### Impact\\n\\nIf anyone has spent time looking at performance audits you scoff at me for pointing out a single rule like the one above, and you would be right to say that on its own the difference between that and an `#id` selector is trivial and measured in thousandths of a second - accumulate all of the CSS selectors of a decent modern app however and we are getting close to that 1 second difference just for not optimising your CSS selectors.\\n\\nKnowing that CSS selectors are read from RIGHT to LEFT we now consider;\\n\\n* The entire DOM is a factor.\\n* Complex rules add levels of computation.\\n* Unused CSS styles are evaluated.\\n* Anything to the left of an ID is irrelevant.\\n* Try to remove anything on the left of a good performing selector.\\n* Clever rules targeting something specific may not have been very smart after all.\\n* Yes, jQuery is also a consideration.\\n\\njQuery has it's own CSS selector subset, but it still interprets right to left. Native JavaScript CSS selectors are the significantly faster `document.querySelector` and `document.querySelectorAll` which leverages the browser CSS engine directly and therefore is subject to the rules set out for your CSS styles.\\n\\n### Preprocessors\\n\\nI'm an everyday user of LESS these days, and i maintain one app using SCSS and have ventured into Stylus a few years back.\\n\\nI am personally pro preprocessors. but they often work against you without you being the wiser.\\n\\nPreprocessors are great productivity tools.\\n\\nConsider LESS: \\n\\n```language-javascript\\nbody {\\n  table {\\n    &.class1 { \\n      tr td {\\n        /* cell styles */\\n      }\\n    } \\n  }\\n  .class2 {\\n    /* class styles */\\n    &.active {\\n      /* conditional styles */\\n    }\\n  }\\n} \\n```\\n\\nCompiled:\\n\\n```language-javascript\\nbody { }\\nbody table.class1 { }\\nbody table.class1 tr { }\\nbody table.class1 tr td { }\\nbody .class2 { }\\nbody .class2.active { }\\n```\\n\\nPreprocessors can offer great structure, and maintainability. But what is wrong here in terms of performance?\\n\\n* Overly complex rules for the browser interpretation\\n* Multiple levels of computation due to the encapsulated classes\\n* Has simple vanilla CSS implementation\\n\\n### Order of efficiency\\n\\nSelectors efficiency may not suite all use cases. Don't eliminate using pseudo selector in fear just optimise their implementation.\\n\\n* ID - example: #unique\\n* Class - example: .reusable\\n* Tags - example: tr\\n* Adjacent sibling - example: tr + th\\n* Child - example: div > p\\n* Descendant - example: div a\\n* Attribute - example: [type=\\\"email\\\"]\\n* Pseudo - example: :focus\\n\\n> Please don't go crazy and style ID's everywhere\\n\\n### Naming Conventions\\n\\nThe awesome front-end developer [Hannah Thompson](https://www.linkedin.com/in/hannahthompsonnz) at [Punters.com.au](https://www.punters.com.au/) whom I work with was responcible for implementing [BEM](http://getbem.com/introduction/) to our already very mature code base so that we could focus more on our components without the confusion of CSS and concern about performance.\\n\\nI encourage you to use BEM, it will change the way you look back at CSS when debugging when you realise how simple it has become.\\n\\n### Conclusion\\n\\nWe've learned that browser read from right to left, and that nesting your preprocessor css is generally not a good idea so keep it 2 levels deep at most. \\n\\nID selectors are best, and close behind class selectors are your optimal choice and work in all naming conventions.\\n\\n##### Enjoyed this?\\n\\nIf you made it here I would love to hear what you thought of the article.\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Back in July 2014 I had the privilege to talk at the first MelbCSS meetup at 99Designs in front of a respectable crowd of front-end developers from beginners to the very experienced in the community.<br>\nAt the end of my talk there was a common theme in the questions, almost all came back to the disbelief on how browsers interpret your CSS rules.</p>\n<h5 id=\"acommonruleexample\">A common rule example</h5>\n<p>Consider</p>\n<pre><code class=\"language-language-javascript\">section#layout-rhs .module div {\n  /* styles */\n}\n</code></pre>\n<p>I put a very similar CSS selector to the audience and asked for someone to guess what is the first interpretation of this rule, specifically, which elements in my DOM would be selected in the dataset of the browsers initial interpretation while parsing this rule.</p>\n<p>Here are some of the answers;</p>\n<ul>\n<li>the child <code>dvi</code> element of element with id of <code>layout-rhs</code></li>\n<li>just the element with id of <code>layout-rhs</code></li>\n<li>All <code>section</code> elements</li>\n<li>element with id of <code>layout-rhs</code> but only if it is a <code>section</code> element</li>\n</ul>\n<p>Unfortunately I had the expectation that the second or third answer would be correct when i asked this question so i was astonished to get 4 incorrect answers.</p>\n<p>My talk was never originally meant to educate on how the browser interprets the CSS selector, as to me that seemed the most basic fundamentals of CSS. I intended to simply engage the audience in my talk with a simple question.</p>\n<p>So being confused by the answers I presented a slide with the following phrase.</p>\n<blockquote>\n<p>All CSS rules are read <em>right</em> to <em>left</em></p>\n</blockquote>\n<p>One person yelled out &quot;no way&quot;, while another was giggling. A young woman in the front row had heard this before and as she stated it to the person beside I continued.</p>\n<h5 id=\"whatwasthefirstelementsselected\">What was the first elements selected?</h5>\n<blockquote>\n<p>All of the <code>div</code> elements in the entire DOM</p>\n</blockquote>\n<p>That may surprise some of you even today.</p>\n<h3 id=\"impact\">Impact</h3>\n<p>If anyone has spent time looking at performance audits you scoff at me for pointing out a single rule like the one above, and you would be right to say that on its own the difference between that and an <code>#id</code> selector is trivial and measured in thousandths of a second - accumulate all of the CSS selectors of a decent modern app however and we are getting close to that 1 second difference just for not optimising your CSS selectors.</p>\n<p>Knowing that CSS selectors are read from RIGHT to LEFT we now consider;</p>\n<ul>\n<li>The entire DOM is a factor.</li>\n<li>Complex rules add levels of computation.</li>\n<li>Unused CSS styles are evaluated.</li>\n<li>Anything to the left of an ID is irrelevant.</li>\n<li>Try to remove anything on the left of a good performing selector.</li>\n<li>Clever rules targeting something specific may not have been very smart after all.</li>\n<li>Yes, jQuery is also a consideration.</li>\n</ul>\n<p>jQuery has it's own CSS selector subset, but it still interprets right to left. Native JavaScript CSS selectors are the significantly faster <code>document.querySelector</code> and <code>document.querySelectorAll</code> which leverages the browser CSS engine directly and therefore is subject to the rules set out for your CSS styles.</p>\n<h3 id=\"preprocessors\">Preprocessors</h3>\n<p>I'm an everyday user of LESS these days, and i maintain one app using SCSS and have ventured into Stylus a few years back.</p>\n<p>I am personally pro preprocessors. but they often work against you without you being the wiser.</p>\n<p>Preprocessors are great productivity tools.</p>\n<p>Consider LESS:</p>\n<pre><code class=\"language-language-javascript\">body {\n  table {\n    &amp;.class1 { \n      tr td {\n        /* cell styles */\n      }\n    } \n  }\n  .class2 {\n    /* class styles */\n    &amp;.active {\n      /* conditional styles */\n    }\n  }\n} \n</code></pre>\n<p>Compiled:</p>\n<pre><code class=\"language-language-javascript\">body { }\nbody table.class1 { }\nbody table.class1 tr { }\nbody table.class1 tr td { }\nbody .class2 { }\nbody .class2.active { }\n</code></pre>\n<p>Preprocessors can offer great structure, and maintainability. But what is wrong here in terms of performance?</p>\n<ul>\n<li>Overly complex rules for the browser interpretation</li>\n<li>Multiple levels of computation due to the encapsulated classes</li>\n<li>Has simple vanilla CSS implementation</li>\n</ul>\n<h3 id=\"orderofefficiency\">Order of efficiency</h3>\n<p>Selectors efficiency may not suite all use cases. Don't eliminate using pseudo selector in fear just optimise their implementation.</p>\n<ul>\n<li>ID - example: #unique</li>\n<li>Class - example: .reusable</li>\n<li>Tags - example: tr</li>\n<li>Adjacent sibling - example: tr + th</li>\n<li>Child - example: div &gt; p</li>\n<li>Descendant - example: div a</li>\n<li>Attribute - example: [type=&quot;email&quot;]</li>\n<li>Pseudo - example: :focus</li>\n</ul>\n<blockquote>\n<p>Please don't go crazy and style ID's everywhere</p>\n</blockquote>\n<h3 id=\"namingconventions\">Naming Conventions</h3>\n<p>The awesome front-end developer <a href=\"https://www.linkedin.com/in/hannahthompsonnz\">Hannah Thompson</a> at <a href=\"https://www.punters.com.au/\">Punters.com.au</a> whom I work with was responcible for implementing <a href=\"http://getbem.com/introduction/\">BEM</a> to our already very mature code base so that we could focus more on our components without the confusion of CSS and concern about performance.</p>\n<p>I encourage you to use BEM, it will change the way you look back at CSS when debugging when you realise how simple it has become.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>We've learned that browser read from right to left, and that nesting your preprocessor css is generally not a good idea so keep it 2 levels deep at most.</p>\n<p>ID selectors are best, and close behind class selectors are your optimal choice and work in all naming conventions.</p>\n<h5 id=\"enjoyedthis\">Enjoyed this?</h5>\n<p>If you made it here I would love to hear what you thought of the article.</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"11","plaintext":"Back in July 2014 I had the privilege to talk at the first MelbCSS meetup at\n99Designs in front of a respectable crowd of front-end developers from beginners\nto the very experienced in the community.\nAt the end of my talk there was a common theme in the questions, almost all came\nback to the disbelief on how browsers interpret your CSS rules.\n\nA common rule example\nConsider\n\nsection#layout-rhs .module div {\n  /* styles */\n}\n\n\nI put a very similar CSS selector to the audience and asked for someone to guess\nwhat is the first interpretation of this rule, specifically, which elements in\nmy DOM would be selected in the dataset of the browsers initial interpretation\nwhile parsing this rule.\n\nHere are some of the answers;\n\n * the child dvi element of element with id of layout-rhs\n * just the element with id of layout-rhs\n * All section elements\n * element with id of layout-rhs but only if it is a section element\n\nUnfortunately I had the expectation that the second or third answer would be\ncorrect when i asked this question so i was astonished to get 4 incorrect\nanswers.\n\nMy talk was never originally meant to educate on how the browser interprets the\nCSS selector, as to me that seemed the most basic fundamentals of CSS. I\nintended to simply engage the audience in my talk with a simple question.\n\nSo being confused by the answers I presented a slide with the following phrase.\n\n> All CSS rules are read right to left\n\n\nOne person yelled out \"no way\", while another was giggling. A young woman in the\nfront row had heard this before and as she stated it to the person beside I\ncontinued.\n\nWhat was the first elements selected?\n> All of the div elements in the entire DOM\n\n\nThat may surprise some of you even today.\n\nImpact\nIf anyone has spent time looking at performance audits you scoff at me for\npointing out a single rule like the one above, and you would be right to say\nthat on its own the difference between that and an #id selector is trivial and\nmeasured in thousandths of a second - accumulate all of the CSS selectors of a\ndecent modern app however and we are getting close to that 1 second difference\njust for not optimising your CSS selectors.\n\nKnowing that CSS selectors are read from RIGHT to LEFT we now consider;\n\n * The entire DOM is a factor.\n * Complex rules add levels of computation.\n * Unused CSS styles are evaluated.\n * Anything to the left of an ID is irrelevant.\n * Try to remove anything on the left of a good performing selector.\n * Clever rules targeting something specific may not have been very smart after\n   all.\n * Yes, jQuery is also a consideration.\n\njQuery has it's own CSS selector subset, but it still interprets right to left.\nNative JavaScript CSS selectors are the significantly faster \ndocument.querySelector and document.querySelectorAll which leverages the browser\nCSS engine directly and therefore is subject to the rules set out for your CSS\nstyles.\n\nPreprocessors\nI'm an everyday user of LESS these days, and i maintain one app using SCSS and\nhave ventured into Stylus a few years back.\n\nI am personally pro preprocessors. but they often work against you without you\nbeing the wiser.\n\nPreprocessors are great productivity tools.\n\nConsider LESS:\n\nbody {\n  table {\n    &.class1 { \n      tr td {\n        /* cell styles */\n      }\n    } \n  }\n  .class2 {\n    /* class styles */\n    &.active {\n      /* conditional styles */\n    }\n  }\n} \n\n\nCompiled:\n\nbody { }\nbody table.class1 { }\nbody table.class1 tr { }\nbody table.class1 tr td { }\nbody .class2 { }\nbody .class2.active { }\n\n\nPreprocessors can offer great structure, and maintainability. But what is wrong\nhere in terms of performance?\n\n * Overly complex rules for the browser interpretation\n * Multiple levels of computation due to the encapsulated classes\n * Has simple vanilla CSS implementation\n\nOrder of efficiency\nSelectors efficiency may not suite all use cases. Don't eliminate using pseudo\nselector in fear just optimise their implementation.\n\n * ID - example: #unique\n * Class - example: .reusable\n * Tags - example: tr\n * Adjacent sibling - example: tr + th\n * Child - example: div > p\n * Descendant - example: div a\n * Attribute - example: [type=\"email\"]\n * Pseudo - example: :focus\n\n> Please don't go crazy and style ID's everywhere\n\n\nNaming Conventions\nThe awesome front-end developer Hannah Thompson\n[https://www.linkedin.com/in/hannahthompsonnz] at Punters.com.au\n[https://www.punters.com.au/] whom I work with was responcible for implementing \nBEM [http://getbem.com/introduction/] to our already very mature code base so\nthat we could focus more on our components without the confusion of CSS and\nconcern about performance.\n\nI encourage you to use BEM, it will change the way you look back at CSS when\ndebugging when you realise how simple it has become.\n\nConclusion\nWe've learned that browser read from right to left, and that nesting your\npreprocessor css is generally not a good idea so keep it 2 levels deep at most.\n\nID selectors are best, and close behind class selectors are your optimal choice\nand work in all naming conventions.\n\nEnjoyed this?\nIf you made it here I would love to hear what you thought of the article.\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-23.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-20 06:57:14","created_by":"1","updated_at":"2021-03-31 14:18:53","updated_by":"1","published_at":"2016-03-20 08:12:32","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc93","uuid":"b6e7c7ef-ba77-493d-9dee-a01430e1d46e","title":"Preventing offline data loss with Web Workers","slug":"preventing-offline-data-loss-with-web-workers","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"### The story\\n\\nYour carefully crafted web app has gone offline and now the code kicks in to manage the data in the browser while the user is offline waiting for them to get back in range.\\n\\nTime goes on and the user had already noticed a performance hit initially when they lost internet but now data is accumulating and the app quickly becomes a mess of UI lock up periods and choppy scrolling.\\n\\nIt's time to explore multi-threading your JavaScript with Web Workers.\\n\\n#### Why I'm writing this\\n\\nWhether I am working on a fun canvas game, a simple content app (like a to do list for example) I like to introduce a Web Worker at the earliest stages to take control of anything that the main UI thread doesn't need to do. This type of thinking is natural to a native App developer but an every JavaScript developer or one focussed on a backend language like Ruby, Java, .NET, or PHP doesn't want to acknowledge the browser has multiple threads available. It's been 10 years already, get your act together peeps.. It's embarrassing.. \\nI've interviewed and worked with many developers do not seem to acknowledge or care about this type of code separation, and they've all never used the App they've developed as a user themselves, right? I can assure you if they had (and they likely have) they would feel like they could never brag about the website to their friends or next employer.\\n\\n> Hey, web and front-end JavaScript developers - be a user for a day, see the UI lock up and USE Web Workers already\\n\\nTake some pride in your own work, don't just be proud of your clever syntax arrangements - that's not art! It's plain ignorant vanity.\\n\\n#### Haven't heard of `Web Workers` yet?\\n\\nI hope you've just started school or decided to dig into some JavaScript this year after learning some HTML and CSS because all browser vendors had fully implemented Web Workers the way they work today back in 2008. And they were available up to 2 years before that in some browsers.\\n\\nIt is 2016 and some of the more advanced among you have heard about the fantastic advancements in offline capabilities using Service Workers, and may have skimmed on the Web Workers (shame on you) or just found it plain confusing. Well It's quite simple what the differences are.\\n\\n> A `Service Worker` is not a `Web Worker`.\\n\\n**A service worker** is a thread apart from the main window, you know the part with the DOM, every browser tab and window operates it's own thread with each DOM. A service worker has been exposed to you so that you may access all the network calls coming out of your main thread with your DOM. Thing about an `<img` tag with an `src` attribute. The service worker implements the ability for you as a developer to _catch_ the network attempt that would `fetch` the image from the location in the `src` and do something, like cache the image so you only need to download it once, or, reply back to the browser with a base64 encoded image if the device has no internet connection awaiting the moment internet is available. Or you could real clever and prevent unauthorised access to resources before they even start to contact the server in the first place.\\n\\n**A Web Worker** Is much simpler, though also a thread apart from the main browser window or tab, and able to make _AJAX_ like network requests, it doesn't offer a developer any deeper access to the usual networking or caching features of a browser. It's uses are more varied, to the point, you use web workers to do all the heavy lifting on the client side so that the processing of such things do not affect the UI or UX in terms of animations, scrolling, drag'n'drop, ect. By setting up your process intensive logic via a web worker you ensure that the UI will feel as snappy and act as fluid as though you were not running any JavaScript at all. \\nA very important consideration on netbooks, tablet, low end mobile phones, and a massive host of users around the world visiting your site on slow internet (_cough_ Australia..)\\n\\n#### Web Worker by example\\n\\nLet's create a way to autosave some page HTML as content along with it's arbitrary data such as name and category for example.\\n\\nThis is a fairly straightforward implementation;\\n\\n**index.html**\\n\\n```language-html\\n<html>\\n  <head>\\n    <!-- basics -->\\n  </head>\\n  <body>\\n    <select class=\\\"autosave\\\">\\n      <option disabled>- Saved Items -</option>\\n    </select>\\n  </body>\\n</html>\\n```\\n\\n**autosave.js**\\n\\n```language-javascript\\nif (!!window.Worker) {\\n  Autosave = function () {\\n    var that = this;\\n    this.Worker = new Worker('/autosave.worker.js');\\n    this.Worker.addEventListener('message', function (e) {\\n      if (e.data === 'Database initialised.') {\\n        console.log(e.data);\\n      }\\n      if (e.data.action === 'add') {\\n        that.addItem((new Date(e.data.result.created)).toLocaleTimeString() + ' [' + e.data.result.category + '] ' + e.data.result.name, e.data.result.id);\\n      }\\n      if (e.data.action === 'list') {\\n        e.data.result.forEach(function(v){\\n          that.addItem((new Date(v.created)).toLocaleTimeString() + ' [' + v.category + '] ' + v.name, v.id);\\n        });\\n      }\\n    });\\n    this.Worker.postMessage({});\\n  };\\n  Autosave.prototype.save = function (category, name, content) {\\n    this.Worker.postMessage({\\n      action: 'add', params: {\\n        category: category,\\n        name: name,\\n        created: +new Date(),\\n        content: content\\n      }\\n    });\\n  };\\n  Autosave.prototype.list = function () {\\n    this.Worker.postMessage({action: 'list'});\\n  };\\n  Autosave.prototype.get = function (id) {\\n    this.Worker.postMessage({action: 'get', params: {id: parseInt(id)}});\\n  };\\n  Autosave.prototype.addItem = function (label, value) {\\n    //example adding a new option to the top of a select in the DOM\\n    $('select.autosave').children().eq(0).after(new Option(label, value));\\n  };\\n}\\n```\\n\\n**autosave.worker.js**\\n\\n```language-javascript\\n(function () {\\n  \\\"use strict\\\";\\n  var dbName    = 'autosave',\\n      dbVersion = 1;\\n\\n  importScripts('store.js');\\n  self.Store = new Store(self, dbName, dbVersion);\\n\\n  self.onmessage = function (e) {\\n    var data     = e.data,\\n        action   = data.action,\\n        params   = data.params || {};\\n\\n    if (action && self.Store[action]) {\\n      self.Store[action].call(self.Store, params, function (result) {\\n        self.postMessage({action: action, params: params, result: result});\\n      });\\n    }\\n  };\\n})();\\n```\\n\\n**store.js**\\n\\n```language-javascript\\nStore = function (worker, name, version) {\\n  var that = this;\\n  this.name = name;\\n  this.version = version || 1;\\n  // Let us open our database\\n  var DBOpenRequest = worker.indexedDB.open(name, version);\\n  // these two event handlers act on the database being opened successfully, or not\\n  DBOpenRequest.onerror = function (e) {\\n    worker.postMessage('Error loading database.');\\n  };\\n  DBOpenRequest.onsuccess = function (e) {\\n    worker.postMessage('Database initialised.');\\n    // store the result of opening the database in the instance variable. This is used a lot below\\n    that.instance = DBOpenRequest.result;\\n  };\\n  // This event handles the event whereby a new version of the database needs to be created\\n  // Either one has not been created before, or a new version number has been submitted via the\\n  // indexedDB.open line above\\n  //it is only implemented in recent browsers\\n  DBOpenRequest.onupgradeneeded = function (e) {\\n    var instance = e.target.result;\\n    instance.onerror = function (e) {\\n      worker.postMessage('Error loading database.');\\n    };\\n    // Create an objectStore for this database\\n    var objectStore = instance.createObjectStore(name, {keyPath: 'id', autoIncrement: true});\\n    // define what data items the objectStore will contain\\n    objectStore.createIndex(\\\"name\\\", \\\"name\\\", {unique: false});\\n    objectStore.createIndex(\\\"category\\\", \\\"category\\\", {unique: false});\\n    objectStore.createIndex(\\\"created\\\", \\\"created\\\", {unique: false});\\n    objectStore.createIndex(\\\"content\\\", \\\"content\\\", {unique: false});\\n\\n    worker.postMessage('Object store created.');\\n  };\\n  return this;\\n};\\nStore.prototype = {\\n  add: function (params, callback) {\\n    var that = this,\\n        newItem = {};\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function () {\\n      console.log('Transaction completed: database add finished.');\\n      typeof callback === 'function' && callback.call(params, params);\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    var objectStore = transaction.objectStore(that.name);\\n    // add our newItem object to the object store\\n    var objectStoreRequest = objectStore.add(params);\\n    objectStoreRequest.onsuccess = function (e) {\\n      // report the success of our new item going into the database\\n      params.id = e.target.result;\\n    };\\n  },\\n  get: function (params, callback) {\\n    var that = this,\\n        id = params.id,\\n        result;\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function () {\\n      console.log('Transaction completed: database get finished.');\\n      typeof callback === 'function' && callback.call(params, result);\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    var objectStore = transaction.objectStore(that.name);\\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\\n    objectStore.openCursor().onsuccess = function (e) {\\n      var cursor = e.target.result;\\n      // if there is still another cursor to go, keep runing this code\\n      if (cursor) {\\n        // if there are no more cursor items to iterate through, say so, and exit the function\\n        if (cursor.value.id === id) {\\n          result = cursor.value;\\n          return;\\n        } else {\\n          // continue on to the next item in the cursor\\n          cursor.continue();\\n        }\\n      }\\n    };\\n  },\\n  truncate: function (params, callback) {\\n    var that = this;\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function () {\\n      console.log('Transaction completed: database truncate finished.');\\n      typeof callback === 'function' && callback.call(params);\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    transaction.objectStore(that.name).clear();\\n  },\\n  update: function (params, callback) {\\n    var that = this;\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function () {\\n      console.log('Transaction completed: database update finished.');\\n      typeof callback === 'function' && callback.call(params, params);\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    transaction.objectStore(that.name).put(params);\\n  },\\n  delete: function (params, callback) {\\n    var that = this,\\n        id = params.id;\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function() {\\n      console.log('Transaction completed: database delete finished.');\\n      typeof callback === 'function' && callback.call(params);\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    transaction.objectStore(that.name).delete(id);\\n  },\\n  list: function (params, callback) {\\n    var that  = this,\\n        items = [];\\n    // open a read/write db transaction, ready for adding the data\\n    var transaction = that.instance.transaction([that.name], \\\"readwrite\\\");\\n    // report on the success of opening the transaction\\n    transaction.oncomplete = function () {\\n      console.log('Transaction completed: database list finished.');\\n    };\\n    transaction.onerror = function () {\\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\\n    };\\n    // call an object store that's already been added to the database\\n    var objectStore = transaction.objectStore(that.name);\\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\\n    objectStore.openCursor().onsuccess = function (e) {\\n      var cursor = e.target.result;\\n      // if there is still another cursor to go, keep runing this code\\n      if (cursor) {\\n        items.push(cursor.value);\\n        // continue on to the next item in the cursor\\n        cursor.continue();\\n        // if there are no more cursor items to iterate through, say so, and exit the function\\n      } else {\\n        typeof callback === 'function' && callback.call(params, items);\\n      }\\n    };\\n  }\\n};\\n```\\n\\nAnd there you have it, an autosave implementation using web workers.\\n\\n\\nI hope this article helps you and saves you time - please spread the knowledge!\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"thestory\">The story</h3>\n<p>Your carefully crafted web app has gone offline and now the code kicks in to manage the data in the browser while the user is offline waiting for them to get back in range.</p>\n<p>Time goes on and the user had already noticed a performance hit initially when they lost internet but now data is accumulating and the app quickly becomes a mess of UI lock up periods and choppy scrolling.</p>\n<p>It's time to explore multi-threading your JavaScript with Web Workers.</p>\n<h4 id=\"whyimwritingthis\">Why I'm writing this</h4>\n<p>Whether I am working on a fun canvas game, a simple content app (like a to do list for example) I like to introduce a Web Worker at the earliest stages to take control of anything that the main UI thread doesn't need to do. This type of thinking is natural to a native App developer but an every JavaScript developer or one focussed on a backend language like Ruby, Java, .NET, or PHP doesn't want to acknowledge the browser has multiple threads available. It's been 10 years already, get your act together peeps.. It's embarrassing..<br>\nI've interviewed and worked with many developers do not seem to acknowledge or care about this type of code separation, and they've all never used the App they've developed as a user themselves, right? I can assure you if they had (and they likely have) they would feel like they could never brag about the website to their friends or next employer.</p>\n<blockquote>\n<p>Hey, web and front-end JavaScript developers - be a user for a day, see the UI lock up and USE Web Workers already</p>\n</blockquote>\n<p>Take some pride in your own work, don't just be proud of your clever syntax arrangements - that's not art! It's plain ignorant vanity.</p>\n<h4 id=\"haventheardofwebworkersyet\">Haven't heard of <code>Web Workers</code> yet?</h4>\n<p>I hope you've just started school or decided to dig into some JavaScript this year after learning some HTML and CSS because all browser vendors had fully implemented Web Workers the way they work today back in 2008. And they were available up to 2 years before that in some browsers.</p>\n<p>It is 2016 and some of the more advanced among you have heard about the fantastic advancements in offline capabilities using Service Workers, and may have skimmed on the Web Workers (shame on you) or just found it plain confusing. Well It's quite simple what the differences are.</p>\n<blockquote>\n<p>A <code>Service Worker</code> is not a <code>Web Worker</code>.</p>\n</blockquote>\n<p><strong>A service worker</strong> is a thread apart from the main window, you know the part with the DOM, every browser tab and window operates it's own thread with each DOM. A service worker has been exposed to you so that you may access all the network calls coming out of your main thread with your DOM. Thing about an <code>&lt;img</code> tag with an <code>src</code> attribute. The service worker implements the ability for you as a developer to <em>catch</em> the network attempt that would <code>fetch</code> the image from the location in the <code>src</code> and do something, like cache the image so you only need to download it once, or, reply back to the browser with a base64 encoded image if the device has no internet connection awaiting the moment internet is available. Or you could real clever and prevent unauthorised access to resources before they even start to contact the server in the first place.</p>\n<p><strong>A Web Worker</strong> Is much simpler, though also a thread apart from the main browser window or tab, and able to make <em>AJAX</em> like network requests, it doesn't offer a developer any deeper access to the usual networking or caching features of a browser. It's uses are more varied, to the point, you use web workers to do all the heavy lifting on the client side so that the processing of such things do not affect the UI or UX in terms of animations, scrolling, drag'n'drop, ect. By setting up your process intensive logic via a web worker you ensure that the UI will feel as snappy and act as fluid as though you were not running any JavaScript at all.<br>\nA very important consideration on netbooks, tablet, low end mobile phones, and a massive host of users around the world visiting your site on slow internet (<em>cough</em> Australia..)</p>\n<h4 id=\"webworkerbyexample\">Web Worker by example</h4>\n<p>Let's create a way to autosave some page HTML as content along with it's arbitrary data such as name and category for example.</p>\n<p>This is a fairly straightforward implementation;</p>\n<p><strong>index.html</strong></p>\n<pre><code class=\"language-language-html\">&lt;html&gt;\n  &lt;head&gt;\n    &lt;!-- basics --&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;select class=&quot;autosave&quot;&gt;\n      &lt;option disabled&gt;- Saved Items -&lt;/option&gt;\n    &lt;/select&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p><strong>autosave.js</strong></p>\n<pre><code class=\"language-language-javascript\">if (!!window.Worker) {\n  Autosave = function () {\n    var that = this;\n    this.Worker = new Worker('/autosave.worker.js');\n    this.Worker.addEventListener('message', function (e) {\n      if (e.data === 'Database initialised.') {\n        console.log(e.data);\n      }\n      if (e.data.action === 'add') {\n        that.addItem((new Date(e.data.result.created)).toLocaleTimeString() + ' [' + e.data.result.category + '] ' + e.data.result.name, e.data.result.id);\n      }\n      if (e.data.action === 'list') {\n        e.data.result.forEach(function(v){\n          that.addItem((new Date(v.created)).toLocaleTimeString() + ' [' + v.category + '] ' + v.name, v.id);\n        });\n      }\n    });\n    this.Worker.postMessage({});\n  };\n  Autosave.prototype.save = function (category, name, content) {\n    this.Worker.postMessage({\n      action: 'add', params: {\n        category: category,\n        name: name,\n        created: +new Date(),\n        content: content\n      }\n    });\n  };\n  Autosave.prototype.list = function () {\n    this.Worker.postMessage({action: 'list'});\n  };\n  Autosave.prototype.get = function (id) {\n    this.Worker.postMessage({action: 'get', params: {id: parseInt(id)}});\n  };\n  Autosave.prototype.addItem = function (label, value) {\n    //example adding a new option to the top of a select in the DOM\n    $('select.autosave').children().eq(0).after(new Option(label, value));\n  };\n}\n</code></pre>\n<p><strong>autosave.worker.js</strong></p>\n<pre><code class=\"language-language-javascript\">(function () {\n  &quot;use strict&quot;;\n  var dbName    = 'autosave',\n      dbVersion = 1;\n\n  importScripts('store.js');\n  self.Store = new Store(self, dbName, dbVersion);\n\n  self.onmessage = function (e) {\n    var data     = e.data,\n        action   = data.action,\n        params   = data.params || {};\n\n    if (action &amp;&amp; self.Store[action]) {\n      self.Store[action].call(self.Store, params, function (result) {\n        self.postMessage({action: action, params: params, result: result});\n      });\n    }\n  };\n})();\n</code></pre>\n<p><strong>store.js</strong></p>\n<pre><code class=\"language-language-javascript\">Store = function (worker, name, version) {\n  var that = this;\n  this.name = name;\n  this.version = version || 1;\n  // Let us open our database\n  var DBOpenRequest = worker.indexedDB.open(name, version);\n  // these two event handlers act on the database being opened successfully, or not\n  DBOpenRequest.onerror = function (e) {\n    worker.postMessage('Error loading database.');\n  };\n  DBOpenRequest.onsuccess = function (e) {\n    worker.postMessage('Database initialised.');\n    // store the result of opening the database in the instance variable. This is used a lot below\n    that.instance = DBOpenRequest.result;\n  };\n  // This event handles the event whereby a new version of the database needs to be created\n  // Either one has not been created before, or a new version number has been submitted via the\n  // indexedDB.open line above\n  //it is only implemented in recent browsers\n  DBOpenRequest.onupgradeneeded = function (e) {\n    var instance = e.target.result;\n    instance.onerror = function (e) {\n      worker.postMessage('Error loading database.');\n    };\n    // Create an objectStore for this database\n    var objectStore = instance.createObjectStore(name, {keyPath: 'id', autoIncrement: true});\n    // define what data items the objectStore will contain\n    objectStore.createIndex(&quot;name&quot;, &quot;name&quot;, {unique: false});\n    objectStore.createIndex(&quot;category&quot;, &quot;category&quot;, {unique: false});\n    objectStore.createIndex(&quot;created&quot;, &quot;created&quot;, {unique: false});\n    objectStore.createIndex(&quot;content&quot;, &quot;content&quot;, {unique: false});\n\n    worker.postMessage('Object store created.');\n  };\n  return this;\n};\nStore.prototype = {\n  add: function (params, callback) {\n    var that = this,\n        newItem = {};\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database add finished.');\n      typeof callback === 'function' &amp;&amp; callback.call(params, params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // add our newItem object to the object store\n    var objectStoreRequest = objectStore.add(params);\n    objectStoreRequest.onsuccess = function (e) {\n      // report the success of our new item going into the database\n      params.id = e.target.result;\n    };\n  },\n  get: function (params, callback) {\n    var that = this,\n        id = params.id,\n        result;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database get finished.');\n      typeof callback === 'function' &amp;&amp; callback.call(params, result);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\n    objectStore.openCursor().onsuccess = function (e) {\n      var cursor = e.target.result;\n      // if there is still another cursor to go, keep runing this code\n      if (cursor) {\n        // if there are no more cursor items to iterate through, say so, and exit the function\n        if (cursor.value.id === id) {\n          result = cursor.value;\n          return;\n        } else {\n          // continue on to the next item in the cursor\n          cursor.continue();\n        }\n      }\n    };\n  },\n  truncate: function (params, callback) {\n    var that = this;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database truncate finished.');\n      typeof callback === 'function' &amp;&amp; callback.call(params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).clear();\n  },\n  update: function (params, callback) {\n    var that = this;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database update finished.');\n      typeof callback === 'function' &amp;&amp; callback.call(params, params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).put(params);\n  },\n  delete: function (params, callback) {\n    var that = this,\n        id = params.id;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function() {\n      console.log('Transaction completed: database delete finished.');\n      typeof callback === 'function' &amp;&amp; callback.call(params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).delete(id);\n  },\n  list: function (params, callback) {\n    var that  = this,\n        items = [];\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], &quot;readwrite&quot;);\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database list finished.');\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\n    objectStore.openCursor().onsuccess = function (e) {\n      var cursor = e.target.result;\n      // if there is still another cursor to go, keep runing this code\n      if (cursor) {\n        items.push(cursor.value);\n        // continue on to the next item in the cursor\n        cursor.continue();\n        // if there are no more cursor items to iterate through, say so, and exit the function\n      } else {\n        typeof callback === 'function' &amp;&amp; callback.call(params, items);\n      }\n    };\n  }\n};\n</code></pre>\n<p>And there you have it, an autosave implementation using web workers.</p>\n<p>I hope this article helps you and saves you time - please spread the knowledge!</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"17","plaintext":"The story\nYour carefully crafted web app has gone offline and now the code kicks in to\nmanage the data in the browser while the user is offline waiting for them to get\nback in range.\n\nTime goes on and the user had already noticed a performance hit initially when\nthey lost internet but now data is accumulating and the app quickly becomes a\nmess of UI lock up periods and choppy scrolling.\n\nIt's time to explore multi-threading your JavaScript with Web Workers.\n\nWhy I'm writing this\nWhether I am working on a fun canvas game, a simple content app (like a to do\nlist for example) I like to introduce a Web Worker at the earliest stages to\ntake control of anything that the main UI thread doesn't need to do. This type\nof thinking is natural to a native App developer but an every JavaScript\ndeveloper or one focussed on a backend language like Ruby, Java, .NET, or PHP\ndoesn't want to acknowledge the browser has multiple threads available. It's\nbeen 10 years already, get your act together peeps.. It's embarrassing..\nI've interviewed and worked with many developers do not seem to acknowledge or\ncare about this type of code separation, and they've all never used the App\nthey've developed as a user themselves, right? I can assure you if they had (and\nthey likely have) they would feel like they could never brag about the website\nto their friends or next employer.\n\n> Hey, web and front-end JavaScript developers - be a user for a day, see the UI\nlock up and USE Web Workers already\n\n\nTake some pride in your own work, don't just be proud of your clever syntax\narrangements - that's not art! It's plain ignorant vanity.\n\nHaven't heard of Web Workers yet?\nI hope you've just started school or decided to dig into some JavaScript this\nyear after learning some HTML and CSS because all browser vendors had fully\nimplemented Web Workers the way they work today back in 2008. And they were\navailable up to 2 years before that in some browsers.\n\nIt is 2016 and some of the more advanced among you have heard about the\nfantastic advancements in offline capabilities using Service Workers, and may\nhave skimmed on the Web Workers (shame on you) or just found it plain confusing.\nWell It's quite simple what the differences are.\n\n> A Service Worker is not a Web Worker.\n\n\nA service worker is a thread apart from the main window, you know the part with\nthe DOM, every browser tab and window operates it's own thread with each DOM. A\nservice worker has been exposed to you so that you may access all the network\ncalls coming out of your main thread with your DOM. Thing about an <img tag with\nan src attribute. The service worker implements the ability for you as a\ndeveloper to catch the network attempt that would fetch the image from the\nlocation in the src and do something, like cache the image so you only need to\ndownload it once, or, reply back to the browser with a base64 encoded image if\nthe device has no internet connection awaiting the moment internet is available.\nOr you could real clever and prevent unauthorised access to resources before\nthey even start to contact the server in the first place.\n\nA Web Worker Is much simpler, though also a thread apart from the main browser\nwindow or tab, and able to make AJAX like network requests, it doesn't offer a\ndeveloper any deeper access to the usual networking or caching features of a\nbrowser. It's uses are more varied, to the point, you use web workers to do all\nthe heavy lifting on the client side so that the processing of such things do\nnot affect the UI or UX in terms of animations, scrolling, drag'n'drop, ect. By\nsetting up your process intensive logic via a web worker you ensure that the UI\nwill feel as snappy and act as fluid as though you were not running any\nJavaScript at all.\nA very important consideration on netbooks, tablet, low end mobile phones, and a\nmassive host of users around the world visiting your site on slow internet (\ncough Australia..)\n\nWeb Worker by example\nLet's create a way to autosave some page HTML as content along with it's\narbitrary data such as name and category for example.\n\nThis is a fairly straightforward implementation;\n\nindex.html\n\n<html>\n  <head>\n    <!-- basics -->\n  </head>\n  <body>\n    <select class=\"autosave\">\n      <option disabled>- Saved Items -</option>\n    </select>\n  </body>\n</html>\n\n\nautosave.js\n\nif (!!window.Worker) {\n  Autosave = function () {\n    var that = this;\n    this.Worker = new Worker('/autosave.worker.js');\n    this.Worker.addEventListener('message', function (e) {\n      if (e.data === 'Database initialised.') {\n        console.log(e.data);\n      }\n      if (e.data.action === 'add') {\n        that.addItem((new Date(e.data.result.created)).toLocaleTimeString() + ' [' + e.data.result.category + '] ' + e.data.result.name, e.data.result.id);\n      }\n      if (e.data.action === 'list') {\n        e.data.result.forEach(function(v){\n          that.addItem((new Date(v.created)).toLocaleTimeString() + ' [' + v.category + '] ' + v.name, v.id);\n        });\n      }\n    });\n    this.Worker.postMessage({});\n  };\n  Autosave.prototype.save = function (category, name, content) {\n    this.Worker.postMessage({\n      action: 'add', params: {\n        category: category,\n        name: name,\n        created: +new Date(),\n        content: content\n      }\n    });\n  };\n  Autosave.prototype.list = function () {\n    this.Worker.postMessage({action: 'list'});\n  };\n  Autosave.prototype.get = function (id) {\n    this.Worker.postMessage({action: 'get', params: {id: parseInt(id)}});\n  };\n  Autosave.prototype.addItem = function (label, value) {\n    //example adding a new option to the top of a select in the DOM\n    $('select.autosave').children().eq(0).after(new Option(label, value));\n  };\n}\n\n\nautosave.worker.js\n\n(function () {\n  \"use strict\";\n  var dbName    = 'autosave',\n      dbVersion = 1;\n\n  importScripts('store.js');\n  self.Store = new Store(self, dbName, dbVersion);\n\n  self.onmessage = function (e) {\n    var data     = e.data,\n        action   = data.action,\n        params   = data.params || {};\n\n    if (action && self.Store[action]) {\n      self.Store[action].call(self.Store, params, function (result) {\n        self.postMessage({action: action, params: params, result: result});\n      });\n    }\n  };\n})();\n\n\nstore.js\n\nStore = function (worker, name, version) {\n  var that = this;\n  this.name = name;\n  this.version = version || 1;\n  // Let us open our database\n  var DBOpenRequest = worker.indexedDB.open(name, version);\n  // these two event handlers act on the database being opened successfully, or not\n  DBOpenRequest.onerror = function (e) {\n    worker.postMessage('Error loading database.');\n  };\n  DBOpenRequest.onsuccess = function (e) {\n    worker.postMessage('Database initialised.');\n    // store the result of opening the database in the instance variable. This is used a lot below\n    that.instance = DBOpenRequest.result;\n  };\n  // This event handles the event whereby a new version of the database needs to be created\n  // Either one has not been created before, or a new version number has been submitted via the\n  // indexedDB.open line above\n  //it is only implemented in recent browsers\n  DBOpenRequest.onupgradeneeded = function (e) {\n    var instance = e.target.result;\n    instance.onerror = function (e) {\n      worker.postMessage('Error loading database.');\n    };\n    // Create an objectStore for this database\n    var objectStore = instance.createObjectStore(name, {keyPath: 'id', autoIncrement: true});\n    // define what data items the objectStore will contain\n    objectStore.createIndex(\"name\", \"name\", {unique: false});\n    objectStore.createIndex(\"category\", \"category\", {unique: false});\n    objectStore.createIndex(\"created\", \"created\", {unique: false});\n    objectStore.createIndex(\"content\", \"content\", {unique: false});\n\n    worker.postMessage('Object store created.');\n  };\n  return this;\n};\nStore.prototype = {\n  add: function (params, callback) {\n    var that = this,\n        newItem = {};\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database add finished.');\n      typeof callback === 'function' && callback.call(params, params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // add our newItem object to the object store\n    var objectStoreRequest = objectStore.add(params);\n    objectStoreRequest.onsuccess = function (e) {\n      // report the success of our new item going into the database\n      params.id = e.target.result;\n    };\n  },\n  get: function (params, callback) {\n    var that = this,\n        id = params.id,\n        result;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database get finished.');\n      typeof callback === 'function' && callback.call(params, result);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\n    objectStore.openCursor().onsuccess = function (e) {\n      var cursor = e.target.result;\n      // if there is still another cursor to go, keep runing this code\n      if (cursor) {\n        // if there are no more cursor items to iterate through, say so, and exit the function\n        if (cursor.value.id === id) {\n          result = cursor.value;\n          return;\n        } else {\n          // continue on to the next item in the cursor\n          cursor.continue();\n        }\n      }\n    };\n  },\n  truncate: function (params, callback) {\n    var that = this;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database truncate finished.');\n      typeof callback === 'function' && callback.call(params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).clear();\n  },\n  update: function (params, callback) {\n    var that = this;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database update finished.');\n      typeof callback === 'function' && callback.call(params, params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).put(params);\n  },\n  delete: function (params, callback) {\n    var that = this,\n        id = params.id;\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function() {\n      console.log('Transaction completed: database delete finished.');\n      typeof callback === 'function' && callback.call(params);\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    transaction.objectStore(that.name).delete(id);\n  },\n  list: function (params, callback) {\n    var that  = this,\n        items = [];\n    // open a read/write db transaction, ready for adding the data\n    var transaction = that.instance.transaction([that.name], \"readwrite\");\n    // report on the success of opening the transaction\n    transaction.oncomplete = function () {\n      console.log('Transaction completed: database list finished.');\n    };\n    transaction.onerror = function () {\n      console.log('Transaction not opened due to error: ' + transaction.error + '');\n    };\n    // call an object store that's already been added to the database\n    var objectStore = transaction.objectStore(that.name);\n    // Open our object store and then get a cursor list of all the different data items in the IDB to iterate through\n    objectStore.openCursor().onsuccess = function (e) {\n      var cursor = e.target.result;\n      // if there is still another cursor to go, keep runing this code\n      if (cursor) {\n        items.push(cursor.value);\n        // continue on to the next item in the cursor\n        cursor.continue();\n        // if there are no more cursor items to iterate through, say so, and exit the function\n      } else {\n        typeof callback === 'function' && callback.call(params, items);\n      }\n    };\n  }\n};\n\n\nAnd there you have it, an autosave implementation using web workers.\n\nI hope this article helps you and saves you time - please spread the knowledge!\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-4.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-03-30 21:29:12","created_by":"1","updated_at":"2021-03-31 14:18:06","updated_by":"1","published_at":"2016-05-24 14:56:04","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc94","uuid":"f947bd16-e195-4fa8-9e0b-a102cdefd52e","title":"What is Application Cache Error event Manifest fetch failed","slug":"what-is-application-cache-error-event-manifest-fetch-failed","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"So you've been building an offline web app using Application Cache and when you test your functionality while offline you encounter this;\\n\\n`Application Cache Error event: Manifest fetch failed (6) https://domain.tld/manifest.appcache` \\n\\nYou're code may look a bit like the following;\\n\\n```language-javascript\\nwindow.addEventListener('load', function(e) {\\n  window.applicationCache.addEventListener('updateready', function(e) {\\n    if (window.applicationCache.status == window.applicationCache.UPDATEREADY) {\\n      window.applicationCache.update();\\n      window.applicationCache.swapCache();\\n    }\\n  }, false);\\n  window.applicationCache.addEventListener('noupdate', function(e){\\n    // start your app here\\n  }, false);\\n  window.applicationCache.addEventListener('cached', function(e){\\n    // start your app here\\n  }, false);\\n  \\n}, false);\\n\\n```\\n\\nAnd you may be accessing the details of the error using this;\\n\\n```language-javascript\\nwindow.applicationCache.addEventListener('error', function(e){\\n    console.log(arguments);\\n  }, false);\\n```\\n\\nBut you're not able to investigate far as the event object is the only data available, and it is brief..\\n\\nYou may first investigate what `ApplicationCacheErrorEvent` is and how to diagnose error cases but if you're here it's likely you were as successful as I was.\\n\\n> `ApplicationCacheErrorEvent` has no public documentation at all\\n\\nThe only information you'll notice change through trial and error might be the `event.type` and `event.reason` properties.\\n\\nThe `reason` encountered when you receive `Application Cache Error event: Manifest fetch failed (6) https://domain.tld/manifest.appcache`  is _manifest_ consistently.\\nThis is good news because we can test and handle this situation easily using;\\n\\n```language-javascript\\nwindow.applicationCache.addEventListener('error', function(e){\\n    if (e.reason === 'manifest') {\\n     // start your app here\\n    }\\n  }, false);\\n```\\n\\nYou might be wondering why I would ignore that there was an error and just start my app? \\nWell why not?\\nI've run through many scenarios online, offline, lost connection during cache update, ect. Yet consistently when I encounter `e.reason === 'manifest'` and continue to run my app it runs flawlessly.\\n\\n#### TL;DR\\n \\nMy conclusion is that the browser throws `Error event: Manifest fetch failed (6)` when it attempts to download your `manifest.appcache` file from the server to check it for freshness and action accordingly thereafter, and that it will attempt to reach the server even when network connectivity is not available. \\n\\n> Keep calm, this is intended normal operation\\n\\nThe fact that javascript execution is not interrupted *and* all assets defined in the `manifest.appcache` are available from a previous online state the error is in fact more of a warning or notice rather than an error that needs resolving.\\n\\nI hope this article helps you and saves you time - please spread the knowledge!\\n\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>So you've been building an offline web app using Application Cache and when you test your functionality while offline you encounter this;</p>\n<p><code>Application Cache Error event: Manifest fetch failed (6) https://domain.tld/manifest.appcache</code></p>\n<p>You're code may look a bit like the following;</p>\n<pre><code class=\"language-language-javascript\">window.addEventListener('load', function(e) {\n  window.applicationCache.addEventListener('updateready', function(e) {\n    if (window.applicationCache.status == window.applicationCache.UPDATEREADY) {\n      window.applicationCache.update();\n      window.applicationCache.swapCache();\n    }\n  }, false);\n  window.applicationCache.addEventListener('noupdate', function(e){\n    // start your app here\n  }, false);\n  window.applicationCache.addEventListener('cached', function(e){\n    // start your app here\n  }, false);\n  \n}, false);\n\n</code></pre>\n<p>And you may be accessing the details of the error using this;</p>\n<pre><code class=\"language-language-javascript\">window.applicationCache.addEventListener('error', function(e){\n    console.log(arguments);\n  }, false);\n</code></pre>\n<p>But you're not able to investigate far as the event object is the only data available, and it is brief..</p>\n<p>You may first investigate what <code>ApplicationCacheErrorEvent</code> is and how to diagnose error cases but if you're here it's likely you were as successful as I was.</p>\n<blockquote>\n<p><code>ApplicationCacheErrorEvent</code> has no public documentation at all</p>\n</blockquote>\n<p>The only information you'll notice change through trial and error might be the <code>event.type</code> and <code>event.reason</code> properties.</p>\n<p>The <code>reason</code> encountered when you receive <code>Application Cache Error event: Manifest fetch failed (6) https://domain.tld/manifest.appcache</code>  is <em>manifest</em> consistently.<br>\nThis is good news because we can test and handle this situation easily using;</p>\n<pre><code class=\"language-language-javascript\">window.applicationCache.addEventListener('error', function(e){\n    if (e.reason === 'manifest') {\n     // start your app here\n    }\n  }, false);\n</code></pre>\n<p>You might be wondering why I would ignore that there was an error and just start my app?<br>\nWell why not?<br>\nI've run through many scenarios online, offline, lost connection during cache update, ect. Yet consistently when I encounter <code>e.reason === 'manifest'</code> and continue to run my app it runs flawlessly.</p>\n<h4 id=\"tldr\">TL;DR</h4>\n<p>My conclusion is that the browser throws <code>Error event: Manifest fetch failed (6)</code> when it attempts to download your <code>manifest.appcache</code> file from the server to check it for freshness and action accordingly thereafter, and that it will attempt to reach the server even when network connectivity is not available.</p>\n<blockquote>\n<p>Keep calm, this is intended normal operation</p>\n</blockquote>\n<p>The fact that javascript execution is not interrupted <em>and</em> all assets defined in the <code>manifest.appcache</code> are available from a previous online state the error is in fact more of a warning or notice rather than an error that needs resolving.</p>\n<p>I hope this article helps you and saves you time - please spread the knowledge!</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"18","plaintext":"So you've been building an offline web app using Application Cache and when you\ntest your functionality while offline you encounter this;\n\nApplication Cache Error event: Manifest fetch failed (6)\nhttps://domain.tld/manifest.appcache\n\nYou're code may look a bit like the following;\n\nwindow.addEventListener('load', function(e) {\n  window.applicationCache.addEventListener('updateready', function(e) {\n    if (window.applicationCache.status == window.applicationCache.UPDATEREADY) {\n      window.applicationCache.update();\n      window.applicationCache.swapCache();\n    }\n  }, false);\n  window.applicationCache.addEventListener('noupdate', function(e){\n    // start your app here\n  }, false);\n  window.applicationCache.addEventListener('cached', function(e){\n    // start your app here\n  }, false);\n  \n}, false);\n\n\n\nAnd you may be accessing the details of the error using this;\n\nwindow.applicationCache.addEventListener('error', function(e){\n    console.log(arguments);\n  }, false);\n\n\nBut you're not able to investigate far as the event object is the only data\navailable, and it is brief..\n\nYou may first investigate what ApplicationCacheErrorEvent is and how to diagnose\nerror cases but if you're here it's likely you were as successful as I was.\n\n> ApplicationCacheErrorEvent has no public documentation at all\n\n\nThe only information you'll notice change through trial and error might be the \nevent.type and event.reason properties.\n\nThe reason encountered when you receive Application Cache Error event: Manifest\nfetch failed (6) https://domain.tld/manifest.appcache is manifest consistently.\nThis is good news because we can test and handle this situation easily using;\n\nwindow.applicationCache.addEventListener('error', function(e){\n    if (e.reason === 'manifest') {\n     // start your app here\n    }\n  }, false);\n\n\nYou might be wondering why I would ignore that there was an error and just start\nmy app?\nWell why not?\nI've run through many scenarios online, offline, lost connection during cache\nupdate, ect. Yet consistently when I encounter e.reason === 'manifest' and\ncontinue to run my app it runs flawlessly.\n\nTL;DR\nMy conclusion is that the browser throws Error event: Manifest fetch failed (6) \nwhen it attempts to download your manifest.appcache file from the server to\ncheck it for freshness and action accordingly thereafter, and that it will\nattempt to reach the server even when network connectivity is not available.\n\n> Keep calm, this is intended normal operation\n\n\nThe fact that javascript execution is not interrupted and all assets defined in\nthe manifest.appcache are available from a previous online state the error is in\nfact more of a warning or notice rather than an error that needs resolving.\n\nI hope this article helps you and saves you time - please spread the knowledge!\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-5.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-05-12 10:09:43","created_by":"1","updated_at":"2021-03-31 14:18:41","updated_by":"1","published_at":"2016-05-12 10:45:33","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc95","uuid":"bcabb103-4a16-4d9d-9e26-2ab7de79a1df","title":"Facebook engineers are the problem with software","slug":"facebook-engineers-are-the-problem-with-software","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Before I get into the observation that Facebook engineers are hurting the software development space unintentionally let me share with you a bit about me.\\n\\nThis site is my 4th blog website, and I've gone beyond 50 personal websites years ago. I'm a speaker at many tech meetups around Sydney and Melbourne and have been working professionally across analyst, database and systems admin, and web development roles for the past 10 years where I am currently a lead software engineer at an innovative company called [Punters](https://www.punters.com.au/) managing a range of tech genius's some of which much smarter than I am in their own areas.\\n\\n> I'm passionate, fast learning, and driven. \\n\\nI'm good at a great many things and a master of very few, my success is certainly due to the experience I've picked up from others great work and I've been lucky to work with amazing individuals throughout my career.\\n\\n### Facebook's contributions\\n#### HHVM \\n###### HipHop Virtual Machine\\nWritten in Hack and PHP, HHVM uses a just-in-time (JIT) compiler to deliver faster than PHP5.x speeds for your PHP source code.\\n\\n#### conceal\\nJava APIs for non-general purpose cryptography on Android.\\n\\n#### Stetho\\nA debug bridge for Android applications using Chrome Developer Tools.\\n\\n#### React.js\\nA Javascript library for your browser intended to abstract the user interface to better handle modern app data flow and user experience.\\n\\n#### React Native\\n###### Not a Javascript library\\nReact Native is used by Facebook to write source code once for their app to work on Android and iOS and much more.\\n\\n### These projects _hurt_ not _help_ software\\n\\nSo lets look at something almost every developer has an option on, the state of \\\"the web\\\" in terms of features and standardisation.\\nIt is fairly chaotic, we've seen browser brands evolve, join together on the same engine, split and fork time and again. We've seen Microsoft fall so far behind the stigma still remains to this day and in a time they've brought us `flexbox` which revolutionised our web UI. In 2016 we've seen Apple Safari (based on webkit) move from being in the worst in terms of modern features to the leader quietly in a single release we never saw coming. \\n\\n> The web has no apparent standards, browsers are the standard in their own right\\n\\nI can see how the Facebook engineers see this mess as a burden and have little faith in the W3c and ECMAScript specifications that are in place to ensure we get the same basic experience on all web browsers. \\n\\n> Facebook want to innovate and build new exciting things\\n\\nThe fact that Facebook intend to deliver a great feature is excellent, however instead of contributing for the better of the wider community they reinvent the wheel and the material the wheel is built with.\\nReact.js is intended to abstract the user interface which is still HTML and CSS so you might try to provide a better user experience. It does this well, and if you've dedicated time in learning the principles and unique syntax the tool is very powerful.\\n\\nA few negative points that are not exclusive to React.js\\n\\n- Deciding to invest time to learn it is a gamble, in another 6 months the ecosystem of the web will be entirely different again. Writing React.js code in your stack today is likely to be technical debt in a year when the next new thing arrives.\\n\\n-  If you're hiring the pool of candidates with the skills required is slim at best, because the library is niche and if you've been exposed it is either as a hobbyist or the company you worked for experimented on a small scale. \\n\\n- You cannot easily find experienced developers in all cities, even in the tech capital/s of your city an expert is like a unicorn.\\n\\nInstead of building a new complex layer (and fail point) into a cluttered ecosystem with an expensive development cost and high chance of redundancy or obsolete, contribute upstream to the code everyone uses, contribute to the standards.\\n\\n> Most bleeding edge libraries quickly become obsolete in a few years\\n\\n#### Humble opinion on a better effort from Facebook\\n\\n#### HHVM \\n\\nContribute to Zend Engine\\n\\nSeriously guys, was it that hard to identify this? Zend Engineers already on their own are managing to provide performance on par with HHVM or beat it outright with the release of PHP7. That is PHP7 core, the code Facebook uses executing as it was meant to be, not some Hack being pre-compiled.\\n\\nFor a moment imagine that Facebook's brilliant engineers focussed the same effort on Zend Engine instead of HHVM, how many years ago would we have PHP7 and where would it be today? \\n\\n#### conceal\\nYet more examples of how the Facebook Engineers could have contributed to Android directly so that _ALL_ Apps running on Android could benefit just because they are on Android.\\n\\n#### Stetho\\nAny developer familiar with creating Apps for Google Play using Android Studio and Chrome Developer Tools to debug would benefit from Stetho being part of Chromium rather than a 3rd party tool you may never have heard of.\\n\\n#### React.js\\nSo many options here for Facebook to have made meaningful an positive effort to better the web ecosystem forever. They could have developed a webkit or blink fork with features that achieve the same performance and user experience benefits using native HTML, CSS, JavaScript and submitted their specifications for all vendors to adopt. \\nLogically the impact is reaching all intended audience by matter of fact, which is something a library could only dream of doing. jQuery eat your heart out (right?).\\n\\n#### React Native\\nAgain, so many options here. The goal of React Native is clear, write code once and it run on multiple platforms. Unity, Cordova PhoneGap, Telerek, Appcelerator, and Xmarin all come to mind instantly but there are so many platforms why would you want to create yet another obscure way to create apps? \\n\\nWhat's worse is now Apple have unleashed Swift as open source and it is highly likely that Google will soon make it so we can develop Android apps with Swift (over Go which has now been ruled out) means that no sooner that React Native arrived it is immediately obsolete, \\n\\nSwift is capable of many programming paradigms and provides some of the most modern and powerful features of any language, it would be crazy to compete a 2nd rate product next to it.\\n\\nFacebook should focus their attention to Swift being the de facto cross platform language not React Native. All they would need to do is help Google and the wider open source community develop a compiler for the Android binaries and perhaps help update Android Studio - which is entirely within their ability because, well, they are pretty rad engineers.\\n\\n### Round up\\n\\nAs a responsible developer, business owner, technology manager, or software engineer, we need to waste less effort developing features that are going to be obsolete too soon for the undertaking to ever be deemed worthy - instead, direct that same energy to contributing the same desires to reach the same outcomes into the standards themselves instead of some specialty raincoat. This way everyone can benefit for years to come.\\n\\n> This was an attempt to raise awareness, thank you\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Before I get into the observation that Facebook engineers are hurting the software development space unintentionally let me share with you a bit about me.</p>\n<p>This site is my 4th blog website, and I've gone beyond 50 personal websites years ago. I'm a speaker at many tech meetups around Sydney and Melbourne and have been working professionally across analyst, database and systems admin, and web development roles for the past 10 years where I am currently a lead software engineer at an innovative company called <a href=\"https://www.punters.com.au/\">Punters</a> managing a range of tech genius's some of which much smarter than I am in their own areas.</p>\n<blockquote>\n<p>I'm passionate, fast learning, and driven.</p>\n</blockquote>\n<p>I'm good at a great many things and a master of very few, my success is certainly due to the experience I've picked up from others great work and I've been lucky to work with amazing individuals throughout my career.</p>\n<h3 id=\"facebookscontributions\">Facebook's contributions</h3>\n<h4 id=\"hhvm\">HHVM</h4>\n<h6 id=\"hiphopvirtualmachine\">HipHop Virtual Machine</h6>\n<p>Written in Hack and PHP, HHVM uses a just-in-time (JIT) compiler to deliver faster than PHP5.x speeds for your PHP source code.</p>\n<h4 id=\"conceal\">conceal</h4>\n<p>Java APIs for non-general purpose cryptography on Android.</p>\n<h4 id=\"stetho\">Stetho</h4>\n<p>A debug bridge for Android applications using Chrome Developer Tools.</p>\n<h4 id=\"reactjs\">React.js</h4>\n<p>A Javascript library for your browser intended to abstract the user interface to better handle modern app data flow and user experience.</p>\n<h4 id=\"reactnative\">React Native</h4>\n<h6 id=\"notajavascriptlibrary\">Not a Javascript library</h6>\n<p>React Native is used by Facebook to write source code once for their app to work on Android and iOS and much more.</p>\n<h3 id=\"theseprojectshurtnothelpsoftware\">These projects <em>hurt</em> not <em>help</em> software</h3>\n<p>So lets look at something almost every developer has an option on, the state of &quot;the web&quot; in terms of features and standardisation.<br>\nIt is fairly chaotic, we've seen browser brands evolve, join together on the same engine, split and fork time and again. We've seen Microsoft fall so far behind the stigma still remains to this day and in a time they've brought us <code>flexbox</code> which revolutionised our web UI. In 2016 we've seen Apple Safari (based on webkit) move from being in the worst in terms of modern features to the leader quietly in a single release we never saw coming.</p>\n<blockquote>\n<p>The web has no apparent standards, browsers are the standard in their own right</p>\n</blockquote>\n<p>I can see how the Facebook engineers see this mess as a burden and have little faith in the W3c and ECMAScript specifications that are in place to ensure we get the same basic experience on all web browsers.</p>\n<blockquote>\n<p>Facebook want to innovate and build new exciting things</p>\n</blockquote>\n<p>The fact that Facebook intend to deliver a great feature is excellent, however instead of contributing for the better of the wider community they reinvent the wheel and the material the wheel is built with.<br>\nReact.js is intended to abstract the user interface which is still HTML and CSS so you might try to provide a better user experience. It does this well, and if you've dedicated time in learning the principles and unique syntax the tool is very powerful.</p>\n<p>A few negative points that are not exclusive to React.js</p>\n<ul>\n<li>\n<p>Deciding to invest time to learn it is a gamble, in another 6 months the ecosystem of the web will be entirely different again. Writing React.js code in your stack today is likely to be technical debt in a year when the next new thing arrives.</p>\n</li>\n<li>\n<p>If you're hiring the pool of candidates with the skills required is slim at best, because the library is niche and if you've been exposed it is either as a hobbyist or the company you worked for experimented on a small scale.</p>\n</li>\n<li>\n<p>You cannot easily find experienced developers in all cities, even in the tech capital/s of your city an expert is like a unicorn.</p>\n</li>\n</ul>\n<p>Instead of building a new complex layer (and fail point) into a cluttered ecosystem with an expensive development cost and high chance of redundancy or obsolete, contribute upstream to the code everyone uses, contribute to the standards.</p>\n<blockquote>\n<p>Most bleeding edge libraries quickly become obsolete in a few years</p>\n</blockquote>\n<h4 id=\"humbleopiniononabettereffortfromfacebook\">Humble opinion on a better effort from Facebook</h4>\n<h4 id=\"hhvm\">HHVM</h4>\n<p>Contribute to Zend Engine</p>\n<p>Seriously guys, was it that hard to identify this? Zend Engineers already on their own are managing to provide performance on par with HHVM or beat it outright with the release of PHP7. That is PHP7 core, the code Facebook uses executing as it was meant to be, not some Hack being pre-compiled.</p>\n<p>For a moment imagine that Facebook's brilliant engineers focussed the same effort on Zend Engine instead of HHVM, how many years ago would we have PHP7 and where would it be today?</p>\n<h4 id=\"conceal\">conceal</h4>\n<p>Yet more examples of how the Facebook Engineers could have contributed to Android directly so that <em>ALL</em> Apps running on Android could benefit just because they are on Android.</p>\n<h4 id=\"stetho\">Stetho</h4>\n<p>Any developer familiar with creating Apps for Google Play using Android Studio and Chrome Developer Tools to debug would benefit from Stetho being part of Chromium rather than a 3rd party tool you may never have heard of.</p>\n<h4 id=\"reactjs\">React.js</h4>\n<p>So many options here for Facebook to have made meaningful an positive effort to better the web ecosystem forever. They could have developed a webkit or blink fork with features that achieve the same performance and user experience benefits using native HTML, CSS, JavaScript and submitted their specifications for all vendors to adopt.<br>\nLogically the impact is reaching all intended audience by matter of fact, which is something a library could only dream of doing. jQuery eat your heart out (right?).</p>\n<h4 id=\"reactnative\">React Native</h4>\n<p>Again, so many options here. The goal of React Native is clear, write code once and it run on multiple platforms. Unity, Cordova PhoneGap, Telerek, Appcelerator, and Xmarin all come to mind instantly but there are so many platforms why would you want to create yet another obscure way to create apps?</p>\n<p>What's worse is now Apple have unleashed Swift as open source and it is highly likely that Google will soon make it so we can develop Android apps with Swift (over Go which has now been ruled out) means that no sooner that React Native arrived it is immediately obsolete,</p>\n<p>Swift is capable of many programming paradigms and provides some of the most modern and powerful features of any language, it would be crazy to compete a 2nd rate product next to it.</p>\n<p>Facebook should focus their attention to Swift being the de facto cross platform language not React Native. All they would need to do is help Google and the wider open source community develop a compiler for the Android binaries and perhaps help update Android Studio - which is entirely within their ability because, well, they are pretty rad engineers.</p>\n<h3 id=\"roundup\">Round up</h3>\n<p>As a responsible developer, business owner, technology manager, or software engineer, we need to waste less effort developing features that are going to be obsolete too soon for the undertaking to ever be deemed worthy - instead, direct that same energy to contributing the same desires to reach the same outcomes into the standards themselves instead of some specialty raincoat. This way everyone can benefit for years to come.</p>\n<blockquote>\n<p>This was an attempt to raise awareness, thank you</p>\n</blockquote>\n<!--kg-card-end: markdown-->","comment_id":"19","plaintext":"Before I get into the observation that Facebook engineers are hurting the\nsoftware development space unintentionally let me share with you a bit about me.\n\nThis site is my 4th blog website, and I've gone beyond 50 personal websites\nyears ago. I'm a speaker at many tech meetups around Sydney and Melbourne and\nhave been working professionally across analyst, database and systems admin, and\nweb development roles for the past 10 years where I am currently a lead software\nengineer at an innovative company called Punters [https://www.punters.com.au/] \nmanaging a range of tech genius's some of which much smarter than I am in their\nown areas.\n\n> I'm passionate, fast learning, and driven.\n\n\nI'm good at a great many things and a master of very few, my success is\ncertainly due to the experience I've picked up from others great work and I've\nbeen lucky to work with amazing individuals throughout my career.\n\nFacebook's contributions\nHHVM\nHipHop Virtual Machine\nWritten in Hack and PHP, HHVM uses a just-in-time (JIT) compiler to deliver\nfaster than PHP5.x speeds for your PHP source code.\n\nconceal\nJava APIs for non-general purpose cryptography on Android.\n\nStetho\nA debug bridge for Android applications using Chrome Developer Tools.\n\nReact.js\nA Javascript library for your browser intended to abstract the user interface to\nbetter handle modern app data flow and user experience.\n\nReact Native\nNot a Javascript library\nReact Native is used by Facebook to write source code once for their app to work\non Android and iOS and much more.\n\nThese projects hurt not help software\nSo lets look at something almost every developer has an option on, the state of\n\"the web\" in terms of features and standardisation.\nIt is fairly chaotic, we've seen browser brands evolve, join together on the\nsame engine, split and fork time and again. We've seen Microsoft fall so far\nbehind the stigma still remains to this day and in a time they've brought us \nflexbox which revolutionised our web UI. In 2016 we've seen Apple Safari (based\non webkit) move from being in the worst in terms of modern features to the\nleader quietly in a single release we never saw coming.\n\n> The web has no apparent standards, browsers are the standard in their own right\n\n\nI can see how the Facebook engineers see this mess as a burden and have little\nfaith in the W3c and ECMAScript specifications that are in place to ensure we\nget the same basic experience on all web browsers.\n\n> Facebook want to innovate and build new exciting things\n\n\nThe fact that Facebook intend to deliver a great feature is excellent, however\ninstead of contributing for the better of the wider community they reinvent the\nwheel and the material the wheel is built with.\nReact.js is intended to abstract the user interface which is still HTML and CSS\nso you might try to provide a better user experience. It does this well, and if\nyou've dedicated time in learning the principles and unique syntax the tool is\nvery powerful.\n\nA few negative points that are not exclusive to React.js\n\n * Deciding to invest time to learn it is a gamble, in another 6 months the\n   ecosystem of the web will be entirely different again. Writing React.js code\n   in your stack today is likely to be technical debt in a year when the next\n   new thing arrives.\n   \n   \n * If you're hiring the pool of candidates with the skills required is slim at\n   best, because the library is niche and if you've been exposed it is either as\n   a hobbyist or the company you worked for experimented on a small scale.\n   \n   \n * You cannot easily find experienced developers in all cities, even in the tech\n   capital/s of your city an expert is like a unicorn.\n   \n   \n\nInstead of building a new complex layer (and fail point) into a cluttered\necosystem with an expensive development cost and high chance of redundancy or\nobsolete, contribute upstream to the code everyone uses, contribute to the\nstandards.\n\n> Most bleeding edge libraries quickly become obsolete in a few years\n\n\nHumble opinion on a better effort from Facebook\nHHVM\nContribute to Zend Engine\n\nSeriously guys, was it that hard to identify this? Zend Engineers already on\ntheir own are managing to provide performance on par with HHVM or beat it\noutright with the release of PHP7. That is PHP7 core, the code Facebook uses\nexecuting as it was meant to be, not some Hack being pre-compiled.\n\nFor a moment imagine that Facebook's brilliant engineers focussed the same\neffort on Zend Engine instead of HHVM, how many years ago would we have PHP7 and\nwhere would it be today?\n\nconceal\nYet more examples of how the Facebook Engineers could have contributed to\nAndroid directly so that ALL Apps running on Android could benefit just because\nthey are on Android.\n\nStetho\nAny developer familiar with creating Apps for Google Play using Android Studio\nand Chrome Developer Tools to debug would benefit from Stetho being part of\nChromium rather than a 3rd party tool you may never have heard of.\n\nReact.js\nSo many options here for Facebook to have made meaningful an positive effort to\nbetter the web ecosystem forever. They could have developed a webkit or blink\nfork with features that achieve the same performance and user experience\nbenefits using native HTML, CSS, JavaScript and submitted their specifications\nfor all vendors to adopt.\nLogically the impact is reaching all intended audience by matter of fact, which\nis something a library could only dream of doing. jQuery eat your heart out\n(right?).\n\nReact Native\nAgain, so many options here. The goal of React Native is clear, write code once\nand it run on multiple platforms. Unity, Cordova PhoneGap, Telerek,\nAppcelerator, and Xmarin all come to mind instantly but there are so many\nplatforms why would you want to create yet another obscure way to create apps?\n\nWhat's worse is now Apple have unleashed Swift as open source and it is highly\nlikely that Google will soon make it so we can develop Android apps with Swift\n(over Go which has now been ruled out) means that no sooner that React Native\narrived it is immediately obsolete,\n\nSwift is capable of many programming paradigms and provides some of the most\nmodern and powerful features of any language, it would be crazy to compete a 2nd\nrate product next to it.\n\nFacebook should focus their attention to Swift being the de facto cross platform\nlanguage not React Native. All they would need to do is help Google and the\nwider open source community develop a compiler for the Android binaries and\nperhaps help update Android Studio - which is entirely within their ability\nbecause, well, they are pretty rad engineers.\n\nRound up\nAs a responsible developer, business owner, technology manager, or software\nengineer, we need to waste less effort developing features that are going to be\nobsolete too soon for the undertaking to ever be deemed worthy - instead, direct\nthat same energy to contributing the same desires to reach the same outcomes\ninto the standards themselves instead of some specialty raincoat. This way\neveryone can benefit for years to come.\n\n> This was an attempt to raise awareness, thank you","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-19.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-05-19 01:12:45","created_by":"1","updated_at":"2021-03-31 14:18:27","updated_by":"1","published_at":"2016-05-19 07:46:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc96","uuid":"16f7a4b7-afd4-4ae6-8627-964ab5c75e3a","title":"What's new in Node.js 6.0","slug":"whats-new-in-node-js-6-0","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This new release of node aims to improve performance, reliability, usability and security.\\n\\nNode 6.0 supports 93% of ES6 features.\\n\\n> Only 6 months after Node Foundation announced version 5.0\\n\\nAnnouncement can be found here;\\nhttps://nodejs.org/en/blog/release/v6.0.0/\\n\\n#### Support for older versions\\n\\nLTS (long-term support) of 4.0 until April 2017.\\n\\nIf you are using any of `0.12`, `0.14`, `5.*` versions of node, you might want to consider upgrading.\\n\\n> October of this year (2016), Node.js v6.0 will be an LTS\\n\\n### Changes\\n\\nI'm going to try and make important notes to save readers the time of digesting the official announcement.\\n\\n#### Buffer\\n\\nThere is a new Buffer API that reduce the risk of bugs and vulnerabilities leaking into applications. \\n\\n- Buffer and SlowBuffer APIs.\\n- Added lastIndexOf method to Buffer prototype chain.\\n\\n#### New ES6 Features\\n\\nProbably the most exciting part of the announcement. \\n\\n###### Default Function Parameters\\n\\nNode.js being JavaScript based, has to be the only programming language that didn't offer such a basic feature.\\n\\n*before 6.0*\\n\\n```language-javascript\\nfunction functionWithDefaultValue(val) {\\n    val = val || {};\\n}\\n```\\n\\n*now in 6.0*\\n\\n```language-javascript\\nfunction functionWithDefaultValue(val = {}) {\\n\\n}\\n```\\n\\n###### Spread Operator\\n\\nThis new sugar lets us extract each array item as function parameter. Before the spread operator, we would do something like this.\\n\\n```language-javascript\\nfunction myFunc(x, y, z) {\\n\\n}\\n\\nvar args = [0, 1, 2];\\nmyFunc.apply(null, args);\\n```\\n\\nThis is how you use a spread operator;\\n\\n```language-javascript\\nfunction myFunc(x, y, z) {\\n\\n}\\n\\nvar args = [0, 1, 2];\\nmyFunc(...args);\\n```\\n\\nThis may seem `arguments`-esq but check out this example;\\n\\n```language-javascript\\nfunction spreadDebug(a, b, ...spread) {\\n    console.log(a, b);\\n    console.log(spread);\\n}\\nspreadDebug(1, 3, 'unknown', 'params');\\n```\\n\\n#### Destructuring\\n\\nPHP devs will see this feature being similar to `list()` but lets take a look at some distinct examples;\\n\\nExtract data from arrays (or objects) into distinct variables.\\n\\n```language-javascript\\nvar x = [0, 1, 2, 3]\\nvar [y, z] = x\\nconsole.log(y) // 0  \\nconsole.log(z) // 1 \\n```\\n\\nSwapping variables\\n\\n```language-javascript\\nvar a = 0;\\nvar b = 99;\\n[a, b] = [b, a];\\nconsole.log(a); // 99\\nconsole.log(b); // 0\\n```\\n\\n###### crypto\\n\\nWith this release there will be some breaking changes form Node.js v5.x\\n\\n- `crypto.Certificate` no longer has _handle property.\\n- The digest parameter for `crypto.pbkdf2()` is now required.\\n- The default encoding for all crypto methods is now utf8.\\n- FIPS-compliant mode is now off by default even if node is built with FIPS-compliance.\\n\\n###### events\\n\\nThe internal event handler storage object `EventEmitter#_events` now inherits from `Object.create(null)` rather than `Object.prototype`.\\nThis prevents issues with events using otherwise reserved property names such as __proto__.\\nThis also means that any properties that modules intentionally add to `Object.prototype` will not be available on `_events`.\\n\\n#### Final words\\n\\nNode.js is used by tens of thousands of organizations and amasses more than 3.5 million active users per month. It is the runtime of choice for high-performance, low latency applications, powering everything from enterprise applications, robots, API engines, cloud stacks and mobile websites.\\n\\n#### Resources\\n\\n- [Download version 6](https://nodejs.org/download/release/v6.0.0/)\\n- [Download current LTS (version 4)](https://nodejs.org/en/download/)\\n- [Technical blog for updates](https://nodejs.org/en/blog/)\\n\\n\\nPlease spread the knowledge!\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>This new release of node aims to improve performance, reliability, usability and security.</p>\n<p>Node 6.0 supports 93% of ES6 features.</p>\n<blockquote>\n<p>Only 6 months after Node Foundation announced version 5.0</p>\n</blockquote>\n<p>Announcement can be found here;<br>\n<a href=\"https://nodejs.org/en/blog/release/v6.0.0/\">https://nodejs.org/en/blog/release/v6.0.0/</a></p>\n<h4 id=\"supportforolderversions\">Support for older versions</h4>\n<p>LTS (long-term support) of 4.0 until April 2017.</p>\n<p>If you are using any of <code>0.12</code>, <code>0.14</code>, <code>5.*</code> versions of node, you might want to consider upgrading.</p>\n<blockquote>\n<p>October of this year (2016), Node.js v6.0 will be an LTS</p>\n</blockquote>\n<h3 id=\"changes\">Changes</h3>\n<p>I'm going to try and make important notes to save readers the time of digesting the official announcement.</p>\n<h4 id=\"buffer\">Buffer</h4>\n<p>There is a new Buffer API that reduce the risk of bugs and vulnerabilities leaking into applications.</p>\n<ul>\n<li>Buffer and SlowBuffer APIs.</li>\n<li>Added lastIndexOf method to Buffer prototype chain.</li>\n</ul>\n<h4 id=\"newes6features\">New ES6 Features</h4>\n<p>Probably the most exciting part of the announcement.</p>\n<h6 id=\"defaultfunctionparameters\">Default Function Parameters</h6>\n<p>Node.js being JavaScript based, has to be the only programming language that didn't offer such a basic feature.</p>\n<p><em>before 6.0</em></p>\n<pre><code class=\"language-language-javascript\">function functionWithDefaultValue(val) {\n    val = val || {};\n}\n</code></pre>\n<p><em>now in 6.0</em></p>\n<pre><code class=\"language-language-javascript\">function functionWithDefaultValue(val = {}) {\n\n}\n</code></pre>\n<h6 id=\"spreadoperator\">Spread Operator</h6>\n<p>This new sugar lets us extract each array item as function parameter. Before the spread operator, we would do something like this.</p>\n<pre><code class=\"language-language-javascript\">function myFunc(x, y, z) {\n\n}\n\nvar args = [0, 1, 2];\nmyFunc.apply(null, args);\n</code></pre>\n<p>This is how you use a spread operator;</p>\n<pre><code class=\"language-language-javascript\">function myFunc(x, y, z) {\n\n}\n\nvar args = [0, 1, 2];\nmyFunc(...args);\n</code></pre>\n<p>This may seem <code>arguments</code>-esq but check out this example;</p>\n<pre><code class=\"language-language-javascript\">function spreadDebug(a, b, ...spread) {\n    console.log(a, b);\n    console.log(spread);\n}\nspreadDebug(1, 3, 'unknown', 'params');\n</code></pre>\n<h4 id=\"destructuring\">Destructuring</h4>\n<p>PHP devs will see this feature being similar to <code>list()</code> but lets take a look at some distinct examples;</p>\n<p>Extract data from arrays (or objects) into distinct variables.</p>\n<pre><code class=\"language-language-javascript\">var x = [0, 1, 2, 3]\nvar [y, z] = x\nconsole.log(y) // 0  \nconsole.log(z) // 1 \n</code></pre>\n<p>Swapping variables</p>\n<pre><code class=\"language-language-javascript\">var a = 0;\nvar b = 99;\n[a, b] = [b, a];\nconsole.log(a); // 99\nconsole.log(b); // 0\n</code></pre>\n<h6 id=\"crypto\">crypto</h6>\n<p>With this release there will be some breaking changes form Node.js v5.x</p>\n<ul>\n<li><code>crypto.Certificate</code> no longer has _handle property.</li>\n<li>The digest parameter for <code>crypto.pbkdf2()</code> is now required.</li>\n<li>The default encoding for all crypto methods is now utf8.</li>\n<li>FIPS-compliant mode is now off by default even if node is built with FIPS-compliance.</li>\n</ul>\n<h6 id=\"events\">events</h6>\n<p>The internal event handler storage object <code>EventEmitter#_events</code> now inherits from <code>Object.create(null)</code> rather than <code>Object.prototype</code>.<br>\nThis prevents issues with events using otherwise reserved property names such as <strong>proto</strong>.<br>\nThis also means that any properties that modules intentionally add to <code>Object.prototype</code> will not be available on <code>_events</code>.</p>\n<h4 id=\"finalwords\">Final words</h4>\n<p>Node.js is used by tens of thousands of organizations and amasses more than 3.5 million active users per month. It is the runtime of choice for high-performance, low latency applications, powering everything from enterprise applications, robots, API engines, cloud stacks and mobile websites.</p>\n<h4 id=\"resources\">Resources</h4>\n<ul>\n<li><a href=\"https://nodejs.org/download/release/v6.0.0/\">Download version 6</a></li>\n<li><a href=\"https://nodejs.org/en/download/\">Download current LTS (version 4)</a></li>\n<li><a href=\"https://nodejs.org/en/blog/\">Technical blog for updates</a></li>\n</ul>\n<p>Please spread the knowledge!</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"20","plaintext":"This new release of node aims to improve performance, reliability, usability and\nsecurity.\n\nNode 6.0 supports 93% of ES6 features.\n\n> Only 6 months after Node Foundation announced version 5.0\n\n\nAnnouncement can be found here;\nhttps://nodejs.org/en/blog/release/v6.0.0/\n\nSupport for older versions\nLTS (long-term support) of 4.0 until April 2017.\n\nIf you are using any of 0.12, 0.14, 5.* versions of node, you might want to\nconsider upgrading.\n\n> October of this year (2016), Node.js v6.0 will be an LTS\n\n\nChanges\nI'm going to try and make important notes to save readers the time of digesting\nthe official announcement.\n\nBuffer\nThere is a new Buffer API that reduce the risk of bugs and vulnerabilities\nleaking into applications.\n\n * Buffer and SlowBuffer APIs.\n * Added lastIndexOf method to Buffer prototype chain.\n\nNew ES6 Features\nProbably the most exciting part of the announcement.\n\nDefault Function Parameters\nNode.js being JavaScript based, has to be the only programming language that\ndidn't offer such a basic feature.\n\nbefore 6.0\n\nfunction functionWithDefaultValue(val) {\n    val = val || {};\n}\n\n\nnow in 6.0\n\nfunction functionWithDefaultValue(val = {}) {\n\n}\n\n\nSpread Operator\nThis new sugar lets us extract each array item as function parameter. Before the\nspread operator, we would do something like this.\n\nfunction myFunc(x, y, z) {\n\n}\n\nvar args = [0, 1, 2];\nmyFunc.apply(null, args);\n\n\nThis is how you use a spread operator;\n\nfunction myFunc(x, y, z) {\n\n}\n\nvar args = [0, 1, 2];\nmyFunc(...args);\n\n\nThis may seem arguments-esq but check out this example;\n\nfunction spreadDebug(a, b, ...spread) {\n    console.log(a, b);\n    console.log(spread);\n}\nspreadDebug(1, 3, 'unknown', 'params');\n\n\nDestructuring\nPHP devs will see this feature being similar to list() but lets take a look at\nsome distinct examples;\n\nExtract data from arrays (or objects) into distinct variables.\n\nvar x = [0, 1, 2, 3]\nvar [y, z] = x\nconsole.log(y) // 0  \nconsole.log(z) // 1 \n\n\nSwapping variables\n\nvar a = 0;\nvar b = 99;\n[a, b] = [b, a];\nconsole.log(a); // 99\nconsole.log(b); // 0\n\n\ncrypto\nWith this release there will be some breaking changes form Node.js v5.x\n\n * crypto.Certificate no longer has _handle property.\n * The digest parameter for crypto.pbkdf2() is now required.\n * The default encoding for all crypto methods is now utf8.\n * FIPS-compliant mode is now off by default even if node is built with\n   FIPS-compliance.\n\nevents\nThe internal event handler storage object EventEmitter#_events now inherits from \nObject.create(null) rather than Object.prototype.\nThis prevents issues with events using otherwise reserved property names such as \nproto.\nThis also means that any properties that modules intentionally add to \nObject.prototype will not be available on _events.\n\nFinal words\nNode.js is used by tens of thousands of organizations and amasses more than 3.5\nmillion active users per month. It is the runtime of choice for\nhigh-performance, low latency applications, powering everything from enterprise\napplications, robots, API engines, cloud stacks and mobile websites.\n\nResources\n * Download version 6 [https://nodejs.org/download/release/v6.0.0/]\n * Download current LTS (version 4) [https://nodejs.org/en/download/]\n * Technical blog for updates [https://nodejs.org/en/blog/]\n\nPlease spread the knowledge!\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-2.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-05-22 21:57:01","created_by":"1","updated_at":"2021-03-31 14:18:17","updated_by":"1","published_at":"2016-05-23 13:01:42","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc97","uuid":"f5b204f4-33cb-46da-a7ca-5dcf25dd695a","title":"Concurrency safe IOPS efficient MySQL with PHP PDO","slug":"concurrency-safe-iops-efficient-mysql-with-php-pdo","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"### The problem\\n\\nI've experience using many solutions for MySQL like Amazon RDS, Google Cloud SQL, Rackspace, Azure, and Openshift. Not all offered the same configuration flexibility but the one bottleneck that all shared was IOPS.\\n\\nIn a startup or a company designing an architecture for a new product with MySQL, they all aim for _scalability_ which is all well and good.\\nFor a developer 'brain' scalability means these things;\\n\\n* Read Replica/s\\n* Sharding or multi-master setup\\n* Cluster db's or data separation\\n* Normalising data in preparation of ETL\\n\\nI've used many frameworks that many if not most PHP products are based on, Laravel, CI, Symfony, Zend, Wordpress, Drupal, Doctrine, the list goes on. \\nThe one thing they all share is the lack of IOPS awareness, worse, you cannot work with them to gain in IOPS considered solutions either.\\n\\nNo developer for any of these frameworks has built in IOPS optimisations because i believe IOPS itself is a mystery to them and more of an Engineer's place to figure out.\\n\\n> In an ecosystem where hardware is software developers must write IOPS considered code\\n\\nI am not university educated, just passionate. So why is it that the most renown frameworks had no developers or engineers identify IOPS issues and build considered solutions?\\n\\n### So what is IOPS\\n\\nWhen buying an SSD you may have seen the IOPS benchmarks and though a bigger number is better. That is like saying putting more fuel in your car makes it drive faster. Now I have your attention let me explain what parts of the term IOPS pertains to developers code.\\n\\nIOPS consists of;\\n\\n* Physical disk operations, read, write, delete (also a write operation), and seek. The later applies only to magnetic disks. I'll reference only SSD for this post.\\n* Queue of operations to perform, to be referred as \\\"Queue Depth\\\"\\n* The controller's ability to manage the operations and optimise them by understanding the queue. \\n\\nFor this article i'll not elaborate much on the third point, other than saying a larger queue depth allows the controller to make optimisations and reach the highest IOPS rates you see in the benchmark figures. So basically at very large queue depths you can reach the IOPS figures on the box, which is like saying when you check in to Git 3 weeks worth of work in 1 commit and tell the customer you did it in a day because that's what the progress report appears to be showing, you had a good day. Well its the same for IOPS, you'll perform at approx 15% of the figure on the box 99% of the time and only reach the peak figures when you are so far in the red that you know you have bigger problems than your hardware to upgrade.\\n\\n#### IOPS in developer terms\\n\\nWhen you run a SELECT statement to find a record and nothing else is going on the SSD controller will likely perform a Sequential Read, that is, Just go fetch what you asked because there isn't anything else in queue. Throw a few more SELECTs, and some UPDATE, INSERT, DELETE statements too and run them the SSD will likely random read while write operations are performed providing performance benefits.\\nNow script to run that 1000 times asynchronously, you now have huge queue depth and what is likely to happen on a busy site. The SSD controller will likely read using Tagged or Native command queue (TCQ or NCQ) logic to keep up with demand. But for an application this leads to observations of delayed reads as the write are prioritized and much slower than a read on an SSD.\\n\\n> Write operations are prioritized and much slower than a read on an SSD\\n\\nMany developers consider using a Read Replica DB at this point. If you're not familiar what a \\\"readrep\\\" is, it's simply all your main database data replicated and in-sync to a copy that you read from. It is very fast, implementing many binary logs, referred to as binlog, to do this by making only the most efficient writes to the database which is usually in memory then to disk. \\nA readrep allows you to perform all of your database SELECT statements (read operations) fast and without INSERTs being prioritised which delays the return of even the simplest of SELECT statements.\\n\\nHowever if you're making many changes to the main DB and the binlog grows, the readrep's queue depth will grow and your read operations are once again delayed. You're back at square one but this time you'll encounter many cases where expectations of sequential write then read in your application are inconsistent and not as was expected by the application. You're likely handling much more capacity now so having a readrep was a valuable decision but what's next? Your options are to look at sharding, clustering, or upgrading hardware because SSD performance for IOPS greatly differs from one model to model.\\n\\nBut are they really what you should do?\\n\\n> Rearchitecture and hardware upgrading doesn't mean you are capable of scalability!\\n\\nInstead of throwing money at the problem look at your code first. Identify that you are doing some crazy things like batch processes doing many UPDATEs that merely change only a timestamp for last access time, or expecting changes but the only data changed upon comparison is a modification time. Look at what is going on with fresh eyes and only move on from a potential redundant SQL UPDATE if it is necessary for a business case.\\n\\nYou may also find that an API is INSERTing duplicate records, and you may initially identify the client application or App is responsible for sending duplicated network calls to your APIs. Do you blame them and ask them to solve your duplicate data concern? No, consider there are likely network issues, offline, or data loss concerns built into the client side and it is a feature not a bug - Your backend code and SQL are really at fault here and need to be addressing this type of concurrency issue correctly. If 2 inserts come in that are duplicates in terms of a business rule, then it is your backend not meeting the business rule.\\n\\n> Ask; is a modification date needed if no other data was changed?\\n\\nAsk questions about the relevancy of each write operation and use the below recommendations to mitigate redundancies.\\n\\n### Quick Wins\\n\\nMySQL engine MyISAM tables are read optimised whereas InnoDB will priorities writes and data integrity features such as key constraints, T-SQL, and PL-SQL. If the features of InnoDB are not utilised on a table and you are writing data less than a ~10 times a second (which is infrequent for a database), then MyISAM would be a far better choice for your table. \\nMyISAM tables usually contain read heavy information that is rarely changed such as user account details information, and you would then have frequently updated pieces of data stored on a InnoDB table such as latest activity timestamp and read from it only when required.\\n\\nAnother quick win would be establishing a read replica database, the benefits are stated well enough above.\\n\\n## Code that mitigates IOPS\\n\\nIf you're establishing a new project the following considerations are going to be vital to a successful scalable application in PHP and MySQL. For existing codebases it isn't an insurmountable task but in most cases it requires a high level of business requirements knowledge to refactor generally speaking and changing SQL is no exception.\\n\\n### Mitigate all IOPS heavy INSERT and UPDATEs\\n\\nWhen executing an INSERT from a user or API requester as the source of the data you should always consider that the data may already exist. \\nGenerally as a developer you would choose InnoDB and use a unique constraint on a key or set of columns to ensure the database isn't polluted with duplicate values. If the unique constraint check fails it can cause unhandled exceptions on the client or server at worst, a bad UX at best.\\n\\n> Unique constraints are not the answer\\n\\nTo ensure we have data integrity with my solution we don't need InnoDB or unique constraints, these are the dependencies of using `ON DUPLICATE KEY UPDATE` functionality. It's nice to not need InnoDB and unique constraints because we can choose to utilise the read optimised MyISAM table and still handle duplicate inserts nicely with the added bonus of IOPS mitigation.\\n\\n#### Handling the INSERT correctly\\n\\nHere is an example of an INSERT with MySQL that is;\\n\\n* MyISAM and InnoDB compatible\\n* Concurrency safe \\n* IOPS efficient\\n\\nYou may be familiar with it without knowing it's benefits!\\nThis will handle the \\\"double post\\\" issue most code struggles with for a relatable use case.\\n\\nPseudo code;\\n\\n```\\nINSERT INTO {$table} ({$columns})\\nSELECT {$values} FROM DUAL\\nWHERE NOT EXISTS(\\n  SELECT 1 FROM {$table} WHERE {$uniqueKeys} LIMIT 1\\n);\\n```\\n\\nA executable example;\\n\\n```language-sql\\nINSERT INTO users (Email, Name, City, Active)\\nSELECT 'chris@example.com', 'Chris', 'Melbourne', 1 FROM DUAL\\nWHERE NOT EXISTS(\\n  SELECT 1 FROM users WHERE Email='chris@example.com' LIMIT 1\\n);\\n```\\n\\nResult for inserting;\\n\\n```\\nQuery OK, 1 rows affected (0.00 sec)\\n```\\n\\nResult when the data exists and no insert was executed;\\n\\n```\\n+-----------+----------------+---------------------+\\n| Email          | Name  | City      | Active\\n+-----------+----------------+---------------------+\\n| chris@ghost.io | Chris | Melbourne | 1\\n+-----------+----------------+---------------------+\\n1 row in set (0.00 sec)\\n```\\n\\nSo what is happening here? The engine will first do an inexpensive read using values you intend to insert, and if the data is not already in there it will INSERT them immediately.\\nYou may think that this is 2x IOPS and less efficient, you would be correct to assume that too. Before dismissing the idea try giving this more context in terms of data that you may want to UPDATE because you had already done a SELECT earlier and you found the data. You have a delay between those 2 queries and a DELETE may have arrived in between and the UPDATE will fail due to nothing more than the I/O of your PHP code talking to MySQL using 2 SQL statements instead of 1.\\n\\nI hope you have followed that well, if not, I'll give a code example of what i briefly talked about above.\\n\\n#### Handling the UPDATE correctly \\n\\nFor UPDATEs, you basically should never update when the only data change is a mod date or last accessed date unless it is the intended functionality. A business case for updating only a modify or access timestamp would be an audit trail or online presence status (Chris was online 2 mins ago), mostly anything else falls into the category of not updating if the only change is a modify or access timestamp.\\n\\nUsing PDO fetch with the result form the INSERT where EXISTS method above we actually get the values stored in the database if the unique data exists, which saved us concurrency concerns already but now we can use if for a comparison before we even decide if an UPDATE needs to be executed without any more than 1 SQL statement being executed so far.\\n\\nI'm not going to demonstrate an UPDATE statement, nor will I provide examples of PHP dong value comparison as it is fairly straight forward.\\n\\nThe key considerations for your comparison code are;\\n\\n* Skipping any column data you've hardcoded for your INSERT, such as NULL and UTC_TIMESTAMP as examples\\n* You might as well also skip comparing the unique column because we ended up at this point because we found a record matching\\n* Omit completely any columns when you know or trust that the INSERTed value is correct or static\\n\\nAll of these considerations are designed to speed up the time it takes to get to the UPDATE execution as soon as possible as the SELECT returned.\\n(you may choose to lock the record if using InnoDB but i advise against that mainly due to bad UX and exception implied handling on clients).\\n\\n### Wrapping this up\\n\\nWe've made a lot of considerations and worked through some examples, you're head is probably spinning so lets take a look at what this might look like when its all put together in a nice reusable single function call in PHP7.\\n\\n> mysql_put: my take on Concurrency safe IOPS efficient MySQL with PHP PDO\\n\\n```language-php\\n  /**\\n   * @param $table string\\n   * @param $mods array keys; \\\"col\\\": required, \\\"bind\\\": false for hardcode, \\\"val\\\": value, \\\"unique\\\": used for exists check and update where clause\\n   * @return array\\n   */\\n  public static function mysql_put(string $table, array $mods): array {\\n    $dbh = $PDOEndpoint; /* @var $dbh \\\\PDO */\\n\\n    $columns = implode(',', array_map(function($data) { return $data['col']; }, $mods));\\n    $params = [];\\n    $select = [];\\n    $update = [];\\n    $unique = [];\\n    $exists = [];\\n    array_map(function($data) use (&$exists, &$unique) {\\n      if ($data['unique'] === true && $data['bind'] !== false) {\\n        $exists[] = $data['col'].' = :'.$data['col'];\\n        $unique[] = $data['col'];\\n      }\\n    }, $mods);\\n    array_map(function($data) use (&$params, &$select, &$unique, &$update) {\\n      if ($data['bind'] !== false) {\\n        $bindParam = ':'.$data['col'];\\n        $params[$bindParam] = $data['val'];\\n        $select[] = $bindParam;\\n        if (!in_array($data['col'], $unique)) {\\n          $update[] = $data['col'].' = '.$bindParam;\\n        }\\n      } else {\\n        $select[] = $data['val'];\\n        if (!in_array($data['col'], $unique)) {\\n          $update[] = $data['col'].' = '.$data['val'];\\n        }\\n      }\\n    }, $mods);\\n    $selectSQL = 'SELECT '.implode(',',$select);\\n    $updateSQL = implode(',', $update);\\n    $existsSQL = implode(' AND ', $exists);\\n\\n    $insertQuery =\\n      \\\"INSERT INTO {$table} ({$columns})\\n        {$selectSQL}\\n       FROM DUAL\\n       WHERE NOT EXISTS(SELECT 1 FROM {$table} WHERE {$existsSQL} LIMIT 1);\\\";\\n    $updateQuery =\\n      \\\"UPDATE {$table} SET\\n        {$updateSQL}\\n       WHERE {$existsSQL}\\n       LIMIT 1;\\\";\\n\\n    try {\\n      $insertSTMT = $dbh->prepare($insertQuery);\\n      foreach ($params as $key => $value) {\\n        $dataType = \\\\PDO::PARAM_STR;\\n        if (is_null($value)) {\\n          $dataType = \\\\PDO::PARAM_NULL;\\n        } elseif (is_int($value)) {\\n          $dataType = \\\\PDO::PARAM_INT;\\n        }\\n        $insertSTMT->bindValue($key, $value, $dataType);\\n      }\\n      $insertSTMT->execute();\\n      $lastId = $dbh->lastInsertId();\\n      if (!is_numeric($lastId)) {\\n        unset($lastId);\\n        $dbResult = $insertSTMT->fetchAll(\\\\PDO::FETCH_ASSOC);\\n        $needsUpdate = false;\\n        foreach ($dbResult[0]??[] as $key => $value) {\\n          if (isset($params[$key]) && $value != $params[$key]) {\\n            $needsUpdate = true;\\n            break;\\n          }\\n        }\\n        if ($needsUpdate) {\\n          $updateSTMT = $dbh->prepare($updateQuery);\\n          foreach ($params as $key => $value) {\\n            $dataType = \\\\PDO::PARAM_STR;\\n            if (is_null($value)) {\\n              $dataType = \\\\PDO::PARAM_NULL;\\n            } elseif (is_int($value)) {\\n              $dataType = \\\\PDO::PARAM_INT;\\n            }\\n            $updateSTMT->bindValue($key, $value, $dataType);\\n          }\\n          $updateSTMT->execute();\\n          $rowCount = $updateSTMT->rowCount();\\n        }\\n      }\\n    } catch(\\\\PDOException $e) {\\n      print('[mysql_put] Error ('.$e->getCode().') '.$e->getMessage());\\n    }\\n    \\n    return [\\n      'insert' => $lastId ?? false,\\n      'needsUpdate' => $needsUpdate ?? null,\\n      'update' => $rowCount ?? false\\n    ];\\n  }\\n\\n```\\n\\nCalling this bad boy looks like this;\\n\\n```language-php \\n  mysql_put('users', [\\n      ['col'=>'userName', 'val' => $userName, 'unique' => true],\\n      ['col'=>'deviceId', 'val' => $session, 'unique' => true],\\n      ['col'=>'removed', 'val' => $removed],\\n      ['col'=>'modDate', 'bind'=>false, 'val' => 'UTC_TIMESTAMP()']\\n    ]);\\n```\\n\\n### Results\\nThe expected IOPS for certain scenarios;\\n\\nInserting data without checking for uniqueness:\\nBefore: 1 write.\\nAfter:  1 read, 1 write.\\n\\nInserting data without checking for uniqueness, double post concurrency issue:\\nBefore: 2 write.\\nAfter:  2 read, 1 write.\\n\\nInserting data with a successful check for uniqueness, double post concurrency issue:\\nBefore: 2 reads, 1 write.\\nAfter:  2 read, 1 write.\\n\\nInserting new data on a unique key;\\nBefore: 1 read, 1 write.\\nAfter:  1 read, 1 write.\\n\\nIf `NOT EXISTS` fails, update data no change;\\nBefore: 1 read, 1 write.\\nAfter:  1 read.\\n\\nIf `NOT EXISTS` fails, update data;\\nBefore: 1 read, 1 write.\\nAfter:  1 read, 1 write.\\n\\n### Conclusion\\n\\nWe've learned that we can keep data integrity all without using InnoDB constraints or hurting IOPS.\\nChanging read heavy tables to MyISAM we gain huge performance benefits directly int he MySQL engine.\\nA read replica database is a certain option for scalability, whereas for clustering and sharding, you'd best explore simpler enhancements first before doing a rearchitecture to handle the fundamental change.\\nAnd finally, utilising some commonly misunderstood SQL techniques, For all of the positive use cases we see IOPS mitigation improvements.\\n\\n\\nI hope this article helps you and saves you time - please spread the knowledge!\\n\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"theproblem\">The problem</h3>\n<p>I've experience using many solutions for MySQL like Amazon RDS, Google Cloud SQL, Rackspace, Azure, and Openshift. Not all offered the same configuration flexibility but the one bottleneck that all shared was IOPS.</p>\n<p>In a startup or a company designing an architecture for a new product with MySQL, they all aim for <em>scalability</em> which is all well and good.<br>\nFor a developer 'brain' scalability means these things;</p>\n<ul>\n<li>Read Replica/s</li>\n<li>Sharding or multi-master setup</li>\n<li>Cluster db's or data separation</li>\n<li>Normalising data in preparation of ETL</li>\n</ul>\n<p>I've used many frameworks that many if not most PHP products are based on, Laravel, CI, Symfony, Zend, Wordpress, Drupal, Doctrine, the list goes on.<br>\nThe one thing they all share is the lack of IOPS awareness, worse, you cannot work with them to gain in IOPS considered solutions either.</p>\n<p>No developer for any of these frameworks has built in IOPS optimisations because i believe IOPS itself is a mystery to them and more of an Engineer's place to figure out.</p>\n<blockquote>\n<p>In an ecosystem where hardware is software developers must write IOPS considered code</p>\n</blockquote>\n<p>I am not university educated, just passionate. So why is it that the most renown frameworks had no developers or engineers identify IOPS issues and build considered solutions?</p>\n<h3 id=\"sowhatisiops\">So what is IOPS</h3>\n<p>When buying an SSD you may have seen the IOPS benchmarks and though a bigger number is better. That is like saying putting more fuel in your car makes it drive faster. Now I have your attention let me explain what parts of the term IOPS pertains to developers code.</p>\n<p>IOPS consists of;</p>\n<ul>\n<li>Physical disk operations, read, write, delete (also a write operation), and seek. The later applies only to magnetic disks. I'll reference only SSD for this post.</li>\n<li>Queue of operations to perform, to be referred as &quot;Queue Depth&quot;</li>\n<li>The controller's ability to manage the operations and optimise them by understanding the queue.</li>\n</ul>\n<p>For this article i'll not elaborate much on the third point, other than saying a larger queue depth allows the controller to make optimisations and reach the highest IOPS rates you see in the benchmark figures. So basically at very large queue depths you can reach the IOPS figures on the box, which is like saying when you check in to Git 3 weeks worth of work in 1 commit and tell the customer you did it in a day because that's what the progress report appears to be showing, you had a good day. Well its the same for IOPS, you'll perform at approx 15% of the figure on the box 99% of the time and only reach the peak figures when you are so far in the red that you know you have bigger problems than your hardware to upgrade.</p>\n<h4 id=\"iopsindeveloperterms\">IOPS in developer terms</h4>\n<p>When you run a SELECT statement to find a record and nothing else is going on the SSD controller will likely perform a Sequential Read, that is, Just go fetch what you asked because there isn't anything else in queue. Throw a few more SELECTs, and some UPDATE, INSERT, DELETE statements too and run them the SSD will likely random read while write operations are performed providing performance benefits.<br>\nNow script to run that 1000 times asynchronously, you now have huge queue depth and what is likely to happen on a busy site. The SSD controller will likely read using Tagged or Native command queue (TCQ or NCQ) logic to keep up with demand. But for an application this leads to observations of delayed reads as the write are prioritized and much slower than a read on an SSD.</p>\n<blockquote>\n<p>Write operations are prioritized and much slower than a read on an SSD</p>\n</blockquote>\n<p>Many developers consider using a Read Replica DB at this point. If you're not familiar what a &quot;readrep&quot; is, it's simply all your main database data replicated and in-sync to a copy that you read from. It is very fast, implementing many binary logs, referred to as binlog, to do this by making only the most efficient writes to the database which is usually in memory then to disk.<br>\nA readrep allows you to perform all of your database SELECT statements (read operations) fast and without INSERTs being prioritised which delays the return of even the simplest of SELECT statements.</p>\n<p>However if you're making many changes to the main DB and the binlog grows, the readrep's queue depth will grow and your read operations are once again delayed. You're back at square one but this time you'll encounter many cases where expectations of sequential write then read in your application are inconsistent and not as was expected by the application. You're likely handling much more capacity now so having a readrep was a valuable decision but what's next? Your options are to look at sharding, clustering, or upgrading hardware because SSD performance for IOPS greatly differs from one model to model.</p>\n<p>But are they really what you should do?</p>\n<blockquote>\n<p>Rearchitecture and hardware upgrading doesn't mean you are capable of scalability!</p>\n</blockquote>\n<p>Instead of throwing money at the problem look at your code first. Identify that you are doing some crazy things like batch processes doing many UPDATEs that merely change only a timestamp for last access time, or expecting changes but the only data changed upon comparison is a modification time. Look at what is going on with fresh eyes and only move on from a potential redundant SQL UPDATE if it is necessary for a business case.</p>\n<p>You may also find that an API is INSERTing duplicate records, and you may initially identify the client application or App is responsible for sending duplicated network calls to your APIs. Do you blame them and ask them to solve your duplicate data concern? No, consider there are likely network issues, offline, or data loss concerns built into the client side and it is a feature not a bug - Your backend code and SQL are really at fault here and need to be addressing this type of concurrency issue correctly. If 2 inserts come in that are duplicates in terms of a business rule, then it is your backend not meeting the business rule.</p>\n<blockquote>\n<p>Ask; is a modification date needed if no other data was changed?</p>\n</blockquote>\n<p>Ask questions about the relevancy of each write operation and use the below recommendations to mitigate redundancies.</p>\n<h3 id=\"quickwins\">Quick Wins</h3>\n<p>MySQL engine MyISAM tables are read optimised whereas InnoDB will priorities writes and data integrity features such as key constraints, T-SQL, and PL-SQL. If the features of InnoDB are not utilised on a table and you are writing data less than a ~10 times a second (which is infrequent for a database), then MyISAM would be a far better choice for your table.<br>\nMyISAM tables usually contain read heavy information that is rarely changed such as user account details information, and you would then have frequently updated pieces of data stored on a InnoDB table such as latest activity timestamp and read from it only when required.</p>\n<p>Another quick win would be establishing a read replica database, the benefits are stated well enough above.</p>\n<h2 id=\"codethatmitigatesiops\">Code that mitigates IOPS</h2>\n<p>If you're establishing a new project the following considerations are going to be vital to a successful scalable application in PHP and MySQL. For existing codebases it isn't an insurmountable task but in most cases it requires a high level of business requirements knowledge to refactor generally speaking and changing SQL is no exception.</p>\n<h3 id=\"mitigatealliopsheavyinsertandupdates\">Mitigate all IOPS heavy INSERT and UPDATEs</h3>\n<p>When executing an INSERT from a user or API requester as the source of the data you should always consider that the data may already exist.<br>\nGenerally as a developer you would choose InnoDB and use a unique constraint on a key or set of columns to ensure the database isn't polluted with duplicate values. If the unique constraint check fails it can cause unhandled exceptions on the client or server at worst, a bad UX at best.</p>\n<blockquote>\n<p>Unique constraints are not the answer</p>\n</blockquote>\n<p>To ensure we have data integrity with my solution we don't need InnoDB or unique constraints, these are the dependencies of using <code>ON DUPLICATE KEY UPDATE</code> functionality. It's nice to not need InnoDB and unique constraints because we can choose to utilise the read optimised MyISAM table and still handle duplicate inserts nicely with the added bonus of IOPS mitigation.</p>\n<h4 id=\"handlingtheinsertcorrectly\">Handling the INSERT correctly</h4>\n<p>Here is an example of an INSERT with MySQL that is;</p>\n<ul>\n<li>MyISAM and InnoDB compatible</li>\n<li>Concurrency safe</li>\n<li>IOPS efficient</li>\n</ul>\n<p>You may be familiar with it without knowing it's benefits!<br>\nThis will handle the &quot;double post&quot; issue most code struggles with for a relatable use case.</p>\n<p>Pseudo code;</p>\n<pre><code>INSERT INTO {$table} ({$columns})\nSELECT {$values} FROM DUAL\nWHERE NOT EXISTS(\n  SELECT 1 FROM {$table} WHERE {$uniqueKeys} LIMIT 1\n);\n</code></pre>\n<p>A executable example;</p>\n<pre><code class=\"language-language-sql\">INSERT INTO users (Email, Name, City, Active)\nSELECT 'chris@example.com', 'Chris', 'Melbourne', 1 FROM DUAL\nWHERE NOT EXISTS(\n  SELECT 1 FROM users WHERE Email='chris@example.com' LIMIT 1\n);\n</code></pre>\n<p>Result for inserting;</p>\n<pre><code>Query OK, 1 rows affected (0.00 sec)\n</code></pre>\n<p>Result when the data exists and no insert was executed;</p>\n<pre><code>+-----------+----------------+---------------------+\n| Email          | Name  | City      | Active\n+-----------+----------------+---------------------+\n| chris@ghost.io | Chris | Melbourne | 1\n+-----------+----------------+---------------------+\n1 row in set (0.00 sec)\n</code></pre>\n<p>So what is happening here? The engine will first do an inexpensive read using values you intend to insert, and if the data is not already in there it will INSERT them immediately.<br>\nYou may think that this is 2x IOPS and less efficient, you would be correct to assume that too. Before dismissing the idea try giving this more context in terms of data that you may want to UPDATE because you had already done a SELECT earlier and you found the data. You have a delay between those 2 queries and a DELETE may have arrived in between and the UPDATE will fail due to nothing more than the I/O of your PHP code talking to MySQL using 2 SQL statements instead of 1.</p>\n<p>I hope you have followed that well, if not, I'll give a code example of what i briefly talked about above.</p>\n<h4 id=\"handlingtheupdatecorrectly\">Handling the UPDATE correctly</h4>\n<p>For UPDATEs, you basically should never update when the only data change is a mod date or last accessed date unless it is the intended functionality. A business case for updating only a modify or access timestamp would be an audit trail or online presence status (Chris was online 2 mins ago), mostly anything else falls into the category of not updating if the only change is a modify or access timestamp.</p>\n<p>Using PDO fetch with the result form the INSERT where EXISTS method above we actually get the values stored in the database if the unique data exists, which saved us concurrency concerns already but now we can use if for a comparison before we even decide if an UPDATE needs to be executed without any more than 1 SQL statement being executed so far.</p>\n<p>I'm not going to demonstrate an UPDATE statement, nor will I provide examples of PHP dong value comparison as it is fairly straight forward.</p>\n<p>The key considerations for your comparison code are;</p>\n<ul>\n<li>Skipping any column data you've hardcoded for your INSERT, such as NULL and UTC_TIMESTAMP as examples</li>\n<li>You might as well also skip comparing the unique column because we ended up at this point because we found a record matching</li>\n<li>Omit completely any columns when you know or trust that the INSERTed value is correct or static</li>\n</ul>\n<p>All of these considerations are designed to speed up the time it takes to get to the UPDATE execution as soon as possible as the SELECT returned.<br>\n(you may choose to lock the record if using InnoDB but i advise against that mainly due to bad UX and exception implied handling on clients).</p>\n<h3 id=\"wrappingthisup\">Wrapping this up</h3>\n<p>We've made a lot of considerations and worked through some examples, you're head is probably spinning so lets take a look at what this might look like when its all put together in a nice reusable single function call in PHP7.</p>\n<blockquote>\n<p>mysql_put: my take on Concurrency safe IOPS efficient MySQL with PHP PDO</p>\n</blockquote>\n<pre><code class=\"language-language-php\">  /**\n   * @param $table string\n   * @param $mods array keys; &quot;col&quot;: required, &quot;bind&quot;: false for hardcode, &quot;val&quot;: value, &quot;unique&quot;: used for exists check and update where clause\n   * @return array\n   */\n  public static function mysql_put(string $table, array $mods): array {\n    $dbh = $PDOEndpoint; /* @var $dbh \\PDO */\n\n    $columns = implode(',', array_map(function($data) { return $data['col']; }, $mods));\n    $params = [];\n    $select = [];\n    $update = [];\n    $unique = [];\n    $exists = [];\n    array_map(function($data) use (&amp;$exists, &amp;$unique) {\n      if ($data['unique'] === true &amp;&amp; $data['bind'] !== false) {\n        $exists[] = $data['col'].' = :'.$data['col'];\n        $unique[] = $data['col'];\n      }\n    }, $mods);\n    array_map(function($data) use (&amp;$params, &amp;$select, &amp;$unique, &amp;$update) {\n      if ($data['bind'] !== false) {\n        $bindParam = ':'.$data['col'];\n        $params[$bindParam] = $data['val'];\n        $select[] = $bindParam;\n        if (!in_array($data['col'], $unique)) {\n          $update[] = $data['col'].' = '.$bindParam;\n        }\n      } else {\n        $select[] = $data['val'];\n        if (!in_array($data['col'], $unique)) {\n          $update[] = $data['col'].' = '.$data['val'];\n        }\n      }\n    }, $mods);\n    $selectSQL = 'SELECT '.implode(',',$select);\n    $updateSQL = implode(',', $update);\n    $existsSQL = implode(' AND ', $exists);\n\n    $insertQuery =\n      &quot;INSERT INTO {$table} ({$columns})\n        {$selectSQL}\n       FROM DUAL\n       WHERE NOT EXISTS(SELECT 1 FROM {$table} WHERE {$existsSQL} LIMIT 1);&quot;;\n    $updateQuery =\n      &quot;UPDATE {$table} SET\n        {$updateSQL}\n       WHERE {$existsSQL}\n       LIMIT 1;&quot;;\n\n    try {\n      $insertSTMT = $dbh-&gt;prepare($insertQuery);\n      foreach ($params as $key =&gt; $value) {\n        $dataType = \\PDO::PARAM_STR;\n        if (is_null($value)) {\n          $dataType = \\PDO::PARAM_NULL;\n        } elseif (is_int($value)) {\n          $dataType = \\PDO::PARAM_INT;\n        }\n        $insertSTMT-&gt;bindValue($key, $value, $dataType);\n      }\n      $insertSTMT-&gt;execute();\n      $lastId = $dbh-&gt;lastInsertId();\n      if (!is_numeric($lastId)) {\n        unset($lastId);\n        $dbResult = $insertSTMT-&gt;fetchAll(\\PDO::FETCH_ASSOC);\n        $needsUpdate = false;\n        foreach ($dbResult[0]??[] as $key =&gt; $value) {\n          if (isset($params[$key]) &amp;&amp; $value != $params[$key]) {\n            $needsUpdate = true;\n            break;\n          }\n        }\n        if ($needsUpdate) {\n          $updateSTMT = $dbh-&gt;prepare($updateQuery);\n          foreach ($params as $key =&gt; $value) {\n            $dataType = \\PDO::PARAM_STR;\n            if (is_null($value)) {\n              $dataType = \\PDO::PARAM_NULL;\n            } elseif (is_int($value)) {\n              $dataType = \\PDO::PARAM_INT;\n            }\n            $updateSTMT-&gt;bindValue($key, $value, $dataType);\n          }\n          $updateSTMT-&gt;execute();\n          $rowCount = $updateSTMT-&gt;rowCount();\n        }\n      }\n    } catch(\\PDOException $e) {\n      print('[mysql_put] Error ('.$e-&gt;getCode().') '.$e-&gt;getMessage());\n    }\n    \n    return [\n      'insert' =&gt; $lastId ?? false,\n      'needsUpdate' =&gt; $needsUpdate ?? null,\n      'update' =&gt; $rowCount ?? false\n    ];\n  }\n\n</code></pre>\n<p>Calling this bad boy looks like this;</p>\n<pre><code class=\"language-language-php\">  mysql_put('users', [\n      ['col'=&gt;'userName', 'val' =&gt; $userName, 'unique' =&gt; true],\n      ['col'=&gt;'deviceId', 'val' =&gt; $session, 'unique' =&gt; true],\n      ['col'=&gt;'removed', 'val' =&gt; $removed],\n      ['col'=&gt;'modDate', 'bind'=&gt;false, 'val' =&gt; 'UTC_TIMESTAMP()']\n    ]);\n</code></pre>\n<h3 id=\"results\">Results</h3>\n<p>The expected IOPS for certain scenarios;</p>\n<p>Inserting data without checking for uniqueness:<br>\nBefore: 1 write.<br>\nAfter:  1 read, 1 write.</p>\n<p>Inserting data without checking for uniqueness, double post concurrency issue:<br>\nBefore: 2 write.<br>\nAfter:  2 read, 1 write.</p>\n<p>Inserting data with a successful check for uniqueness, double post concurrency issue:<br>\nBefore: 2 reads, 1 write.<br>\nAfter:  2 read, 1 write.</p>\n<p>Inserting new data on a unique key;<br>\nBefore: 1 read, 1 write.<br>\nAfter:  1 read, 1 write.</p>\n<p>If <code>NOT EXISTS</code> fails, update data no change;<br>\nBefore: 1 read, 1 write.<br>\nAfter:  1 read.</p>\n<p>If <code>NOT EXISTS</code> fails, update data;<br>\nBefore: 1 read, 1 write.<br>\nAfter:  1 read, 1 write.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>We've learned that we can keep data integrity all without using InnoDB constraints or hurting IOPS.<br>\nChanging read heavy tables to MyISAM we gain huge performance benefits directly int he MySQL engine.<br>\nA read replica database is a certain option for scalability, whereas for clustering and sharding, you'd best explore simpler enhancements first before doing a rearchitecture to handle the fundamental change.<br>\nAnd finally, utilising some commonly misunderstood SQL techniques, For all of the positive use cases we see IOPS mitigation improvements.</p>\n<p>I hope this article helps you and saves you time - please spread the knowledge!</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"21","plaintext":"The problem\nI've experience using many solutions for MySQL like Amazon RDS, Google Cloud\nSQL, Rackspace, Azure, and Openshift. Not all offered the same configuration\nflexibility but the one bottleneck that all shared was IOPS.\n\nIn a startup or a company designing an architecture for a new product with\nMySQL, they all aim for scalability which is all well and good.\nFor a developer 'brain' scalability means these things;\n\n * Read Replica/s\n * Sharding or multi-master setup\n * Cluster db's or data separation\n * Normalising data in preparation of ETL\n\nI've used many frameworks that many if not most PHP products are based on,\nLaravel, CI, Symfony, Zend, Wordpress, Drupal, Doctrine, the list goes on.\nThe one thing they all share is the lack of IOPS awareness, worse, you cannot\nwork with them to gain in IOPS considered solutions either.\n\nNo developer for any of these frameworks has built in IOPS optimisations because\ni believe IOPS itself is a mystery to them and more of an Engineer's place to\nfigure out.\n\n> In an ecosystem where hardware is software developers must write IOPS considered\ncode\n\n\nI am not university educated, just passionate. So why is it that the most renown\nframeworks had no developers or engineers identify IOPS issues and build\nconsidered solutions?\n\nSo what is IOPS\nWhen buying an SSD you may have seen the IOPS benchmarks and though a bigger\nnumber is better. That is like saying putting more fuel in your car makes it\ndrive faster. Now I have your attention let me explain what parts of the term\nIOPS pertains to developers code.\n\nIOPS consists of;\n\n * Physical disk operations, read, write, delete (also a write operation), and\n   seek. The later applies only to magnetic disks. I'll reference only SSD for\n   this post.\n * Queue of operations to perform, to be referred as \"Queue Depth\"\n * The controller's ability to manage the operations and optimise them by\n   understanding the queue.\n\nFor this article i'll not elaborate much on the third point, other than saying a\nlarger queue depth allows the controller to make optimisations and reach the\nhighest IOPS rates you see in the benchmark figures. So basically at very large\nqueue depths you can reach the IOPS figures on the box, which is like saying\nwhen you check in to Git 3 weeks worth of work in 1 commit and tell the customer\nyou did it in a day because that's what the progress report appears to be\nshowing, you had a good day. Well its the same for IOPS, you'll perform at\napprox 15% of the figure on the box 99% of the time and only reach the peak\nfigures when you are so far in the red that you know you have bigger problems\nthan your hardware to upgrade.\n\nIOPS in developer terms\nWhen you run a SELECT statement to find a record and nothing else is going on\nthe SSD controller will likely perform a Sequential Read, that is, Just go fetch\nwhat you asked because there isn't anything else in queue. Throw a few more\nSELECTs, and some UPDATE, INSERT, DELETE statements too and run them the SSD\nwill likely random read while write operations are performed providing\nperformance benefits.\nNow script to run that 1000 times asynchronously, you now have huge queue depth\nand what is likely to happen on a busy site. The SSD controller will likely read\nusing Tagged or Native command queue (TCQ or NCQ) logic to keep up with demand.\nBut for an application this leads to observations of delayed reads as the write\nare prioritized and much slower than a read on an SSD.\n\n> Write operations are prioritized and much slower than a read on an SSD\n\n\nMany developers consider using a Read Replica DB at this point. If you're not\nfamiliar what a \"readrep\" is, it's simply all your main database data replicated\nand in-sync to a copy that you read from. It is very fast, implementing many\nbinary logs, referred to as binlog, to do this by making only the most efficient\nwrites to the database which is usually in memory then to disk.\nA readrep allows you to perform all of your database SELECT statements (read\noperations) fast and without INSERTs being prioritised which delays the return\nof even the simplest of SELECT statements.\n\nHowever if you're making many changes to the main DB and the binlog grows, the\nreadrep's queue depth will grow and your read operations are once again delayed.\nYou're back at square one but this time you'll encounter many cases where\nexpectations of sequential write then read in your application are inconsistent\nand not as was expected by the application. You're likely handling much more\ncapacity now so having a readrep was a valuable decision but what's next? Your\noptions are to look at sharding, clustering, or upgrading hardware because SSD\nperformance for IOPS greatly differs from one model to model.\n\nBut are they really what you should do?\n\n> Rearchitecture and hardware upgrading doesn't mean you are capable of\nscalability!\n\n\nInstead of throwing money at the problem look at your code first. Identify that\nyou are doing some crazy things like batch processes doing many UPDATEs that\nmerely change only a timestamp for last access time, or expecting changes but\nthe only data changed upon comparison is a modification time. Look at what is\ngoing on with fresh eyes and only move on from a potential redundant SQL UPDATE\nif it is necessary for a business case.\n\nYou may also find that an API is INSERTing duplicate records, and you may\ninitially identify the client application or App is responsible for sending\nduplicated network calls to your APIs. Do you blame them and ask them to solve\nyour duplicate data concern? No, consider there are likely network issues,\noffline, or data loss concerns built into the client side and it is a feature\nnot a bug - Your backend code and SQL are really at fault here and need to be\naddressing this type of concurrency issue correctly. If 2 inserts come in that\nare duplicates in terms of a business rule, then it is your backend not meeting\nthe business rule.\n\n> Ask; is a modification date needed if no other data was changed?\n\n\nAsk questions about the relevancy of each write operation and use the below\nrecommendations to mitigate redundancies.\n\nQuick Wins\nMySQL engine MyISAM tables are read optimised whereas InnoDB will priorities\nwrites and data integrity features such as key constraints, T-SQL, and PL-SQL.\nIf the features of InnoDB are not utilised on a table and you are writing data\nless than a ~10 times a second (which is infrequent for a database), then MyISAM\nwould be a far better choice for your table.\nMyISAM tables usually contain read heavy information that is rarely changed such\nas user account details information, and you would then have frequently updated\npieces of data stored on a InnoDB table such as latest activity timestamp and\nread from it only when required.\n\nAnother quick win would be establishing a read replica database, the benefits\nare stated well enough above.\n\nCode that mitigates IOPS\nIf you're establishing a new project the following considerations are going to\nbe vital to a successful scalable application in PHP and MySQL. For existing\ncodebases it isn't an insurmountable task but in most cases it requires a high\nlevel of business requirements knowledge to refactor generally speaking and\nchanging SQL is no exception.\n\nMitigate all IOPS heavy INSERT and UPDATEs\nWhen executing an INSERT from a user or API requester as the source of the data\nyou should always consider that the data may already exist.\nGenerally as a developer you would choose InnoDB and use a unique constraint on\na key or set of columns to ensure the database isn't polluted with duplicate\nvalues. If the unique constraint check fails it can cause unhandled exceptions\non the client or server at worst, a bad UX at best.\n\n> Unique constraints are not the answer\n\n\nTo ensure we have data integrity with my solution we don't need InnoDB or unique\nconstraints, these are the dependencies of using ON DUPLICATE KEY UPDATE \nfunctionality. It's nice to not need InnoDB and unique constraints because we\ncan choose to utilise the read optimised MyISAM table and still handle duplicate\ninserts nicely with the added bonus of IOPS mitigation.\n\nHandling the INSERT correctly\nHere is an example of an INSERT with MySQL that is;\n\n * MyISAM and InnoDB compatible\n * Concurrency safe\n * IOPS efficient\n\nYou may be familiar with it without knowing it's benefits!\nThis will handle the \"double post\" issue most code struggles with for a\nrelatable use case.\n\nPseudo code;\n\nINSERT INTO {$table} ({$columns})\nSELECT {$values} FROM DUAL\nWHERE NOT EXISTS(\n  SELECT 1 FROM {$table} WHERE {$uniqueKeys} LIMIT 1\n);\n\n\nA executable example;\n\nINSERT INTO users (Email, Name, City, Active)\nSELECT 'chris@example.com', 'Chris', 'Melbourne', 1 FROM DUAL\nWHERE NOT EXISTS(\n  SELECT 1 FROM users WHERE Email='chris@example.com' LIMIT 1\n);\n\n\nResult for inserting;\n\nQuery OK, 1 rows affected (0.00 sec)\n\n\nResult when the data exists and no insert was executed;\n\n+-----------+----------------+---------------------+\n| Email          | Name  | City      | Active\n+-----------+----------------+---------------------+\n| chris@ghost.io | Chris | Melbourne | 1\n+-----------+----------------+---------------------+\n1 row in set (0.00 sec)\n\n\nSo what is happening here? The engine will first do an inexpensive read using\nvalues you intend to insert, and if the data is not already in there it will\nINSERT them immediately.\nYou may think that this is 2x IOPS and less efficient, you would be correct to\nassume that too. Before dismissing the idea try giving this more context in\nterms of data that you may want to UPDATE because you had already done a SELECT\nearlier and you found the data. You have a delay between those 2 queries and a\nDELETE may have arrived in between and the UPDATE will fail due to nothing more\nthan the I/O of your PHP code talking to MySQL using 2 SQL statements instead of\n1.\n\nI hope you have followed that well, if not, I'll give a code example of what i\nbriefly talked about above.\n\nHandling the UPDATE correctly\nFor UPDATEs, you basically should never update when the only data change is a\nmod date or last accessed date unless it is the intended functionality. A\nbusiness case for updating only a modify or access timestamp would be an audit\ntrail or online presence status (Chris was online 2 mins ago), mostly anything\nelse falls into the category of not updating if the only change is a modify or\naccess timestamp.\n\nUsing PDO fetch with the result form the INSERT where EXISTS method above we\nactually get the values stored in the database if the unique data exists, which\nsaved us concurrency concerns already but now we can use if for a comparison\nbefore we even decide if an UPDATE needs to be executed without any more than 1\nSQL statement being executed so far.\n\nI'm not going to demonstrate an UPDATE statement, nor will I provide examples of\nPHP dong value comparison as it is fairly straight forward.\n\nThe key considerations for your comparison code are;\n\n * Skipping any column data you've hardcoded for your INSERT, such as NULL and\n   UTC_TIMESTAMP as examples\n * You might as well also skip comparing the unique column because we ended up\n   at this point because we found a record matching\n * Omit completely any columns when you know or trust that the INSERTed value is\n   correct or static\n\nAll of these considerations are designed to speed up the time it takes to get to\nthe UPDATE execution as soon as possible as the SELECT returned.\n(you may choose to lock the record if using InnoDB but i advise against that\nmainly due to bad UX and exception implied handling on clients).\n\nWrapping this up\nWe've made a lot of considerations and worked through some examples, you're head\nis probably spinning so lets take a look at what this might look like when its\nall put together in a nice reusable single function call in PHP7.\n\n> mysql_put: my take on Concurrency safe IOPS efficient MySQL with PHP PDO\n\n\n  /**\n   * @param $table string\n   * @param $mods array keys; \"col\": required, \"bind\": false for hardcode, \"val\": value, \"unique\": used for exists check and update where clause\n   * @return array\n   */\n  public static function mysql_put(string $table, array $mods): array {\n    $dbh = $PDOEndpoint; /* @var $dbh \\PDO */\n\n    $columns = implode(',', array_map(function($data) { return $data['col']; }, $mods));\n    $params = [];\n    $select = [];\n    $update = [];\n    $unique = [];\n    $exists = [];\n    array_map(function($data) use (&$exists, &$unique) {\n      if ($data['unique'] === true && $data['bind'] !== false) {\n        $exists[] = $data['col'].' = :'.$data['col'];\n        $unique[] = $data['col'];\n      }\n    }, $mods);\n    array_map(function($data) use (&$params, &$select, &$unique, &$update) {\n      if ($data['bind'] !== false) {\n        $bindParam = ':'.$data['col'];\n        $params[$bindParam] = $data['val'];\n        $select[] = $bindParam;\n        if (!in_array($data['col'], $unique)) {\n          $update[] = $data['col'].' = '.$bindParam;\n        }\n      } else {\n        $select[] = $data['val'];\n        if (!in_array($data['col'], $unique)) {\n          $update[] = $data['col'].' = '.$data['val'];\n        }\n      }\n    }, $mods);\n    $selectSQL = 'SELECT '.implode(',',$select);\n    $updateSQL = implode(',', $update);\n    $existsSQL = implode(' AND ', $exists);\n\n    $insertQuery =\n      \"INSERT INTO {$table} ({$columns})\n        {$selectSQL}\n       FROM DUAL\n       WHERE NOT EXISTS(SELECT 1 FROM {$table} WHERE {$existsSQL} LIMIT 1);\";\n    $updateQuery =\n      \"UPDATE {$table} SET\n        {$updateSQL}\n       WHERE {$existsSQL}\n       LIMIT 1;\";\n\n    try {\n      $insertSTMT = $dbh->prepare($insertQuery);\n      foreach ($params as $key => $value) {\n        $dataType = \\PDO::PARAM_STR;\n        if (is_null($value)) {\n          $dataType = \\PDO::PARAM_NULL;\n        } elseif (is_int($value)) {\n          $dataType = \\PDO::PARAM_INT;\n        }\n        $insertSTMT->bindValue($key, $value, $dataType);\n      }\n      $insertSTMT->execute();\n      $lastId = $dbh->lastInsertId();\n      if (!is_numeric($lastId)) {\n        unset($lastId);\n        $dbResult = $insertSTMT->fetchAll(\\PDO::FETCH_ASSOC);\n        $needsUpdate = false;\n        foreach ($dbResult[0]??[] as $key => $value) {\n          if (isset($params[$key]) && $value != $params[$key]) {\n            $needsUpdate = true;\n            break;\n          }\n        }\n        if ($needsUpdate) {\n          $updateSTMT = $dbh->prepare($updateQuery);\n          foreach ($params as $key => $value) {\n            $dataType = \\PDO::PARAM_STR;\n            if (is_null($value)) {\n              $dataType = \\PDO::PARAM_NULL;\n            } elseif (is_int($value)) {\n              $dataType = \\PDO::PARAM_INT;\n            }\n            $updateSTMT->bindValue($key, $value, $dataType);\n          }\n          $updateSTMT->execute();\n          $rowCount = $updateSTMT->rowCount();\n        }\n      }\n    } catch(\\PDOException $e) {\n      print('[mysql_put] Error ('.$e->getCode().') '.$e->getMessage());\n    }\n    \n    return [\n      'insert' => $lastId ?? false,\n      'needsUpdate' => $needsUpdate ?? null,\n      'update' => $rowCount ?? false\n    ];\n  }\n\n\n\nCalling this bad boy looks like this;\n\n  mysql_put('users', [\n      ['col'=>'userName', 'val' => $userName, 'unique' => true],\n      ['col'=>'deviceId', 'val' => $session, 'unique' => true],\n      ['col'=>'removed', 'val' => $removed],\n      ['col'=>'modDate', 'bind'=>false, 'val' => 'UTC_TIMESTAMP()']\n    ]);\n\n\nResults\nThe expected IOPS for certain scenarios;\n\nInserting data without checking for uniqueness:\nBefore: 1 write.\nAfter: 1 read, 1 write.\n\nInserting data without checking for uniqueness, double post concurrency issue:\nBefore: 2 write.\nAfter: 2 read, 1 write.\n\nInserting data with a successful check for uniqueness, double post concurrency\nissue:\nBefore: 2 reads, 1 write.\nAfter: 2 read, 1 write.\n\nInserting new data on a unique key;\nBefore: 1 read, 1 write.\nAfter: 1 read, 1 write.\n\nIf NOT EXISTS fails, update data no change;\nBefore: 1 read, 1 write.\nAfter: 1 read.\n\nIf NOT EXISTS fails, update data;\nBefore: 1 read, 1 write.\nAfter: 1 read, 1 write.\n\nConclusion\nWe've learned that we can keep data integrity all without using InnoDB\nconstraints or hurting IOPS.\nChanging read heavy tables to MyISAM we gain huge performance benefits directly\nint he MySQL engine.\nA read replica database is a certain option for scalability, whereas for\nclustering and sharding, you'd best explore simpler enhancements first before\ndoing a rearchitecture to handle the fundamental change.\nAnd finally, utilising some commonly misunderstood SQL techniques, For all of\nthe positive use cases we see IOPS mitigation improvements.\n\nI hope this article helps you and saves you time - please spread the knowledge!\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-7.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-05-30 03:26:22","created_by":"1","updated_at":"2021-03-31 14:16:09","updated_by":"1","published_at":"2016-06-03 11:21:52","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc98","uuid":"5ccedb96-6ff2-4879-9e37-872c17f7e0ff","title":"You dont need JavaScript for that","slug":"you-dont-need-javascript-for-that","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"### Image Slider\\n\\n<script async src=\\\"http://jsfiddle.net/chrisdlangton/1csxpj9x/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/daa2317d2c5813841d1a7c7a1ab9c38f.js\\\"></script>\\n\\n### Pushable Buttons\\n\\n<script async src=\\\"https://jsfiddle.net/chrisdlangton/p0wq35by/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/d497a7f302a1f69f30600bcd8ff274c5.js\\\"></script>\\n\\n### Animated Progress Bars\\n\\n<script async src=\\\"https://jsfiddle.net/chrisdlangton/aq4vbL4h/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/3a238f374070a73e87606ed7b94b4637.js\\\"></script>\\n\\n### Tooltips\\n\\n<script async src=\\\"https://jsfiddle.net/chrisdlangton/45guyp9p/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/531f07147ba3f5b0aae5fffe43045cb7.js\\\"></script>\\n\\n### Visibility Toggle\\n\\n<script async src=\\\"https://jsfiddle.net/chrisdlangton/kxoubt4o/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/b6662941b44dc750a5160e2b43db0b78.js\\\"></script>\\n\\n### Drop Down Menu\\n\\n<script async src=\\\"https://jsfiddle.net/chrisdlangton/1kmc7qs4/embed/html,css,result/dark/\\\"></script>\\n\\n**Gist**\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/405aa7cdf10e0f2b1e9bf59c5f44d517.js\\\"></script>\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"imageslider\">Image Slider</h3>\n<script async src=\"http://jsfiddle.net/chrisdlangton/1csxpj9x/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/daa2317d2c5813841d1a7c7a1ab9c38f.js\"></script>\n<h3 id=\"pushablebuttons\">Pushable Buttons</h3>\n<script async src=\"https://jsfiddle.net/chrisdlangton/p0wq35by/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/d497a7f302a1f69f30600bcd8ff274c5.js\"></script>\n<h3 id=\"animatedprogressbars\">Animated Progress Bars</h3>\n<script async src=\"https://jsfiddle.net/chrisdlangton/aq4vbL4h/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/3a238f374070a73e87606ed7b94b4637.js\"></script>\n<h3 id=\"tooltips\">Tooltips</h3>\n<script async src=\"https://jsfiddle.net/chrisdlangton/45guyp9p/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/531f07147ba3f5b0aae5fffe43045cb7.js\"></script>\n<h3 id=\"visibilitytoggle\">Visibility Toggle</h3>\n<script async src=\"https://jsfiddle.net/chrisdlangton/kxoubt4o/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/b6662941b44dc750a5160e2b43db0b78.js\"></script>\n<h3 id=\"dropdownmenu\">Drop Down Menu</h3>\n<script async src=\"https://jsfiddle.net/chrisdlangton/1kmc7qs4/embed/html,css,result/dark/\"></script>\n<p><strong>Gist</strong></p>\n<script src=\"https://gist.github.com/chrisdlangton/405aa7cdf10e0f2b1e9bf59c5f44d517.js\"></script><!--kg-card-end: markdown-->","comment_id":"22","plaintext":"Image Slider\nGist\n\nPushable Buttons\nGist\n\nAnimated Progress Bars\nGist\n\nTooltips\nGist\n\nVisibility Toggle\nGist\n\nDrop Down Menu\nGist","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-17.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-04 12:50:50","created_by":"1","updated_at":"2021-03-31 14:16:48","updated_by":"1","published_at":"2016-05-25 09:23:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc99","uuid":"2c48332b-b857-4a6e-a8e6-8dd1ee2f5fb2","title":"Misconception on CPU: Node.js vs PHP blocking web requests","slug":"misconception-on-cpu-node-js-vs-php-blocking-web-requests","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We know the Node.js engine runs our code asynchronously, and we know it does that using an event loop. When Node.js is used to handle web requests it is capable of managing them over each CPU available via its cluster functionality achieving incredible performance over rival server-side languages.\\n\\nPHP on the other hand by nature is parallel not asynchronous, nor is it capable of achieving multi-threading. We all know this, but were you aware how your web requests are really managed? The Zend engine that is PHP is thread safe, what that means is the engine is capable of processing simultaneously many threads at once without any 2 threads colliding on resources. So, regardless if you are using Apache or Nginx or any other web server, that web server actually spawns a new PHP managed thread for every web request received.\\n\\nThe difference between PHP and Node.js is PHP's engine can be using resources from many threads at once, but all web requests that Node.js handle are managed on only 1 thread per available CPU.\\n\\nWhat this means to you is as a Node.js _or_ PHP developer all you need to worry about on 1 PHP script in terms of CPU is that your script doesn't effectively utilise 100% of the CPU if you care about not interrupting execution of other threads on the same server including those that are managed for incoming web requests.\\n\\n> As a Node.js developer you have 1 additional concern that a PHP developer never will.\\n\\n**Blocking the event loop**, blocks CPU operations and interrupts execution for incoming web requests.\\n\\n**Why is not a PHP issue?**\\n\\nDifferent requests from different clients are ran in different instances of PHP invoked by the web server, and those instances share nothing in memory or resources. Collisions could only happen on persisted data such as files or databases. Most databases handle such collisions very well, and with files you have to be more careful.\\n\\n**Node.js gotcha - Blocking the event loop**\\n\\nJavaScript in Node.js (just like in the browser) provides a single threaded environment. This means that no two parts of your application run in parallel; instead, concurrency is achieved through the handling of I/O bound operations asynchronously. For example, a request from Node.js to the database engine to fetch some document is what allows Node.js to focus on some other part of the application:\\n\\nTrying to fetch an user object from the database. Node.js is free to run other parts of the code from the moment this function is invoked;\\n\\n```language-javascript\\ndb.User.get(userId, function(err, user) {\\n\\t// .. until the moment the user object has been retrieved here\\n})\\n```\\n\\nThis is what draws many developers to Node.js, it is powerful and performant.\\n\\nHowever, a piece of CPU-bound code (synchronous code) in a Node.js instance with thousands of clients connected is all it takes to block the event loop, making all the clients wait. CPU-bound codes include attempting to sort a large array, running an extremely long loop, and so on. For example:\\n\\n```language-javascript\\nfunction sortUsersByCountry(users) {\\n\\tusers.sort(function(a, b) {\\n\\t\\treturn a.country < b.country ? -1 : 1\\n\\t})\\n}\\n```\\n\\nInvoking this `sortUsersByCountry` function may be fine if run on a small `users` array, but with a large array, it will have a horrible impact on the overall performance. If this is something that absolutely must be done, and you are certain that there will be nothing else waiting on the event loop (for example, if this was part of a command-line tool that you are building with Node.js, and it wouldn’t matter if the entire thing ran synchronously), then this may not be an issue. However, in a Node.js web-server instance trying to serve thousands of users at a time, such a pattern can prove fatal.\\n\\nIf this array of users was being retrieved from the database, the ideal solution in Node.js would be to fetch it already sorted directly from the database. If the event loop was being blocked by a loop written to compute the sum of a long history of financial transaction data, it could be deferred to some external worker/queue setup to avoid hogging the event loop.\\nWe all know that the bottleneck in all web based environments is the database, and for a PHP developer we commonly avoid sorting, grouping, and Scalar functions in SQL databases for the symple pure fact the server side language is better and faster at that sort of thing then a SQL engine is, and we want our database to have the least impact as is possible.\\n\\n#### Conclusion\\n\\nAs you can see, there is no silver-bullet solution to this kind of Node.js problem, rather each case needs to be addressed individually. The fundamental idea is to not do CPU intensive work within the front facing Node.js instances - the ones clients connect to concurrently.\\n\\nThe real challenge is trusting that you, and your developers if you are responsible for any are keenly aware of what \\\"CPU intensive work\\\" is in all cases.\\n\\n\\nI hope this article helps you and saves you time - please spread the knowledge!\\n\\n\\n<a href=\\\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\\\" class=\\\"twitter-mention-button\\\" data-size=\\\"large\\\" data-related=\\\"chrisdlangton\\\">Tweet to @chrisdlangton</a>\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>We know the Node.js engine runs our code asynchronously, and we know it does that using an event loop. When Node.js is used to handle web requests it is capable of managing them over each CPU available via its cluster functionality achieving incredible performance over rival server-side languages.</p>\n<p>PHP on the other hand by nature is parallel not asynchronous, nor is it capable of achieving multi-threading. We all know this, but were you aware how your web requests are really managed? The Zend engine that is PHP is thread safe, what that means is the engine is capable of processing simultaneously many threads at once without any 2 threads colliding on resources. So, regardless if you are using Apache or Nginx or any other web server, that web server actually spawns a new PHP managed thread for every web request received.</p>\n<p>The difference between PHP and Node.js is PHP's engine can be using resources from many threads at once, but all web requests that Node.js handle are managed on only 1 thread per available CPU.</p>\n<p>What this means to you is as a Node.js <em>or</em> PHP developer all you need to worry about on 1 PHP script in terms of CPU is that your script doesn't effectively utilise 100% of the CPU if you care about not interrupting execution of other threads on the same server including those that are managed for incoming web requests.</p>\n<blockquote>\n<p>As a Node.js developer you have 1 additional concern that a PHP developer never will.</p>\n</blockquote>\n<p><strong>Blocking the event loop</strong>, blocks CPU operations and interrupts execution for incoming web requests.</p>\n<p><strong>Why is not a PHP issue?</strong></p>\n<p>Different requests from different clients are ran in different instances of PHP invoked by the web server, and those instances share nothing in memory or resources. Collisions could only happen on persisted data such as files or databases. Most databases handle such collisions very well, and with files you have to be more careful.</p>\n<p><strong>Node.js gotcha - Blocking the event loop</strong></p>\n<p>JavaScript in Node.js (just like in the browser) provides a single threaded environment. This means that no two parts of your application run in parallel; instead, concurrency is achieved through the handling of I/O bound operations asynchronously. For example, a request from Node.js to the database engine to fetch some document is what allows Node.js to focus on some other part of the application:</p>\n<p>Trying to fetch an user object from the database. Node.js is free to run other parts of the code from the moment this function is invoked;</p>\n<pre><code class=\"language-language-javascript\">db.User.get(userId, function(err, user) {\n\t// .. until the moment the user object has been retrieved here\n})\n</code></pre>\n<p>This is what draws many developers to Node.js, it is powerful and performant.</p>\n<p>However, a piece of CPU-bound code (synchronous code) in a Node.js instance with thousands of clients connected is all it takes to block the event loop, making all the clients wait. CPU-bound codes include attempting to sort a large array, running an extremely long loop, and so on. For example:</p>\n<pre><code class=\"language-language-javascript\">function sortUsersByCountry(users) {\n\tusers.sort(function(a, b) {\n\t\treturn a.country &lt; b.country ? -1 : 1\n\t})\n}\n</code></pre>\n<p>Invoking this <code>sortUsersByCountry</code> function may be fine if run on a small <code>users</code> array, but with a large array, it will have a horrible impact on the overall performance. If this is something that absolutely must be done, and you are certain that there will be nothing else waiting on the event loop (for example, if this was part of a command-line tool that you are building with Node.js, and it wouldn’t matter if the entire thing ran synchronously), then this may not be an issue. However, in a Node.js web-server instance trying to serve thousands of users at a time, such a pattern can prove fatal.</p>\n<p>If this array of users was being retrieved from the database, the ideal solution in Node.js would be to fetch it already sorted directly from the database. If the event loop was being blocked by a loop written to compute the sum of a long history of financial transaction data, it could be deferred to some external worker/queue setup to avoid hogging the event loop.<br>\nWe all know that the bottleneck in all web based environments is the database, and for a PHP developer we commonly avoid sorting, grouping, and Scalar functions in SQL databases for the symple pure fact the server side language is better and faster at that sort of thing then a SQL engine is, and we want our database to have the least impact as is possible.</p>\n<h4 id=\"conclusion\">Conclusion</h4>\n<p>As you can see, there is no silver-bullet solution to this kind of Node.js problem, rather each case needs to be addressed individually. The fundamental idea is to not do CPU intensive work within the front facing Node.js instances - the ones clients connect to concurrently.</p>\n<p>The real challenge is trusting that you, and your developers if you are responsible for any are keenly aware of what &quot;CPU intensive work&quot; is in all cases.</p>\n<p>I hope this article helps you and saves you time - please spread the knowledge!</p>\n<p><a href=\"https://twitter.com/intent/tweet?screen_name=chrisdlangton\" class=\"twitter-mention-button\" data-size=\"large\" data-related=\"chrisdlangton\">Tweet to @chrisdlangton</a></p>\n<!--kg-card-end: markdown-->","comment_id":"23","plaintext":"We know the Node.js engine runs our code asynchronously, and we know it does\nthat using an event loop. When Node.js is used to handle web requests it is\ncapable of managing them over each CPU available via its cluster functionality\nachieving incredible performance over rival server-side languages.\n\nPHP on the other hand by nature is parallel not asynchronous, nor is it capable\nof achieving multi-threading. We all know this, but were you aware how your web\nrequests are really managed? The Zend engine that is PHP is thread safe, what\nthat means is the engine is capable of processing simultaneously many threads at\nonce without any 2 threads colliding on resources. So, regardless if you are\nusing Apache or Nginx or any other web server, that web server actually spawns a\nnew PHP managed thread for every web request received.\n\nThe difference between PHP and Node.js is PHP's engine can be using resources\nfrom many threads at once, but all web requests that Node.js handle are managed\non only 1 thread per available CPU.\n\nWhat this means to you is as a Node.js or PHP developer all you need to worry\nabout on 1 PHP script in terms of CPU is that your script doesn't effectively\nutilise 100% of the CPU if you care about not interrupting execution of other\nthreads on the same server including those that are managed for incoming web\nrequests.\n\n> As a Node.js developer you have 1 additional concern that a PHP developer never\nwill.\n\n\nBlocking the event loop, blocks CPU operations and interrupts execution for\nincoming web requests.\n\nWhy is not a PHP issue?\n\nDifferent requests from different clients are ran in different instances of PHP\ninvoked by the web server, and those instances share nothing in memory or\nresources. Collisions could only happen on persisted data such as files or\ndatabases. Most databases handle such collisions very well, and with files you\nhave to be more careful.\n\nNode.js gotcha - Blocking the event loop\n\nJavaScript in Node.js (just like in the browser) provides a single threaded\nenvironment. This means that no two parts of your application run in parallel;\ninstead, concurrency is achieved through the handling of I/O bound operations\nasynchronously. For example, a request from Node.js to the database engine to\nfetch some document is what allows Node.js to focus on some other part of the\napplication:\n\nTrying to fetch an user object from the database. Node.js is free to run other\nparts of the code from the moment this function is invoked;\n\ndb.User.get(userId, function(err, user) {\n\t// .. until the moment the user object has been retrieved here\n})\n\n\nThis is what draws many developers to Node.js, it is powerful and performant.\n\nHowever, a piece of CPU-bound code (synchronous code) in a Node.js instance with\nthousands of clients connected is all it takes to block the event loop, making\nall the clients wait. CPU-bound codes include attempting to sort a large array,\nrunning an extremely long loop, and so on. For example:\n\nfunction sortUsersByCountry(users) {\n\tusers.sort(function(a, b) {\n\t\treturn a.country < b.country ? -1 : 1\n\t})\n}\n\n\nInvoking this sortUsersByCountry function may be fine if run on a small users \narray, but with a large array, it will have a horrible impact on the overall\nperformance. If this is something that absolutely must be done, and you are\ncertain that there will be nothing else waiting on the event loop (for example,\nif this was part of a command-line tool that you are building with Node.js, and\nit wouldn’t matter if the entire thing ran synchronously), then this may not be\nan issue. However, in a Node.js web-server instance trying to serve thousands of\nusers at a time, such a pattern can prove fatal.\n\nIf this array of users was being retrieved from the database, the ideal solution\nin Node.js would be to fetch it already sorted directly from the database. If\nthe event loop was being blocked by a loop written to compute the sum of a long\nhistory of financial transaction data, it could be deferred to some external\nworker/queue setup to avoid hogging the event loop.\nWe all know that the bottleneck in all web based environments is the database,\nand for a PHP developer we commonly avoid sorting, grouping, and Scalar\nfunctions in SQL databases for the symple pure fact the server side language is\nbetter and faster at that sort of thing then a SQL engine is, and we want our\ndatabase to have the least impact as is possible.\n\nConclusion\nAs you can see, there is no silver-bullet solution to this kind of Node.js\nproblem, rather each case needs to be addressed individually. The fundamental\nidea is to not do CPU intensive work within the front facing Node.js instances -\nthe ones clients connect to concurrently.\n\nThe real challenge is trusting that you, and your developers if you are\nresponsible for any are keenly aware of what \"CPU intensive work\" is in all\ncases.\n\nI hope this article helps you and saves you time - please spread the knowledge!\n\nTweet to @chrisdlangton\n[https://twitter.com/intent/tweet?screen_name=chrisdlangton]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-10.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-04 12:50:56","created_by":"1","updated_at":"2021-03-31 14:16:29","updated_by":"1","published_at":"2016-06-04 13:50:48","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9a","uuid":"62c0e557-3d73-462e-9490-17a193ac2035","title":"When is consistent an anti-pattern","slug":"html-when-is-consistent-an-anti-pattern","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As my click-bait title suggests, I'm going to explore when the idea of writing consistent code can be harmful.\\n\\nI've been reading a lot on blogs of the most renown tech leaders, and also in the tech mailing lists such as HTML5 Weekly, many occasions where the idea of _consistency_ in conveyed as the most ideal of code styles.\\nIn one case, the very influential blogger made a blanket statement, profound in its importance to the article, that **all** HTML element attributes **must** put the `class` attribute first, followed by the `id` then other attributes may follow. Other than the do's and dont's written in code, there was only one reasoning described in words, and i quote;\\n\\n~ \\\"The order follows what the most common attributes are. Almost every element will have class, most will have an id, some will have data attributes, etc. This makes it easier to mentally parse.\\\"\\n\\n> One does not simply say it is more common therefore it is best.\\n\\nIn terms of easier to mentally parse, if you had been writing, editing, refactoring, renaming, finding, replacing, ...repeat anything in HTML you would know (as well as you know your native spoken language) that all your `a` tags have the `href` first, followed by the optional `target`, then any other attributes. If you don't immediately agree that is ok, I believe strongly that you already subconsciously know having written as much HMTL as i have, but I'll explain it here in case you have not.\\n\\nBut before that, let me introduce some other HTML tags so that you might come to the knowledge yourself. For an `img` tag, `src` first makes sense, then the optional `alt`, and then other attributes. I could go onto other elements that have similar unique attributes and make the same point; `label[for]` `input[name]` though I sense through the ether of internet, time, and space; that you get the picture by now. \\n\\nSo all is left is the main thing; is why?\\n\\nNot because the attribute is common like class or id, and not because it is logical either (event if it is :P), but the unique attribute for the elemetn should be first only because of the most important of reasons;\\n\\n**Readability**: you identify immediately with the element and its purpose when you see;\\n```language-html\\n<a href=\\\"/sub/content\\\" target=\\\"_parent\\\" class=\\\"bem-it__up\\\" id=\\\"actionHook\\\" data-other=\\\"stuff\\\">More<a/>\\n```\\nOver something the article author had suggested;\\n```language-html\\n<a class=\\\"bem-it__up\\\" id=\\\"actionHook\\\" href=\\\"/sub/content\\\" target=\\\"_parent\\\"  data-other=\\\"stuff\\\">More<a/>\\n```\\nIt's effort to see what is happening in the second example, whereas the first is elegant in its simplicity to know what that element if for.\\n\\nAs a bonus, a novice developer can easily refactor any `a` tag `href` without regex, yet using regex also became straight-forward with example 1.\\n\\nBasically;\\n* Readability is better than consistency at the cost of identifiability\\n* Unique attributes are expected; expected is a form of being consistent\\n* Enabling the novice with a simple style; Simplicity is better than consistency at the cost of unproductivity.\\n\\nSo I ask anyone who blindly promotes _consistency_ please do explain to yourself why, but simply questioning why would all elements in all cases benefit from the class and id attributes being first tells you very little, you need to question why not be consistent, what benefits would i lose by blindly choosing to be _consistent_.\\n\\nIf consistency is the only _why_, then perhaps in this case consistency is an anti-pattern for simple readable HTML and therefore you should be thinking about it's harmful outcomes instead.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>As my click-bait title suggests, I'm going to explore when the idea of writing consistent code can be harmful.</p>\n<p>I've been reading a lot on blogs of the most renown tech leaders, and also in the tech mailing lists such as HTML5 Weekly, many occasions where the idea of <em>consistency</em> in conveyed as the most ideal of code styles.<br>\nIn one case, the very influential blogger made a blanket statement, profound in its importance to the article, that <strong>all</strong> HTML element attributes <strong>must</strong> put the <code>class</code> attribute first, followed by the <code>id</code> then other attributes may follow. Other than the do's and dont's written in code, there was only one reasoning described in words, and i quote;</p>\n<p>~ &quot;The order follows what the most common attributes are. Almost every element will have class, most will have an id, some will have data attributes, etc. This makes it easier to mentally parse.&quot;</p>\n<blockquote>\n<p>One does not simply say it is more common therefore it is best.</p>\n</blockquote>\n<p>In terms of easier to mentally parse, if you had been writing, editing, refactoring, renaming, finding, replacing, ...repeat anything in HTML you would know (as well as you know your native spoken language) that all your <code>a</code> tags have the <code>href</code> first, followed by the optional <code>target</code>, then any other attributes. If you don't immediately agree that is ok, I believe strongly that you already subconsciously know having written as much HMTL as i have, but I'll explain it here in case you have not.</p>\n<p>But before that, let me introduce some other HTML tags so that you might come to the knowledge yourself. For an <code>img</code> tag, <code>src</code> first makes sense, then the optional <code>alt</code>, and then other attributes. I could go onto other elements that have similar unique attributes and make the same point; <code>label[for]</code> <code>input[name]</code> though I sense through the ether of internet, time, and space; that you get the picture by now.</p>\n<p>So all is left is the main thing; is why?</p>\n<p>Not because the attribute is common like class or id, and not because it is logical either (event if it is :P), but the unique attribute for the elemetn should be first only because of the most important of reasons;</p>\n<p><strong>Readability</strong>: you identify immediately with the element and its purpose when you see;</p>\n<pre><code class=\"language-language-html\">&lt;a href=&quot;/sub/content&quot; target=&quot;_parent&quot; class=&quot;bem-it__up&quot; id=&quot;actionHook&quot; data-other=&quot;stuff&quot;&gt;More&lt;a/&gt;\n</code></pre>\n<p>Over something the article author had suggested;</p>\n<pre><code class=\"language-language-html\">&lt;a class=&quot;bem-it__up&quot; id=&quot;actionHook&quot; href=&quot;/sub/content&quot; target=&quot;_parent&quot;  data-other=&quot;stuff&quot;&gt;More&lt;a/&gt;\n</code></pre>\n<p>It's effort to see what is happening in the second example, whereas the first is elegant in its simplicity to know what that element if for.</p>\n<p>As a bonus, a novice developer can easily refactor any <code>a</code> tag <code>href</code> without regex, yet using regex also became straight-forward with example 1.</p>\n<p>Basically;</p>\n<ul>\n<li>Readability is better than consistency at the cost of identifiability</li>\n<li>Unique attributes are expected; expected is a form of being consistent</li>\n<li>Enabling the novice with a simple style; Simplicity is better than consistency at the cost of unproductivity.</li>\n</ul>\n<p>So I ask anyone who blindly promotes <em>consistency</em> please do explain to yourself why, but simply questioning why would all elements in all cases benefit from the class and id attributes being first tells you very little, you need to question why not be consistent, what benefits would i lose by blindly choosing to be <em>consistent</em>.</p>\n<p>If consistency is the only <em>why</em>, then perhaps in this case consistency is an anti-pattern for simple readable HTML and therefore you should be thinking about it's harmful outcomes instead.</p>\n<!--kg-card-end: markdown-->","comment_id":"24","plaintext":"As my click-bait title suggests, I'm going to explore when the idea of writing\nconsistent code can be harmful.\n\nI've been reading a lot on blogs of the most renown tech leaders, and also in\nthe tech mailing lists such as HTML5 Weekly, many occasions where the idea of \nconsistency in conveyed as the most ideal of code styles.\nIn one case, the very influential blogger made a blanket statement, profound in\nits importance to the article, that all HTML element attributes must put the \nclass attribute first, followed by the id then other attributes may follow.\nOther than the do's and dont's written in code, there was only one reasoning\ndescribed in words, and i quote;\n\n~ \"The order follows what the most common attributes are. Almost every element\nwill have class, most will have an id, some will have data attributes, etc. This\nmakes it easier to mentally parse.\"\n\n> One does not simply say it is more common therefore it is best.\n\n\nIn terms of easier to mentally parse, if you had been writing, editing,\nrefactoring, renaming, finding, replacing, ...repeat anything in HTML you would\nknow (as well as you know your native spoken language) that all your a tags have\nthe href first, followed by the optional target, then any other attributes. If\nyou don't immediately agree that is ok, I believe strongly that you already\nsubconsciously know having written as much HMTL as i have, but I'll explain it\nhere in case you have not.\n\nBut before that, let me introduce some other HTML tags so that you might come to\nthe knowledge yourself. For an img tag, src first makes sense, then the optional \nalt, and then other attributes. I could go onto other elements that have similar\nunique attributes and make the same point; label[for] input[name] though I sense\nthrough the ether of internet, time, and space; that you get the picture by now.\n\nSo all is left is the main thing; is why?\n\nNot because the attribute is common like class or id, and not because it is\nlogical either (event if it is :P), but the unique attribute for the elemetn\nshould be first only because of the most important of reasons;\n\nReadability: you identify immediately with the element and its purpose when you\nsee;\n\n<a href=\"/sub/content\" target=\"_parent\" class=\"bem-it__up\" id=\"actionHook\" data-other=\"stuff\">More<a/>\n\n\nOver something the article author had suggested;\n\n<a class=\"bem-it__up\" id=\"actionHook\" href=\"/sub/content\" target=\"_parent\"  data-other=\"stuff\">More<a/>\n\n\nIt's effort to see what is happening in the second example, whereas the first is\nelegant in its simplicity to know what that element if for.\n\nAs a bonus, a novice developer can easily refactor any a tag href without regex,\nyet using regex also became straight-forward with example 1.\n\nBasically;\n\n * Readability is better than consistency at the cost of identifiability\n * Unique attributes are expected; expected is a form of being consistent\n * Enabling the novice with a simple style; Simplicity is better than\n   consistency at the cost of unproductivity.\n\nSo I ask anyone who blindly promotes consistency please do explain to yourself\nwhy, but simply questioning why would all elements in all cases benefit from the\nclass and id attributes being first tells you very little, you need to question\nwhy not be consistent, what benefits would i lose by blindly choosing to be \nconsistent.\n\nIf consistency is the only why, then perhaps in this case consistency is an\nanti-pattern for simple readable HTML and therefore you should be thinking about\nit's harmful outcomes instead.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-14 06:44:35","created_by":"1","updated_at":"2021-03-31 14:15:46","updated_by":"1","published_at":"2016-06-14 07:08:16","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9b","uuid":"f2b64b02-0819-4358-8e2e-c817ab75673a","title":"Correctly checking property exists in JavaScript","slug":"correctly-checking-property-exists-in-javascript","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"While maintaining some code recently I found myself writing out of pure habit `Object.hasOwnProperty.call(obj, 'prop')` as is expected in almost all OSS I've contributed to, and quickly editing that to just `!!obj.prop` which I chose because it worked more succinctly in my use case but I generally cringe whenever I read code using that technique. It had me wondering if the OSS project owners who prefer `hasOwnProperty` over `!!` actually understand the differences, because I've seen pull requests rejected simply because they used `!!` and the repo maintainer didn't like the syntax.\\n\\nI'm going to demonstrate to you 3 ways to check for properties in JavaScript Objects and explain how each can be useful in their own unique way and why `hasOwnProperty` isn't always the correct option.\\n\\n### Using type coercion\\n\\nRegarded widely as evil, JavaScript allows us to coerce any value to a boolean using bang bang (`!!`) which can be troublesome. For the same functionality that allows coercion we have the dilemma that with a _truthy_ value we get the expected `true` boolean with the type coercion technique, however falsey values will be coerced to a `false` boolean appropriately, but this isn't useful when simply checking for the existence of an objects property which would also have the expected `false`, yet the property exists with a falsey value.\\n\\n```language-javascript\\nvar Obj = { foo: 'bar' };\\nfunction _hasFoo() {\\n  return !!this.foo;\\n}\\n_hasFoo.call(Obj, 'foo'); // true\\nObj.foo = 0;\\n_hasFoo.call(Obj, 'foo'); // false\\nObj.foo = false;\\n_hasFoo.call(Obj, 'foo'); // false\\nObj.foo = undefined;\\n_hasFoo.call(Obj, 'foo'); // false\\ndelete Obj.foo;\\n_hasFoo.call(Obj, 'foo'); // false\\nObj.foo = true;\\n_hasFoo.call(Obj, 'foo'); // true\\n```\\n\\nAs you can see in the tests the type coercion technique will not appropriately report that the property exists, so type coercion should never be used in that manner. Instead, consider using type coercion only to test for property existence as well as testing the value is set (as a truthy value). \\n\\n```language-javascript\\nvar RedisDAL = Object.create({ \\n  canConnect: function(){\\n    return !!this.endpoint;\\n  },\\n  endpoint: ''\\n});\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = 'myclustername.xxx.0001.region.cache.amazonaws.com:port';\\nRedisDAL.canConnect(); // true\\n```\\n\\nAs demonstrated, type coercion can be a succinct solution, if you intend to test for more than just property existence.\\n\\n### The hasOwnProperty method\\n\\nThe accepted standard way to check for property existence in JavaScript, let me demonstrate;\\n\\n```language-javascript\\nvar RedisDAL = { \\n  canConnect: function(){\\n    return Object.hasOwnProperty.call(this, 'endpoint');\\n  },\\n  endpoint: ''\\n};\\nRedisDAL.canConnect(); // true\\nRedisDAL.endpoint = false;\\nRedisDAL.canConnect(); // true\\ndelete RedisDAL.endpoint;\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = undefined;\\nRedisDAL.canConnect(); // true\\n```\\n\\nWhether the value is truthy or otherwise, using `hasOwnProperty` will consistently report concisely if the object has the property defined.\\n\\nThere is a gotcha, `hasOwnProperty` cannot read when the property is inherited through the prototypal chain which can be demonstrated using `Object.create` below;  \\n\\n```language-javascript\\nvar RedisDAL = Object.create({ \\n  canConnect: function(){\\n    return Object.hasOwnProperty.call(this, 'endpoint');\\n  },\\n  endpoint: ''\\n});\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = false;\\nRedisDAL.canConnect(); // true\\ndelete RedisDAL.endpoint;\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = undefined;\\nRedisDAL.canConnect(); // true\\n```\\n\\nAs you can see with the same tests performed the first test reports `false` because it could not read the property which `RedisDAL` inherited through its prototype.\\n\\n### With the in operator\\n\\nThe only way to be 100% sure that the object and its prototypal inherited properties are checked is using the `in` operator like so;\\n\\n```language-javascript\\nvar RedisDAL = { \\n  canConnect: function(){\\n    return 'endpoint' in this;\\n  },\\n  endpoint: ''\\n};\\nRedisDAL.canConnect(); // true\\nRedisDAL.endpoint = false;\\nRedisDAL.canConnect(); // true\\ndelete RedisDAL.endpoint;\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = undefined;\\nRedisDAL.canConnect(); // true\\n```\\n\\nAnd now using `Object.create` to demonstrate inherited properties also;\\n\\n```language-javascript\\nvar RedisDAL = Object.create({ \\n  canConnect: function(){\\n    return 'endpoint' in this;\\n  },\\n  endpoint: ''\\n});\\nRedisDAL.canConnect(); // true\\nRedisDAL.endpoint = false;\\nRedisDAL.canConnect(); // true\\ndelete RedisDAL.endpoint;\\nRedisDAL.canConnect(); // false\\nRedisDAL.endpoint = undefined;\\nRedisDAL.canConnect(); // true\\n```\\n\\nNow we can see that both test scenarios are consistently reporting their results, but this may not be the desired use case. You may not wish to validate inherited properties for good reason.\\n\\n#### In short\\n\\nIt should be up to you as a developer to choose the appropriate technique for your particular use case as none are the same and each have specific pro's and con's. \\nIf you document your code well the choice to use one over the others, then there should be no reason to have a Pull Request denied. All 3 are as valid as one another regardless of a maintainers personal opinions or code standards (lack-there-of) and styles.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>While maintaining some code recently I found myself writing out of pure habit <code>Object.hasOwnProperty.call(obj, 'prop')</code> as is expected in almost all OSS I've contributed to, and quickly editing that to just <code>!!obj.prop</code> which I chose because it worked more succinctly in my use case but I generally cringe whenever I read code using that technique. It had me wondering if the OSS project owners who prefer <code>hasOwnProperty</code> over <code>!!</code> actually understand the differences, because I've seen pull requests rejected simply because they used <code>!!</code> and the repo maintainer didn't like the syntax.</p>\n<p>I'm going to demonstrate to you 3 ways to check for properties in JavaScript Objects and explain how each can be useful in their own unique way and why <code>hasOwnProperty</code> isn't always the correct option.</p>\n<h3 id=\"usingtypecoercion\">Using type coercion</h3>\n<p>Regarded widely as evil, JavaScript allows us to coerce any value to a boolean using bang bang (<code>!!</code>) which can be troublesome. For the same functionality that allows coercion we have the dilemma that with a <em>truthy</em> value we get the expected <code>true</code> boolean with the type coercion technique, however falsey values will be coerced to a <code>false</code> boolean appropriately, but this isn't useful when simply checking for the existence of an objects property which would also have the expected <code>false</code>, yet the property exists with a falsey value.</p>\n<pre><code class=\"language-language-javascript\">var Obj = { foo: 'bar' };\nfunction _hasFoo() {\n  return !!this.foo;\n}\n_hasFoo.call(Obj, 'foo'); // true\nObj.foo = 0;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = false;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = undefined;\n_hasFoo.call(Obj, 'foo'); // false\ndelete Obj.foo;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = true;\n_hasFoo.call(Obj, 'foo'); // true\n</code></pre>\n<p>As you can see in the tests the type coercion technique will not appropriately report that the property exists, so type coercion should never be used in that manner. Instead, consider using type coercion only to test for property existence as well as testing the value is set (as a truthy value).</p>\n<pre><code class=\"language-language-javascript\">var RedisDAL = Object.create({ \n  canConnect: function(){\n    return !!this.endpoint;\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = 'myclustername.xxx.0001.region.cache.amazonaws.com:port';\nRedisDAL.canConnect(); // true\n</code></pre>\n<p>As demonstrated, type coercion can be a succinct solution, if you intend to test for more than just property existence.</p>\n<h3 id=\"thehasownpropertymethod\">The hasOwnProperty method</h3>\n<p>The accepted standard way to check for property existence in JavaScript, let me demonstrate;</p>\n<pre><code class=\"language-language-javascript\">var RedisDAL = { \n  canConnect: function(){\n    return Object.hasOwnProperty.call(this, 'endpoint');\n  },\n  endpoint: ''\n};\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n</code></pre>\n<p>Whether the value is truthy or otherwise, using <code>hasOwnProperty</code> will consistently report concisely if the object has the property defined.</p>\n<p>There is a gotcha, <code>hasOwnProperty</code> cannot read when the property is inherited through the prototypal chain which can be demonstrated using <code>Object.create</code> below;</p>\n<pre><code class=\"language-language-javascript\">var RedisDAL = Object.create({ \n  canConnect: function(){\n    return Object.hasOwnProperty.call(this, 'endpoint');\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n</code></pre>\n<p>As you can see with the same tests performed the first test reports <code>false</code> because it could not read the property which <code>RedisDAL</code> inherited through its prototype.</p>\n<h3 id=\"withtheinoperator\">With the in operator</h3>\n<p>The only way to be 100% sure that the object and its prototypal inherited properties are checked is using the <code>in</code> operator like so;</p>\n<pre><code class=\"language-language-javascript\">var RedisDAL = { \n  canConnect: function(){\n    return 'endpoint' in this;\n  },\n  endpoint: ''\n};\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n</code></pre>\n<p>And now using <code>Object.create</code> to demonstrate inherited properties also;</p>\n<pre><code class=\"language-language-javascript\">var RedisDAL = Object.create({ \n  canConnect: function(){\n    return 'endpoint' in this;\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n</code></pre>\n<p>Now we can see that both test scenarios are consistently reporting their results, but this may not be the desired use case. You may not wish to validate inherited properties for good reason.</p>\n<h4 id=\"inshort\">In short</h4>\n<p>It should be up to you as a developer to choose the appropriate technique for your particular use case as none are the same and each have specific pro's and con's.<br>\nIf you document your code well the choice to use one over the others, then there should be no reason to have a Pull Request denied. All 3 are as valid as one another regardless of a maintainers personal opinions or code standards (lack-there-of) and styles.</p>\n<!--kg-card-end: markdown-->","comment_id":"25","plaintext":"While maintaining some code recently I found myself writing out of pure habit \nObject.hasOwnProperty.call(obj, 'prop') as is expected in almost all OSS I've\ncontributed to, and quickly editing that to just !!obj.prop which I chose\nbecause it worked more succinctly in my use case but I generally cringe whenever\nI read code using that technique. It had me wondering if the OSS project owners\nwho prefer hasOwnProperty over !! actually understand the differences, because\nI've seen pull requests rejected simply because they used !! and the repo\nmaintainer didn't like the syntax.\n\nI'm going to demonstrate to you 3 ways to check for properties in JavaScript\nObjects and explain how each can be useful in their own unique way and why \nhasOwnProperty isn't always the correct option.\n\nUsing type coercion\nRegarded widely as evil, JavaScript allows us to coerce any value to a boolean\nusing bang bang (!!) which can be troublesome. For the same functionality that\nallows coercion we have the dilemma that with a truthy value we get the expected \ntrue boolean with the type coercion technique, however falsey values will be\ncoerced to a false boolean appropriately, but this isn't useful when simply\nchecking for the existence of an objects property which would also have the\nexpected false, yet the property exists with a falsey value.\n\nvar Obj = { foo: 'bar' };\nfunction _hasFoo() {\n  return !!this.foo;\n}\n_hasFoo.call(Obj, 'foo'); // true\nObj.foo = 0;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = false;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = undefined;\n_hasFoo.call(Obj, 'foo'); // false\ndelete Obj.foo;\n_hasFoo.call(Obj, 'foo'); // false\nObj.foo = true;\n_hasFoo.call(Obj, 'foo'); // true\n\n\nAs you can see in the tests the type coercion technique will not appropriately\nreport that the property exists, so type coercion should never be used in that\nmanner. Instead, consider using type coercion only to test for property\nexistence as well as testing the value is set (as a truthy value).\n\nvar RedisDAL = Object.create({ \n  canConnect: function(){\n    return !!this.endpoint;\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = 'myclustername.xxx.0001.region.cache.amazonaws.com:port';\nRedisDAL.canConnect(); // true\n\n\nAs demonstrated, type coercion can be a succinct solution, if you intend to test\nfor more than just property existence.\n\nThe hasOwnProperty method\nThe accepted standard way to check for property existence in JavaScript, let me\ndemonstrate;\n\nvar RedisDAL = { \n  canConnect: function(){\n    return Object.hasOwnProperty.call(this, 'endpoint');\n  },\n  endpoint: ''\n};\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n\n\nWhether the value is truthy or otherwise, using hasOwnProperty will consistently\nreport concisely if the object has the property defined.\n\nThere is a gotcha, hasOwnProperty cannot read when the property is inherited\nthrough the prototypal chain which can be demonstrated using Object.create \nbelow;\n\nvar RedisDAL = Object.create({ \n  canConnect: function(){\n    return Object.hasOwnProperty.call(this, 'endpoint');\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n\n\nAs you can see with the same tests performed the first test reports false \nbecause it could not read the property which RedisDAL inherited through its\nprototype.\n\nWith the in operator\nThe only way to be 100% sure that the object and its prototypal inherited\nproperties are checked is using the in operator like so;\n\nvar RedisDAL = { \n  canConnect: function(){\n    return 'endpoint' in this;\n  },\n  endpoint: ''\n};\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n\n\nAnd now using Object.create to demonstrate inherited properties also;\n\nvar RedisDAL = Object.create({ \n  canConnect: function(){\n    return 'endpoint' in this;\n  },\n  endpoint: ''\n});\nRedisDAL.canConnect(); // true\nRedisDAL.endpoint = false;\nRedisDAL.canConnect(); // true\ndelete RedisDAL.endpoint;\nRedisDAL.canConnect(); // false\nRedisDAL.endpoint = undefined;\nRedisDAL.canConnect(); // true\n\n\nNow we can see that both test scenarios are consistently reporting their\nresults, but this may not be the desired use case. You may not wish to validate\ninherited properties for good reason.\n\nIn short\nIt should be up to you as a developer to choose the appropriate technique for\nyour particular use case as none are the same and each have specific pro's and\ncon's.\nIf you document your code well the choice to use one over the others, then there\nshould be no reason to have a Pull Request denied. All 3 are as valid as one\nanother regardless of a maintainers personal opinions or code standards\n(lack-there-of) and styles.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-8.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-15 14:35:33","created_by":"1","updated_at":"2021-03-31 14:16:17","updated_by":"1","published_at":"2016-06-16 13:10:18","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9c","uuid":"97b26d47-9049-4f61-ab43-1196c195a9a4","title":"Mobile Web: Cost of JavaScript Frameworks","slug":"cost-of-javascript-frameworks","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In many of my previous posts I've discussed the importance of mobile, and the company where I am employed as the lead software engineer; the users have shown us over the past couple of years that they prefer to visit our site on their mobile over their desktop.\\nIt's clear to me that we need to not only understand that our users want a good mobile experience, we actually need to be proactive in supplying the best experience possible to keep up with users expectations, or the users will go where their expectations are met.\\n\\nA mobile device doesn't have the processing power of desktop or laptop computer, we know this, but do we actually do anything with that knowledge? If you use a JavaScript framework like Polymer, Angular, React, Backbone, or Ember; have you benchmarked it on mobile compared to the desktop experience? If you haven't already, look up benchmarks done already or perform your own. You will find vanilla JavaScript outperforms all frameworks. By an unbelievable margin.\\n\\n> tl;dr Mobile devices are slow, Frameworks are _all_ proven to be slower than vanilla JavaScript. I'm going to discuss how I have made changes to a large scale site to remedy this issue.\\n\\nLibraries can be swapped out and replaced if they prove problematic. Frameworks on the other hand, tend to be a lot more difficult to swap, often requiring a rebuild of the app in question and inherently a lot bigger and under-utilised. \\n\\nI’m concerned by the costs of frameworks on mobile, so I'll focus on frameworks and not libraries.\\n\\nI see the ergonomic benefits in frameworks, but I can’t help but feel that, for many developers, investing in knowledge of the web platform (JavaScript) itself is the best long-term investment. Frameworks come and go, they do contribute ideas and patterns but if you ever find that the one you use no longer works for you, or has a bug that remains unfixed, being able to understand the platform that the framework abstracts will help enormously.\\n\\nYour project code should functionally provide a feature your users interact with, Frameworks only provide developers a tool to start writing that code not the feature itself. \\n\\n> Good developers use frameworks, great developers _understand_ how frameworks abstract the real vanilla functionality.\\n\\nFrameworks are an inversion of control. They control the lifecycle of the app, and give you entry points where your code runs. You’re still responsible for the final code, but you’re not in control. One day you’re just coding, developing your app, and in the console you see a warning telling you that there’s been a deprecation. That often requires figuring out what the latest version of the framework is and what it needs you to do differently, and that may also mean refactoring projects. You have to take time out to learn any new or updated framework: what it means to write _idiomatic_ code for that framework; its patterns, practices, and ways of doing things.\\n\\nThat's more control taken from you by Frameworks, you can't even focus on your app sporadically and those deadlines or stakeholders or clients are calling while you're upgrading, refactoring, and re-learning.\\n\\n> You could be creating features or fixing UX bugs with that valuable time wasted\\n\\n##### But how to avoid frameworks effectively?\\n\\nI'm going to share some tips on how to tackle the benefits frameworks provide while remaining vanilla JavaScript.\\n\\nOne of the most helpful Framework features is abstraction, which is commonly said to mitigate complexity. It does mitigate complexities that arise I admit, yet a lot of complex code I've seen is actually written to circumvent the strangle hold frameworks themselves impose on you. Frameworks aside though, complex code is inevitable and the key to handling complexity is in understanding the difference between abstracting and SoC. Look to design patterns rather than frameworks.\\n\\nAbstraction is for simple, single purpose, reused functionality, it will allow you to be DRY. SoC can be misconstrued, put simply SoC can be DRY and it can be code duplication. Being DRY is not always best practice, and abstraction can be the downfall of a project. \\nComplex functionality should be respected, do not try to abstract it as abstractions are inherently complex due to imperative programming fundamentals and are an unnatural thought process for humans. Respect the complex functionality and do not attempt to make it reusable for the sake of _one day_ it might be reused. Functional programming patterns are great tools for dealing with complexity as it allows a human mind to process concisely what is happening, the real challenge is first learning all of the possible patterns and you will be able to easily identify how to best tackle any coding problem you encounter with the right pattern.\\n\\nThe idea of a design pattern is an attempt to standardize what are already accepted best practices. Effective software design requires considering issues that may not become visible until later in the implementation. Reusing design patterns helps to prevent subtle issues that can cause major problems and improves code readability for coders and architects familiar with the patterns.\\n\\nOften, people only understand how to apply certain software design techniques to certain problems. These techniques are difficult to apply to a broader range of problems. Design patterns provide general solutions, documented in a format that doesn't require specifics tied to a particular problem.\\n\\nIn addition, patterns allow developers to communicate using well-known, well understood names for software interactions. Common design patterns can be improved over time, making them more robust than ad-hoc designs.\\n\\nAll of this is actually how the frameworks are written, yet these frameworks don't provide your product and users any features on their own, whereas you as a developer can cut out the framework entirely by employing the patterns yourself in your own code. \\n\\n> Instead of learning a framework, learn design patterns.\\n\\nIt's been talked about above briefly once or twice, but I strongly believe that all developers would benefit greatly if they invested the same time it took them to learn, refactor, update, re-learn a framework; and instead use that valuable time to learn the platform itself, learn JavaScript. jQuery is not JavaScript, Angular is not JavaScript, any framework or library you've used is not JavaScript. \\nTake the time to learn Web Workers (not to be confused with the awesome new Service Workers), they're more than 10 years old and I doubt you know the first thing about them, they offer multithreading and mitigate everyday issues with laggy animations, golty scrolling, and page lock ups. Learn about prefetching and prerendering, use them if you've read about them. Create a manifest.json and utilise Progressive App features like geo-fencing and Push notifications. Learn about OOLO, prototypal inheritance is a unique and powerful beast, You're still sleeping and you need to be woken up from your nightmare - JavaScript is not evil and not untamable, it is elegant and really powerful when it is used as it was meant to be used, not hacked to be something it isn't meant to be, such as classical OOP, it is not designed for it, it is designed for prototypal programming.\\n\\n#### Looking forward\\n\\nIf we can achieve fast-booting, low-memory, smooth-executing frameworks with good ergonomics we’ll be onto a winner. Basically, the best framework can only be one that is available natively on the platform; I just wish one day the developers who toil at frameworks contribute back to the greater community by enhancing the open web itself.\\n\\nUntil then, for mobile at least, I’ll be sticking to the vanilla web platform.\\n\\n#### Final words\\n\\nLearn pure JavaScript, use prototypes; you will finally see the truth and until then you'll doubt everything written on this page.\\n\\nJust remember, the best way to be the best developer is to learn the underlying platform and all of the design patterns, and hire those whom you know respect the language as you have come to by doing the same.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In many of my previous posts I've discussed the importance of mobile, and the company where I am employed as the lead software engineer; the users have shown us over the past couple of years that they prefer to visit our site on their mobile over their desktop.<br>\nIt's clear to me that we need to not only understand that our users want a good mobile experience, we actually need to be proactive in supplying the best experience possible to keep up with users expectations, or the users will go where their expectations are met.</p>\n<p>A mobile device doesn't have the processing power of desktop or laptop computer, we know this, but do we actually do anything with that knowledge? If you use a JavaScript framework like Polymer, Angular, React, Backbone, or Ember; have you benchmarked it on mobile compared to the desktop experience? If you haven't already, look up benchmarks done already or perform your own. You will find vanilla JavaScript outperforms all frameworks. By an unbelievable margin.</p>\n<blockquote>\n<p>tl;dr Mobile devices are slow, Frameworks are <em>all</em> proven to be slower than vanilla JavaScript. I'm going to discuss how I have made changes to a large scale site to remedy this issue.</p>\n</blockquote>\n<p>Libraries can be swapped out and replaced if they prove problematic. Frameworks on the other hand, tend to be a lot more difficult to swap, often requiring a rebuild of the app in question and inherently a lot bigger and under-utilised.</p>\n<p>I’m concerned by the costs of frameworks on mobile, so I'll focus on frameworks and not libraries.</p>\n<p>I see the ergonomic benefits in frameworks, but I can’t help but feel that, for many developers, investing in knowledge of the web platform (JavaScript) itself is the best long-term investment. Frameworks come and go, they do contribute ideas and patterns but if you ever find that the one you use no longer works for you, or has a bug that remains unfixed, being able to understand the platform that the framework abstracts will help enormously.</p>\n<p>Your project code should functionally provide a feature your users interact with, Frameworks only provide developers a tool to start writing that code not the feature itself.</p>\n<blockquote>\n<p>Good developers use frameworks, great developers <em>understand</em> how frameworks abstract the real vanilla functionality.</p>\n</blockquote>\n<p>Frameworks are an inversion of control. They control the lifecycle of the app, and give you entry points where your code runs. You’re still responsible for the final code, but you’re not in control. One day you’re just coding, developing your app, and in the console you see a warning telling you that there’s been a deprecation. That often requires figuring out what the latest version of the framework is and what it needs you to do differently, and that may also mean refactoring projects. You have to take time out to learn any new or updated framework: what it means to write <em>idiomatic</em> code for that framework; its patterns, practices, and ways of doing things.</p>\n<p>That's more control taken from you by Frameworks, you can't even focus on your app sporadically and those deadlines or stakeholders or clients are calling while you're upgrading, refactoring, and re-learning.</p>\n<blockquote>\n<p>You could be creating features or fixing UX bugs with that valuable time wasted</p>\n</blockquote>\n<h5 id=\"buthowtoavoidframeworkseffectively\">But how to avoid frameworks effectively?</h5>\n<p>I'm going to share some tips on how to tackle the benefits frameworks provide while remaining vanilla JavaScript.</p>\n<p>One of the most helpful Framework features is abstraction, which is commonly said to mitigate complexity. It does mitigate complexities that arise I admit, yet a lot of complex code I've seen is actually written to circumvent the strangle hold frameworks themselves impose on you. Frameworks aside though, complex code is inevitable and the key to handling complexity is in understanding the difference between abstracting and SoC. Look to design patterns rather than frameworks.</p>\n<p>Abstraction is for simple, single purpose, reused functionality, it will allow you to be DRY. SoC can be misconstrued, put simply SoC can be DRY and it can be code duplication. Being DRY is not always best practice, and abstraction can be the downfall of a project.<br>\nComplex functionality should be respected, do not try to abstract it as abstractions are inherently complex due to imperative programming fundamentals and are an unnatural thought process for humans. Respect the complex functionality and do not attempt to make it reusable for the sake of <em>one day</em> it might be reused. Functional programming patterns are great tools for dealing with complexity as it allows a human mind to process concisely what is happening, the real challenge is first learning all of the possible patterns and you will be able to easily identify how to best tackle any coding problem you encounter with the right pattern.</p>\n<p>The idea of a design pattern is an attempt to standardize what are already accepted best practices. Effective software design requires considering issues that may not become visible until later in the implementation. Reusing design patterns helps to prevent subtle issues that can cause major problems and improves code readability for coders and architects familiar with the patterns.</p>\n<p>Often, people only understand how to apply certain software design techniques to certain problems. These techniques are difficult to apply to a broader range of problems. Design patterns provide general solutions, documented in a format that doesn't require specifics tied to a particular problem.</p>\n<p>In addition, patterns allow developers to communicate using well-known, well understood names for software interactions. Common design patterns can be improved over time, making them more robust than ad-hoc designs.</p>\n<p>All of this is actually how the frameworks are written, yet these frameworks don't provide your product and users any features on their own, whereas you as a developer can cut out the framework entirely by employing the patterns yourself in your own code.</p>\n<blockquote>\n<p>Instead of learning a framework, learn design patterns.</p>\n</blockquote>\n<p>It's been talked about above briefly once or twice, but I strongly believe that all developers would benefit greatly if they invested the same time it took them to learn, refactor, update, re-learn a framework; and instead use that valuable time to learn the platform itself, learn JavaScript. jQuery is not JavaScript, Angular is not JavaScript, any framework or library you've used is not JavaScript.<br>\nTake the time to learn Web Workers (not to be confused with the awesome new Service Workers), they're more than 10 years old and I doubt you know the first thing about them, they offer multithreading and mitigate everyday issues with laggy animations, golty scrolling, and page lock ups. Learn about prefetching and prerendering, use them if you've read about them. Create a manifest.json and utilise Progressive App features like geo-fencing and Push notifications. Learn about OOLO, prototypal inheritance is a unique and powerful beast, You're still sleeping and you need to be woken up from your nightmare - JavaScript is not evil and not untamable, it is elegant and really powerful when it is used as it was meant to be used, not hacked to be something it isn't meant to be, such as classical OOP, it is not designed for it, it is designed for prototypal programming.</p>\n<h4 id=\"lookingforward\">Looking forward</h4>\n<p>If we can achieve fast-booting, low-memory, smooth-executing frameworks with good ergonomics we’ll be onto a winner. Basically, the best framework can only be one that is available natively on the platform; I just wish one day the developers who toil at frameworks contribute back to the greater community by enhancing the open web itself.</p>\n<p>Until then, for mobile at least, I’ll be sticking to the vanilla web platform.</p>\n<h4 id=\"finalwords\">Final words</h4>\n<p>Learn pure JavaScript, use prototypes; you will finally see the truth and until then you'll doubt everything written on this page.</p>\n<p>Just remember, the best way to be the best developer is to learn the underlying platform and all of the design patterns, and hire those whom you know respect the language as you have come to by doing the same.</p>\n<!--kg-card-end: markdown-->","comment_id":"26","plaintext":"In many of my previous posts I've discussed the importance of mobile, and the\ncompany where I am employed as the lead software engineer; the users have shown\nus over the past couple of years that they prefer to visit our site on their\nmobile over their desktop.\nIt's clear to me that we need to not only understand that our users want a good\nmobile experience, we actually need to be proactive in supplying the best\nexperience possible to keep up with users expectations, or the users will go\nwhere their expectations are met.\n\nA mobile device doesn't have the processing power of desktop or laptop computer,\nwe know this, but do we actually do anything with that knowledge? If you use a\nJavaScript framework like Polymer, Angular, React, Backbone, or Ember; have you\nbenchmarked it on mobile compared to the desktop experience? If you haven't\nalready, look up benchmarks done already or perform your own. You will find\nvanilla JavaScript outperforms all frameworks. By an unbelievable margin.\n\n> tl;dr Mobile devices are slow, Frameworks are all proven to be slower than\nvanilla JavaScript. I'm going to discuss how I have made changes to a large\nscale site to remedy this issue.\n\n\nLibraries can be swapped out and replaced if they prove problematic. Frameworks\non the other hand, tend to be a lot more difficult to swap, often requiring a\nrebuild of the app in question and inherently a lot bigger and under-utilised.\n\nI’m concerned by the costs of frameworks on mobile, so I'll focus on frameworks\nand not libraries.\n\nI see the ergonomic benefits in frameworks, but I can’t help but feel that, for\nmany developers, investing in knowledge of the web platform (JavaScript) itself\nis the best long-term investment. Frameworks come and go, they do contribute\nideas and patterns but if you ever find that the one you use no longer works for\nyou, or has a bug that remains unfixed, being able to understand the platform\nthat the framework abstracts will help enormously.\n\nYour project code should functionally provide a feature your users interact\nwith, Frameworks only provide developers a tool to start writing that code not\nthe feature itself.\n\n> Good developers use frameworks, great developers understand how frameworks\nabstract the real vanilla functionality.\n\n\nFrameworks are an inversion of control. They control the lifecycle of the app,\nand give you entry points where your code runs. You’re still responsible for the\nfinal code, but you’re not in control. One day you’re just coding, developing\nyour app, and in the console you see a warning telling you that there’s been a\ndeprecation. That often requires figuring out what the latest version of the\nframework is and what it needs you to do differently, and that may also mean\nrefactoring projects. You have to take time out to learn any new or updated\nframework: what it means to write idiomatic code for that framework; its\npatterns, practices, and ways of doing things.\n\nThat's more control taken from you by Frameworks, you can't even focus on your\napp sporadically and those deadlines or stakeholders or clients are calling\nwhile you're upgrading, refactoring, and re-learning.\n\n> You could be creating features or fixing UX bugs with that valuable time wasted\n\n\nBut how to avoid frameworks effectively?\nI'm going to share some tips on how to tackle the benefits frameworks provide\nwhile remaining vanilla JavaScript.\n\nOne of the most helpful Framework features is abstraction, which is commonly\nsaid to mitigate complexity. It does mitigate complexities that arise I admit,\nyet a lot of complex code I've seen is actually written to circumvent the\nstrangle hold frameworks themselves impose on you. Frameworks aside though,\ncomplex code is inevitable and the key to handling complexity is in\nunderstanding the difference between abstracting and SoC. Look to design\npatterns rather than frameworks.\n\nAbstraction is for simple, single purpose, reused functionality, it will allow\nyou to be DRY. SoC can be misconstrued, put simply SoC can be DRY and it can be\ncode duplication. Being DRY is not always best practice, and abstraction can be\nthe downfall of a project.\nComplex functionality should be respected, do not try to abstract it as\nabstractions are inherently complex due to imperative programming fundamentals\nand are an unnatural thought process for humans. Respect the complex\nfunctionality and do not attempt to make it reusable for the sake of one day it\nmight be reused. Functional programming patterns are great tools for dealing\nwith complexity as it allows a human mind to process concisely what is\nhappening, the real challenge is first learning all of the possible patterns and\nyou will be able to easily identify how to best tackle any coding problem you\nencounter with the right pattern.\n\nThe idea of a design pattern is an attempt to standardize what are already\naccepted best practices. Effective software design requires considering issues\nthat may not become visible until later in the implementation. Reusing design\npatterns helps to prevent subtle issues that can cause major problems and\nimproves code readability for coders and architects familiar with the patterns.\n\nOften, people only understand how to apply certain software design techniques to\ncertain problems. These techniques are difficult to apply to a broader range of\nproblems. Design patterns provide general solutions, documented in a format that\ndoesn't require specifics tied to a particular problem.\n\nIn addition, patterns allow developers to communicate using well-known, well\nunderstood names for software interactions. Common design patterns can be\nimproved over time, making them more robust than ad-hoc designs.\n\nAll of this is actually how the frameworks are written, yet these frameworks\ndon't provide your product and users any features on their own, whereas you as a\ndeveloper can cut out the framework entirely by employing the patterns yourself\nin your own code.\n\n> Instead of learning a framework, learn design patterns.\n\n\nIt's been talked about above briefly once or twice, but I strongly believe that\nall developers would benefit greatly if they invested the same time it took them\nto learn, refactor, update, re-learn a framework; and instead use that valuable\ntime to learn the platform itself, learn JavaScript. jQuery is not JavaScript,\nAngular is not JavaScript, any framework or library you've used is not\nJavaScript.\nTake the time to learn Web Workers (not to be confused with the awesome new\nService Workers), they're more than 10 years old and I doubt you know the first\nthing about them, they offer multithreading and mitigate everyday issues with\nlaggy animations, golty scrolling, and page lock ups. Learn about prefetching\nand prerendering, use them if you've read about them. Create a manifest.json and\nutilise Progressive App features like geo-fencing and Push notifications. Learn\nabout OOLO, prototypal inheritance is a unique and powerful beast, You're still\nsleeping and you need to be woken up from your nightmare - JavaScript is not\nevil and not untamable, it is elegant and really powerful when it is used as it\nwas meant to be used, not hacked to be something it isn't meant to be, such as\nclassical OOP, it is not designed for it, it is designed for prototypal\nprogramming.\n\nLooking forward\nIf we can achieve fast-booting, low-memory, smooth-executing frameworks with\ngood ergonomics we’ll be onto a winner. Basically, the best framework can only\nbe one that is available natively on the platform; I just wish one day the\ndevelopers who toil at frameworks contribute back to the greater community by\nenhancing the open web itself.\n\nUntil then, for mobile at least, I’ll be sticking to the vanilla web platform.\n\nFinal words\nLearn pure JavaScript, use prototypes; you will finally see the truth and until\nthen you'll doubt everything written on this page.\n\nJust remember, the best way to be the best developer is to learn the underlying\nplatform and all of the design patterns, and hire those whom you know respect\nthe language as you have come to by doing the same.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-3.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-19 02:31:53","created_by":"1","updated_at":"2021-03-31 14:15:35","updated_by":"1","published_at":"2016-06-19 11:28:32","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9d","uuid":"b3c6915d-2a55-4b1a-93c2-a9ed5adf6c65","title":"Unicode source code in PHP","slug":"unicode-source-code-in-php","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Yeah, php can be evil.\\n\\n```language-php\\n<?php\\nnew class {\\n      static $foo = 1;\\n      static $f​oo = 2;\\n      static $fo​o = 3;\\n      public function __construct() {\\n         echo self::$foo; // 1\\n         echo self::$f​oo; // 2\\n         echo self::$fo​o; // 3\\n      }\\n   };\\n```\\n\\nThis is actually;\\n\\n```\\n<?php\\nnew class {\\n      static $foo = 1;\\n      static $fu+200b​oo = 2;\\n      static $fou+200b​o = 3;\\n      public function __construct() {\\n         echo self::$foo;\\n         echo self::$fu+200b​oo;\\n         echo self::$fo​u+200bo;\\n      }\\n   };\\n```\\n\\nWelcome to the era of unicode source code..\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Yeah, php can be evil.</p>\n<pre><code class=\"language-language-php\">&lt;?php\nnew class {\n      static $foo = 1;\n      static $f​oo = 2;\n      static $fo​o = 3;\n      public function __construct() {\n         echo self::$foo; // 1\n         echo self::$f​oo; // 2\n         echo self::$fo​o; // 3\n      }\n   };\n</code></pre>\n<p>This is actually;</p>\n<pre><code>&lt;?php\nnew class {\n      static $foo = 1;\n      static $fu+200b​oo = 2;\n      static $fou+200b​o = 3;\n      public function __construct() {\n         echo self::$foo;\n         echo self::$fu+200b​oo;\n         echo self::$fo​u+200bo;\n      }\n   };\n</code></pre>\n<p>Welcome to the era of unicode source code..</p>\n<!--kg-card-end: markdown-->","comment_id":"27","plaintext":"Yeah, php can be evil.\n\n<?php\nnew class {\n      static $foo = 1;\n      static $f​oo = 2;\n      static $fo​o = 3;\n      public function __construct() {\n         echo self::$foo; // 1\n         echo self::$f​oo; // 2\n         echo self::$fo​o; // 3\n      }\n   };\n\n\nThis is actually;\n\n<?php\nnew class {\n      static $foo = 1;\n      static $fu+200b​oo = 2;\n      static $fou+200b​o = 3;\n      public function __construct() {\n         echo self::$foo;\n         echo self::$fu+200b​oo;\n         echo self::$fo​u+200bo;\n      }\n   };\n\n\nWelcome to the era of unicode source code..","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-11.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-19 22:21:34","created_by":"1","updated_at":"2021-03-31 14:15:10","updated_by":"1","published_at":"2016-06-19 22:24:28","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9e","uuid":"c04328d7-d681-443d-a978-b58ecdd73ad3","title":"Functional Programing with PHP7","slug":"php-functional-programing","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Many developers like to talk about functional programming, but most have never written a working implementation, nor understand the concepts practically without putting in the practice itself. The reason is quite simple: we are taught to think in an imperative manner when we first start learning to program and any developer or engineer who thinks highly of themselves refuse to accept they need to actually forget what they think they know and take the time to challenge themselves and re-learn everything again from scratch.\\n\\nI'm proud to know the developers I've hired are open-minded, humble, and thirsty for new challenges! And I expect nothing less of myself.\\n\\n### So what is Functional Programming?\\n\\nOther than being the method in which a regular human brain computes, functional programming streamlines the applications logic around abstractions such as higher order functions which are inherently stateless and allow you to be as DRY as possible while remaining readable and easy to follow with your logic. \\n\\n> In short; Functional Programming allows for complexities to be trivial to follow.\\n\\nBecause your abstractions are stateless it is up to your main application logic to maintain a certain clarity over what you would normally expect as a 'state', because essentially Functional Programming is stateless too. Instead of maintaining a central state for each lifecycle of your processing you would instead pass the state along the chain of processing in an ever forward momentum of executions.\\n\\nAlthough Functional Programming itself is only the main application logic PHP however is Imperative fundamentally so it is our job to create the abstractions that allow us to be as functional as possible with our main application logic.\\n\\n> Recursion = PHP without imperative programming\\n\\nWe'll also see a rise in lambda or anonymous function usage (i.e. closures; to be covered in detail further on) as we will be expecting all of our functions to take a function as a parameter, return a function, or both.\\n\\nThis will lead you to rely less on OOP in general and rather focus abstractions on more functional friendly collections, factories, and yes singletons too.\\n\\nIf you aren't already familiar with the concepts of general OOP design patterns, in particular collections and factories, now is the time to learn them as they will make your abstractions possible and therefore provide what is needed to create a truly functional application.\\n\\n### Recursion\\n\\nFunctional PHP would not include any loops, foreach, for, while, do while, are all taken away from you and replaced with the concept of recursion.\\n\\nSuppose we want to write a function to find the sum of all elements in an array (forget that `array_sum` exists for the time being). In a functional style, we would write;\\n\\n```language-php\\nfunction sum($array) {\\n  if (empty($array)) {\\n    return 0;\\n  } else {\\n    return $array[0] + sum(array_slice($array, 1));\\n  }\\n}\\n\\n$total = sum([1, 2, 3]); // 6\\n```\\n\\nAn empty list will return 0, which is our base condition. For an array with more than one value, it will return the results of adding the first element with the recursive sum of all other elements.\\n\\n> Fun fact: I included this in the [Punters.com.au](https://www.punters.com.au/) coding challenge and to date no candidate has demonstrated functional programming\\n\\n### Functional building blocks\\n\\nThere are tools you must first provide PHP with before you can get started employing Functional Programming.\\n\\nI've put the entire project on GitHub for you, It is production ready.\\n\\n<blockquote class=\\\"embedly-card\\\" data-card-key=\\\"3b2010fca1a74d2496154aad6248f9cc\\\" data-card-theme=\\\"dark\\\" data-card-branding=\\\"0\\\" data-card-type=\\\"article\\\"><h4><a href=\\\"https://github.com/chrisdlangton/php-functional-programming\\\">chrisdlangton/php-functional-programming</a></h4><p>php-functional-programming - The Building Blocks for PHP to be able to provide true Functional Programming</p></blockquote>\\n\\nWe'll start with a higher order function called `array_each` which abstracts looping and allows you to perform your usually looped actions when a collection or return value isn't required and explain how it is different from the `array_map` function that does return a value.\\n\\n```language-php\\nfunction array_each(array $items, $func): void {\\n  foreach ($items as $key => $item) {\\n    $func($item, $key);\\n  }\\n}\\n\\n$users = [\\n  [\\n    \\\"id\\\" => \\\"1\\\",\\n    \\\"username\\\" => \\\"John\\\",\\n    \\\"email\\\" => \\\"john@domain.tld\\\"\\n  ],[\\n    \\\"id\\\" => \\\"2\\\",\\n    \\\"username\\\" => \\\"Jane\\\",\\n    \\\"email\\\" => \\\"jane@domain.tld\\\"\\n  ]\\n];\\n\\narray_each($users, function($user) {\\n  emailTo($user['email'], 'Welcome to my blog')\\n});\\n```\\n\\nYou can see clearly that the `array_each` function provides no return value as it iterates, it's purpose is to consume the data and do something with each item it finds - brilliant!\\n\\nIf you intend to extract certain information for later use as a collection, perhaps a list of username's, you would instead need to utilise `array_map` like so;\\n\\n```language-php\\nfunction array_map(array $items, $func): array {\\n  $results = [];\\n  foreach ($items as $key => $item) {\\n    $results[] = $func($item, $key);\\n  }\\n  return $results;\\n}\\n\\n$users = [\\n  [\\n    \\\"id\\\" => \\\"1\\\",\\n    \\\"username\\\" => \\\"John\\\",\\n    \\\"email\\\" => \\\"john@domain.tld\\\"\\n  ],[\\n    \\\"id\\\" => \\\"2\\\",\\n    \\\"username\\\" => \\\"Jane\\\",\\n    \\\"email\\\" => \\\"jane@domain.tld\\\"\\n  ]\\n];\\n\\n$usernames = array_map($users, function($user){\\n  return $user['username'];\\n});\\n\\nprint( implode(', ', $usernames) ); // John, Jane\\n```\\n\\nIn your implementation you'd have these higher order functions referenced by your script and therefore you'd only be writing the 3 lines of usage, clearly functional is easier to read #amiright\\n\\n### More advanced examples\\n\\nLooking deep at these implementations you might already see that we have much more expressive power than has been demonstrated. For example, you can use `array_each` and `array_map` on multidimensional arrays as well as make use of PHP's `use` on closures and of course pass into your closure scope another variable by reference if need be - to demonstrate;\\n\\n```language-php\\n$data = [\\n  \\\"foo\\\" = \\\"bar\\\",\\n  \\\"baz\\\" = true,\\n  \\\"users\\\" => [\\n    [\\n      \\\"id\\\" => \\\"1\\\",\\n      \\\"username\\\" => \\\"John\\\",\\n      \\\"email\\\" => \\\"john@domain.tld\\\"\\n    ],[\\n      \\\"id\\\" => \\\"2\\\",\\n      \\\"username\\\" => \\\"Jane\\\",\\n      \\\"email\\\" => \\\"jane@domain.tld\\\"\\n    ]\\n  ]\\n];\\n\\n$usernames = [];\\narray_each($data, function($item, $key) use (&$usernames) {\\n  if ($key === 'users') {\\n    $usernames = array_map($item['users'], function($user) {\\n      return $user['username'];\\n    });\\n  }\\n});\\n\\nprint( implode(', ', $usernames) ); // John, Jane\\n```\\nIn PHP functions can be passed as arguments to other functions and a function can return other functions \\u0001(a feature called higher-order functions). Both user-defined and built-in functions can be referenced by a variable and invoked dynamically.\\n\\n### On higher-order functions\\n\\nThe most common usage of higher-order functions is when implementing a strategy pattern. \\n\\n```language-php\\n$input = [1, 2, 3, 4, 5, 6];\\n\\n$filter_even = function($item) {\\n    return ($item % 2) == 0;\\n};\\n\\n$output = array_filter($input, $filter_even);\\n\\nprint_r( implode(', ', $output) ); // 2, 4, 6\\n```\\n\\nI've mentioned closures a few times and I'll take the time now to explain them so you can see for yourself their usefulness, actually, their importance to implement functional programming.\\nA closure is an anonymous function that can access variables imported from the outside scope without using any global variables. Basicly, a closure is a function with some arguments fixed (closed) by the environment when it is defined. Closures can work around variable scope restrictions in a clean way readable way that the human can understand just by glancing at the code, a core point of employing functional programming.\\n\\nIn the next example we use closures to define a higher-order function returning a single filtering function, to be used by PHP's `array_filter`;\\n\\n```language-php\\n$greater_than = function($min) {\\n    return function($item) use ($min) {\\n        return $item > $min;\\n    };\\n};\\n\\n$input = [1, 2, 3, 4, 5, 6];\\n$output = array_filter($input, $greater_than(3));\\n\\nprint_r( implode(', ', $output) ); // 4, 5, 6\\n```\\n\\nEarly binding is used by default for importing $min variable into the created function. For true closures with late binding one should use a reference when importing. Imagine a templating or input validation library, where closure is defined to capture variables in scope and access them later when the anonymous function is evaluated.\\n\\n> The fun stuff; Immutability\\n\\n### Immutability\\n\\nNot so fun in PHP actually. \\nImmutability is the behavior that a value of a variable cannot be changed once it is defined. Different languages have different ways of achieving this; in PHP, for example, the only way to make a variable immutable is to define it as a constant.\\n\\n> An immutable object cannot be modified by accident\\n\\nThis also has an extra benefit of making testing easier because complex objects do not need to be rebuilt. Because all objects are safe from modification by default, and all changes are explicit, a significant amount of cognitive load can be reduced by not having to follow complex method chains to see where changes occur.\\n\\nPHP is fairly unique among popular programming languages in that the array type can be used in so many ways. All of the following are arrays in PHP;\\n\\n```language-php\\n$dictionary = [\\n    'rabbit' => 'fun pet',\\n    'szechuan' => 'spicy deliciousness',\\n    'php7' => 'not php as you know it'\\n];\\n$orderedList = ['amazing', 'functional', 'functional', 'programming'];\\n$unorderedList = [0, 8, 2, 32, 4];\\n$set = ['total war', 'sins of a solar empire', '0 AD'];\\n```\\n\\nData type structures:\\n\\n- **Dictionary**, an associative array that consists of a key and a value.\\n- **OrderedList**, a list that contains any number of values in a defined order.\\n- **UnorderedList**, a list that contains any number of values in an undefined order.\\n- **Set**, a set of values that contains no duplicates.\\n\\nWhile the PHP approach provides a great amount of flexibility it also makes it more difficult to validate data that is being passed through an application. By using more strict data structures, we can avoid these problems and have more testable code. Anyone can assign a value in an unexpected way with new functionality given an array is representative of these 4 structures.\\n\\nIn my repo you'll find these data type structures and several building blocks needed to kick start your dive into Functional Programming with PHP;\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/add6e1d5769b26b687548ea2776d78fe.js\\\"></script>\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Many developers like to talk about functional programming, but most have never written a working implementation, nor understand the concepts practically without putting in the practice itself. The reason is quite simple: we are taught to think in an imperative manner when we first start learning to program and any developer or engineer who thinks highly of themselves refuse to accept they need to actually forget what they think they know and take the time to challenge themselves and re-learn everything again from scratch.</p>\n<p>I'm proud to know the developers I've hired are open-minded, humble, and thirsty for new challenges! And I expect nothing less of myself.</p>\n<h3 id=\"sowhatisfunctionalprogramming\">So what is Functional Programming?</h3>\n<p>Other than being the method in which a regular human brain computes, functional programming streamlines the applications logic around abstractions such as higher order functions which are inherently stateless and allow you to be as DRY as possible while remaining readable and easy to follow with your logic.</p>\n<blockquote>\n<p>In short; Functional Programming allows for complexities to be trivial to follow.</p>\n</blockquote>\n<p>Because your abstractions are stateless it is up to your main application logic to maintain a certain clarity over what you would normally expect as a 'state', because essentially Functional Programming is stateless too. Instead of maintaining a central state for each lifecycle of your processing you would instead pass the state along the chain of processing in an ever forward momentum of executions.</p>\n<p>Although Functional Programming itself is only the main application logic PHP however is Imperative fundamentally so it is our job to create the abstractions that allow us to be as functional as possible with our main application logic.</p>\n<blockquote>\n<p>Recursion = PHP without imperative programming</p>\n</blockquote>\n<p>We'll also see a rise in lambda or anonymous function usage (i.e. closures; to be covered in detail further on) as we will be expecting all of our functions to take a function as a parameter, return a function, or both.</p>\n<p>This will lead you to rely less on OOP in general and rather focus abstractions on more functional friendly collections, factories, and yes singletons too.</p>\n<p>If you aren't already familiar with the concepts of general OOP design patterns, in particular collections and factories, now is the time to learn them as they will make your abstractions possible and therefore provide what is needed to create a truly functional application.</p>\n<h3 id=\"recursion\">Recursion</h3>\n<p>Functional PHP would not include any loops, foreach, for, while, do while, are all taken away from you and replaced with the concept of recursion.</p>\n<p>Suppose we want to write a function to find the sum of all elements in an array (forget that <code>array_sum</code> exists for the time being). In a functional style, we would write;</p>\n<pre><code class=\"language-language-php\">function sum($array) {\n  if (empty($array)) {\n    return 0;\n  } else {\n    return $array[0] + sum(array_slice($array, 1));\n  }\n}\n\n$total = sum([1, 2, 3]); // 6\n</code></pre>\n<p>An empty list will return 0, which is our base condition. For an array with more than one value, it will return the results of adding the first element with the recursive sum of all other elements.</p>\n<blockquote>\n<p>Fun fact: I included this in the <a href=\"https://www.punters.com.au/\">Punters.com.au</a> coding challenge and to date no candidate has demonstrated functional programming</p>\n</blockquote>\n<h3 id=\"functionalbuildingblocks\">Functional building blocks</h3>\n<p>There are tools you must first provide PHP with before you can get started employing Functional Programming.</p>\n<p>I've put the entire project on GitHub for you, It is production ready.</p>\n<blockquote class=\"embedly-card\" data-card-key=\"3b2010fca1a74d2496154aad6248f9cc\" data-card-theme=\"dark\" data-card-branding=\"0\" data-card-type=\"article\"><h4><a href=\"https://github.com/chrisdlangton/php-functional-programming\">chrisdlangton/php-functional-programming</a></h4><p>php-functional-programming - The Building Blocks for PHP to be able to provide true Functional Programming</p></blockquote>\n<p>We'll start with a higher order function called <code>array_each</code> which abstracts looping and allows you to perform your usually looped actions when a collection or return value isn't required and explain how it is different from the <code>array_map</code> function that does return a value.</p>\n<pre><code class=\"language-language-php\">function array_each(array $items, $func): void {\n  foreach ($items as $key =&gt; $item) {\n    $func($item, $key);\n  }\n}\n\n$users = [\n  [\n    &quot;id&quot; =&gt; &quot;1&quot;,\n    &quot;username&quot; =&gt; &quot;John&quot;,\n    &quot;email&quot; =&gt; &quot;john@domain.tld&quot;\n  ],[\n    &quot;id&quot; =&gt; &quot;2&quot;,\n    &quot;username&quot; =&gt; &quot;Jane&quot;,\n    &quot;email&quot; =&gt; &quot;jane@domain.tld&quot;\n  ]\n];\n\narray_each($users, function($user) {\n  emailTo($user['email'], 'Welcome to my blog')\n});\n</code></pre>\n<p>You can see clearly that the <code>array_each</code> function provides no return value as it iterates, it's purpose is to consume the data and do something with each item it finds - brilliant!</p>\n<p>If you intend to extract certain information for later use as a collection, perhaps a list of username's, you would instead need to utilise <code>array_map</code> like so;</p>\n<pre><code class=\"language-language-php\">function array_map(array $items, $func): array {\n  $results = [];\n  foreach ($items as $key =&gt; $item) {\n    $results[] = $func($item, $key);\n  }\n  return $results;\n}\n\n$users = [\n  [\n    &quot;id&quot; =&gt; &quot;1&quot;,\n    &quot;username&quot; =&gt; &quot;John&quot;,\n    &quot;email&quot; =&gt; &quot;john@domain.tld&quot;\n  ],[\n    &quot;id&quot; =&gt; &quot;2&quot;,\n    &quot;username&quot; =&gt; &quot;Jane&quot;,\n    &quot;email&quot; =&gt; &quot;jane@domain.tld&quot;\n  ]\n];\n\n$usernames = array_map($users, function($user){\n  return $user['username'];\n});\n\nprint( implode(', ', $usernames) ); // John, Jane\n</code></pre>\n<p>In your implementation you'd have these higher order functions referenced by your script and therefore you'd only be writing the 3 lines of usage, clearly functional is easier to read #amiright</p>\n<h3 id=\"moreadvancedexamples\">More advanced examples</h3>\n<p>Looking deep at these implementations you might already see that we have much more expressive power than has been demonstrated. For example, you can use <code>array_each</code> and <code>array_map</code> on multidimensional arrays as well as make use of PHP's <code>use</code> on closures and of course pass into your closure scope another variable by reference if need be - to demonstrate;</p>\n<pre><code class=\"language-language-php\">$data = [\n  &quot;foo&quot; = &quot;bar&quot;,\n  &quot;baz&quot; = true,\n  &quot;users&quot; =&gt; [\n    [\n      &quot;id&quot; =&gt; &quot;1&quot;,\n      &quot;username&quot; =&gt; &quot;John&quot;,\n      &quot;email&quot; =&gt; &quot;john@domain.tld&quot;\n    ],[\n      &quot;id&quot; =&gt; &quot;2&quot;,\n      &quot;username&quot; =&gt; &quot;Jane&quot;,\n      &quot;email&quot; =&gt; &quot;jane@domain.tld&quot;\n    ]\n  ]\n];\n\n$usernames = [];\narray_each($data, function($item, $key) use (&amp;$usernames) {\n  if ($key === 'users') {\n    $usernames = array_map($item['users'], function($user) {\n      return $user['username'];\n    });\n  }\n});\n\nprint( implode(', ', $usernames) ); // John, Jane\n</code></pre>\n<p>In PHP functions can be passed as arguments to other functions and a function can return other functions \u0001(a feature called higher-order functions). Both user-defined and built-in functions can be referenced by a variable and invoked dynamically.</p>\n<h3 id=\"onhigherorderfunctions\">On higher-order functions</h3>\n<p>The most common usage of higher-order functions is when implementing a strategy pattern.</p>\n<pre><code class=\"language-language-php\">$input = [1, 2, 3, 4, 5, 6];\n\n$filter_even = function($item) {\n    return ($item % 2) == 0;\n};\n\n$output = array_filter($input, $filter_even);\n\nprint_r( implode(', ', $output) ); // 2, 4, 6\n</code></pre>\n<p>I've mentioned closures a few times and I'll take the time now to explain them so you can see for yourself their usefulness, actually, their importance to implement functional programming.<br>\nA closure is an anonymous function that can access variables imported from the outside scope without using any global variables. Basicly, a closure is a function with some arguments fixed (closed) by the environment when it is defined. Closures can work around variable scope restrictions in a clean way readable way that the human can understand just by glancing at the code, a core point of employing functional programming.</p>\n<p>In the next example we use closures to define a higher-order function returning a single filtering function, to be used by PHP's <code>array_filter</code>;</p>\n<pre><code class=\"language-language-php\">$greater_than = function($min) {\n    return function($item) use ($min) {\n        return $item &gt; $min;\n    };\n};\n\n$input = [1, 2, 3, 4, 5, 6];\n$output = array_filter($input, $greater_than(3));\n\nprint_r( implode(', ', $output) ); // 4, 5, 6\n</code></pre>\n<p>Early binding is used by default for importing $min variable into the created function. For true closures with late binding one should use a reference when importing. Imagine a templating or input validation library, where closure is defined to capture variables in scope and access them later when the anonymous function is evaluated.</p>\n<blockquote>\n<p>The fun stuff; Immutability</p>\n</blockquote>\n<h3 id=\"immutability\">Immutability</h3>\n<p>Not so fun in PHP actually.<br>\nImmutability is the behavior that a value of a variable cannot be changed once it is defined. Different languages have different ways of achieving this; in PHP, for example, the only way to make a variable immutable is to define it as a constant.</p>\n<blockquote>\n<p>An immutable object cannot be modified by accident</p>\n</blockquote>\n<p>This also has an extra benefit of making testing easier because complex objects do not need to be rebuilt. Because all objects are safe from modification by default, and all changes are explicit, a significant amount of cognitive load can be reduced by not having to follow complex method chains to see where changes occur.</p>\n<p>PHP is fairly unique among popular programming languages in that the array type can be used in so many ways. All of the following are arrays in PHP;</p>\n<pre><code class=\"language-language-php\">$dictionary = [\n    'rabbit' =&gt; 'fun pet',\n    'szechuan' =&gt; 'spicy deliciousness',\n    'php7' =&gt; 'not php as you know it'\n];\n$orderedList = ['amazing', 'functional', 'functional', 'programming'];\n$unorderedList = [0, 8, 2, 32, 4];\n$set = ['total war', 'sins of a solar empire', '0 AD'];\n</code></pre>\n<p>Data type structures:</p>\n<ul>\n<li><strong>Dictionary</strong>, an associative array that consists of a key and a value.</li>\n<li><strong>OrderedList</strong>, a list that contains any number of values in a defined order.</li>\n<li><strong>UnorderedList</strong>, a list that contains any number of values in an undefined order.</li>\n<li><strong>Set</strong>, a set of values that contains no duplicates.</li>\n</ul>\n<p>While the PHP approach provides a great amount of flexibility it also makes it more difficult to validate data that is being passed through an application. By using more strict data structures, we can avoid these problems and have more testable code. Anyone can assign a value in an unexpected way with new functionality given an array is representative of these 4 structures.</p>\n<p>In my repo you'll find these data type structures and several building blocks needed to kick start your dive into Functional Programming with PHP;</p>\n<script src=\"https://gist.github.com/chrisdlangton/add6e1d5769b26b687548ea2776d78fe.js\"></script><!--kg-card-end: markdown-->","comment_id":"28","plaintext":"Many developers like to talk about functional programming, but most have never\nwritten a working implementation, nor understand the concepts practically\nwithout putting in the practice itself. The reason is quite simple: we are\ntaught to think in an imperative manner when we first start learning to program\nand any developer or engineer who thinks highly of themselves refuse to accept\nthey need to actually forget what they think they know and take the time to\nchallenge themselves and re-learn everything again from scratch.\n\nI'm proud to know the developers I've hired are open-minded, humble, and thirsty\nfor new challenges! And I expect nothing less of myself.\n\nSo what is Functional Programming?\nOther than being the method in which a regular human brain computes, functional\nprogramming streamlines the applications logic around abstractions such as\nhigher order functions which are inherently stateless and allow you to be as DRY\nas possible while remaining readable and easy to follow with your logic.\n\n> In short; Functional Programming allows for complexities to be trivial to\nfollow.\n\n\nBecause your abstractions are stateless it is up to your main application logic\nto maintain a certain clarity over what you would normally expect as a 'state',\nbecause essentially Functional Programming is stateless too. Instead of\nmaintaining a central state for each lifecycle of your processing you would\ninstead pass the state along the chain of processing in an ever forward momentum\nof executions.\n\nAlthough Functional Programming itself is only the main application logic PHP\nhowever is Imperative fundamentally so it is our job to create the abstractions\nthat allow us to be as functional as possible with our main application logic.\n\n> Recursion = PHP without imperative programming\n\n\nWe'll also see a rise in lambda or anonymous function usage (i.e. closures; to\nbe covered in detail further on) as we will be expecting all of our functions to\ntake a function as a parameter, return a function, or both.\n\nThis will lead you to rely less on OOP in general and rather focus abstractions\non more functional friendly collections, factories, and yes singletons too.\n\nIf you aren't already familiar with the concepts of general OOP design patterns,\nin particular collections and factories, now is the time to learn them as they\nwill make your abstractions possible and therefore provide what is needed to\ncreate a truly functional application.\n\nRecursion\nFunctional PHP would not include any loops, foreach, for, while, do while, are\nall taken away from you and replaced with the concept of recursion.\n\nSuppose we want to write a function to find the sum of all elements in an array\n(forget that array_sum exists for the time being). In a functional style, we\nwould write;\n\nfunction sum($array) {\n  if (empty($array)) {\n    return 0;\n  } else {\n    return $array[0] + sum(array_slice($array, 1));\n  }\n}\n\n$total = sum([1, 2, 3]); // 6\n\n\nAn empty list will return 0, which is our base condition. For an array with more\nthan one value, it will return the results of adding the first element with the\nrecursive sum of all other elements.\n\n> Fun fact: I included this in the Punters.com.au [https://www.punters.com.au/] \ncoding challenge and to date no candidate has demonstrated functional\nprogramming\n\n\nFunctional building blocks\nThere are tools you must first provide PHP with before you can get started\nemploying Functional Programming.\n\nI've put the entire project on GitHub for you, It is production ready.\n\n> chrisdlangton/php-functional-programming\n[https://github.com/chrisdlangton/php-functional-programming]\nphp-functional-programming - The Building Blocks for PHP to be able to provide\ntrue Functional Programming\n\n\nWe'll start with a higher order function called array_each which abstracts\nlooping and allows you to perform your usually looped actions when a collection\nor return value isn't required and explain how it is different from the \narray_map function that does return a value.\n\nfunction array_each(array $items, $func): void {\n  foreach ($items as $key => $item) {\n    $func($item, $key);\n  }\n}\n\n$users = [\n  [\n    \"id\" => \"1\",\n    \"username\" => \"John\",\n    \"email\" => \"john@domain.tld\"\n  ],[\n    \"id\" => \"2\",\n    \"username\" => \"Jane\",\n    \"email\" => \"jane@domain.tld\"\n  ]\n];\n\narray_each($users, function($user) {\n  emailTo($user['email'], 'Welcome to my blog')\n});\n\n\nYou can see clearly that the array_each function provides no return value as it\niterates, it's purpose is to consume the data and do something with each item it\nfinds - brilliant!\n\nIf you intend to extract certain information for later use as a collection,\nperhaps a list of username's, you would instead need to utilise array_map like\nso;\n\nfunction array_map(array $items, $func): array {\n  $results = [];\n  foreach ($items as $key => $item) {\n    $results[] = $func($item, $key);\n  }\n  return $results;\n}\n\n$users = [\n  [\n    \"id\" => \"1\",\n    \"username\" => \"John\",\n    \"email\" => \"john@domain.tld\"\n  ],[\n    \"id\" => \"2\",\n    \"username\" => \"Jane\",\n    \"email\" => \"jane@domain.tld\"\n  ]\n];\n\n$usernames = array_map($users, function($user){\n  return $user['username'];\n});\n\nprint( implode(', ', $usernames) ); // John, Jane\n\n\nIn your implementation you'd have these higher order functions referenced by\nyour script and therefore you'd only be writing the 3 lines of usage, clearly\nfunctional is easier to read #amiright\n\nMore advanced examples\nLooking deep at these implementations you might already see that we have much\nmore expressive power than has been demonstrated. For example, you can use \narray_each and array_map on multidimensional arrays as well as make use of PHP's \nuse on closures and of course pass into your closure scope another variable by\nreference if need be - to demonstrate;\n\n$data = [\n  \"foo\" = \"bar\",\n  \"baz\" = true,\n  \"users\" => [\n    [\n      \"id\" => \"1\",\n      \"username\" => \"John\",\n      \"email\" => \"john@domain.tld\"\n    ],[\n      \"id\" => \"2\",\n      \"username\" => \"Jane\",\n      \"email\" => \"jane@domain.tld\"\n    ]\n  ]\n];\n\n$usernames = [];\narray_each($data, function($item, $key) use (&$usernames) {\n  if ($key === 'users') {\n    $usernames = array_map($item['users'], function($user) {\n      return $user['username'];\n    });\n  }\n});\n\nprint( implode(', ', $usernames) ); // John, Jane\n\n\nIn PHP functions can be passed as arguments to other functions and a function\ncan return other functions \u0001(a feature called higher-order functions). Both\nuser-defined and built-in functions can be referenced by a variable and invoked\ndynamically.\n\nOn higher-order functions\nThe most common usage of higher-order functions is when implementing a strategy\npattern.\n\n$input = [1, 2, 3, 4, 5, 6];\n\n$filter_even = function($item) {\n    return ($item % 2) == 0;\n};\n\n$output = array_filter($input, $filter_even);\n\nprint_r( implode(', ', $output) ); // 2, 4, 6\n\n\nI've mentioned closures a few times and I'll take the time now to explain them\nso you can see for yourself their usefulness, actually, their importance to\nimplement functional programming.\nA closure is an anonymous function that can access variables imported from the\noutside scope without using any global variables. Basicly, a closure is a\nfunction with some arguments fixed (closed) by the environment when it is\ndefined. Closures can work around variable scope restrictions in a clean way\nreadable way that the human can understand just by glancing at the code, a core\npoint of employing functional programming.\n\nIn the next example we use closures to define a higher-order function returning\na single filtering function, to be used by PHP's array_filter;\n\n$greater_than = function($min) {\n    return function($item) use ($min) {\n        return $item > $min;\n    };\n};\n\n$input = [1, 2, 3, 4, 5, 6];\n$output = array_filter($input, $greater_than(3));\n\nprint_r( implode(', ', $output) ); // 4, 5, 6\n\n\nEarly binding is used by default for importing $min variable into the created\nfunction. For true closures with late binding one should use a reference when\nimporting. Imagine a templating or input validation library, where closure is\ndefined to capture variables in scope and access them later when the anonymous\nfunction is evaluated.\n\n> The fun stuff; Immutability\n\n\nImmutability\nNot so fun in PHP actually.\nImmutability is the behavior that a value of a variable cannot be changed once\nit is defined. Different languages have different ways of achieving this; in\nPHP, for example, the only way to make a variable immutable is to define it as a\nconstant.\n\n> An immutable object cannot be modified by accident\n\n\nThis also has an extra benefit of making testing easier because complex objects\ndo not need to be rebuilt. Because all objects are safe from modification by\ndefault, and all changes are explicit, a significant amount of cognitive load\ncan be reduced by not having to follow complex method chains to see where\nchanges occur.\n\nPHP is fairly unique among popular programming languages in that the array type\ncan be used in so many ways. All of the following are arrays in PHP;\n\n$dictionary = [\n    'rabbit' => 'fun pet',\n    'szechuan' => 'spicy deliciousness',\n    'php7' => 'not php as you know it'\n];\n$orderedList = ['amazing', 'functional', 'functional', 'programming'];\n$unorderedList = [0, 8, 2, 32, 4];\n$set = ['total war', 'sins of a solar empire', '0 AD'];\n\n\nData type structures:\n\n * Dictionary, an associative array that consists of a key and a value.\n * OrderedList, a list that contains any number of values in a defined order.\n * UnorderedList, a list that contains any number of values in an undefined\n   order.\n * Set, a set of values that contains no duplicates.\n\nWhile the PHP approach provides a great amount of flexibility it also makes it\nmore difficult to validate data that is being passed through an application. By\nusing more strict data structures, we can avoid these problems and have more\ntestable code. Anyone can assign a value in an unexpected way with new\nfunctionality given an array is representative of these 4 structures.\n\nIn my repo you'll find these data type structures and several building blocks\nneeded to kick start your dive into Functional Programming with PHP;","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-6.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-20 02:49:54","created_by":"1","updated_at":"2021-03-31 14:15:26","updated_by":"1","published_at":"2016-06-21 16:59:47","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbc9f","uuid":"e34e8cb2-a275-4e46-988e-c3bea5b57cff","title":"Access browser stored passwords via Credentials API","slug":"access-browser-stored-passwords-via-credentials-api","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The latest version of Chrome 51 (as of writing) supports the Credential Management API. It’s a proposal at the W3C that gives developers programmatic access to a browser’s credential manager and helps users sign in more easily.\\n\\n> Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView release 51\\n\\nBrowsers have for a long time stored your passwords, in chrome they offer them to you and highlight forms in an ugly yellow color.\\nAs a user, you can identify visually if a site you are on has the ability to access these passwords by noticing the key symbol in the navigation bar;\\n\\n![Chrome showing you have stored passwords for this site](__GHOST_URL__/content/images/2016/06/Selection_038.png)\\n\\nIf you see this, then you will be able to access the form data the browser remembered in the past, but now, the website developers now also can access that same form data programmatically; with an exception to a password entered in the form that used the proper password HTML input;\\n\\n```language-html\\n<input type=\\\"password\\\">\\n```\\n\\nIf they used the correct input, then as a user you would not have seen the password as you type. But like all passwords you trust they are not tracking your plain text password in some other way. Moving on.\\n\\n### Using PasswordCredential\\n\\n> Before continuing it's worth mentioning that you may only access the `PasswordCredential` via `fetch` and the endpoint for fetch must be on the same origin.\\n\\nA basic login form;\\n\\n```language-html\\n<form action=\\\"/\\\" method=\\\"POST\\\" id=\\\"theForm\\\">\\n  <label for=\\\"id\\\">Name</label>\\n  <input type=\\\"text\\\" name=\\\"id\\\" id=\\\"id\\\" value=\\\"\\\">\\n  <label for=\\\"password\\\">New Password</label>\\n  <input type=\\\"text\\\" name=\\\"password\\\" id=\\\"password\\\" value=\\\"\\\">\\n  <input type=\\\"submit\\\">\\n</form>\\n```\\nAnd the JavaScript needed to validate with the server;\\n\\n```language-javascript\\ndocument.querySelector('#theForm').addEventListener(\\\"submit\\\", function(e) {\\n  if (navigator.credentials) {\\n    var creds = new PasswordCredential(e.target);\\n    fetch(e.target.action, {\\n      method: \\\"POST\\\",\\n      credentials: creds\\n    }).then(function() {\\n      navigator.credentials.store(creds);\\n    });\\n  }\\n});\\n```\\n\\nThis so far shows a normal login form, nothing here is new in terms of functionality. The user gets a form, fills it in completely and sends their credentials to your server, which you would validate and log them in. Normally the user's browser would ask them if they wish to store the form data in the browser and if they chose yes the browser would fill in the form the next time they visited. However, the developer has not the ability to answer the question \\\"would you like Chrome to save the form data\\\" using `navigator.credentials.store` shown above. Neat!\\n\\nThe data accessible to you in the browser from the what the browser saved looks like this;\\n\\n```language-json\\n{\\n  \\\"additionalData\\\":null,\\n  \\\"iconURL\\\":\\\"\\\",\\n  \\\"id\\\":\\\"person@gmail.com\\\",\\n  \\\"idName\\\":\\\"username\\\",\\n  \\\"name\\\":\\\"\\\",\\n  \\\"passwordName\\\":\\\"password\\\",\\n  \\\"type\\\":\\\"password\\\"\\n}\\n```\\n\\nNo actual password can be read in the browser due to the `PasswordCredential` and `fetch` security functions, validation should still be performed on the server, always.\\n\\nIn addition, the website might need to store some more data in the browser without it being part of the form inputs, such as a SCRF token from the server. You can do this too! Using `additionalData` property as part of the `PasswordCredential`;\\n\\n```language-javascript\\ncreds.additionalData = new FormData();\\ncreds.additionalData.append(\\\"csrf\\\", \\\"[server token value goes here]\\\")\\n```\\n\\n### Auto-login\\n\\nBy auto-login I mean the user won't need to interact with your site, but Chrome will ask for their permission;\\n\\n![Chrome asking the user for permission to use the stored password](__GHOST_URL__/content/images/2016/06/Selection39.png)\\n\\n```language-javascript\\nnavigator.credentials.get({ password: true }).then(function(credential) {\\n  if (!credential) {\\n    return;\\n  }\\n  if (credential.type == \\\"password\\\") {\\n    fetch(\\\"/\\\", {\\n      credentials: credential,\\n      method: \\\"POST\\\"\\n    }).then(function (response) {\\n      // let the user know\\n    });\\n  }\\n});\\n```\\n\\nThis example will access the stored data in the browser form the first time the user filled out your form, sends it to your server for validation, and if that returns successful the website should consider the process of login successful.\\n\\n### Google or Facebook Federated Login\\n\\nYou can examine the `type` property of the credential object to see if it’s `PasswordCredential` type 'password' or `FederatedCredential` type 'federated'.\\n\\nIf the credential is a `FederatedCredential`, you can call the appropriate API using information it contains.\\n\\nHere is a modified example of the above;\\n\\n```language-javascript\\nnavigator.credentials.get({  \\n  password: true,\\n  federated: {\\n    providers: [\\n      'https://accounts.google.com',  \\n      'https://www.facebook.com'  \\n    ]  \\n  }  \\n}).then(function(cred) {\\n  // call the appropriate API\\n});\\n``` \\n\\nI won't go into using these providers APIs as they are not part of the Credentials API.\\n\\n### Logout\\n\\nWhen a user signs out from your website, it’s your responsibility to ensure that the user will not be automatically signed back in. To ensure this, the Credential Management API provides a mechanism called mediation. You can enable mediation mode by calling `navigator.credentials.requireUserMediation`.\\n\\n```language-javascript\\nnavigator.credentials.requireUserMediation();\\n```\\n\\nOr in the `navigator.credentials.get` you can pass `unmediated: false`;\\n\\n```language-javascript\\nnavigator.credentials.get({\\n  password: true,\\n  unmediated: false\\n}).then(function(cred) {\\n  // logged in this time, but will not auto-login\\n});\\n```\\n\\nAnd there you have it, I hope this makes it's way into more browsers as it will not only pave the way to solve the password problem on the web but also drive usage and development of `fetch`.\\n\\nDon't forget to share on Twitter below the comments if you liked this. \\nFollow me to stay updated with my newest content. Thank you.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The latest version of Chrome 51 (as of writing) supports the Credential Management API. It’s a proposal at the W3C that gives developers programmatic access to a browser’s credential manager and helps users sign in more easily.</p>\n<blockquote>\n<p>Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView release 51</p>\n</blockquote>\n<p>Browsers have for a long time stored your passwords, in chrome they offer them to you and highlight forms in an ugly yellow color.<br>\nAs a user, you can identify visually if a site you are on has the ability to access these passwords by noticing the key symbol in the navigation bar;</p>\n<p><img src=\"__GHOST_URL__/content/images/2016/06/Selection_038.png\" alt=\"Chrome showing you have stored passwords for this site\" loading=\"lazy\"></p>\n<p>If you see this, then you will be able to access the form data the browser remembered in the past, but now, the website developers now also can access that same form data programmatically; with an exception to a password entered in the form that used the proper password HTML input;</p>\n<pre><code class=\"language-language-html\">&lt;input type=&quot;password&quot;&gt;\n</code></pre>\n<p>If they used the correct input, then as a user you would not have seen the password as you type. But like all passwords you trust they are not tracking your plain text password in some other way. Moving on.</p>\n<h3 id=\"usingpasswordcredential\">Using PasswordCredential</h3>\n<blockquote>\n<p>Before continuing it's worth mentioning that you may only access the <code>PasswordCredential</code> via <code>fetch</code> and the endpoint for fetch must be on the same origin.</p>\n</blockquote>\n<p>A basic login form;</p>\n<pre><code class=\"language-language-html\">&lt;form action=&quot;/&quot; method=&quot;POST&quot; id=&quot;theForm&quot;&gt;\n  &lt;label for=&quot;id&quot;&gt;Name&lt;/label&gt;\n  &lt;input type=&quot;text&quot; name=&quot;id&quot; id=&quot;id&quot; value=&quot;&quot;&gt;\n  &lt;label for=&quot;password&quot;&gt;New Password&lt;/label&gt;\n  &lt;input type=&quot;text&quot; name=&quot;password&quot; id=&quot;password&quot; value=&quot;&quot;&gt;\n  &lt;input type=&quot;submit&quot;&gt;\n&lt;/form&gt;\n</code></pre>\n<p>And the JavaScript needed to validate with the server;</p>\n<pre><code class=\"language-language-javascript\">document.querySelector('#theForm').addEventListener(&quot;submit&quot;, function(e) {\n  if (navigator.credentials) {\n    var creds = new PasswordCredential(e.target);\n    fetch(e.target.action, {\n      method: &quot;POST&quot;,\n      credentials: creds\n    }).then(function() {\n      navigator.credentials.store(creds);\n    });\n  }\n});\n</code></pre>\n<p>This so far shows a normal login form, nothing here is new in terms of functionality. The user gets a form, fills it in completely and sends their credentials to your server, which you would validate and log them in. Normally the user's browser would ask them if they wish to store the form data in the browser and if they chose yes the browser would fill in the form the next time they visited. However, the developer has not the ability to answer the question &quot;would you like Chrome to save the form data&quot; using <code>navigator.credentials.store</code> shown above. Neat!</p>\n<p>The data accessible to you in the browser from the what the browser saved looks like this;</p>\n<pre><code class=\"language-language-json\">{\n  &quot;additionalData&quot;:null,\n  &quot;iconURL&quot;:&quot;&quot;,\n  &quot;id&quot;:&quot;person@gmail.com&quot;,\n  &quot;idName&quot;:&quot;username&quot;,\n  &quot;name&quot;:&quot;&quot;,\n  &quot;passwordName&quot;:&quot;password&quot;,\n  &quot;type&quot;:&quot;password&quot;\n}\n</code></pre>\n<p>No actual password can be read in the browser due to the <code>PasswordCredential</code> and <code>fetch</code> security functions, validation should still be performed on the server, always.</p>\n<p>In addition, the website might need to store some more data in the browser without it being part of the form inputs, such as a SCRF token from the server. You can do this too! Using <code>additionalData</code> property as part of the <code>PasswordCredential</code>;</p>\n<pre><code class=\"language-language-javascript\">creds.additionalData = new FormData();\ncreds.additionalData.append(&quot;csrf&quot;, &quot;[server token value goes here]&quot;)\n</code></pre>\n<h3 id=\"autologin\">Auto-login</h3>\n<p>By auto-login I mean the user won't need to interact with your site, but Chrome will ask for their permission;</p>\n<p><img src=\"__GHOST_URL__/content/images/2016/06/Selection39.png\" alt=\"Chrome asking the user for permission to use the stored password\" loading=\"lazy\"></p>\n<pre><code class=\"language-language-javascript\">navigator.credentials.get({ password: true }).then(function(credential) {\n  if (!credential) {\n    return;\n  }\n  if (credential.type == &quot;password&quot;) {\n    fetch(&quot;/&quot;, {\n      credentials: credential,\n      method: &quot;POST&quot;\n    }).then(function (response) {\n      // let the user know\n    });\n  }\n});\n</code></pre>\n<p>This example will access the stored data in the browser form the first time the user filled out your form, sends it to your server for validation, and if that returns successful the website should consider the process of login successful.</p>\n<h3 id=\"googleorfacebookfederatedlogin\">Google or Facebook Federated Login</h3>\n<p>You can examine the <code>type</code> property of the credential object to see if it’s <code>PasswordCredential</code> type 'password' or <code>FederatedCredential</code> type 'federated'.</p>\n<p>If the credential is a <code>FederatedCredential</code>, you can call the appropriate API using information it contains.</p>\n<p>Here is a modified example of the above;</p>\n<pre><code class=\"language-language-javascript\">navigator.credentials.get({  \n  password: true,\n  federated: {\n    providers: [\n      'https://accounts.google.com',  \n      'https://www.facebook.com'  \n    ]  \n  }  \n}).then(function(cred) {\n  // call the appropriate API\n});\n</code></pre>\n<p>I won't go into using these providers APIs as they are not part of the Credentials API.</p>\n<h3 id=\"logout\">Logout</h3>\n<p>When a user signs out from your website, it’s your responsibility to ensure that the user will not be automatically signed back in. To ensure this, the Credential Management API provides a mechanism called mediation. You can enable mediation mode by calling <code>navigator.credentials.requireUserMediation</code>.</p>\n<pre><code class=\"language-language-javascript\">navigator.credentials.requireUserMediation();\n</code></pre>\n<p>Or in the <code>navigator.credentials.get</code> you can pass <code>unmediated: false</code>;</p>\n<pre><code class=\"language-language-javascript\">navigator.credentials.get({\n  password: true,\n  unmediated: false\n}).then(function(cred) {\n  // logged in this time, but will not auto-login\n});\n</code></pre>\n<p>And there you have it, I hope this makes it's way into more browsers as it will not only pave the way to solve the password problem on the web but also drive usage and development of <code>fetch</code>.</p>\n<p>Don't forget to share on Twitter below the comments if you liked this.<br>\nFollow me to stay updated with my newest content. Thank you.</p>\n<!--kg-card-end: markdown-->","comment_id":"29","plaintext":"The latest version of Chrome 51 (as of writing) supports the Credential\nManagement API. It’s a proposal at the W3C that gives developers programmatic\naccess to a browser’s credential manager and helps users sign in more easily.\n\n> Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView\nrelease 51\n\n\nBrowsers have for a long time stored your passwords, in chrome they offer them\nto you and highlight forms in an ugly yellow color.\nAs a user, you can identify visually if a site you are on has the ability to\naccess these passwords by noticing the key symbol in the navigation bar;\n\n\n\nIf you see this, then you will be able to access the form data the browser\nremembered in the past, but now, the website developers now also can access that\nsame form data programmatically; with an exception to a password entered in the\nform that used the proper password HTML input;\n\n<input type=\"password\">\n\n\nIf they used the correct input, then as a user you would not have seen the\npassword as you type. But like all passwords you trust they are not tracking\nyour plain text password in some other way. Moving on.\n\nUsing PasswordCredential\n> Before continuing it's worth mentioning that you may only access the \nPasswordCredential via fetch and the endpoint for fetch must be on the same\norigin.\n\n\nA basic login form;\n\n<form action=\"/\" method=\"POST\" id=\"theForm\">\n  <label for=\"id\">Name</label>\n  <input type=\"text\" name=\"id\" id=\"id\" value=\"\">\n  <label for=\"password\">New Password</label>\n  <input type=\"text\" name=\"password\" id=\"password\" value=\"\">\n  <input type=\"submit\">\n</form>\n\n\nAnd the JavaScript needed to validate with the server;\n\ndocument.querySelector('#theForm').addEventListener(\"submit\", function(e) {\n  if (navigator.credentials) {\n    var creds = new PasswordCredential(e.target);\n    fetch(e.target.action, {\n      method: \"POST\",\n      credentials: creds\n    }).then(function() {\n      navigator.credentials.store(creds);\n    });\n  }\n});\n\n\nThis so far shows a normal login form, nothing here is new in terms of\nfunctionality. The user gets a form, fills it in completely and sends their\ncredentials to your server, which you would validate and log them in. Normally\nthe user's browser would ask them if they wish to store the form data in the\nbrowser and if they chose yes the browser would fill in the form the next time\nthey visited. However, the developer has not the ability to answer the question\n\"would you like Chrome to save the form data\" using navigator.credentials.store \nshown above. Neat!\n\nThe data accessible to you in the browser from the what the browser saved looks\nlike this;\n\n{\n  \"additionalData\":null,\n  \"iconURL\":\"\",\n  \"id\":\"person@gmail.com\",\n  \"idName\":\"username\",\n  \"name\":\"\",\n  \"passwordName\":\"password\",\n  \"type\":\"password\"\n}\n\n\nNo actual password can be read in the browser due to the PasswordCredential and \nfetch security functions, validation should still be performed on the server,\nalways.\n\nIn addition, the website might need to store some more data in the browser\nwithout it being part of the form inputs, such as a SCRF token from the server.\nYou can do this too! Using additionalData property as part of the \nPasswordCredential;\n\ncreds.additionalData = new FormData();\ncreds.additionalData.append(\"csrf\", \"[server token value goes here]\")\n\n\nAuto-login\nBy auto-login I mean the user won't need to interact with your site, but Chrome\nwill ask for their permission;\n\n\n\nnavigator.credentials.get({ password: true }).then(function(credential) {\n  if (!credential) {\n    return;\n  }\n  if (credential.type == \"password\") {\n    fetch(\"/\", {\n      credentials: credential,\n      method: \"POST\"\n    }).then(function (response) {\n      // let the user know\n    });\n  }\n});\n\n\nThis example will access the stored data in the browser form the first time the\nuser filled out your form, sends it to your server for validation, and if that\nreturns successful the website should consider the process of login successful.\n\nGoogle or Facebook Federated Login\nYou can examine the type property of the credential object to see if it’s \nPasswordCredential type 'password' or FederatedCredential type 'federated'.\n\nIf the credential is a FederatedCredential, you can call the appropriate API\nusing information it contains.\n\nHere is a modified example of the above;\n\nnavigator.credentials.get({  \n  password: true,\n  federated: {\n    providers: [\n      'https://accounts.google.com',  \n      'https://www.facebook.com'  \n    ]  \n  }  \n}).then(function(cred) {\n  // call the appropriate API\n});\n\n\nI won't go into using these providers APIs as they are not part of the\nCredentials API.\n\nLogout\nWhen a user signs out from your website, it’s your responsibility to ensure that\nthe user will not be automatically signed back in. To ensure this, the\nCredential Management API provides a mechanism called mediation. You can enable\nmediation mode by calling navigator.credentials.requireUserMediation.\n\nnavigator.credentials.requireUserMediation();\n\n\nOr in the navigator.credentials.get you can pass unmediated: false;\n\nnavigator.credentials.get({\n  password: true,\n  unmediated: false\n}).then(function(cred) {\n  // logged in this time, but will not auto-login\n});\n\n\nAnd there you have it, I hope this makes it's way into more browsers as it will\nnot only pave the way to solve the password problem on the web but also drive\nusage and development of fetch.\n\nDon't forget to share on Twitter below the comments if you liked this.\nFollow me to stay updated with my newest content. Thank you.","feature_image":"https://s3.amazonaws.com/digitaltrends-uploads-prod/2015/03/body-lock.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-23 05:43:48","created_by":"1","updated_at":"2021-03-31 14:15:00","updated_by":"1","published_at":"2016-06-23 07:34:26","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca0","uuid":"d2fb1e0e-4225-4d24-bf23-896feabe9381","title":"Use Passive Event Listeners to prevent scroll interruptions","slug":"use-passive-event-listeners-to-prevent-scroll-interruptions","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"For years we've complained and asked for a way to bind to touch and mouse events that do not require and should not change the built in scrolling actions.\\n\\n> Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView release 51\\n\\nFinally it has landed on our devices and as a dev i'm pretty excited, but there are a few gotcha's as usual.\\n\\n### Simple usage\\n\\nThe third existing option on `addEventListener` has been replaced, it is no longer a boolean for the `capture` argument but rather a new object called `EventListenerOptions` which has some new properties; `capture` as expected and the new `passive` which both take boolean values. \\n\\nIn a number of common scenarios events don't need to block scrolling - for instance:\\n\\n- User activity monitoring which just wants to know when the user was last active\\n- `touchstart` handlers that hide some active UI (like tooltips)\\n- `touchstart` and `touchend` handlers that style UI elements (without suppressing the click event).\\n\\nFor these scenarios, the passive option can be added (with appropriate feature detection) without any other code changes, resulting in a significantly smoother scrolling experience.\\n\\n### Disable scrolling, touch, or wheel events\\n\\nThere are a few complicated scenarios where the handler only wants to suppress scrolling under certain conditions, such as:\\n\\n- A UI element (like YouTube's volume slider) which slides on horizontal wheel events without changing the scrolling behavior on vertical wheel events\\n- Swiping horizontally to rotate a carousel, dismiss an item or reveal a drawer, while still permitting vertical scrolling\\n\\nSince there is no equivalent of \\\"touch action\\\" for wheel events, you can implement them with non-passive wheel listeners.\\n\\nSome options include;\\n\\nIn this case, use `touch-action: pan-y` to declaratively disable scrolling that starts along the horizontal axis without having to call `preventDefault()`, and using `touch-action: pan-x` will work the same on the vertical axis. \\n\\nTo consistently disable scrolling by cancelling all touch or wheel events like;\\n\\n- Panning and zooming a map\\n- Full-page/full-screen games\\n\\nUse the `touch-action: none` on your `EventListenerOptions`.\\n\\n### Gotcha 1\\n#### Scroll modifications are disabled in handlers\\n\\nBy using this feature you effectively tell the browser that you will not be calling `e.preventDefault()` in your event handler which normally allows scrolling to start (at a later stage) while still executing your handler.\\n\\nTherefore `e.preventDefault()` becomes redundant;\\n\\n```language-javascript\\naddEventListener(document, \\\"touchstart\\\", function(e) {\\n  console.log(e.defaultPrevented);  // will be false\\n  e.preventDefault();   // does nothing since the listener is passive\\n  console.log(e.defaultPrevented);  // still false\\n}, {passive: true});\\n```\\n\\nCalling it while using `passive: true` has no impact and can be safely omitted.\\n\\n### Gotcha 2\\n#### Backwards compatibility is broken\\n\\nAs you might suspect from the new object `EventListenerOptions` replacing the third argument we have not only a cross browser conflict but also a supporting browser backwards compatibility issue. \\n\\nIf your app doesn't currently use the third option and you know for certain that it won't in future, you still cannot use passive event listeners now because by passing up the `EventListenerOptions` you are breaking your event listener on non-supporting browsers.\\n\\nYou will need to manually handle feature detection or use a [polyfil](https://github.com/WICG/EventListenerOptions/blob/gh-pages/EventListenerOptions.polyfill.js).\\n\\n### Gotcha 3\\n#### Feature detection is a thread blocking check\\n\\nThe fact that the way we need to check for the feature is thread blocking makes this troublesome. \\n\\nTry using the feature to ensure it is available;\\n\\n```language-javascript\\nvar supportsPassive = false;\\ntry {\\n  var opts = Object.defineProperty({}, 'passive', {\\n    get: function() {\\n      supportsPassive = true;\\n    }\\n  });\\n  window.addEventListener(\\\"test\\\", null, opts);\\n} catch (e) {}\\n\\nelem.addEventListener('touchstart', fn, supportsPassive ? { passive: true } : false); \\n```\\n\\nYou might need to consider doing this only once, immediately after you've run all of your DOM ready functionality so as to not block initial rendering, and then defer all of your event listeners until last. This to me is an anti-pattern and unfortunately there is no better performing way to manage this limitation, you could make the decision to block initially rendering and do the check immediately before ready state 4, but that's your choice.\\n\\nAlternatively you could use the defacto JavaScript feature detection library Modernizr but it's implementation is hidden and you'll not be sure if it is blocking or otherwise;\\n\\n```language-javascript\\nelem.addEventListener('touchstart', fn, Modernizr.passiveeventlisteners ? {passive:true} : false);\\n```\\n\\n### How about making passive default?\\n\\nNot only have we been given a brilliant new feature that breaks backwards compatibility, we are also forced to opt-in to use it as it is not set by default. Considering the minority of use cases where you would not want to use passive one would expect the default be set to passive!\\n\\nSome quick dev has put together a tiny Github repo to enable the setting by default when you call `addEventListener`, you can [find it here](https://github.com/zzarcon/default-passive-events).\\n\\n### In short\\n\\nWhen talking about performance, scrolling is a key aspect of your app, this means that as soon as the users perceives a janky scroll on your page they will quickly bounce and not likely spread the word about what you've built unless in frustration.\\n\\nI strongly suggest all people in a position to implement this do so ASAP.\\n\\n> Read all about [how it works](https://developers.google.com/web/updates/2016/06/passive-event-listeners?hl=en) on Google Developers.\\n\\nYou can find the source code on [Github](https://github.com/Google/WebFundamentals/tree/master/src/content/en/updates/posts/2016/06/passive-event-listeners.markdown)\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>For years we've complained and asked for a way to bind to touch and mouse events that do not require and should not change the built in scrolling actions.</p>\n<blockquote>\n<p>Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView release 51</p>\n</blockquote>\n<p>Finally it has landed on our devices and as a dev i'm pretty excited, but there are a few gotcha's as usual.</p>\n<h3 id=\"simpleusage\">Simple usage</h3>\n<p>The third existing option on <code>addEventListener</code> has been replaced, it is no longer a boolean for the <code>capture</code> argument but rather a new object called <code>EventListenerOptions</code> which has some new properties; <code>capture</code> as expected and the new <code>passive</code> which both take boolean values.</p>\n<p>In a number of common scenarios events don't need to block scrolling - for instance:</p>\n<ul>\n<li>User activity monitoring which just wants to know when the user was last active</li>\n<li><code>touchstart</code> handlers that hide some active UI (like tooltips)</li>\n<li><code>touchstart</code> and <code>touchend</code> handlers that style UI elements (without suppressing the click event).</li>\n</ul>\n<p>For these scenarios, the passive option can be added (with appropriate feature detection) without any other code changes, resulting in a significantly smoother scrolling experience.</p>\n<h3 id=\"disablescrollingtouchorwheelevents\">Disable scrolling, touch, or wheel events</h3>\n<p>There are a few complicated scenarios where the handler only wants to suppress scrolling under certain conditions, such as:</p>\n<ul>\n<li>A UI element (like YouTube's volume slider) which slides on horizontal wheel events without changing the scrolling behavior on vertical wheel events</li>\n<li>Swiping horizontally to rotate a carousel, dismiss an item or reveal a drawer, while still permitting vertical scrolling</li>\n</ul>\n<p>Since there is no equivalent of &quot;touch action&quot; for wheel events, you can implement them with non-passive wheel listeners.</p>\n<p>Some options include;</p>\n<p>In this case, use <code>touch-action: pan-y</code> to declaratively disable scrolling that starts along the horizontal axis without having to call <code>preventDefault()</code>, and using <code>touch-action: pan-x</code> will work the same on the vertical axis.</p>\n<p>To consistently disable scrolling by cancelling all touch or wheel events like;</p>\n<ul>\n<li>Panning and zooming a map</li>\n<li>Full-page/full-screen games</li>\n</ul>\n<p>Use the <code>touch-action: none</code> on your <code>EventListenerOptions</code>.</p>\n<h3 id=\"gotcha1\">Gotcha 1</h3>\n<h4 id=\"scrollmodificationsaredisabledinhandlers\">Scroll modifications are disabled in handlers</h4>\n<p>By using this feature you effectively tell the browser that you will not be calling <code>e.preventDefault()</code> in your event handler which normally allows scrolling to start (at a later stage) while still executing your handler.</p>\n<p>Therefore <code>e.preventDefault()</code> becomes redundant;</p>\n<pre><code class=\"language-language-javascript\">addEventListener(document, &quot;touchstart&quot;, function(e) {\n  console.log(e.defaultPrevented);  // will be false\n  e.preventDefault();   // does nothing since the listener is passive\n  console.log(e.defaultPrevented);  // still false\n}, {passive: true});\n</code></pre>\n<p>Calling it while using <code>passive: true</code> has no impact and can be safely omitted.</p>\n<h3 id=\"gotcha2\">Gotcha 2</h3>\n<h4 id=\"backwardscompatibilityisbroken\">Backwards compatibility is broken</h4>\n<p>As you might suspect from the new object <code>EventListenerOptions</code> replacing the third argument we have not only a cross browser conflict but also a supporting browser backwards compatibility issue.</p>\n<p>If your app doesn't currently use the third option and you know for certain that it won't in future, you still cannot use passive event listeners now because by passing up the <code>EventListenerOptions</code> you are breaking your event listener on non-supporting browsers.</p>\n<p>You will need to manually handle feature detection or use a <a href=\"https://github.com/WICG/EventListenerOptions/blob/gh-pages/EventListenerOptions.polyfill.js\">polyfil</a>.</p>\n<h3 id=\"gotcha3\">Gotcha 3</h3>\n<h4 id=\"featuredetectionisathreadblockingcheck\">Feature detection is a thread blocking check</h4>\n<p>The fact that the way we need to check for the feature is thread blocking makes this troublesome.</p>\n<p>Try using the feature to ensure it is available;</p>\n<pre><code class=\"language-language-javascript\">var supportsPassive = false;\ntry {\n  var opts = Object.defineProperty({}, 'passive', {\n    get: function() {\n      supportsPassive = true;\n    }\n  });\n  window.addEventListener(&quot;test&quot;, null, opts);\n} catch (e) {}\n\nelem.addEventListener('touchstart', fn, supportsPassive ? { passive: true } : false); \n</code></pre>\n<p>You might need to consider doing this only once, immediately after you've run all of your DOM ready functionality so as to not block initial rendering, and then defer all of your event listeners until last. This to me is an anti-pattern and unfortunately there is no better performing way to manage this limitation, you could make the decision to block initially rendering and do the check immediately before ready state 4, but that's your choice.</p>\n<p>Alternatively you could use the defacto JavaScript feature detection library Modernizr but it's implementation is hidden and you'll not be sure if it is blocking or otherwise;</p>\n<pre><code class=\"language-language-javascript\">elem.addEventListener('touchstart', fn, Modernizr.passiveeventlisteners ? {passive:true} : false);\n</code></pre>\n<h3 id=\"howaboutmakingpassivedefault\">How about making passive default?</h3>\n<p>Not only have we been given a brilliant new feature that breaks backwards compatibility, we are also forced to opt-in to use it as it is not set by default. Considering the minority of use cases where you would not want to use passive one would expect the default be set to passive!</p>\n<p>Some quick dev has put together a tiny Github repo to enable the setting by default when you call <code>addEventListener</code>, you can <a href=\"https://github.com/zzarcon/default-passive-events\">find it here</a>.</p>\n<h3 id=\"inshort\">In short</h3>\n<p>When talking about performance, scrolling is a key aspect of your app, this means that as soon as the users perceives a janky scroll on your page they will quickly bounce and not likely spread the word about what you've built unless in frustration.</p>\n<p>I strongly suggest all people in a position to implement this do so ASAP.</p>\n<blockquote>\n<p>Read all about <a href=\"https://developers.google.com/web/updates/2016/06/passive-event-listeners?hl=en\">how it works</a> on Google Developers.</p>\n</blockquote>\n<p>You can find the source code on <a href=\"https://github.com/Google/WebFundamentals/tree/master/src/content/en/updates/posts/2016/06/passive-event-listeners.markdown\">Github</a></p>\n<!--kg-card-end: markdown-->","comment_id":"30","plaintext":"For years we've complained and asked for a way to bind to touch and mouse events\nthat do not require and should not change the built in scrolling actions.\n\n> Available on Chrome 51, Chrome Mobile 51, Safari Mobile 51, and Android WebView\nrelease 51\n\n\nFinally it has landed on our devices and as a dev i'm pretty excited, but there\nare a few gotcha's as usual.\n\nSimple usage\nThe third existing option on addEventListener has been replaced, it is no longer\na boolean for the capture argument but rather a new object called \nEventListenerOptions which has some new properties; capture as expected and the\nnew passive which both take boolean values.\n\nIn a number of common scenarios events don't need to block scrolling - for\ninstance:\n\n * User activity monitoring which just wants to know when the user was last\n   active\n * touchstart handlers that hide some active UI (like tooltips)\n * touchstart and touchend handlers that style UI elements (without suppressing\n   the click event).\n\nFor these scenarios, the passive option can be added (with appropriate feature\ndetection) without any other code changes, resulting in a significantly smoother\nscrolling experience.\n\nDisable scrolling, touch, or wheel events\nThere are a few complicated scenarios where the handler only wants to suppress\nscrolling under certain conditions, such as:\n\n * A UI element (like YouTube's volume slider) which slides on horizontal wheel\n   events without changing the scrolling behavior on vertical wheel events\n * Swiping horizontally to rotate a carousel, dismiss an item or reveal a\n   drawer, while still permitting vertical scrolling\n\nSince there is no equivalent of \"touch action\" for wheel events, you can\nimplement them with non-passive wheel listeners.\n\nSome options include;\n\nIn this case, use touch-action: pan-y to declaratively disable scrolling that\nstarts along the horizontal axis without having to call preventDefault(), and\nusing touch-action: pan-x will work the same on the vertical axis.\n\nTo consistently disable scrolling by cancelling all touch or wheel events like;\n\n * Panning and zooming a map\n * Full-page/full-screen games\n\nUse the touch-action: none on your EventListenerOptions.\n\nGotcha 1\nScroll modifications are disabled in handlers\nBy using this feature you effectively tell the browser that you will not be\ncalling e.preventDefault() in your event handler which normally allows scrolling\nto start (at a later stage) while still executing your handler.\n\nTherefore e.preventDefault() becomes redundant;\n\naddEventListener(document, \"touchstart\", function(e) {\n  console.log(e.defaultPrevented);  // will be false\n  e.preventDefault();   // does nothing since the listener is passive\n  console.log(e.defaultPrevented);  // still false\n}, {passive: true});\n\n\nCalling it while using passive: true has no impact and can be safely omitted.\n\nGotcha 2\nBackwards compatibility is broken\nAs you might suspect from the new object EventListenerOptions replacing the\nthird argument we have not only a cross browser conflict but also a supporting\nbrowser backwards compatibility issue.\n\nIf your app doesn't currently use the third option and you know for certain that\nit won't in future, you still cannot use passive event listeners now because by\npassing up the EventListenerOptions you are breaking your event listener on\nnon-supporting browsers.\n\nYou will need to manually handle feature detection or use a polyfil\n[https://github.com/WICG/EventListenerOptions/blob/gh-pages/EventListenerOptions.polyfill.js]\n.\n\nGotcha 3\nFeature detection is a thread blocking check\nThe fact that the way we need to check for the feature is thread blocking makes\nthis troublesome.\n\nTry using the feature to ensure it is available;\n\nvar supportsPassive = false;\ntry {\n  var opts = Object.defineProperty({}, 'passive', {\n    get: function() {\n      supportsPassive = true;\n    }\n  });\n  window.addEventListener(\"test\", null, opts);\n} catch (e) {}\n\nelem.addEventListener('touchstart', fn, supportsPassive ? { passive: true } : false); \n\n\nYou might need to consider doing this only once, immediately after you've run\nall of your DOM ready functionality so as to not block initial rendering, and\nthen defer all of your event listeners until last. This to me is an anti-pattern\nand unfortunately there is no better performing way to manage this limitation,\nyou could make the decision to block initially rendering and do the check\nimmediately before ready state 4, but that's your choice.\n\nAlternatively you could use the defacto JavaScript feature detection library\nModernizr but it's implementation is hidden and you'll not be sure if it is\nblocking or otherwise;\n\nelem.addEventListener('touchstart', fn, Modernizr.passiveeventlisteners ? {passive:true} : false);\n\n\nHow about making passive default?\nNot only have we been given a brilliant new feature that breaks backwards\ncompatibility, we are also forced to opt-in to use it as it is not set by\ndefault. Considering the minority of use cases where you would not want to use\npassive one would expect the default be set to passive!\n\nSome quick dev has put together a tiny Github repo to enable the setting by\ndefault when you call addEventListener, you can find it here\n[https://github.com/zzarcon/default-passive-events].\n\nIn short\nWhen talking about performance, scrolling is a key aspect of your app, this\nmeans that as soon as the users perceives a janky scroll on your page they will\nquickly bounce and not likely spread the word about what you've built unless in\nfrustration.\n\nI strongly suggest all people in a position to implement this do so ASAP.\n\n> Read all about how it works\n[https://developers.google.com/web/updates/2016/06/passive-event-listeners?hl=en] \non Google Developers.\n\n\nYou can find the source code on Github\n[https://github.com/Google/WebFundamentals/tree/master/src/content/en/updates/posts/2016/06/passive-event-listeners.markdown]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/maxresdefault.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-26 22:17:11","created_by":"1","updated_at":"2021-03-31 14:14:07","updated_by":"1","published_at":"2016-07-04 22:37:19","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca1","uuid":"234648d6-e149-423e-a722-30858376724d","title":"AWS Autoscaling Best Practices","slug":"aws-autoscaling-best-practices-for-startups","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"#### Welcome to Auto Scaling in the Amazon Cloud\\n\\nIf the fundamental premise of the \\\"cloud\\\" is \\n\\n> use only the resources you need for as long as you need them\\n\\nSo for a website you save money when traffic is low; dynamic scaling based on traffic.\\nEnterprise SaaS is great for AWS since customers are using your product during typical business hours, resulting in very low traffic at night. \\n\\nSo what does it take to make the switch?\\n\\nIdeally, you want to be dealing with a stateless application, where terminating one node won’t produce side effects on user experience. Typical stateless apps don’t rely on keeping session state in memory on the same server that processes the web request.\\n\\n> Unfortunately not all software is created equal. \\n\\nOne thing with dynamically terminating server instances is that you can’t rely on them being accessible at any time, two main considerations have to be made;\\nProvisioning an instance is automated through simple CloudFormation templates, and logs need to be forwarded to a remote host.\\n\\nConsider what would happen if one instance were to terminate with active sessions; you’re impacting user experience. But there is a solution; using the Elastic Load Balancer (ELB) to which we’ll come back shortly.\\n\\n#### Architecting the app around Auto Scaling\\n\\nFirst and foremost, you'll need the app to properly report your chosen autoscaling metric to CloudWatch. There are some built-in metrics such as memory utilisation or CPU, network, etc. but I assure you that these measurements alone will work only for the very limited few apps and you will need to identify and create your own custom metrics to be reported after just a few days of running in an autoscaled environment.\\n\\nI chose to develop an algorithm which gathers metrics from all running instances in the autoscale group and reports how efficient that group will operate with the current load if we were to remove just one single instance. Using this custom metric we compare the value to what we have set as the scale up policy (when we would need to add one instance) and if we would not trigger the scale up policy by removing one instance we consider the scale down event to be positive and therefore remove one instance. Immediately, the metric adjusts to the new count every time an instance is added or removed.\\n\\n### On EC2 Auto Scale groups\\n\\nThe EC2 Auto-Scale group detects \\\"unhealthy\\\" instances and terminates them, then adjusting (sometimes taking no action) dependant on the current \\\"desired\\\" instance count. The desired count is determined by the current state of the group which uses your defined CloudWatch metric measurements in your auto scale policies.\\n\\nSome consultants and developer bloggers will tell you that the Auto Scale group needs the instance to respond a 200 HTTP status code when it is polled by the Auto Scale group or it will be terminated, this is simply untrue. There is no HTTP request being made, that implies a running web server must be installed on the instance, and in turn that the fact a running webserver is the definition of \\\"healthy\\\" which is simply preposterous. EC2 instances are not exclusive for web server use.\\n\\nIn fact, these people are getting confused with the EC2 Elastic Load Balancer service (ELB) health check; which will never be concerned about auto scale groups or CloudWatch metrics and deals only with incoming HTTP requests routing them to a healthy EC2 instance with a running web server. The ELB health check to determine \\\"healthy\\\" does indeed expect the EC2 instance to have a running web server of some sort; on a specific port; and returning that HTTP 200 status for a specified file name. Then, if this ELB health check fails the instance _is not_ terminated as some would suggest, the ELB simply recognises that it is unhealthy and will not route any new incoming HTTP requests to that instance until it is healthy.\\n\\nSo back to the Auto Scale group in terms of how the  instance health check is done and terminates unhealthy hosts, this is not very well documented by AWS unfortunately, leading to the misunderstandings among many developers and consultants. But as an engineer I have evaluated through experience that an instance is considered unhealthy if one of the following occurs;\\n\\n- An action performed by the underlying EC2 system is not carried out successfully. For example; if an auto scale event is started and was unable to complete successfully.\\n\\n- EC2 predefined CloudWatch metrics fail to report, i.e. INSUFFICIENT_DATA. This may include network, CPU, or memory.\\n\\n- The instance becomes unresponsive, Which is the most interesting of the 3;\\n\\nFor an example of what I mean by _unresponsive_; the EC2 console makes requests to the EC2 backend when you do things like 'refresh' a view, and this, in turn, requires the EC2 backend to communicate with the instance and here it may be determined as unresponsive. I encountered this when the internal AWS networks in the Sydney region became irregular during floods. I could SSH into my instance using the public IP from the office in Melbourne but I was not able to communicate over the internal AWS network using a private IP from another instance in the same availability zone and the Auto-Scale group eventually terminated all of the running and publicly accessible instances!\\n\\n### Capacity planning\\n\\nIn terms of what needs autoscaling, also covering what can be put behind ELBs, and what can operate without ELBs.\\n\\n**Memory optimised**: Auto scaled instances that do not process web requests. The best example that comes to mind is on-demand 3rd party data processing. \\nFor example, you may have a requirement to gather information about flights and translate the multiple data sources into a standard your application can recognise and process. The resource requirements would likely fluctuate based on the estimated departure or arrival times, e.g. You might need to check terminal and gate information from a static data source, which would need to be done less often in the days leading to the time of arrival/departure but more far frequently in the hour before the arrival/departure when the data is likely to change more suddenly. This priority queueing would be high demand during peak periods for the airport and almost negligible during the hours the airport is in night curfew (for some cities). \\n\\nYou will likely have a lot of stateful long-running processes in this scenario which require little in terms of CPU or HDD space but need loads of RAM memory. Choosing a memory optimised instance type works really well.\\n\\n**General purpose**: great for a web stack behind an ELB, this is fairly self-explanatory so I'll be brief. Considerations for high demand are usually based on daylight hours and depending on the domain you operate in you might also need to react to certain events, for example, the airing of an interview or advertisement on a free to air TV show where you'll go from superficial traffic to a flood of interested and engaging traffic.\\n\\nThere are arguments on Memory optimised vs CPU optimised instances, this generally comes down to a decision of being Highly Available (HA) and granular or not HA and prepared for the heavy load.\\n\\nI should make it clear that any web server should have a minimum of 2 CPUs, even when implementing _granular_ scaling methodologies. The webserver (Apache or Nginx) should be able to operate effectively while the code is executing. Running one CPU quickly leads to the ELB queueing incoming requests while the single CPU instance manages the routeing of web requests, processing them, and serving them, all with minimum traffic conditions. The results are sporadic and exaggerated metric reporting on resources triggering far too many conflicting scale actions. Having 2 CPUs is more stable as the instance can route and respond while requests are processed asynchronously.\\n\\n**Compute Optimised**; Being generic here, these instances are not recommended for autoscale with a few exceptions. I would generally advise using compute optimised for a batch processing server, where; you deal with extremely short scripts that do one job without consideration for resource usage, like sending emails, running ETLs, doing backups, push notifications, etc. Basically, anything that runs for a single purpose on a schedule (e.g. cron) and needs to perform its task immediately.\\nIf you are looking at using compute optimised instances in an auto scale group you're going to be outside what I can help you with here. Things like DNA processing and Machine Learning or Decision Tree territory (from personal experience) and needs more analysis that I feel is appropriate for this article.\\n\\n**On being Granular**: This idea is based on getting the most of the resources you operate on, and using (paying for) only what you need to operate efficiently. \\nPersonally i support the idea of being granular, but, I generally have doubts that anyone can truly be prepared for all eventualities whilst sticking to being purely granular. In my experience, high traffic events on a web stack can not be predicted in all cases, therefore you rely almost entirely on the fact you have the right auto scale rules in place to deal with the unknown an inevitable high traffic events. If i sugest you might need to scale from 100 concurrent users to 1000 in a matter of 1min, you can prepare for that because it has been addressed, what i am talking about is being able to stay online and be granular during unforeseeable events we cannot predict to address in advance. this is where your senior management needs to acknowledge that if the unforeseeable occurs it is an acceptable risk, or, you implements HA architecture as well as being granular in preparation of these unforeseeable events.\\n\\n**On being Highly Available**\\nThe considerations for being Highly Available could be;\\n\\n- Physical faults in one availability zone (AZ) fall back to an alternate AZ.\\n- Major network problems in regions will fallback to alternate a region.\\n- High traffic events on your ELB distribute evenly across multiple alternate AZ. Because an auto scale group is per a single AZ.\\n\\nThe single obvious difference between granular and HA is granular is localised to an auto scale group on a single AZ, and HA is having redundancies which basically means capacity needs to be 100% more than current demand at any given time.\\nOr more simply, granular intends to save money at the cost of being failure resistant, and HA aims to ensure that during the event of unforeseeable disasters you still operate at 100%.\\n\\nYou should not choose between these but rather strive to achieve a combination of both!\\nBeing HA only means that you operate 2x what you would have setup up granular or otherwise, granular just ensures you aren't wasting any resources.\\n\\n### Load Balancing\\n\\nHow might you normally manage SSL certificates for HTTPS? An ELB has the ability to manage your SSL and cyphers for you, I have experience using the misunderstood multi-site (UCC) SSL cert. but I won't go into the details on how beneficial UCC will be for you only that I strongly recommend using one for your ELB as it is almost definitely going to be needed as you scale your business.\\n\\nBasically, the ELB will handle all your incoming port 443 HTTPS requests and route them to the EC2 instance over the encrypted internal AWS private network for your web server to then handle and respond to. The web server usually requires developers to manage the SSL certificates in an unintuitive way prone to errors and usually expire suddenly and forgotten.\\n\\nWith the ELB this archaic pattern can be left to the data center geezers, because we are already communicating across the AWS private network and the ELB can actually route all of its incoming HTTPS port 443 to your EC2 instance as HTTP on port 80, meaning your web servers don't need to manage the SSL certs any longer but external connects are still HTTPS> This centralises the one cert on the ELB instead of the management of SSL for each instance. If your security alarms are ringing I'd suggest only 1 thing, if you already use EC2 you must trust AWS, configuring the ELB in this way is no more insecure than simply using the same EC2 instance.\\n\\nThe ELB also provides a feature called _connection draining_. This is particularly important for an auto scale group because when a scale event terminates an EC2 instance the ELB only knows about it when it is too late, so instead of terminating requests which to a user appears like you are offline, instead the setting allows the EC2 instance to remain responsive for as long as it takes to serve its current open incoming connections from the ELB and the ELB will not send any new requests to this instance.\\n\\n### Mitigating the bottleneck (database)\\n\\nWhen we talk about auto scaling and being efficient, we must address the elephant in the room. All businesses have a bottleneck and it is likely that yours is database I/O. \\n\\nI have written about database optimisations previously so I'll not go into the specifics here. basically, we should try to reduce the time our auto-scaled instances spend talking to RDS (or any other database) and do their job so they can operate as efficient as possible and scale less.\\n\\nAWS have tools specifically designed for this;\\nCloudFront, Elasticache, and DynamoDB. \\n\\n**CloudFront (CF)**: is a CDN, but it can be more than that;\\n\\n- CF is most commonly utilised to serve static assets (CSS, JS, and Images) stored in S3 buckets. S3 is not distributed, your assets are located in a single region, CF will cache the asset from S3 in edge locations all around the world closest to the user requesting it for faster delivery to the browser and will only contact the S3 bucket if the expiry of the cached resource has expired.  \\n- You might need to deliver _streaming video_ transcoded for consumption on the web or using Apple's HLS for iOS apps.\\n- In addition to static assets, it is also a consideration to have CF cache configuration files for your Apps, or even the structured data (JSON, SOAP, XML) responses from your APIs. This means your servers will be hit less often to deliver content you deem cacheable (for a time).\\n- Similar to the last point, you might also wish to cache certain web pages, some examples may include templates typically requested by service workers or dumped in the DOM for an ajax call to hook up with data at a later stage. These are excellent candidates. Other possibilities are pages that attract superficial traffic, so pages like about us, contact us, careers, and generally any page that is considered to not have on demand dynamic content should be cached for at least 7 hours (per Facebook's recommendation).\\n- More on superficial traffic; if you have most of your website dynamic features available only to logged in users, on pages that can also be accessed when not logged in but provide a less than dynamic experience, it will be extremely beneficial to have CF cache these pages for users that are not logged in for at least 2mins (again a Facebook recommendation), this may give the superficial traffic a slightly stale page whereas logged in users get to pass-through CF while holding a session and get on-demand access to the dynamic features, which is great news for your Auto Scale group as it will receive far less hits from these superficial users.\\n\\n**Elasticache** is a service that abstracts Memcache or Redis for scalable in-memory caching.\\nIf you've ever setup either Memcache or Redis you may have the impression it is best used for simple data like storing sessions for a clustered Node.js app or for storing simple stateful objects for when a user is load balanced across multiple servers.\\n\\nAlthough these are common uses of the technology, they barely scrape the surface of usefulness. You can utilise Elasticache in a way that effectively makes your database utilisation drop in such a way that any single piece of data is only requested from the DB directly only as often as it changes, with one exception; the Elasticache optimisation algorithms sometimes purge infrequently accessed data from memory, at which point your database will be hit for the requested data even if it is unchanged since it was last accessed.\\n\\nThe key to implementing this type of technique has two major focusses;\\n\\na) Well structured cache keys; for storing the relevant lookup data in the key itself so the data can be targeted for easy fetching and invalidation.\\n\\nb) Cache stampeding; This occurs when highly depended upon objects in memory have suddenly expired and there is a stampede of requests incoming for that data all at virtually the same moment. This will result in the following pattern;\\n\\n1) Request cache exists > cache expired\\n2) Request data from DB, wait for it to be received during which more requests have com in and are doing the same.\\n3) Tell Elasticache to store the new data. the all other requests that came through before also tell Elasticache to store _thier_ version of the data from the DB.\\n4) Following requests get served from the cache. \\n\\nThis is obviously inefficient, and in fact, can generate unwanted read IOPS on your DB and crippling write IOPS concerns for your Elasticache nodes.\\n\\nTo fix this we need to tell Elasticache to store our objects a little longer then the applications needs, for instance, when we intend to store our object for 7hrs we have that 7hr value stored in our application but when our call goes to Elasticache simply take the 7hrs and add 10 seconds. This is so that later after the 7hrs has passed when our application expects the object to be expired we still can access that stored object for 10 seconds longer! It'll make more sense in a moment. \\nFor our application will be able to have this we to actually store two pieces of data into Elasticache; our object data, and when it was stored. Our application code is already aware that this object is to be stored for 7hrs because it is coded into the the request so that when it encounters expired caches it can request and then store the new data for the appropriate time, therefore, by having the created time stored with the object we actually want we can instantly return the requested object to our requester and spawn off another thread to update the object from the DB with fresh data without interfering with the requester.\\n \\nThis also allows the cache stampede to be mitigated because the object is still returned, and during the few moments after the first requester is taking to update the object we still can serve it from cache while it is being updated!\\n\\nI use this not only to reduce I/O from my Auto Scaled instances to my RDS, but to mitigate the IOPS limitations imposed by AWS on their RDS product. This Elasticache layer is far cheaper than PIOPS and considerably more performant and powerful than RDS.\\n\\nI mentioned session data is a candidate for usual Memcache or Redis implementations earlier, I'd like to make it clear that although sessions are commonly saved to these technologies they are by far not appropriate for Elasticache. For one reason alone, AWS controls the policy for purging data to keep the service optimised, whereas your own installation of Redis or Memcache can be de-optimised so as to not aggressively purge the important session data. \\nThe last thing you want is to have your logged in users kicked out sporadically. \\n\\nElasticache is a powerful tool and should be respected for its use cases and sessions are better stored in a more static centralised way, for example, a document storage DB;\\n\\n**DynamoDB** is built on top of MongoDB, a document storage database, ready for scalability through abstraction but nonetheless still an amazing document storage database.\\n\\nI have experience using DynamoDB for high demand, real-time, and reliable data. DynamoDB costings can easily get out of hand so you must be mindful that the data stored in DynamoDB actually belongs there.\\n\\nOther than Session data being a great candidate due to sessions lasting usually no longer than a few hours a day, another great candidate for DynamoDB is auction data.\\n\\nAny great candidates for DynamoDB are data that is only meaningful for a defined timeframe, and may be aggressively updated whilst just as aggressively being requested. \\n\\nThink about the data for an eBay auction, it's current highest bid and all of the participating user's max bids that are only meaningful before the auction closes. After the auction this data becomes static, it won't change anymore and can be stored more permanently in RDS.\\n\\nDynamoDB is suited perfectly for this task as it is fast and handles concurrent changes well without blocking requests. Also, the cost is a non-issue if you are dealing with a single object for all your changes and remove the data as soon as it is no longer meaningful.\\n\\nUsing DynamoDB in this way relieves tremendous pressure off your RDS and generally the entire web stack, so I urge you to identify data such as this in your own applications and utilise document storage the right way. Technology like MongoDB is not best suited to store all of your data, your application and business will be better off using relational databases over document storage for long-term data needs.\\n\\n### To sum up\\n\\nI've covered a huge amount of topics and given you many techniques to consider that it's likely making your head spin, so let me bullet point this for you;\\n\\n- Ensure your EC2 instances are able to be thrown away (terminated) by centralising logs in S3 and delegate responsibility for data and state storage elsewhere\\n\\n- Craft your auto scale actions and CloudWatch metrics carefully. Be proactive in reporting engineered data for your metrics as scaling should be intimately unique for your particular business case.\\n\\n- Choose the right EC2 instance for the task it performs. Make sure you understand being granular might affect the instances ability to do its job, i.e. ensure web server instances have at least 2x CPUs\\n\\n- Have the ELB manage your SSL cert to reduce complexity in each instance\\n\\n- Implement HA methodology, it is actually more important to do this for granular setups contrary to what you may have been told before.\\n\\n- Use CloundFront, it is very cheap and provides the most significant performance bang for buck.\\n\\n- Identify and store all data in Elasicache, bake it into the way you communicate with your relational database/s that are your bottleneck, always. Your IOPS will become significantly better.\\n\\n- Don't be afraid of the cost to use DynamoDB, in fact, it can be cheaper than relational or memory alternatives for the right data when you consider the impact that data might have on your RDS throughput, and the user reliability and performance benefits are second to none.\\n\\nUnderstanding how to make the right decision on AWS will save you unmeasurably when you are able to scale without incident. \\nChoosing MVPs and outsourcing development over proper AWS planning and architectured the right way could be the difference between success and failure for a startup.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h4 id=\"welcometoautoscalingintheamazoncloud\">Welcome to Auto Scaling in the Amazon Cloud</h4>\n<p>If the fundamental premise of the &quot;cloud&quot; is</p>\n<blockquote>\n<p>use only the resources you need for as long as you need them</p>\n</blockquote>\n<p>So for a website you save money when traffic is low; dynamic scaling based on traffic.<br>\nEnterprise SaaS is great for AWS since customers are using your product during typical business hours, resulting in very low traffic at night.</p>\n<p>So what does it take to make the switch?</p>\n<p>Ideally, you want to be dealing with a stateless application, where terminating one node won’t produce side effects on user experience. Typical stateless apps don’t rely on keeping session state in memory on the same server that processes the web request.</p>\n<blockquote>\n<p>Unfortunately not all software is created equal.</p>\n</blockquote>\n<p>One thing with dynamically terminating server instances is that you can’t rely on them being accessible at any time, two main considerations have to be made;<br>\nProvisioning an instance is automated through simple CloudFormation templates, and logs need to be forwarded to a remote host.</p>\n<p>Consider what would happen if one instance were to terminate with active sessions; you’re impacting user experience. But there is a solution; using the Elastic Load Balancer (ELB) to which we’ll come back shortly.</p>\n<h4 id=\"architectingtheapparoundautoscaling\">Architecting the app around Auto Scaling</h4>\n<p>First and foremost, you'll need the app to properly report your chosen autoscaling metric to CloudWatch. There are some built-in metrics such as memory utilisation or CPU, network, etc. but I assure you that these measurements alone will work only for the very limited few apps and you will need to identify and create your own custom metrics to be reported after just a few days of running in an autoscaled environment.</p>\n<p>I chose to develop an algorithm which gathers metrics from all running instances in the autoscale group and reports how efficient that group will operate with the current load if we were to remove just one single instance. Using this custom metric we compare the value to what we have set as the scale up policy (when we would need to add one instance) and if we would not trigger the scale up policy by removing one instance we consider the scale down event to be positive and therefore remove one instance. Immediately, the metric adjusts to the new count every time an instance is added or removed.</p>\n<h3 id=\"onec2autoscalegroups\">On EC2 Auto Scale groups</h3>\n<p>The EC2 Auto-Scale group detects &quot;unhealthy&quot; instances and terminates them, then adjusting (sometimes taking no action) dependant on the current &quot;desired&quot; instance count. The desired count is determined by the current state of the group which uses your defined CloudWatch metric measurements in your auto scale policies.</p>\n<p>Some consultants and developer bloggers will tell you that the Auto Scale group needs the instance to respond a 200 HTTP status code when it is polled by the Auto Scale group or it will be terminated, this is simply untrue. There is no HTTP request being made, that implies a running web server must be installed on the instance, and in turn that the fact a running webserver is the definition of &quot;healthy&quot; which is simply preposterous. EC2 instances are not exclusive for web server use.</p>\n<p>In fact, these people are getting confused with the EC2 Elastic Load Balancer service (ELB) health check; which will never be concerned about auto scale groups or CloudWatch metrics and deals only with incoming HTTP requests routing them to a healthy EC2 instance with a running web server. The ELB health check to determine &quot;healthy&quot; does indeed expect the EC2 instance to have a running web server of some sort; on a specific port; and returning that HTTP 200 status for a specified file name. Then, if this ELB health check fails the instance <em>is not</em> terminated as some would suggest, the ELB simply recognises that it is unhealthy and will not route any new incoming HTTP requests to that instance until it is healthy.</p>\n<p>So back to the Auto Scale group in terms of how the  instance health check is done and terminates unhealthy hosts, this is not very well documented by AWS unfortunately, leading to the misunderstandings among many developers and consultants. But as an engineer I have evaluated through experience that an instance is considered unhealthy if one of the following occurs;</p>\n<ul>\n<li>\n<p>An action performed by the underlying EC2 system is not carried out successfully. For example; if an auto scale event is started and was unable to complete successfully.</p>\n</li>\n<li>\n<p>EC2 predefined CloudWatch metrics fail to report, i.e. INSUFFICIENT_DATA. This may include network, CPU, or memory.</p>\n</li>\n<li>\n<p>The instance becomes unresponsive, Which is the most interesting of the 3;</p>\n</li>\n</ul>\n<p>For an example of what I mean by <em>unresponsive</em>; the EC2 console makes requests to the EC2 backend when you do things like 'refresh' a view, and this, in turn, requires the EC2 backend to communicate with the instance and here it may be determined as unresponsive. I encountered this when the internal AWS networks in the Sydney region became irregular during floods. I could SSH into my instance using the public IP from the office in Melbourne but I was not able to communicate over the internal AWS network using a private IP from another instance in the same availability zone and the Auto-Scale group eventually terminated all of the running and publicly accessible instances!</p>\n<h3 id=\"capacityplanning\">Capacity planning</h3>\n<p>In terms of what needs autoscaling, also covering what can be put behind ELBs, and what can operate without ELBs.</p>\n<p><strong>Memory optimised</strong>: Auto scaled instances that do not process web requests. The best example that comes to mind is on-demand 3rd party data processing.<br>\nFor example, you may have a requirement to gather information about flights and translate the multiple data sources into a standard your application can recognise and process. The resource requirements would likely fluctuate based on the estimated departure or arrival times, e.g. You might need to check terminal and gate information from a static data source, which would need to be done less often in the days leading to the time of arrival/departure but more far frequently in the hour before the arrival/departure when the data is likely to change more suddenly. This priority queueing would be high demand during peak periods for the airport and almost negligible during the hours the airport is in night curfew (for some cities).</p>\n<p>You will likely have a lot of stateful long-running processes in this scenario which require little in terms of CPU or HDD space but need loads of RAM memory. Choosing a memory optimised instance type works really well.</p>\n<p><strong>General purpose</strong>: great for a web stack behind an ELB, this is fairly self-explanatory so I'll be brief. Considerations for high demand are usually based on daylight hours and depending on the domain you operate in you might also need to react to certain events, for example, the airing of an interview or advertisement on a free to air TV show where you'll go from superficial traffic to a flood of interested and engaging traffic.</p>\n<p>There are arguments on Memory optimised vs CPU optimised instances, this generally comes down to a decision of being Highly Available (HA) and granular or not HA and prepared for the heavy load.</p>\n<p>I should make it clear that any web server should have a minimum of 2 CPUs, even when implementing <em>granular</em> scaling methodologies. The webserver (Apache or Nginx) should be able to operate effectively while the code is executing. Running one CPU quickly leads to the ELB queueing incoming requests while the single CPU instance manages the routeing of web requests, processing them, and serving them, all with minimum traffic conditions. The results are sporadic and exaggerated metric reporting on resources triggering far too many conflicting scale actions. Having 2 CPUs is more stable as the instance can route and respond while requests are processed asynchronously.</p>\n<p><strong>Compute Optimised</strong>; Being generic here, these instances are not recommended for autoscale with a few exceptions. I would generally advise using compute optimised for a batch processing server, where; you deal with extremely short scripts that do one job without consideration for resource usage, like sending emails, running ETLs, doing backups, push notifications, etc. Basically, anything that runs for a single purpose on a schedule (e.g. cron) and needs to perform its task immediately.<br>\nIf you are looking at using compute optimised instances in an auto scale group you're going to be outside what I can help you with here. Things like DNA processing and Machine Learning or Decision Tree territory (from personal experience) and needs more analysis that I feel is appropriate for this article.</p>\n<p><strong>On being Granular</strong>: This idea is based on getting the most of the resources you operate on, and using (paying for) only what you need to operate efficiently.<br>\nPersonally i support the idea of being granular, but, I generally have doubts that anyone can truly be prepared for all eventualities whilst sticking to being purely granular. In my experience, high traffic events on a web stack can not be predicted in all cases, therefore you rely almost entirely on the fact you have the right auto scale rules in place to deal with the unknown an inevitable high traffic events. If i sugest you might need to scale from 100 concurrent users to 1000 in a matter of 1min, you can prepare for that because it has been addressed, what i am talking about is being able to stay online and be granular during unforeseeable events we cannot predict to address in advance. this is where your senior management needs to acknowledge that if the unforeseeable occurs it is an acceptable risk, or, you implements HA architecture as well as being granular in preparation of these unforeseeable events.</p>\n<p><strong>On being Highly Available</strong><br>\nThe considerations for being Highly Available could be;</p>\n<ul>\n<li>Physical faults in one availability zone (AZ) fall back to an alternate AZ.</li>\n<li>Major network problems in regions will fallback to alternate a region.</li>\n<li>High traffic events on your ELB distribute evenly across multiple alternate AZ. Because an auto scale group is per a single AZ.</li>\n</ul>\n<p>The single obvious difference between granular and HA is granular is localised to an auto scale group on a single AZ, and HA is having redundancies which basically means capacity needs to be 100% more than current demand at any given time.<br>\nOr more simply, granular intends to save money at the cost of being failure resistant, and HA aims to ensure that during the event of unforeseeable disasters you still operate at 100%.</p>\n<p>You should not choose between these but rather strive to achieve a combination of both!<br>\nBeing HA only means that you operate 2x what you would have setup up granular or otherwise, granular just ensures you aren't wasting any resources.</p>\n<h3 id=\"loadbalancing\">Load Balancing</h3>\n<p>How might you normally manage SSL certificates for HTTPS? An ELB has the ability to manage your SSL and cyphers for you, I have experience using the misunderstood multi-site (UCC) SSL cert. but I won't go into the details on how beneficial UCC will be for you only that I strongly recommend using one for your ELB as it is almost definitely going to be needed as you scale your business.</p>\n<p>Basically, the ELB will handle all your incoming port 443 HTTPS requests and route them to the EC2 instance over the encrypted internal AWS private network for your web server to then handle and respond to. The web server usually requires developers to manage the SSL certificates in an unintuitive way prone to errors and usually expire suddenly and forgotten.</p>\n<p>With the ELB this archaic pattern can be left to the data center geezers, because we are already communicating across the AWS private network and the ELB can actually route all of its incoming HTTPS port 443 to your EC2 instance as HTTP on port 80, meaning your web servers don't need to manage the SSL certs any longer but external connects are still HTTPS&gt; This centralises the one cert on the ELB instead of the management of SSL for each instance. If your security alarms are ringing I'd suggest only 1 thing, if you already use EC2 you must trust AWS, configuring the ELB in this way is no more insecure than simply using the same EC2 instance.</p>\n<p>The ELB also provides a feature called <em>connection draining</em>. This is particularly important for an auto scale group because when a scale event terminates an EC2 instance the ELB only knows about it when it is too late, so instead of terminating requests which to a user appears like you are offline, instead the setting allows the EC2 instance to remain responsive for as long as it takes to serve its current open incoming connections from the ELB and the ELB will not send any new requests to this instance.</p>\n<h3 id=\"mitigatingthebottleneckdatabase\">Mitigating the bottleneck (database)</h3>\n<p>When we talk about auto scaling and being efficient, we must address the elephant in the room. All businesses have a bottleneck and it is likely that yours is database I/O.</p>\n<p>I have written about database optimisations previously so I'll not go into the specifics here. basically, we should try to reduce the time our auto-scaled instances spend talking to RDS (or any other database) and do their job so they can operate as efficient as possible and scale less.</p>\n<p>AWS have tools specifically designed for this;<br>\nCloudFront, Elasticache, and DynamoDB.</p>\n<p><strong>CloudFront (CF)</strong>: is a CDN, but it can be more than that;</p>\n<ul>\n<li>CF is most commonly utilised to serve static assets (CSS, JS, and Images) stored in S3 buckets. S3 is not distributed, your assets are located in a single region, CF will cache the asset from S3 in edge locations all around the world closest to the user requesting it for faster delivery to the browser and will only contact the S3 bucket if the expiry of the cached resource has expired.</li>\n<li>You might need to deliver <em>streaming video</em> transcoded for consumption on the web or using Apple's HLS for iOS apps.</li>\n<li>In addition to static assets, it is also a consideration to have CF cache configuration files for your Apps, or even the structured data (JSON, SOAP, XML) responses from your APIs. This means your servers will be hit less often to deliver content you deem cacheable (for a time).</li>\n<li>Similar to the last point, you might also wish to cache certain web pages, some examples may include templates typically requested by service workers or dumped in the DOM for an ajax call to hook up with data at a later stage. These are excellent candidates. Other possibilities are pages that attract superficial traffic, so pages like about us, contact us, careers, and generally any page that is considered to not have on demand dynamic content should be cached for at least 7 hours (per Facebook's recommendation).</li>\n<li>More on superficial traffic; if you have most of your website dynamic features available only to logged in users, on pages that can also be accessed when not logged in but provide a less than dynamic experience, it will be extremely beneficial to have CF cache these pages for users that are not logged in for at least 2mins (again a Facebook recommendation), this may give the superficial traffic a slightly stale page whereas logged in users get to pass-through CF while holding a session and get on-demand access to the dynamic features, which is great news for your Auto Scale group as it will receive far less hits from these superficial users.</li>\n</ul>\n<p><strong>Elasticache</strong> is a service that abstracts Memcache or Redis for scalable in-memory caching.<br>\nIf you've ever setup either Memcache or Redis you may have the impression it is best used for simple data like storing sessions for a clustered Node.js app or for storing simple stateful objects for when a user is load balanced across multiple servers.</p>\n<p>Although these are common uses of the technology, they barely scrape the surface of usefulness. You can utilise Elasticache in a way that effectively makes your database utilisation drop in such a way that any single piece of data is only requested from the DB directly only as often as it changes, with one exception; the Elasticache optimisation algorithms sometimes purge infrequently accessed data from memory, at which point your database will be hit for the requested data even if it is unchanged since it was last accessed.</p>\n<p>The key to implementing this type of technique has two major focusses;</p>\n<p>a) Well structured cache keys; for storing the relevant lookup data in the key itself so the data can be targeted for easy fetching and invalidation.</p>\n<p>b) Cache stampeding; This occurs when highly depended upon objects in memory have suddenly expired and there is a stampede of requests incoming for that data all at virtually the same moment. This will result in the following pattern;</p>\n<ol>\n<li>Request cache exists &gt; cache expired</li>\n<li>Request data from DB, wait for it to be received during which more requests have com in and are doing the same.</li>\n<li>Tell Elasticache to store the new data. the all other requests that came through before also tell Elasticache to store <em>thier</em> version of the data from the DB.</li>\n<li>Following requests get served from the cache.</li>\n</ol>\n<p>This is obviously inefficient, and in fact, can generate unwanted read IOPS on your DB and crippling write IOPS concerns for your Elasticache nodes.</p>\n<p>To fix this we need to tell Elasticache to store our objects a little longer then the applications needs, for instance, when we intend to store our object for 7hrs we have that 7hr value stored in our application but when our call goes to Elasticache simply take the 7hrs and add 10 seconds. This is so that later after the 7hrs has passed when our application expects the object to be expired we still can access that stored object for 10 seconds longer! It'll make more sense in a moment.<br>\nFor our application will be able to have this we to actually store two pieces of data into Elasticache; our object data, and when it was stored. Our application code is already aware that this object is to be stored for 7hrs because it is coded into the the request so that when it encounters expired caches it can request and then store the new data for the appropriate time, therefore, by having the created time stored with the object we actually want we can instantly return the requested object to our requester and spawn off another thread to update the object from the DB with fresh data without interfering with the requester.</p>\n<p>This also allows the cache stampede to be mitigated because the object is still returned, and during the few moments after the first requester is taking to update the object we still can serve it from cache while it is being updated!</p>\n<p>I use this not only to reduce I/O from my Auto Scaled instances to my RDS, but to mitigate the IOPS limitations imposed by AWS on their RDS product. This Elasticache layer is far cheaper than PIOPS and considerably more performant and powerful than RDS.</p>\n<p>I mentioned session data is a candidate for usual Memcache or Redis implementations earlier, I'd like to make it clear that although sessions are commonly saved to these technologies they are by far not appropriate for Elasticache. For one reason alone, AWS controls the policy for purging data to keep the service optimised, whereas your own installation of Redis or Memcache can be de-optimised so as to not aggressively purge the important session data.<br>\nThe last thing you want is to have your logged in users kicked out sporadically.</p>\n<p>Elasticache is a powerful tool and should be respected for its use cases and sessions are better stored in a more static centralised way, for example, a document storage DB;</p>\n<p><strong>DynamoDB</strong> is built on top of MongoDB, a document storage database, ready for scalability through abstraction but nonetheless still an amazing document storage database.</p>\n<p>I have experience using DynamoDB for high demand, real-time, and reliable data. DynamoDB costings can easily get out of hand so you must be mindful that the data stored in DynamoDB actually belongs there.</p>\n<p>Other than Session data being a great candidate due to sessions lasting usually no longer than a few hours a day, another great candidate for DynamoDB is auction data.</p>\n<p>Any great candidates for DynamoDB are data that is only meaningful for a defined timeframe, and may be aggressively updated whilst just as aggressively being requested.</p>\n<p>Think about the data for an eBay auction, it's current highest bid and all of the participating user's max bids that are only meaningful before the auction closes. After the auction this data becomes static, it won't change anymore and can be stored more permanently in RDS.</p>\n<p>DynamoDB is suited perfectly for this task as it is fast and handles concurrent changes well without blocking requests. Also, the cost is a non-issue if you are dealing with a single object for all your changes and remove the data as soon as it is no longer meaningful.</p>\n<p>Using DynamoDB in this way relieves tremendous pressure off your RDS and generally the entire web stack, so I urge you to identify data such as this in your own applications and utilise document storage the right way. Technology like MongoDB is not best suited to store all of your data, your application and business will be better off using relational databases over document storage for long-term data needs.</p>\n<h3 id=\"tosumup\">To sum up</h3>\n<p>I've covered a huge amount of topics and given you many techniques to consider that it's likely making your head spin, so let me bullet point this for you;</p>\n<ul>\n<li>\n<p>Ensure your EC2 instances are able to be thrown away (terminated) by centralising logs in S3 and delegate responsibility for data and state storage elsewhere</p>\n</li>\n<li>\n<p>Craft your auto scale actions and CloudWatch metrics carefully. Be proactive in reporting engineered data for your metrics as scaling should be intimately unique for your particular business case.</p>\n</li>\n<li>\n<p>Choose the right EC2 instance for the task it performs. Make sure you understand being granular might affect the instances ability to do its job, i.e. ensure web server instances have at least 2x CPUs</p>\n</li>\n<li>\n<p>Have the ELB manage your SSL cert to reduce complexity in each instance</p>\n</li>\n<li>\n<p>Implement HA methodology, it is actually more important to do this for granular setups contrary to what you may have been told before.</p>\n</li>\n<li>\n<p>Use CloundFront, it is very cheap and provides the most significant performance bang for buck.</p>\n</li>\n<li>\n<p>Identify and store all data in Elasicache, bake it into the way you communicate with your relational database/s that are your bottleneck, always. Your IOPS will become significantly better.</p>\n</li>\n<li>\n<p>Don't be afraid of the cost to use DynamoDB, in fact, it can be cheaper than relational or memory alternatives for the right data when you consider the impact that data might have on your RDS throughput, and the user reliability and performance benefits are second to none.</p>\n</li>\n</ul>\n<p>Understanding how to make the right decision on AWS will save you unmeasurably when you are able to scale without incident.<br>\nChoosing MVPs and outsourcing development over proper AWS planning and architectured the right way could be the difference between success and failure for a startup.</p>\n<!--kg-card-end: markdown-->","comment_id":"31","plaintext":"Welcome to Auto Scaling in the Amazon Cloud\nIf the fundamental premise of the \"cloud\" is\n\n> use only the resources you need for as long as you need them\n\n\nSo for a website you save money when traffic is low; dynamic scaling based on\ntraffic.\nEnterprise SaaS is great for AWS since customers are using your product during\ntypical business hours, resulting in very low traffic at night.\n\nSo what does it take to make the switch?\n\nIdeally, you want to be dealing with a stateless application, where terminating\none node won’t produce side effects on user experience. Typical stateless apps\ndon’t rely on keeping session state in memory on the same server that processes\nthe web request.\n\n> Unfortunately not all software is created equal.\n\n\nOne thing with dynamically terminating server instances is that you can’t rely\non them being accessible at any time, two main considerations have to be made;\nProvisioning an instance is automated through simple CloudFormation templates,\nand logs need to be forwarded to a remote host.\n\nConsider what would happen if one instance were to terminate with active\nsessions; you’re impacting user experience. But there is a solution; using the\nElastic Load Balancer (ELB) to which we’ll come back shortly.\n\nArchitecting the app around Auto Scaling\nFirst and foremost, you'll need the app to properly report your chosen\nautoscaling metric to CloudWatch. There are some built-in metrics such as memory\nutilisation or CPU, network, etc. but I assure you that these measurements alone\nwill work only for the very limited few apps and you will need to identify and\ncreate your own custom metrics to be reported after just a few days of running\nin an autoscaled environment.\n\nI chose to develop an algorithm which gathers metrics from all running instances\nin the autoscale group and reports how efficient that group will operate with\nthe current load if we were to remove just one single instance. Using this\ncustom metric we compare the value to what we have set as the scale up policy\n(when we would need to add one instance) and if we would not trigger the scale\nup policy by removing one instance we consider the scale down event to be\npositive and therefore remove one instance. Immediately, the metric adjusts to\nthe new count every time an instance is added or removed.\n\nOn EC2 Auto Scale groups\nThe EC2 Auto-Scale group detects \"unhealthy\" instances and terminates them, then\nadjusting (sometimes taking no action) dependant on the current \"desired\"\ninstance count. The desired count is determined by the current state of the\ngroup which uses your defined CloudWatch metric measurements in your auto scale\npolicies.\n\nSome consultants and developer bloggers will tell you that the Auto Scale group\nneeds the instance to respond a 200 HTTP status code when it is polled by the\nAuto Scale group or it will be terminated, this is simply untrue. There is no\nHTTP request being made, that implies a running web server must be installed on\nthe instance, and in turn that the fact a running webserver is the definition of\n\"healthy\" which is simply preposterous. EC2 instances are not exclusive for web\nserver use.\n\nIn fact, these people are getting confused with the EC2 Elastic Load Balancer\nservice (ELB) health check; which will never be concerned about auto scale\ngroups or CloudWatch metrics and deals only with incoming HTTP requests routing\nthem to a healthy EC2 instance with a running web server. The ELB health check\nto determine \"healthy\" does indeed expect the EC2 instance to have a running web\nserver of some sort; on a specific port; and returning that HTTP 200 status for\na specified file name. Then, if this ELB health check fails the instance is not \nterminated as some would suggest, the ELB simply recognises that it is unhealthy\nand will not route any new incoming HTTP requests to that instance until it is\nhealthy.\n\nSo back to the Auto Scale group in terms of how the instance health check is\ndone and terminates unhealthy hosts, this is not very well documented by AWS\nunfortunately, leading to the misunderstandings among many developers and\nconsultants. But as an engineer I have evaluated through experience that an\ninstance is considered unhealthy if one of the following occurs;\n\n * An action performed by the underlying EC2 system is not carried out\n   successfully. For example; if an auto scale event is started and was unable\n   to complete successfully.\n   \n   \n * EC2 predefined CloudWatch metrics fail to report, i.e. INSUFFICIENT_DATA.\n   This may include network, CPU, or memory.\n   \n   \n * The instance becomes unresponsive, Which is the most interesting of the 3;\n   \n   \n\nFor an example of what I mean by unresponsive; the EC2 console makes requests to\nthe EC2 backend when you do things like 'refresh' a view, and this, in turn,\nrequires the EC2 backend to communicate with the instance and here it may be\ndetermined as unresponsive. I encountered this when the internal AWS networks in\nthe Sydney region became irregular during floods. I could SSH into my instance\nusing the public IP from the office in Melbourne but I was not able to\ncommunicate over the internal AWS network using a private IP from another\ninstance in the same availability zone and the Auto-Scale group eventually\nterminated all of the running and publicly accessible instances!\n\nCapacity planning\nIn terms of what needs autoscaling, also covering what can be put behind ELBs,\nand what can operate without ELBs.\n\nMemory optimised: Auto scaled instances that do not process web requests. The\nbest example that comes to mind is on-demand 3rd party data processing.\nFor example, you may have a requirement to gather information about flights and\ntranslate the multiple data sources into a standard your application can\nrecognise and process. The resource requirements would likely fluctuate based on\nthe estimated departure or arrival times, e.g. You might need to check terminal\nand gate information from a static data source, which would need to be done less\noften in the days leading to the time of arrival/departure but more far\nfrequently in the hour before the arrival/departure when the data is likely to\nchange more suddenly. This priority queueing would be high demand during peak\nperiods for the airport and almost negligible during the hours the airport is in\nnight curfew (for some cities).\n\nYou will likely have a lot of stateful long-running processes in this scenario\nwhich require little in terms of CPU or HDD space but need loads of RAM memory.\nChoosing a memory optimised instance type works really well.\n\nGeneral purpose: great for a web stack behind an ELB, this is fairly\nself-explanatory so I'll be brief. Considerations for high demand are usually\nbased on daylight hours and depending on the domain you operate in you might\nalso need to react to certain events, for example, the airing of an interview or\nadvertisement on a free to air TV show where you'll go from superficial traffic\nto a flood of interested and engaging traffic.\n\nThere are arguments on Memory optimised vs CPU optimised instances, this\ngenerally comes down to a decision of being Highly Available (HA) and granular\nor not HA and prepared for the heavy load.\n\nI should make it clear that any web server should have a minimum of 2 CPUs, even\nwhen implementing granular scaling methodologies. The webserver (Apache or\nNginx) should be able to operate effectively while the code is executing.\nRunning one CPU quickly leads to the ELB queueing incoming requests while the\nsingle CPU instance manages the routeing of web requests, processing them, and\nserving them, all with minimum traffic conditions. The results are sporadic and\nexaggerated metric reporting on resources triggering far too many conflicting\nscale actions. Having 2 CPUs is more stable as the instance can route and\nrespond while requests are processed asynchronously.\n\nCompute Optimised; Being generic here, these instances are not recommended for\nautoscale with a few exceptions. I would generally advise using compute\noptimised for a batch processing server, where; you deal with extremely short\nscripts that do one job without consideration for resource usage, like sending\nemails, running ETLs, doing backups, push notifications, etc. Basically,\nanything that runs for a single purpose on a schedule (e.g. cron) and needs to\nperform its task immediately.\nIf you are looking at using compute optimised instances in an auto scale group\nyou're going to be outside what I can help you with here. Things like DNA\nprocessing and Machine Learning or Decision Tree territory (from personal\nexperience) and needs more analysis that I feel is appropriate for this article.\n\nOn being Granular: This idea is based on getting the most of the resources you\noperate on, and using (paying for) only what you need to operate efficiently.\nPersonally i support the idea of being granular, but, I generally have doubts\nthat anyone can truly be prepared for all eventualities whilst sticking to being\npurely granular. In my experience, high traffic events on a web stack can not be\npredicted in all cases, therefore you rely almost entirely on the fact you have\nthe right auto scale rules in place to deal with the unknown an inevitable high\ntraffic events. If i sugest you might need to scale from 100 concurrent users to\n1000 in a matter of 1min, you can prepare for that because it has been\naddressed, what i am talking about is being able to stay online and be granular\nduring unforeseeable events we cannot predict to address in advance. this is\nwhere your senior management needs to acknowledge that if the unforeseeable\noccurs it is an acceptable risk, or, you implements HA architecture as well as\nbeing granular in preparation of these unforeseeable events.\n\nOn being Highly Available\nThe considerations for being Highly Available could be;\n\n * Physical faults in one availability zone (AZ) fall back to an alternate AZ.\n * Major network problems in regions will fallback to alternate a region.\n * High traffic events on your ELB distribute evenly across multiple alternate\n   AZ. Because an auto scale group is per a single AZ.\n\nThe single obvious difference between granular and HA is granular is localised\nto an auto scale group on a single AZ, and HA is having redundancies which\nbasically means capacity needs to be 100% more than current demand at any given\ntime.\nOr more simply, granular intends to save money at the cost of being failure\nresistant, and HA aims to ensure that during the event of unforeseeable\ndisasters you still operate at 100%.\n\nYou should not choose between these but rather strive to achieve a combination\nof both!\nBeing HA only means that you operate 2x what you would have setup up granular or\notherwise, granular just ensures you aren't wasting any resources.\n\nLoad Balancing\nHow might you normally manage SSL certificates for HTTPS? An ELB has the ability\nto manage your SSL and cyphers for you, I have experience using the\nmisunderstood multi-site (UCC) SSL cert. but I won't go into the details on how\nbeneficial UCC will be for you only that I strongly recommend using one for your\nELB as it is almost definitely going to be needed as you scale your business.\n\nBasically, the ELB will handle all your incoming port 443 HTTPS requests and\nroute them to the EC2 instance over the encrypted internal AWS private network\nfor your web server to then handle and respond to. The web server usually\nrequires developers to manage the SSL certificates in an unintuitive way prone\nto errors and usually expire suddenly and forgotten.\n\nWith the ELB this archaic pattern can be left to the data center geezers,\nbecause we are already communicating across the AWS private network and the ELB\ncan actually route all of its incoming HTTPS port 443 to your EC2 instance as\nHTTP on port 80, meaning your web servers don't need to manage the SSL certs any\nlonger but external connects are still HTTPS> This centralises the one cert on\nthe ELB instead of the management of SSL for each instance. If your security\nalarms are ringing I'd suggest only 1 thing, if you already use EC2 you must\ntrust AWS, configuring the ELB in this way is no more insecure than simply using\nthe same EC2 instance.\n\nThe ELB also provides a feature called connection draining. This is particularly\nimportant for an auto scale group because when a scale event terminates an EC2\ninstance the ELB only knows about it when it is too late, so instead of\nterminating requests which to a user appears like you are offline, instead the\nsetting allows the EC2 instance to remain responsive for as long as it takes to\nserve its current open incoming connections from the ELB and the ELB will not\nsend any new requests to this instance.\n\nMitigating the bottleneck (database)\nWhen we talk about auto scaling and being efficient, we must address the\nelephant in the room. All businesses have a bottleneck and it is likely that\nyours is database I/O.\n\nI have written about database optimisations previously so I'll not go into the\nspecifics here. basically, we should try to reduce the time our auto-scaled\ninstances spend talking to RDS (or any other database) and do their job so they\ncan operate as efficient as possible and scale less.\n\nAWS have tools specifically designed for this;\nCloudFront, Elasticache, and DynamoDB.\n\nCloudFront (CF): is a CDN, but it can be more than that;\n\n * CF is most commonly utilised to serve static assets (CSS, JS, and Images)\n   stored in S3 buckets. S3 is not distributed, your assets are located in a\n   single region, CF will cache the asset from S3 in edge locations all around\n   the world closest to the user requesting it for faster delivery to the\n   browser and will only contact the S3 bucket if the expiry of the cached\n   resource has expired.\n * You might need to deliver streaming video transcoded for consumption on the\n   web or using Apple's HLS for iOS apps.\n * In addition to static assets, it is also a consideration to have CF cache\n   configuration files for your Apps, or even the structured data (JSON, SOAP,\n   XML) responses from your APIs. This means your servers will be hit less often\n   to deliver content you deem cacheable (for a time).\n * Similar to the last point, you might also wish to cache certain web pages,\n   some examples may include templates typically requested by service workers or\n   dumped in the DOM for an ajax call to hook up with data at a later stage.\n   These are excellent candidates. Other possibilities are pages that attract\n   superficial traffic, so pages like about us, contact us, careers, and\n   generally any page that is considered to not have on demand dynamic content\n   should be cached for at least 7 hours (per Facebook's recommendation).\n * More on superficial traffic; if you have most of your website dynamic\n   features available only to logged in users, on pages that can also be\n   accessed when not logged in but provide a less than dynamic experience, it\n   will be extremely beneficial to have CF cache these pages for users that are\n   not logged in for at least 2mins (again a Facebook recommendation), this may\n   give the superficial traffic a slightly stale page whereas logged in users\n   get to pass-through CF while holding a session and get on-demand access to\n   the dynamic features, which is great news for your Auto Scale group as it\n   will receive far less hits from these superficial users.\n\nElasticache is a service that abstracts Memcache or Redis for scalable in-memory\ncaching.\nIf you've ever setup either Memcache or Redis you may have the impression it is\nbest used for simple data like storing sessions for a clustered Node.js app or\nfor storing simple stateful objects for when a user is load balanced across\nmultiple servers.\n\nAlthough these are common uses of the technology, they barely scrape the surface\nof usefulness. You can utilise Elasticache in a way that effectively makes your\ndatabase utilisation drop in such a way that any single piece of data is only\nrequested from the DB directly only as often as it changes, with one exception;\nthe Elasticache optimisation algorithms sometimes purge infrequently accessed\ndata from memory, at which point your database will be hit for the requested\ndata even if it is unchanged since it was last accessed.\n\nThe key to implementing this type of technique has two major focusses;\n\na) Well structured cache keys; for storing the relevant lookup data in the key\nitself so the data can be targeted for easy fetching and invalidation.\n\nb) Cache stampeding; This occurs when highly depended upon objects in memory\nhave suddenly expired and there is a stampede of requests incoming for that data\nall at virtually the same moment. This will result in the following pattern;\n\n 1. Request cache exists > cache expired\n 2. Request data from DB, wait for it to be received during which more requests\n    have com in and are doing the same.\n 3. Tell Elasticache to store the new data. the all other requests that came\n    through before also tell Elasticache to store thier version of the data from\n    the DB.\n 4. Following requests get served from the cache.\n\nThis is obviously inefficient, and in fact, can generate unwanted read IOPS on\nyour DB and crippling write IOPS concerns for your Elasticache nodes.\n\nTo fix this we need to tell Elasticache to store our objects a little longer\nthen the applications needs, for instance, when we intend to store our object\nfor 7hrs we have that 7hr value stored in our application but when our call goes\nto Elasticache simply take the 7hrs and add 10 seconds. This is so that later\nafter the 7hrs has passed when our application expects the object to be expired\nwe still can access that stored object for 10 seconds longer! It'll make more\nsense in a moment.\nFor our application will be able to have this we to actually store two pieces of\ndata into Elasticache; our object data, and when it was stored. Our application\ncode is already aware that this object is to be stored for 7hrs because it is\ncoded into the the request so that when it encounters expired caches it can\nrequest and then store the new data for the appropriate time, therefore, by\nhaving the created time stored with the object we actually want we can instantly\nreturn the requested object to our requester and spawn off another thread to\nupdate the object from the DB with fresh data without interfering with the\nrequester.\n\nThis also allows the cache stampede to be mitigated because the object is still\nreturned, and during the few moments after the first requester is taking to\nupdate the object we still can serve it from cache while it is being updated!\n\nI use this not only to reduce I/O from my Auto Scaled instances to my RDS, but\nto mitigate the IOPS limitations imposed by AWS on their RDS product. This\nElasticache layer is far cheaper than PIOPS and considerably more performant and\npowerful than RDS.\n\nI mentioned session data is a candidate for usual Memcache or Redis\nimplementations earlier, I'd like to make it clear that although sessions are\ncommonly saved to these technologies they are by far not appropriate for\nElasticache. For one reason alone, AWS controls the policy for purging data to\nkeep the service optimised, whereas your own installation of Redis or Memcache\ncan be de-optimised so as to not aggressively purge the important session data.\nThe last thing you want is to have your logged in users kicked out sporadically.\n\nElasticache is a powerful tool and should be respected for its use cases and\nsessions are better stored in a more static centralised way, for example, a\ndocument storage DB;\n\nDynamoDB is built on top of MongoDB, a document storage database, ready for\nscalability through abstraction but nonetheless still an amazing document\nstorage database.\n\nI have experience using DynamoDB for high demand, real-time, and reliable data.\nDynamoDB costings can easily get out of hand so you must be mindful that the\ndata stored in DynamoDB actually belongs there.\n\nOther than Session data being a great candidate due to sessions lasting usually\nno longer than a few hours a day, another great candidate for DynamoDB is\nauction data.\n\nAny great candidates for DynamoDB are data that is only meaningful for a defined\ntimeframe, and may be aggressively updated whilst just as aggressively being\nrequested.\n\nThink about the data for an eBay auction, it's current highest bid and all of\nthe participating user's max bids that are only meaningful before the auction\ncloses. After the auction this data becomes static, it won't change anymore and\ncan be stored more permanently in RDS.\n\nDynamoDB is suited perfectly for this task as it is fast and handles concurrent\nchanges well without blocking requests. Also, the cost is a non-issue if you are\ndealing with a single object for all your changes and remove the data as soon as\nit is no longer meaningful.\n\nUsing DynamoDB in this way relieves tremendous pressure off your RDS and\ngenerally the entire web stack, so I urge you to identify data such as this in\nyour own applications and utilise document storage the right way. Technology\nlike MongoDB is not best suited to store all of your data, your application and\nbusiness will be better off using relational databases over document storage for\nlong-term data needs.\n\nTo sum up\nI've covered a huge amount of topics and given you many techniques to consider\nthat it's likely making your head spin, so let me bullet point this for you;\n\n * Ensure your EC2 instances are able to be thrown away (terminated) by\n   centralising logs in S3 and delegate responsibility for data and state\n   storage elsewhere\n   \n   \n * Craft your auto scale actions and CloudWatch metrics carefully. Be proactive\n   in reporting engineered data for your metrics as scaling should be intimately\n   unique for your particular business case.\n   \n   \n * Choose the right EC2 instance for the task it performs. Make sure you\n   understand being granular might affect the instances ability to do its job,\n   i.e. ensure web server instances have at least 2x CPUs\n   \n   \n * Have the ELB manage your SSL cert to reduce complexity in each instance\n   \n   \n * Implement HA methodology, it is actually more important to do this for\n   granular setups contrary to what you may have been told before.\n   \n   \n * Use CloundFront, it is very cheap and provides the most significant\n   performance bang for buck.\n   \n   \n * Identify and store all data in Elasicache, bake it into the way you\n   communicate with your relational database/s that are your bottleneck, always.\n   Your IOPS will become significantly better.\n   \n   \n * Don't be afraid of the cost to use DynamoDB, in fact, it can be cheaper than\n   relational or memory alternatives for the right data when you consider the\n   impact that data might have on your RDS throughput, and the user reliability\n   and performance benefits are second to none.\n   \n   \n\nUnderstanding how to make the right decision on AWS will save you unmeasurably\nwhen you are able to scale without incident.\nChoosing MVPs and outsourcing development over proper AWS planning and\narchitectured the right way could be the difference between success and failure\nfor a startup.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/AWS_01_1500@2x-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-06-26 22:18:29","created_by":"1","updated_at":"2021-03-31 14:14:47","updated_by":"1","published_at":"2016-07-01 14:05:46","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca2","uuid":"e46b0e63-cfa3-45aa-b2c5-03ac95cf7c84","title":"Strategy To Manage Cache At Scale","slug":"strategy-to-manage-cache-at-scale","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"One of the major challenges we face as modern developers in our infrastructure is how do we cache effectively so that we don't have to perform heavy I/O to serve identical requests.\\n\\n> The application is only as fast as its slowest component, usually that is relational databases\\n\\nIf we have a solid caching strategy the database is not only relieved of immense pressures we also increase performance as the slowest cog in some cases may never be contacted.\\n\\nI'll explain two main strategies you might follow to manage cache at scale in an agnostic to technology way but first lets explore what a cache is quickly and what our expectations are when we use the word \\\"cache\\\".\\n\\n### So what exactly is a distributed cache\\n\\nThe difference between a cache and a distributed cache is the distributed version takes traditional cache and provides its functions at scale, that is to say the cache technology is designed to scale indefinitely based on demand whereas the traditional cache will only exist in the same server as the application using it, sharing memory, and limited memory space.\\n\\n### What functions does a cache provide\\n\\nCaches are pretty simple in terms of functionality they expose, the common ones are;\\n\\n* **store**: takes a datam and a key to identify it\\n  * The datam is usually a string\\n  * When a key that was previously used is given the new data will over-write the existing data for that key\\n  * The cache has no constraints on datam uniqueness, it is happy to store the same datam for as many keys it is sent\\n  * TTL: some cache's offer an expiry time for the given key, if a request is received after the set expiry the the datam is purged \\n* **retrieve**: takes a key identifier and attempts to return stored datam\\n  * If there was never any datam stored for the given key or if the TTL has passed the retrieve will fail\\n* **delete**: takes a key identifier and attempts to purge stored datam\\n  * If there was never any datam stored for the given key this function still runs successfully in most cache's\\n* **LRU**: Least Recently Used algorithm\\n * This is a common case for cache's; due to most cache's having a set amount of memory space it must free up space so that newer or prioritised datam's can be served\\n* **Locking**: a function that deals with race conditions in a cache that is not sequential/transactional by design\\n * This is not common but it exists so if you're goal is to be scalable you might want to consider a cache that uses record locking for one that is going to be reliably scalable\\n* **O(1)**: commonly referred to as the Big-O\\n * This is a function that is a scalability dream, it is a proven method to ensure _ALL_ functions are performed consistently at an identical timing as opposed to 2 different functions taking (n)seconds to complete.\\n  * A caveat to this is when you are asking the cache to delete something that is not even set, or you attempt to retrieve data that never existed, it is going to take the same time as if it did the task and it would be better performance and scale wise to never have made that request to the cache in the first place.\\n\\n### Node discovery\\n\\nIf the cache doesn't provide node discovery there is a simple way to implement it yourself in code;\\n\\n* Simple maths utilising modulus\\n* crc32; [Consistent hashing](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.192&rank=4&q=consistent%20hash&osm=&ossid=), sometimes called a continuum due to its method of plotting being based on the idea of an analogue clock.\\n\\n> This is essential for a distributed cache, without it the cache cannot effectively scale.\\n\\nMany just go straight onto using the modulus method because developers are generally comfortable with Math, but the engineer in me saw the flaw in its use and needed to master consistent hashing.\\n\\nTurns out there is [already a PHP implementation](https://github.com/pda/flexihash) of Consistent hashing, [a Python one](https://gist.github.com/reorx/8470123), and [one in pure JavaScript too](https://github.com/dakatsuka/node-consistent-hashing), so I never did get to write one myself.\\n\\nUsing modulus you would put your N cache node server IPs in an array and pick one using key % N.\\nNow the problem with the modulus method that crc32 doesn't have is as soon as you add or remove a node(server) i.e. change N, most of your cache will become invalid and any key that continues to work is pure coincidence.\\nThis leads to some pretty crazy race conditions as your data sources get smashed as the cache hydrates, this is what we refer to in the cache world as cache stampede.\\n\\nWith consistent hashing the only datam that cannot be retrieved is the data that was stored on the removed node.\\n\\n### Cache keys\\n\\nAs mentioned previously, the cache will not prevent you storing non-unique datam in the store, it only cares about the key to identify a relationship to arbitrary datam.\\n\\nHowever, we use caches to uniquely store our data conveniently and uniquely. Let focus on \\\"uniqueness\\\", because what the cache understands as unique is the key and what we expect to be unique is the datam.\\n\\n> The responsibility of uniqueness is placed entirely on the application, the cache doesn't care\\n\\nThere are many ways for an application to generate a key, and every application will define their expectation of uniqueness entirely on their own use cases, however there is a way to ensure uniqueness and it requires having a _key identifier store_ which manages things like these keys, and an assortment of application logic bound to the cache which might include strategies to invalidate associative data which is discussed in more detail within the invalidation section.\\n\\n### Cache key generation\\n\\nComparing uniqueness of the options you have for a caches cache key;\\n\\n* **String**\\n  * Generated by the app, usually inline for every item you cache, this article proposes below to standardise its creation\\n  * Only as unique as your app logic defines it\\n  * Character limits are not an issue, write performance will be consistent regardless of key size\\n  * memcached max key size is 1MB\\n  * redis; max 512MB size for cache keys, (interesting because the max data size is also 512MB) \\n* **hash**: \\n  * Less unique than using app string above without hashing\\n  * Hashing functions become more prone to collisions as the inputs compared become longer\\n* **UUID**: \\n  * Always unique concurrently across hosts, down to the microsecond or better entropy;\\n  * Basically, your chances of collision in the foreseeable future of our computing power is inconceivable.\\n\\nThe go-to is usually the worst choice, a hash function like `md5` or `sha1`. \\nI strongly argue that a hash is problematic and redundant for use as a cache key.\\n\\nThe fact that your hash seed must be the way you define your expectation of uniqueness, the hashing itself has no additional purpose to the cache. Using the seed as the key is all the cache actually needs, hashing is simply unnecessary overhead and in context of scalability (why we use caches) any unnecessary overhead should be eliminated.\\n\\n> Hashing functions are problematic and redundant for use as a cache key\\n\\nAn example in php would be `sha1(json_encode($_REQUEST))` and one who uses this would argue that is not manual, yet you have not actually done what you think you have done. Your goal in fact was to ensure that the cache key is unique for this use case, and by using this key you will fetch the expected data. My question is do you know the unique key to use to fetch the data saved with the above method? You might think you do, but you do not really know I assure you.\\n\\nWhen you save data with this method you are opening the way for things like cache-busting to populate the cache with N duplications.\\n\\nLet's say you have [\\\"userId\\\"=>123] as your input; the cache key would be `5b9dfbd8045853c992f66e9caa1ef3c7cd76cf4a` and if you used that key you would get the expected data, however the data you get was not unique for that key, there are multiple keys and duplications of that data in the cache. Hashing in this way is having no control methods to prevent N more of the same data being stored for different keys generated with unhandled input. Your data was created by the handled input but your key creation method allowed any unhandled input to generate more unique keys to store the same data.\\n\\nThis problem isn't noticeable and may go undetected because the application is doing what it was designed to do. \\n\\nThis is actually a massive performance problem as your cache is now full of data you will never request, this might lead to premature nodes being spawned to handle the extra demand, it might even lead to unmanageable IOPS while this extra storing happens and as the LRU runs to clear out this bad data over time. \\n \\nNeedless to say, If you've not created and enforced cache key generation strategicly you are open to a lot of unwanted problems.\\n\\nBack onto uniqueness, consider that the way a data set is considered unique, I mentioned earlier uniqueness is an expectation that differs with every application. If you have sessions there is a high chance that you're going to want to provide an active session different data to an anonymous user, both users have the same entry point to the data, and in fact there is only the difference that one has an active session and the other doesn't, you're going to have to properly handle this in your cache key generation or you will encounter the issue of non-active session users seeing the cached version stored by a user with an active session, or worse, one active session sees the data from another active session as their own.\\nThere are many more complexities to consider if you must manually generate your cache keys and these could be time based for things like auctions or events, and could even have geolocation reasons to have 1 resource slightly different for 2 users.\\n\\nWe haven't addressed the fact that you should ever even need to care about manually creating a cache key that is unique at all when writing code.\\n\\nLet me introduce you to UUID.\\nA UUID takes no input from the application, it guarantees uniqueness across hosts simultaneously down to the microsecond or better. \\nUsing a UUID for the cache key there is no need to introduce complex rules and assumptions into your application at all, the cache technology you choose gets the uniqueness _IT_ cares about.\\nBut the uniqueness our application cares about is still to be clarified, to ensure we enforce our uniqueness we need a cache key strategy which acts as a way to have a lookup to retrieve the keys we used (the UUIDs) with logic our application understands.\\nA cache key store gives us not only the ability to prevent mistakes in our code by making bad keys or not handling keys correctly for our expected outcome, but it also provides a programmatic way to access associated keys for when we need to invalidate related data, something the hashing method also cannot achieve.\\n\\n### Cache key store\\n\\nDue to the Big-O, caches do not expose a way to iterate through the keys it is using, doing so would be a resource drain, time consuming, and with LRU and time factors the iterable list would be extremely difficult to implement with its changing state. It is far better for your application logic to maintain it's own store that if the need arises provide you the ability to iterate through keys.\\n\\nQuickly on iteration, when you think you need to do it you're more likely really in need of some Regex like ﻿`preg_match_all` in PHP, or `re.findall` in Python to extract the keys you actually wish to iterate instead of all known keys.\\n\\nThis idea of a cache key store is likely new to you and the fact the cache is using UUIDs you might not yet grasp how any application using UUIDs could possibly operate, so i'll provide an OO-esq interface to ease the learning curve for some of you;\\n\\n```\\nprotocol CacheKeyStore\\n  prop storeName = 'my cache key store name'\\n  prop delimiter = '|'\\n  func <void> \\n       set => <str> cacheKey,\\n              <str> json\\n  func <str|cacheKey> \\n       makeKey => <str> __CLASS__, \\n                  <str> __METHOD__, \\n                  <str> handledInput\\n  func <str>\\n       exists => <bool>  \\n  func <str>\\n       fetch => <str> cacheKey  \\n  func <void>\\n       delete => <str> cacheKey  \\n  func <void>\\n       deleteAssoc => <str> cacheKeyPart\\n```\\n\\nThe only part of that self-documenting abstract that needs clarification is the delimiter; which is used to create logical ways to identify keys by parts.\\n\\nThis is a visual aid of where that code sits in your stack;\\n\\n![Cache Key Strategy](__GHOST_URL__/content/images/2016/09/cache-strategy-diagram.jpg)\\n\\nA general purpose use case in PHP would be;\\n\\n```language-php\\nclass User {\\n  function getUserById($id) {\\n    $cacheKey = CacheKeyStore::makeKey(__CLASS__, __FUNCTION__, \\\"userId/$id\\\");\\n    if (CacheKeyStore::exists($cacheKey)) {\\n      $data = CacheKeyStore::fetch($cacheKey);\\n    } else {\\n      $data = $someModel->someGetter();\\n      CacheKeyStore::set($cacheKey, $data);\\n    }\\n  }\\n}\\n```\\n\\nYou would add additional uniqueness rules to the `makeKey` function based on application specific goals, such as time of day, geolocation, sessions to name a few but how that is done is up to you, the important thing here is there is no unnecessary hashing and there is no implied knowledge at all how you need to create the seed for your hash to ensure uniqueness of the data, nor is there going to be unwanted duplication in data being stored accidentally.\\n\\nAll of these benefits with a small, simple, reusable bit of code, and without a hash. You could go as far as to build it inherently in the `$someModel->someGetter()` to remain **DRY**.\\n\\nUsing such a technique instead of straight up hashing not only handled the majority of edge cases, we just removed all need to know anything at all about how the cache works, it is now completely enclosed with fetching data which you would fundamentally do well already in your application.\\n\\nThis may seem too good to be true, and it is in practise not this easy. You still need to consider how you define TTL and most importantly how to implement an invalidation strategy that works as seamlessly as our above `DRY` example.\\n\\nTo answer the question of expiry i'll refer to the policy used at Facebook AFAIK it breaks down into 2 use cases;\\n\\n1) 7hrs; Data that can be invalidated programatically;\\n2) 2mins; for data that cannot\\n\\nFor data we cannot invalidate we are likely thinking of time based data, for example a list of the last x sold items. This is not practical because we cannot predict the input which is a yet unknown timestamp.\\nSo we either cache it for 2 mins and allow it to naturally become fresh via the TTL or we don't cache it at all putting pressure on the data source for every request. Not caching something should typically only be done for existence checks, therefore this theoretical example would be cached for 2mins.\\n\\nAll other data should be cached for 7 hours because the theory is; if we can invalidate it we trust our application keeps it fresh with our implementation, if this is ever untrue the problem extends beyond any single use case.\\n\\nTo address cache invalidation strategies; one could only theorise numerous ways this might be done because there are limitless possibilities in terms of what your uniqueness parameters will be for your application, and any invalidation strategy is directly bound to what is considered unique.\\n\\nA simple use case would be a users list, assuming there are no other factors around uniqueness like sessions. One would implement invalidation something like this;\\n\\nRoute: POST /user/:unique_constraint {arbitrary user data}\\nRoute: GET /user/:unique_constraint {user data}\\n\\n* POST /user/chris {\\\"name\\\":\\\"stoff\\\",\\\"role\\\"=>\\\"Admin\\\"}\\n* POST /user/ben {\\\"name\\\":\\\"ben\\\",\\\"role\\\"=>\\\"Std\\\"}\\n* POST /user/rodney {\\\"name\\\":\\\"mckay\\\",\\\"role\\\"=>\\\"Std\\\"}\\n\\ngetting data;\\n\\n* GET /user/chris\\n\\nreturns;\\n{\\\"name\\\":\\\"stoff\\\"}\\n\\nRoute: GET /users {list of users}\\nreturns;\\n[{\\\"name\\\":\\\"stoff\\\"}, {\\\"name\\\":\\\"ben\\\"}, {\\\"name\\\":\\\"mckay\\\"}]\\n\\nRoute: GET /users/standard {list of users with role Std}\\nreturns;\\n[{\\\"name\\\":\\\"ben\\\"}, {\\\"name\\\":\\\"mckay\\\"}]\\n\\nDELETE /user/mckay\\n\\nGET /users\\n[{\\\"name\\\":\\\"stoff\\\"}, {\\\"name\\\":\\\"ben\\\"}]\\n\\nGET /users/standard\\n[{\\\"name\\\":\\\"ben\\\"}]\\n\\nTo implement invalidation of the collections you need to be able to find the right keys to invalidate, this is true for the hashing technique and the cache key store technique with one very important difference, you need to find all collections on you own if you use hashing, maybe all related collections can be `grep`d if the devs were strict enough to name them a certain way, but with the cache key store you only need to ask for it to give you all the keys for associated collections (correction, you dont care about the UUID keys, they are just invalidated), way easier and will not leave any collection missed by a dev building their own hashes to send to the cache delete api.\\n\\n### Conclusion\\n\\nNot only are hashes used as cache keys redundant even in the most zealot of use cases arguing for them, the alternative UUID method can be completely silent in practice if implemented correctly and have none of the side effects or edge cases that come with using hashes.\\n\\nThe implementation of the above concepts has come to be based on extremely high traffic conditions and demand on the cache, combined with growing teams and applications. You simply cannot afford a bad cache strategy at scale, and having the right one before you have heavy load conditions can save you immeasurably in cold hard cash when the scale is inevitably necessary.\\n\\nThe biggest benefit for using a cache key store is that you can change from one cache technology to another without the application itself needing a refactor, all you do is implement the wrapping abstraction methods for the chosen technology. This can save you if say you chose memcached early on for its ease of use but now need to scale and have now decided redis or AWS Elasticache are better for the business.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>One of the major challenges we face as modern developers in our infrastructure is how do we cache effectively so that we don't have to perform heavy I/O to serve identical requests.</p>\n<blockquote>\n<p>The application is only as fast as its slowest component, usually that is relational databases</p>\n</blockquote>\n<p>If we have a solid caching strategy the database is not only relieved of immense pressures we also increase performance as the slowest cog in some cases may never be contacted.</p>\n<p>I'll explain two main strategies you might follow to manage cache at scale in an agnostic to technology way but first lets explore what a cache is quickly and what our expectations are when we use the word &quot;cache&quot;.</p>\n<h3 id=\"sowhatexactlyisadistributedcache\">So what exactly is a distributed cache</h3>\n<p>The difference between a cache and a distributed cache is the distributed version takes traditional cache and provides its functions at scale, that is to say the cache technology is designed to scale indefinitely based on demand whereas the traditional cache will only exist in the same server as the application using it, sharing memory, and limited memory space.</p>\n<h3 id=\"whatfunctionsdoesacacheprovide\">What functions does a cache provide</h3>\n<p>Caches are pretty simple in terms of functionality they expose, the common ones are;</p>\n<ul>\n<li><strong>store</strong>: takes a datam and a key to identify it\n<ul>\n<li>The datam is usually a string</li>\n<li>When a key that was previously used is given the new data will over-write the existing data for that key</li>\n<li>The cache has no constraints on datam uniqueness, it is happy to store the same datam for as many keys it is sent</li>\n<li>TTL: some cache's offer an expiry time for the given key, if a request is received after the set expiry the the datam is purged</li>\n</ul>\n</li>\n<li><strong>retrieve</strong>: takes a key identifier and attempts to return stored datam\n<ul>\n<li>If there was never any datam stored for the given key or if the TTL has passed the retrieve will fail</li>\n</ul>\n</li>\n<li><strong>delete</strong>: takes a key identifier and attempts to purge stored datam\n<ul>\n<li>If there was never any datam stored for the given key this function still runs successfully in most cache's</li>\n</ul>\n</li>\n<li><strong>LRU</strong>: Least Recently Used algorithm</li>\n<li>This is a common case for cache's; due to most cache's having a set amount of memory space it must free up space so that newer or prioritised datam's can be served</li>\n<li><strong>Locking</strong>: a function that deals with race conditions in a cache that is not sequential/transactional by design</li>\n<li>This is not common but it exists so if you're goal is to be scalable you might want to consider a cache that uses record locking for one that is going to be reliably scalable</li>\n<li><strong>O(1)</strong>: commonly referred to as the Big-O</li>\n<li>This is a function that is a scalability dream, it is a proven method to ensure <em>ALL</em> functions are performed consistently at an identical timing as opposed to 2 different functions taking (n)seconds to complete.</li>\n<li>A caveat to this is when you are asking the cache to delete something that is not even set, or you attempt to retrieve data that never existed, it is going to take the same time as if it did the task and it would be better performance and scale wise to never have made that request to the cache in the first place.</li>\n</ul>\n<h3 id=\"nodediscovery\">Node discovery</h3>\n<p>If the cache doesn't provide node discovery there is a simple way to implement it yourself in code;</p>\n<ul>\n<li>Simple maths utilising modulus</li>\n<li>crc32; <a href=\"http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.192&amp;rank=4&amp;q=consistent%20hash&amp;osm=&amp;ossid=\">Consistent hashing</a>, sometimes called a continuum due to its method of plotting being based on the idea of an analogue clock.</li>\n</ul>\n<blockquote>\n<p>This is essential for a distributed cache, without it the cache cannot effectively scale.</p>\n</blockquote>\n<p>Many just go straight onto using the modulus method because developers are generally comfortable with Math, but the engineer in me saw the flaw in its use and needed to master consistent hashing.</p>\n<p>Turns out there is <a href=\"https://github.com/pda/flexihash\">already a PHP implementation</a> of Consistent hashing, <a href=\"https://gist.github.com/reorx/8470123\">a Python one</a>, and <a href=\"https://github.com/dakatsuka/node-consistent-hashing\">one in pure JavaScript too</a>, so I never did get to write one myself.</p>\n<p>Using modulus you would put your N cache node server IPs in an array and pick one using key % N.<br>\nNow the problem with the modulus method that crc32 doesn't have is as soon as you add or remove a node(server) i.e. change N, most of your cache will become invalid and any key that continues to work is pure coincidence.<br>\nThis leads to some pretty crazy race conditions as your data sources get smashed as the cache hydrates, this is what we refer to in the cache world as cache stampede.</p>\n<p>With consistent hashing the only datam that cannot be retrieved is the data that was stored on the removed node.</p>\n<h3 id=\"cachekeys\">Cache keys</h3>\n<p>As mentioned previously, the cache will not prevent you storing non-unique datam in the store, it only cares about the key to identify a relationship to arbitrary datam.</p>\n<p>However, we use caches to uniquely store our data conveniently and uniquely. Let focus on &quot;uniqueness&quot;, because what the cache understands as unique is the key and what we expect to be unique is the datam.</p>\n<blockquote>\n<p>The responsibility of uniqueness is placed entirely on the application, the cache doesn't care</p>\n</blockquote>\n<p>There are many ways for an application to generate a key, and every application will define their expectation of uniqueness entirely on their own use cases, however there is a way to ensure uniqueness and it requires having a <em>key identifier store</em> which manages things like these keys, and an assortment of application logic bound to the cache which might include strategies to invalidate associative data which is discussed in more detail within the invalidation section.</p>\n<h3 id=\"cachekeygeneration\">Cache key generation</h3>\n<p>Comparing uniqueness of the options you have for a caches cache key;</p>\n<ul>\n<li><strong>String</strong>\n<ul>\n<li>Generated by the app, usually inline for every item you cache, this article proposes below to standardise its creation</li>\n<li>Only as unique as your app logic defines it</li>\n<li>Character limits are not an issue, write performance will be consistent regardless of key size</li>\n<li>memcached max key size is 1MB</li>\n<li>redis; max 512MB size for cache keys, (interesting because the max data size is also 512MB)</li>\n</ul>\n</li>\n<li><strong>hash</strong>:\n<ul>\n<li>Less unique than using app string above without hashing</li>\n<li>Hashing functions become more prone to collisions as the inputs compared become longer</li>\n</ul>\n</li>\n<li><strong>UUID</strong>:\n<ul>\n<li>Always unique concurrently across hosts, down to the microsecond or better entropy;</li>\n<li>Basically, your chances of collision in the foreseeable future of our computing power is inconceivable.</li>\n</ul>\n</li>\n</ul>\n<p>The go-to is usually the worst choice, a hash function like <code>md5</code> or <code>sha1</code>.<br>\nI strongly argue that a hash is problematic and redundant for use as a cache key.</p>\n<p>The fact that your hash seed must be the way you define your expectation of uniqueness, the hashing itself has no additional purpose to the cache. Using the seed as the key is all the cache actually needs, hashing is simply unnecessary overhead and in context of scalability (why we use caches) any unnecessary overhead should be eliminated.</p>\n<blockquote>\n<p>Hashing functions are problematic and redundant for use as a cache key</p>\n</blockquote>\n<p>An example in php would be <code>sha1(json_encode($_REQUEST))</code> and one who uses this would argue that is not manual, yet you have not actually done what you think you have done. Your goal in fact was to ensure that the cache key is unique for this use case, and by using this key you will fetch the expected data. My question is do you know the unique key to use to fetch the data saved with the above method? You might think you do, but you do not really know I assure you.</p>\n<p>When you save data with this method you are opening the way for things like cache-busting to populate the cache with N duplications.</p>\n<p>Let's say you have [&quot;userId&quot;=&gt;123] as your input; the cache key would be <code>5b9dfbd8045853c992f66e9caa1ef3c7cd76cf4a</code> and if you used that key you would get the expected data, however the data you get was not unique for that key, there are multiple keys and duplications of that data in the cache. Hashing in this way is having no control methods to prevent N more of the same data being stored for different keys generated with unhandled input. Your data was created by the handled input but your key creation method allowed any unhandled input to generate more unique keys to store the same data.</p>\n<p>This problem isn't noticeable and may go undetected because the application is doing what it was designed to do.</p>\n<p>This is actually a massive performance problem as your cache is now full of data you will never request, this might lead to premature nodes being spawned to handle the extra demand, it might even lead to unmanageable IOPS while this extra storing happens and as the LRU runs to clear out this bad data over time.</p>\n<p>Needless to say, If you've not created and enforced cache key generation strategicly you are open to a lot of unwanted problems.</p>\n<p>Back onto uniqueness, consider that the way a data set is considered unique, I mentioned earlier uniqueness is an expectation that differs with every application. If you have sessions there is a high chance that you're going to want to provide an active session different data to an anonymous user, both users have the same entry point to the data, and in fact there is only the difference that one has an active session and the other doesn't, you're going to have to properly handle this in your cache key generation or you will encounter the issue of non-active session users seeing the cached version stored by a user with an active session, or worse, one active session sees the data from another active session as their own.<br>\nThere are many more complexities to consider if you must manually generate your cache keys and these could be time based for things like auctions or events, and could even have geolocation reasons to have 1 resource slightly different for 2 users.</p>\n<p>We haven't addressed the fact that you should ever even need to care about manually creating a cache key that is unique at all when writing code.</p>\n<p>Let me introduce you to UUID.<br>\nA UUID takes no input from the application, it guarantees uniqueness across hosts simultaneously down to the microsecond or better.<br>\nUsing a UUID for the cache key there is no need to introduce complex rules and assumptions into your application at all, the cache technology you choose gets the uniqueness <em>IT</em> cares about.<br>\nBut the uniqueness our application cares about is still to be clarified, to ensure we enforce our uniqueness we need a cache key strategy which acts as a way to have a lookup to retrieve the keys we used (the UUIDs) with logic our application understands.<br>\nA cache key store gives us not only the ability to prevent mistakes in our code by making bad keys or not handling keys correctly for our expected outcome, but it also provides a programmatic way to access associated keys for when we need to invalidate related data, something the hashing method also cannot achieve.</p>\n<h3 id=\"cachekeystore\">Cache key store</h3>\n<p>Due to the Big-O, caches do not expose a way to iterate through the keys it is using, doing so would be a resource drain, time consuming, and with LRU and time factors the iterable list would be extremely difficult to implement with its changing state. It is far better for your application logic to maintain it's own store that if the need arises provide you the ability to iterate through keys.</p>\n<p>Quickly on iteration, when you think you need to do it you're more likely really in need of some Regex like ﻿<code>preg_match_all</code> in PHP, or <code>re.findall</code> in Python to extract the keys you actually wish to iterate instead of all known keys.</p>\n<p>This idea of a cache key store is likely new to you and the fact the cache is using UUIDs you might not yet grasp how any application using UUIDs could possibly operate, so i'll provide an OO-esq interface to ease the learning curve for some of you;</p>\n<pre><code>protocol CacheKeyStore\n  prop storeName = 'my cache key store name'\n  prop delimiter = '|'\n  func &lt;void&gt; \n       set =&gt; &lt;str&gt; cacheKey,\n              &lt;str&gt; json\n  func &lt;str|cacheKey&gt; \n       makeKey =&gt; &lt;str&gt; __CLASS__, \n                  &lt;str&gt; __METHOD__, \n                  &lt;str&gt; handledInput\n  func &lt;str&gt;\n       exists =&gt; &lt;bool&gt;  \n  func &lt;str&gt;\n       fetch =&gt; &lt;str&gt; cacheKey  \n  func &lt;void&gt;\n       delete =&gt; &lt;str&gt; cacheKey  \n  func &lt;void&gt;\n       deleteAssoc =&gt; &lt;str&gt; cacheKeyPart\n</code></pre>\n<p>The only part of that self-documenting abstract that needs clarification is the delimiter; which is used to create logical ways to identify keys by parts.</p>\n<p>This is a visual aid of where that code sits in your stack;</p>\n<p><img src=\"__GHOST_URL__/content/images/2016/09/cache-strategy-diagram.jpg\" alt=\"Cache Key Strategy\" loading=\"lazy\"></p>\n<p>A general purpose use case in PHP would be;</p>\n<pre><code class=\"language-language-php\">class User {\n  function getUserById($id) {\n    $cacheKey = CacheKeyStore::makeKey(__CLASS__, __FUNCTION__, &quot;userId/$id&quot;);\n    if (CacheKeyStore::exists($cacheKey)) {\n      $data = CacheKeyStore::fetch($cacheKey);\n    } else {\n      $data = $someModel-&gt;someGetter();\n      CacheKeyStore::set($cacheKey, $data);\n    }\n  }\n}\n</code></pre>\n<p>You would add additional uniqueness rules to the <code>makeKey</code> function based on application specific goals, such as time of day, geolocation, sessions to name a few but how that is done is up to you, the important thing here is there is no unnecessary hashing and there is no implied knowledge at all how you need to create the seed for your hash to ensure uniqueness of the data, nor is there going to be unwanted duplication in data being stored accidentally.</p>\n<p>All of these benefits with a small, simple, reusable bit of code, and without a hash. You could go as far as to build it inherently in the <code>$someModel-&gt;someGetter()</code> to remain <strong>DRY</strong>.</p>\n<p>Using such a technique instead of straight up hashing not only handled the majority of edge cases, we just removed all need to know anything at all about how the cache works, it is now completely enclosed with fetching data which you would fundamentally do well already in your application.</p>\n<p>This may seem too good to be true, and it is in practise not this easy. You still need to consider how you define TTL and most importantly how to implement an invalidation strategy that works as seamlessly as our above <code>DRY</code> example.</p>\n<p>To answer the question of expiry i'll refer to the policy used at Facebook AFAIK it breaks down into 2 use cases;</p>\n<ol>\n<li>7hrs; Data that can be invalidated programatically;</li>\n<li>2mins; for data that cannot</li>\n</ol>\n<p>For data we cannot invalidate we are likely thinking of time based data, for example a list of the last x sold items. This is not practical because we cannot predict the input which is a yet unknown timestamp.<br>\nSo we either cache it for 2 mins and allow it to naturally become fresh via the TTL or we don't cache it at all putting pressure on the data source for every request. Not caching something should typically only be done for existence checks, therefore this theoretical example would be cached for 2mins.</p>\n<p>All other data should be cached for 7 hours because the theory is; if we can invalidate it we trust our application keeps it fresh with our implementation, if this is ever untrue the problem extends beyond any single use case.</p>\n<p>To address cache invalidation strategies; one could only theorise numerous ways this might be done because there are limitless possibilities in terms of what your uniqueness parameters will be for your application, and any invalidation strategy is directly bound to what is considered unique.</p>\n<p>A simple use case would be a users list, assuming there are no other factors around uniqueness like sessions. One would implement invalidation something like this;</p>\n<p>Route: POST /user/:unique_constraint {arbitrary user data}<br>\nRoute: GET /user/:unique_constraint {user data}</p>\n<ul>\n<li>POST /user/chris {&quot;name&quot;:&quot;stoff&quot;,&quot;role&quot;=&gt;&quot;Admin&quot;}</li>\n<li>POST /user/ben {&quot;name&quot;:&quot;ben&quot;,&quot;role&quot;=&gt;&quot;Std&quot;}</li>\n<li>POST /user/rodney {&quot;name&quot;:&quot;mckay&quot;,&quot;role&quot;=&gt;&quot;Std&quot;}</li>\n</ul>\n<p>getting data;</p>\n<ul>\n<li>GET /user/chris</li>\n</ul>\n<p>returns;<br>\n{&quot;name&quot;:&quot;stoff&quot;}</p>\n<p>Route: GET /users {list of users}<br>\nreturns;<br>\n[{&quot;name&quot;:&quot;stoff&quot;}, {&quot;name&quot;:&quot;ben&quot;}, {&quot;name&quot;:&quot;mckay&quot;}]</p>\n<p>Route: GET /users/standard {list of users with role Std}<br>\nreturns;<br>\n[{&quot;name&quot;:&quot;ben&quot;}, {&quot;name&quot;:&quot;mckay&quot;}]</p>\n<p>DELETE /user/mckay</p>\n<p>GET /users<br>\n[{&quot;name&quot;:&quot;stoff&quot;}, {&quot;name&quot;:&quot;ben&quot;}]</p>\n<p>GET /users/standard<br>\n[{&quot;name&quot;:&quot;ben&quot;}]</p>\n<p>To implement invalidation of the collections you need to be able to find the right keys to invalidate, this is true for the hashing technique and the cache key store technique with one very important difference, you need to find all collections on you own if you use hashing, maybe all related collections can be <code>grep</code>d if the devs were strict enough to name them a certain way, but with the cache key store you only need to ask for it to give you all the keys for associated collections (correction, you dont care about the UUID keys, they are just invalidated), way easier and will not leave any collection missed by a dev building their own hashes to send to the cache delete api.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>Not only are hashes used as cache keys redundant even in the most zealot of use cases arguing for them, the alternative UUID method can be completely silent in practice if implemented correctly and have none of the side effects or edge cases that come with using hashes.</p>\n<p>The implementation of the above concepts has come to be based on extremely high traffic conditions and demand on the cache, combined with growing teams and applications. You simply cannot afford a bad cache strategy at scale, and having the right one before you have heavy load conditions can save you immeasurably in cold hard cash when the scale is inevitably necessary.</p>\n<p>The biggest benefit for using a cache key store is that you can change from one cache technology to another without the application itself needing a refactor, all you do is implement the wrapping abstraction methods for the chosen technology. This can save you if say you chose memcached early on for its ease of use but now need to scale and have now decided redis or AWS Elasticache are better for the business.</p>\n<!--kg-card-end: markdown-->","comment_id":"34","plaintext":"One of the major challenges we face as modern developers in our infrastructure\nis how do we cache effectively so that we don't have to perform heavy I/O to\nserve identical requests.\n\n> The application is only as fast as its slowest component, usually that is\nrelational databases\n\n\nIf we have a solid caching strategy the database is not only relieved of immense\npressures we also increase performance as the slowest cog in some cases may\nnever be contacted.\n\nI'll explain two main strategies you might follow to manage cache at scale in an\nagnostic to technology way but first lets explore what a cache is quickly and\nwhat our expectations are when we use the word \"cache\".\n\nSo what exactly is a distributed cache\nThe difference between a cache and a distributed cache is the distributed\nversion takes traditional cache and provides its functions at scale, that is to\nsay the cache technology is designed to scale indefinitely based on demand\nwhereas the traditional cache will only exist in the same server as the\napplication using it, sharing memory, and limited memory space.\n\nWhat functions does a cache provide\nCaches are pretty simple in terms of functionality they expose, the common ones\nare;\n\n * store: takes a datam and a key to identify it * The datam is usually a string\n    * When a key that was\n      previously used is given the new data will over-write the existing data\n      for that key\n    * The cache has no constraints\n      on datam uniqueness, it is happy to store the same datam for as many keys\n      it is sent\n    * TTL: some cache's offer an\n      expiry time for the given key, if a request is received after the set\n      expiry the the datam is purged\n   \n   \n * retrieve: takes a key identifier and attempts to return stored datam * If\n      there was never any datam stored for the given key or if the TTL has\n      passed the retrieve will fail\n   \n   \n * delete: takes a key identifier and attempts to purge stored datam * If there\n      was never any datam stored for the given key this function still runs\n      successfully in most cache's\n   \n   \n * LRU: Least Recently Used algorithm\n * This is a common case for cache's; due to most cache's having a set amount of\n   memory space it must free up space so that newer or prioritised datam's can\n   be served\n * Locking: a function that deals with race conditions in a cache that is not\n   sequential/transactional by design\n * This is not common but it exists so if you're goal is to be scalable you\n   might want to consider a cache that uses record locking for one that is going\n   to be reliably scalable\n * O(1): commonly referred to as the Big-O\n * This is a function that is a scalability dream, it is a proven method to\n   ensure ALL functions are performed consistently at an identical timing as\n   opposed to 2 different functions taking (n)seconds to complete.\n * A caveat to this is when you are asking the cache to delete something that is\n   not even set, or you attempt to retrieve data that never existed, it is going\n   to take the same time as if it did the task and it would be better\n   performance and scale wise to never have made that request to the cache in\n   the first place.\n\nNode discovery\nIf the cache doesn't provide node discovery there is a simple way to implement\nit yourself in code;\n\n * Simple maths utilising modulus\n * crc32; Consistent hashing\n   [http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.192&rank=4&q=consistent%20hash&osm=&ossid=]\n   , sometimes called a continuum due to its method of plotting being based on\n   the idea of an analogue clock.\n\n> This is essential for a distributed cache, without it the cache cannot\neffectively scale.\n\n\nMany just go straight onto using the modulus method because developers are\ngenerally comfortable with Math, but the engineer in me saw the flaw in its use\nand needed to master consistent hashing.\n\nTurns out there is already a PHP implementation\n[https://github.com/pda/flexihash] of Consistent hashing, a Python one\n[https://gist.github.com/reorx/8470123], and one in pure JavaScript too\n[https://github.com/dakatsuka/node-consistent-hashing], so I never did get to\nwrite one myself.\n\nUsing modulus you would put your N cache node server IPs in an array and pick\none using key % N.\nNow the problem with the modulus method that crc32 doesn't have is as soon as\nyou add or remove a node(server) i.e. change N, most of your cache will become\ninvalid and any key that continues to work is pure coincidence.\nThis leads to some pretty crazy race conditions as your data sources get smashed\nas the cache hydrates, this is what we refer to in the cache world as cache\nstampede.\n\nWith consistent hashing the only datam that cannot be retrieved is the data that\nwas stored on the removed node.\n\nCache keys\nAs mentioned previously, the cache will not prevent you storing non-unique datam\nin the store, it only cares about the key to identify a relationship to\narbitrary datam.\n\nHowever, we use caches to uniquely store our data conveniently and uniquely. Let\nfocus on \"uniqueness\", because what the cache understands as unique is the key\nand what we expect to be unique is the datam.\n\n> The responsibility of uniqueness is placed entirely on the application, the\ncache doesn't care\n\n\nThere are many ways for an application to generate a key, and every application\nwill define their expectation of uniqueness entirely on their own use cases,\nhowever there is a way to ensure uniqueness and it requires having a key\nidentifier store which manages things like these keys, and an assortment of\napplication logic bound to the cache which might include strategies to\ninvalidate associative data which is discussed in more detail within the\ninvalidation section.\n\nCache key generation\nComparing uniqueness of the options you have for a caches cache key;\n\n * String * Generated by the app, usually inline for every item you cache, this\n      article proposes below to standardise its creation\n    * Only as unique as your app logic defines it\n    * Character limits are not an issue, write performance will be\n      consistent regardless of key size\n    * memcached max key size is 1MB\n    * redis; max 512MB size for cache keys, (interesting because the max\n      data size is also 512MB)\n   \n   \n * hash: * Less unique than using app string above without hashing\n    * Hashing functions become more prone to collisions as the inputs\n      compared become longer\n   \n   \n * UUID: * Always unique concurrently across hosts, down to the microsecond or\n      better entropy;\n    * Basically, your chances of collision in the foreseeable future of our\n      computing power is inconceivable.\n   \n   \n\nThe go-to is usually the worst choice, a hash function like md5 or sha1.\nI strongly argue that a hash is problematic and redundant for use as a cache\nkey.\n\nThe fact that your hash seed must be the way you define your expectation of\nuniqueness, the hashing itself has no additional purpose to the cache. Using the\nseed as the key is all the cache actually needs, hashing is simply unnecessary\noverhead and in context of scalability (why we use caches) any unnecessary\noverhead should be eliminated.\n\n> Hashing functions are problematic and redundant for use as a cache key\n\n\nAn example in php would be sha1(json_encode($_REQUEST)) and one who uses this\nwould argue that is not manual, yet you have not actually done what you think\nyou have done. Your goal in fact was to ensure that the cache key is unique for\nthis use case, and by using this key you will fetch the expected data. My\nquestion is do you know the unique key to use to fetch the data saved with the\nabove method? You might think you do, but you do not really know I assure you.\n\nWhen you save data with this method you are opening the way for things like\ncache-busting to populate the cache with N duplications.\n\nLet's say you have [\"userId\"=>123] as your input; the cache key would be \n5b9dfbd8045853c992f66e9caa1ef3c7cd76cf4a and if you used that key you would get\nthe expected data, however the data you get was not unique for that key, there\nare multiple keys and duplications of that data in the cache. Hashing in this\nway is having no control methods to prevent N more of the same data being stored\nfor different keys generated with unhandled input. Your data was created by the\nhandled input but your key creation method allowed any unhandled input to\ngenerate more unique keys to store the same data.\n\nThis problem isn't noticeable and may go undetected because the application is\ndoing what it was designed to do.\n\nThis is actually a massive performance problem as your cache is now full of data\nyou will never request, this might lead to premature nodes being spawned to\nhandle the extra demand, it might even lead to unmanageable IOPS while this\nextra storing happens and as the LRU runs to clear out this bad data over time.\n\nNeedless to say, If you've not created and enforced cache key generation\nstrategicly you are open to a lot of unwanted problems.\n\nBack onto uniqueness, consider that the way a data set is considered unique, I\nmentioned earlier uniqueness is an expectation that differs with every\napplication. If you have sessions there is a high chance that you're going to\nwant to provide an active session different data to an anonymous user, both\nusers have the same entry point to the data, and in fact there is only the\ndifference that one has an active session and the other doesn't, you're going to\nhave to properly handle this in your cache key generation or you will encounter\nthe issue of non-active session users seeing the cached version stored by a user\nwith an active session, or worse, one active session sees the data from another\nactive session as their own.\nThere are many more complexities to consider if you must manually generate your\ncache keys and these could be time based for things like auctions or events, and\ncould even have geolocation reasons to have 1 resource slightly different for 2\nusers.\n\nWe haven't addressed the fact that you should ever even need to care about\nmanually creating a cache key that is unique at all when writing code.\n\nLet me introduce you to UUID.\nA UUID takes no input from the application, it guarantees uniqueness across\nhosts simultaneously down to the microsecond or better.\nUsing a UUID for the cache key there is no need to introduce complex rules and\nassumptions into your application at all, the cache technology you choose gets\nthe uniqueness IT cares about.\nBut the uniqueness our application cares about is still to be clarified, to\nensure we enforce our uniqueness we need a cache key strategy which acts as a\nway to have a lookup to retrieve the keys we used (the UUIDs) with logic our\napplication understands.\nA cache key store gives us not only the ability to prevent mistakes in our code\nby making bad keys or not handling keys correctly for our expected outcome, but\nit also provides a programmatic way to access associated keys for when we need\nto invalidate related data, something the hashing method also cannot achieve.\n\nCache key store\nDue to the Big-O, caches do not expose a way to iterate through the keys it is\nusing, doing so would be a resource drain, time consuming, and with LRU and time\nfactors the iterable list would be extremely difficult to implement with its\nchanging state. It is far better for your application logic to maintain it's own\nstore that if the need arises provide you the ability to iterate through keys.\n\nQuickly on iteration, when you think you need to do it you're more likely really\nin need of some Regex likepreg_match_all in PHP, or re.findall in Python to\nextract the keys you actually wish to iterate instead of all known keys.\n\nThis idea of a cache key store is likely new to you and the fact the cache is\nusing UUIDs you might not yet grasp how any application using UUIDs could\npossibly operate, so i'll provide an OO-esq interface to ease the learning curve\nfor some of you;\n\nprotocol CacheKeyStore\n  prop storeName = 'my cache key store name'\n  prop delimiter = '|'\n  func <void> \n       set => <str> cacheKey,\n              <str> json\n  func <str|cacheKey> \n       makeKey => <str> __CLASS__, \n                  <str> __METHOD__, \n                  <str> handledInput\n  func <str>\n       exists => <bool>  \n  func <str>\n       fetch => <str> cacheKey  \n  func <void>\n       delete => <str> cacheKey  \n  func <void>\n       deleteAssoc => <str> cacheKeyPart\n\n\nThe only part of that self-documenting abstract that needs clarification is the\ndelimiter; which is used to create logical ways to identify keys by parts.\n\nThis is a visual aid of where that code sits in your stack;\n\n\n\nA general purpose use case in PHP would be;\n\nclass User {\n  function getUserById($id) {\n    $cacheKey = CacheKeyStore::makeKey(__CLASS__, __FUNCTION__, \"userId/$id\");\n    if (CacheKeyStore::exists($cacheKey)) {\n      $data = CacheKeyStore::fetch($cacheKey);\n    } else {\n      $data = $someModel->someGetter();\n      CacheKeyStore::set($cacheKey, $data);\n    }\n  }\n}\n\n\nYou would add additional uniqueness rules to the makeKey function based on\napplication specific goals, such as time of day, geolocation, sessions to name a\nfew but how that is done is up to you, the important thing here is there is no\nunnecessary hashing and there is no implied knowledge at all how you need to\ncreate the seed for your hash to ensure uniqueness of the data, nor is there\ngoing to be unwanted duplication in data being stored accidentally.\n\nAll of these benefits with a small, simple, reusable bit of code, and without a\nhash. You could go as far as to build it inherently in the \n$someModel->someGetter() to remain DRY.\n\nUsing such a technique instead of straight up hashing not only handled the\nmajority of edge cases, we just removed all need to know anything at all about\nhow the cache works, it is now completely enclosed with fetching data which you\nwould fundamentally do well already in your application.\n\nThis may seem too good to be true, and it is in practise not this easy. You\nstill need to consider how you define TTL and most importantly how to implement\nan invalidation strategy that works as seamlessly as our above DRY example.\n\nTo answer the question of expiry i'll refer to the policy used at Facebook AFAIK\nit breaks down into 2 use cases;\n\n 1. 7hrs; Data that can be invalidated programatically;\n 2. 2mins; for data that cannot\n\nFor data we cannot invalidate we are likely thinking of time based data, for\nexample a list of the last x sold items. This is not practical because we cannot\npredict the input which is a yet unknown timestamp.\nSo we either cache it for 2 mins and allow it to naturally become fresh via the\nTTL or we don't cache it at all putting pressure on the data source for every\nrequest. Not caching something should typically only be done for existence\nchecks, therefore this theoretical example would be cached for 2mins.\n\nAll other data should be cached for 7 hours because the theory is; if we can\ninvalidate it we trust our application keeps it fresh with our implementation,\nif this is ever untrue the problem extends beyond any single use case.\n\nTo address cache invalidation strategies; one could only theorise numerous ways\nthis might be done because there are limitless possibilities in terms of what\nyour uniqueness parameters will be for your application, and any invalidation\nstrategy is directly bound to what is considered unique.\n\nA simple use case would be a users list, assuming there are no other factors\naround uniqueness like sessions. One would implement invalidation something like\nthis;\n\nRoute: POST /user/:unique_constraint {arbitrary user data}\nRoute: GET /user/:unique_constraint {user data}\n\n * POST /user/chris {\"name\":\"stoff\",\"role\"=>\"Admin\"}\n * POST /user/ben {\"name\":\"ben\",\"role\"=>\"Std\"}\n * POST /user/rodney {\"name\":\"mckay\",\"role\"=>\"Std\"}\n\ngetting data;\n\n * GET /user/chris\n\nreturns;\n{\"name\":\"stoff\"}\n\nRoute: GET /users {list of users}\nreturns;\n[{\"name\":\"stoff\"}, {\"name\":\"ben\"}, {\"name\":\"mckay\"}]\n\nRoute: GET /users/standard {list of users with role Std}\nreturns;\n[{\"name\":\"ben\"}, {\"name\":\"mckay\"}]\n\nDELETE /user/mckay\n\nGET /users\n[{\"name\":\"stoff\"}, {\"name\":\"ben\"}]\n\nGET /users/standard\n[{\"name\":\"ben\"}]\n\nTo implement invalidation of the collections you need to be able to find the\nright keys to invalidate, this is true for the hashing technique and the cache\nkey store technique with one very important difference, you need to find all\ncollections on you own if you use hashing, maybe all related collections can be \ngrepd if the devs were strict enough to name them a certain way, but with the\ncache key store you only need to ask for it to give you all the keys for\nassociated collections (correction, you dont care about the UUID keys, they are\njust invalidated), way easier and will not leave any collection missed by a dev\nbuilding their own hashes to send to the cache delete api.\n\nConclusion\nNot only are hashes used as cache keys redundant even in the most zealot of use\ncases arguing for them, the alternative UUID method can be completely silent in\npractice if implemented correctly and have none of the side effects or edge\ncases that come with using hashes.\n\nThe implementation of the above concepts has come to be based on extremely high\ntraffic conditions and demand on the cache, combined with growing teams and\napplications. You simply cannot afford a bad cache strategy at scale, and having\nthe right one before you have heavy load conditions can save you immeasurably in\ncold hard cash when the scale is inevitably necessary.\n\nThe biggest benefit for using a cache key store is that you can change from one\ncache technology to another without the application itself needing a refactor,\nall you do is implement the wrapping abstraction methods for the chosen\ntechnology. This can save you if say you chose memcached early on for its ease\nof use but now need to scale and have now decided redis or AWS Elasticache are\nbetter for the business.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/C_23-DPWsAAL7lP.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-09-05 05:10:04","created_by":"1","updated_at":"2021-03-31 14:13:44","updated_by":"1","published_at":"2016-08-31 15:21:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca3","uuid":"a1dd65fb-0f13-4202-949b-32498103e8db","title":"range over channels in go - Fibonacci example","slug":"range-over-channels-in-go-fibonacci-example","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Fibonacci. \\nThis sequence of numbers occurs in nature, and has many uses.\\n \\n> In a Fibonacci sequence, each number is equal to the previous two numbers added together.\\n\\nFibonacci numbers are easily computed in modern programming languages. \\n\\nThis computational task helps us learn about how a channel works in go and how the range keyword can help make more complex tasks easier.\\n\\n```\\npackage main\\nimport \\\"fmt\\\"\\nfunc Fibonacci(ch chan int, count int) {\\n    f2, f1 := 0, 1\\n    for count >= 0 {\\n        ch <- f2\\n        count--\\n        f2, f1 = f1, f2+f1\\n    }\\n    close(ch)\\n}\\nfunc main() {\\n    ch := make(chan int)\\n    go Fibonacci(ch, 10)\\n    i := 0\\n    for num := range ch {\\n        fmt.Printf(\\\"F(%d): \\\\t%d\\\\n\\\", i, num)\\n        i++\\n    }\\n}\\n```\\n\\nThere you have it, a very basic go program.\\n\\nIn our main `func` we first allocate a channel, and pass it into our Fibonacci `func` along with an argument of `10` telling Fibonacci to produce 10 numbers from the Fibonacci sequence.\\n\\nNext we range over the channel and print any values that channel produces for as long as it is not closed.\\n\\nIn our Fibonacci function we simply declare a few variables to hole our default values for the first number in our Fibonacci sequence, then we `for` loop as many iterations as we were provided in our second argument (in this example it was `10`).\\n\\nIn each loop we send to the channel our current number in the sequence, decrement our counter, and calculate the next number in the sequence. We do this until our counter reaches zero and the last calculated number in the sequence has been sent through the channel.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h2 id=\"fibonacci\">Fibonacci.</h2>\n<p>This sequence of numbers occurs in nature, and has many uses.</p>\n<blockquote>\n<p>In a Fibonacci sequence, each number is equal to the previous two numbers added together.</p>\n</blockquote>\n<p>Fibonacci numbers are easily computed in modern programming languages.</p>\n<p>This computational task helps us learn about how a channel works in go and how the range keyword can help make more complex tasks easier.</p>\n<pre><code>package main\nimport &quot;fmt&quot;\nfunc Fibonacci(ch chan int, count int) {\n    f2, f1 := 0, 1\n    for count &gt;= 0 {\n        ch &lt;- f2\n        count--\n        f2, f1 = f1, f2+f1\n    }\n    close(ch)\n}\nfunc main() {\n    ch := make(chan int)\n    go Fibonacci(ch, 10)\n    i := 0\n    for num := range ch {\n        fmt.Printf(&quot;F(%d): \\t%d\\n&quot;, i, num)\n        i++\n    }\n}\n</code></pre>\n<p>There you have it, a very basic go program.</p>\n<p>In our main <code>func</code> we first allocate a channel, and pass it into our Fibonacci <code>func</code> along with an argument of <code>10</code> telling Fibonacci to produce 10 numbers from the Fibonacci sequence.</p>\n<p>Next we range over the channel and print any values that channel produces for as long as it is not closed.</p>\n<p>In our Fibonacci function we simply declare a few variables to hole our default values for the first number in our Fibonacci sequence, then we <code>for</code> loop as many iterations as we were provided in our second argument (in this example it was <code>10</code>).</p>\n<p>In each loop we send to the channel our current number in the sequence, decrement our counter, and calculate the next number in the sequence. We do this until our counter reaches zero and the last calculated number in the sequence has been sent through the channel.</p>\n<!--kg-card-end: markdown-->","comment_id":"36","plaintext":"Fibonacci.\nThis sequence of numbers occurs in nature, and has many uses.\n\n> In a Fibonacci sequence, each number is equal to the previous two numbers added\ntogether.\n\n\nFibonacci numbers are easily computed in modern programming languages.\n\nThis computational task helps us learn about how a channel works in go and how\nthe range keyword can help make more complex tasks easier.\n\npackage main\nimport \"fmt\"\nfunc Fibonacci(ch chan int, count int) {\n    f2, f1 := 0, 1\n    for count >= 0 {\n        ch <- f2\n        count--\n        f2, f1 = f1, f2+f1\n    }\n    close(ch)\n}\nfunc main() {\n    ch := make(chan int)\n    go Fibonacci(ch, 10)\n    i := 0\n    for num := range ch {\n        fmt.Printf(\"F(%d): \\t%d\\n\", i, num)\n        i++\n    }\n}\n\n\nThere you have it, a very basic go program.\n\nIn our main func we first allocate a channel, and pass it into our Fibonacci \nfunc along with an argument of 10 telling Fibonacci to produce 10 numbers from\nthe Fibonacci sequence.\n\nNext we range over the channel and print any values that channel produces for as\nlong as it is not closed.\n\nIn our Fibonacci function we simply declare a few variables to hole our default\nvalues for the first number in our Fibonacci sequence, then we for loop as many\niterations as we were provided in our second argument (in this example it was 10\n).\n\nIn each loop we send to the channel our current number in the sequence,\ndecrement our counter, and calculate the next number in the sequence. We do this\nuntil our counter reaches zero and the last calculated number in the sequence\nhas been sent through the channel.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/The_Golden_Ratio_R03.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-11-21 03:18:46","created_by":"1","updated_at":"2021-03-31 14:14:38","updated_by":"1","published_at":"2016-11-27 11:49:13","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca4","uuid":"a6782018-1a7d-4037-b7bd-11650a1d9d27","title":"AWS EC2 Scaling gotchas no one told you","slug":"aws-ec2-scaling-gotchas-no-one-told-you","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Here are some things I wish someone told me before I needed to manage over 5 million connections a day, 35k concurrently on average (Melbourne Cup). If that sounds like a lot to you, it is, the Commonwealth Bank had [less than 1 million hits a day this month](https://www.similarweb.com/website/commbank.com.au#overview) and other than myself there was only 1 other tech person in my little start up team to deal with the complexities of scaling.\\n\\nAWS can look overwhelming. There are now over 40 different AWS services, and many concepts to think about when building your app. AWS provides [excellent documentation](http://aws.amazon.com/documentation/) on each of their services, [helpful guides](http://aws.amazon.com/solutions/) and [white papers](http://aws.amazon.com/whitepapers/), [reference architectures](http://aws.amazon.com/architecture/) for common apps, and even a [program especially for startups](http://aws.amazon.com/activate/) where startups can receive one month free to speak with AWS Cloud Support Engineers.\\n\\nMy target audience is the developer who has started using AWS with scalability in mind, or perhaps one that has used AWS for a long time for some small app but is looking to scale it now.\\n\\n### AWS involves a paradigm shift in thinking\\n\\nMoving from physical servers to the \\\"cloud\\\" is not always obvious what needs to be done. Architecting your service for scalability is less obvious than that still.\\n\\n> Scalability is not so much of \\\"what\\\" you are using, but rather \\\"how\\\" you use it\\n\\nIt's one thing to architect a stack to be ready for scale, and another to build an application that is scalable within that stack. You've probably chosen all of the correct services already, configured them in a fair way to address highly available and fault tolerance, but your apps seem to under perform or the AWS bill seem disproportionally high to what was expected.\\n\\nIt's time to analyse where you went wrong.\\n\\n### Application Development\\n\\nYou've handled the obvious things like **storing no application state** on your ephemeral servers, **centralised logs**, in production **disabled SSH access to all servers** to focus on CI/CD, avoided EIP for auto scaled instances, scaled horizontally, scale granularly, and Multi-AZ.\\n\\nNow let's talk about some of the concerns that are not in the mainstream.\\n\\n#### Production code release to an auto-scaled stack\\n\\nThere are 2 main methods to do this without taking your autoscale group offline, and which you choose to do is comes down to only 1 consideration;\\n\\n- Are your AMI's created with the code on it, ready to run immediately after provisioning?\\n- OR; Is your AMI simply the environment, and it will pull in the latest code when it is provisioned?\\n\\nWith this answered, you are going to need to perform  the following corresponding methods;\\n\\n- Build the new AMI with the new code, update the autoscale group configuration with the new AMI ID (CloudFormation template), launch it into the autoscale group by detaching an existing instance that it replaces. This is called cycling your autoscale group.\\n- Using Git, synchronise with all servers, git pull the repo files in a temp directory and when you're files are ready to run, link them to your working directory being served. Doing this allows any running scripts to complete in memory on the old files, and any new ones utilise the new code.\\n\\nGotchas for the first option; doing this also requires you to utilise connection draining on the ELB. Failing to do this terminates open customer connections abruptly, giving the appearance of being offline. If you use a cache in front of your instances (and you should) you run the risk of an error page being cached for a long time. Which to users has the appearance of an outage.\\nAnother drawback of the first option is having to cycle servers, leaving some servers on old code whilst new ones are introduced to the stack. This can have unforeseeable symptoms, often leading to unreproducible bug reports.\\n\\nThe second option is by far what you should attempt to achieve, but it is the least traditional and seemingly risky until you understand how inode components get manipulated. This technique is synchronised so no consistency issues or error pages to concern you, there isn't ELB connection draining. This method depends on your server side language to serve each request in its own process. Having Apache/Nginx in front of a language like PHP are perfect, but if your server side language handles all requests through the main process (Ruby, Node.js) this option if off the table for you entirely. \\n\\n### Granular Auto scaling rules\\n\\nWe all want to be granular, and if we are on AWS we are not running our stacks 24/7 or we'd have chosen the cheaper dedicated hardware. No, in AWS we know we have large periods of low activity. Sometimes we may encounter scale up followed by an instant scale down event. This symptom is consistently due to poorly considered scale policies.\\n\\nHere is a simple policy based on CPU;\\n\\n* Add 1 instance when Average CPU > 60% for 3 consecutive periods of 5mins.\\n* Remove 1 instance when Average CPU < 60% for 3 consecutive periods of 5mins.\\n\\n![ec2 scale policy data](__GHOST_URL__/content/images/2016/11/ec2-scale-policy-data-1.png)\\n\\nThe above is an example where server load steadily increased and yet we encounter a scale up followed by a scale down and then an out of CPU capacity event.\\n\\nThis is a very common example, and it is always caused due to reliance on the base EC2 CPU metric. Don't fall into the trap, scale your stack based on metrics that make sense for your business case, report these metrics to CloudWatch, and create one (and only one) scale policy for each scale up and scale down event.\\n\\n> Create your own predictive algorithm\\n\\nIf you finally decide that CPU is all you need, might I suggest you compose your own CPU metric using a basic algorithm to predict the state of the stack in its current load:- what it would look like for the remaining instances if you were to reduce its capacity. Use this new CPU based metric to decide if a scale event is going to be a positive outcome.\\n\\n### Finally\\n\\nPlease Leave comments below or reach me on twitter.\\n\\nI will be adding more to this article in the coming days.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Here are some things I wish someone told me before I needed to manage over 5 million connections a day, 35k concurrently on average (Melbourne Cup). If that sounds like a lot to you, it is, the Commonwealth Bank had <a href=\"https://www.similarweb.com/website/commbank.com.au#overview\">less than 1 million hits a day this month</a> and other than myself there was only 1 other tech person in my little start up team to deal with the complexities of scaling.</p>\n<p>AWS can look overwhelming. There are now over 40 different AWS services, and many concepts to think about when building your app. AWS provides <a href=\"http://aws.amazon.com/documentation/\">excellent documentation</a> on each of their services, <a href=\"http://aws.amazon.com/solutions/\">helpful guides</a> and <a href=\"http://aws.amazon.com/whitepapers/\">white papers</a>, <a href=\"http://aws.amazon.com/architecture/\">reference architectures</a> for common apps, and even a <a href=\"http://aws.amazon.com/activate/\">program especially for startups</a> where startups can receive one month free to speak with AWS Cloud Support Engineers.</p>\n<p>My target audience is the developer who has started using AWS with scalability in mind, or perhaps one that has used AWS for a long time for some small app but is looking to scale it now.</p>\n<h3 id=\"awsinvolvesaparadigmshiftinthinking\">AWS involves a paradigm shift in thinking</h3>\n<p>Moving from physical servers to the &quot;cloud&quot; is not always obvious what needs to be done. Architecting your service for scalability is less obvious than that still.</p>\n<blockquote>\n<p>Scalability is not so much of &quot;what&quot; you are using, but rather &quot;how&quot; you use it</p>\n</blockquote>\n<p>It's one thing to architect a stack to be ready for scale, and another to build an application that is scalable within that stack. You've probably chosen all of the correct services already, configured them in a fair way to address highly available and fault tolerance, but your apps seem to under perform or the AWS bill seem disproportionally high to what was expected.</p>\n<p>It's time to analyse where you went wrong.</p>\n<h3 id=\"applicationdevelopment\">Application Development</h3>\n<p>You've handled the obvious things like <strong>storing no application state</strong> on your ephemeral servers, <strong>centralised logs</strong>, in production <strong>disabled SSH access to all servers</strong> to focus on CI/CD, avoided EIP for auto scaled instances, scaled horizontally, scale granularly, and Multi-AZ.</p>\n<p>Now let's talk about some of the concerns that are not in the mainstream.</p>\n<h4 id=\"productioncodereleasetoanautoscaledstack\">Production code release to an auto-scaled stack</h4>\n<p>There are 2 main methods to do this without taking your autoscale group offline, and which you choose to do is comes down to only 1 consideration;</p>\n<ul>\n<li>Are your AMI's created with the code on it, ready to run immediately after provisioning?</li>\n<li>OR; Is your AMI simply the environment, and it will pull in the latest code when it is provisioned?</li>\n</ul>\n<p>With this answered, you are going to need to perform  the following corresponding methods;</p>\n<ul>\n<li>Build the new AMI with the new code, update the autoscale group configuration with the new AMI ID (CloudFormation template), launch it into the autoscale group by detaching an existing instance that it replaces. This is called cycling your autoscale group.</li>\n<li>Using Git, synchronise with all servers, git pull the repo files in a temp directory and when you're files are ready to run, link them to your working directory being served. Doing this allows any running scripts to complete in memory on the old files, and any new ones utilise the new code.</li>\n</ul>\n<p>Gotchas for the first option; doing this also requires you to utilise connection draining on the ELB. Failing to do this terminates open customer connections abruptly, giving the appearance of being offline. If you use a cache in front of your instances (and you should) you run the risk of an error page being cached for a long time. Which to users has the appearance of an outage.<br>\nAnother drawback of the first option is having to cycle servers, leaving some servers on old code whilst new ones are introduced to the stack. This can have unforeseeable symptoms, often leading to unreproducible bug reports.</p>\n<p>The second option is by far what you should attempt to achieve, but it is the least traditional and seemingly risky until you understand how inode components get manipulated. This technique is synchronised so no consistency issues or error pages to concern you, there isn't ELB connection draining. This method depends on your server side language to serve each request in its own process. Having Apache/Nginx in front of a language like PHP are perfect, but if your server side language handles all requests through the main process (Ruby, Node.js) this option if off the table for you entirely.</p>\n<h3 id=\"granularautoscalingrules\">Granular Auto scaling rules</h3>\n<p>We all want to be granular, and if we are on AWS we are not running our stacks 24/7 or we'd have chosen the cheaper dedicated hardware. No, in AWS we know we have large periods of low activity. Sometimes we may encounter scale up followed by an instant scale down event. This symptom is consistently due to poorly considered scale policies.</p>\n<p>Here is a simple policy based on CPU;</p>\n<ul>\n<li>Add 1 instance when Average CPU &gt; 60% for 3 consecutive periods of 5mins.</li>\n<li>Remove 1 instance when Average CPU &lt; 60% for 3 consecutive periods of 5mins.</li>\n</ul>\n<p><img src=\"__GHOST_URL__/content/images/2016/11/ec2-scale-policy-data-1.png\" alt=\"ec2 scale policy data\" loading=\"lazy\"></p>\n<p>The above is an example where server load steadily increased and yet we encounter a scale up followed by a scale down and then an out of CPU capacity event.</p>\n<p>This is a very common example, and it is always caused due to reliance on the base EC2 CPU metric. Don't fall into the trap, scale your stack based on metrics that make sense for your business case, report these metrics to CloudWatch, and create one (and only one) scale policy for each scale up and scale down event.</p>\n<blockquote>\n<p>Create your own predictive algorithm</p>\n</blockquote>\n<p>If you finally decide that CPU is all you need, might I suggest you compose your own CPU metric using a basic algorithm to predict the state of the stack in its current load:- what it would look like for the remaining instances if you were to reduce its capacity. Use this new CPU based metric to decide if a scale event is going to be a positive outcome.</p>\n<h3 id=\"finally\">Finally</h3>\n<p>Please Leave comments below or reach me on twitter.</p>\n<p>I will be adding more to this article in the coming days.</p>\n<!--kg-card-end: markdown-->","comment_id":"37","plaintext":"Here are some things I wish someone told me before I needed to manage over 5\nmillion connections a day, 35k concurrently on average (Melbourne Cup). If that\nsounds like a lot to you, it is, the Commonwealth Bank had less than 1 million\nhits a day this month\n[https://www.similarweb.com/website/commbank.com.au#overview] and other than\nmyself there was only 1 other tech person in my little start up team to deal\nwith the complexities of scaling.\n\nAWS can look overwhelming. There are now over 40 different AWS services, and\nmany concepts to think about when building your app. AWS provides excellent\ndocumentation [http://aws.amazon.com/documentation/] on each of their services, \nhelpful guides [http://aws.amazon.com/solutions/] and white papers\n[http://aws.amazon.com/whitepapers/], reference architectures\n[http://aws.amazon.com/architecture/] for common apps, and even a program\nespecially for startups [http://aws.amazon.com/activate/] where startups can\nreceive one month free to speak with AWS Cloud Support Engineers.\n\nMy target audience is the developer who has started using AWS with scalability\nin mind, or perhaps one that has used AWS for a long time for some small app but\nis looking to scale it now.\n\nAWS involves a paradigm shift in thinking\nMoving from physical servers to the \"cloud\" is not always obvious what needs to\nbe done. Architecting your service for scalability is less obvious than that\nstill.\n\n> Scalability is not so much of \"what\" you are using, but rather \"how\" you use it\n\n\nIt's one thing to architect a stack to be ready for scale, and another to build\nan application that is scalable within that stack. You've probably chosen all of\nthe correct services already, configured them in a fair way to address highly\navailable and fault tolerance, but your apps seem to under perform or the AWS\nbill seem disproportionally high to what was expected.\n\nIt's time to analyse where you went wrong.\n\nApplication Development\nYou've handled the obvious things like storing no application state on your\nephemeral servers, centralised logs, in production disabled SSH access to all\nservers to focus on CI/CD, avoided EIP for auto scaled instances, scaled\nhorizontally, scale granularly, and Multi-AZ.\n\nNow let's talk about some of the concerns that are not in the mainstream.\n\nProduction code release to an auto-scaled stack\nThere are 2 main methods to do this without taking your autoscale group offline,\nand which you choose to do is comes down to only 1 consideration;\n\n * Are your AMI's created with the code on it, ready to run immediately after\n   provisioning?\n * OR; Is your AMI simply the environment, and it will pull in the latest code\n   when it is provisioned?\n\nWith this answered, you are going to need to perform the following corresponding\nmethods;\n\n * Build the new AMI with the new code, update the autoscale group configuration\n   with the new AMI ID (CloudFormation template), launch it into the autoscale\n   group by detaching an existing instance that it replaces. This is called\n   cycling your autoscale group.\n * Using Git, synchronise with all servers, git pull the repo files in a temp\n   directory and when you're files are ready to run, link them to your working\n   directory being served. Doing this allows any running scripts to complete in\n   memory on the old files, and any new ones utilise the new code.\n\nGotchas for the first option; doing this also requires you to utilise connection\ndraining on the ELB. Failing to do this terminates open customer connections\nabruptly, giving the appearance of being offline. If you use a cache in front of\nyour instances (and you should) you run the risk of an error page being cached\nfor a long time. Which to users has the appearance of an outage.\nAnother drawback of the first option is having to cycle servers, leaving some\nservers on old code whilst new ones are introduced to the stack. This can have\nunforeseeable symptoms, often leading to unreproducible bug reports.\n\nThe second option is by far what you should attempt to achieve, but it is the\nleast traditional and seemingly risky until you understand how inode components\nget manipulated. This technique is synchronised so no consistency issues or\nerror pages to concern you, there isn't ELB connection draining. This method\ndepends on your server side language to serve each request in its own process.\nHaving Apache/Nginx in front of a language like PHP are perfect, but if your\nserver side language handles all requests through the main process (Ruby,\nNode.js) this option if off the table for you entirely.\n\nGranular Auto scaling rules\nWe all want to be granular, and if we are on AWS we are not running our stacks\n24/7 or we'd have chosen the cheaper dedicated hardware. No, in AWS we know we\nhave large periods of low activity. Sometimes we may encounter scale up followed\nby an instant scale down event. This symptom is consistently due to poorly\nconsidered scale policies.\n\nHere is a simple policy based on CPU;\n\n * Add 1 instance when Average CPU > 60% for 3 consecutive periods of 5mins.\n * Remove 1 instance when Average CPU < 60% for 3 consecutive periods of 5mins.\n\n\n\nThe above is an example where server load steadily increased and yet we\nencounter a scale up followed by a scale down and then an out of CPU capacity\nevent.\n\nThis is a very common example, and it is always caused due to reliance on the\nbase EC2 CPU metric. Don't fall into the trap, scale your stack based on metrics\nthat make sense for your business case, report these metrics to CloudWatch, and\ncreate one (and only one) scale policy for each scale up and scale down event.\n\n> Create your own predictive algorithm\n\n\nIf you finally decide that CPU is all you need, might I suggest you compose your\nown CPU metric using a basic algorithm to predict the state of the stack in its\ncurrent load:- what it would look like for the remaining instances if you were\nto reduce its capacity. Use this new CPU based metric to decide if a scale event\nis going to be a positive outcome.\n\nFinally\nPlease Leave comments below or reach me on twitter.\n\nI will be adding more to this article in the coming days.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/AWS_01_1500@2x-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2016-11-27 12:08:36","created_by":"1","updated_at":"2021-03-31 14:13:55","updated_by":"1","published_at":"2016-11-27 14:28:19","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca5","uuid":"eecda888-f2df-4283-9232-cfc61264c74c","title":"Browser Compatbility","slug":"browser-compatbility","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]],\"ghostVersion\":\"3.0\"}","html":null,"comment_id":"38","plaintext":null,"feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"6064789120c4c500017fbc6f","created_at":"2017-03-26 09:51:38","created_by":"1","updated_at":"2017-03-26 09:54:52","updated_by":null,"published_at":"2012-01-01 12:59:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca6","uuid":"0c64b1b9-e8e8-471a-9757-a96bebec1ff9","title":"About Me","slug":"about","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Senior Information Security Consultant at CMD Solutions Australia, responsibile enterprise risk assessments and offering consultancy on DevSecOps projects.\\n\\n**What I do;**\\n\\n* All things AWS and DevOps.\\n* Talks at Technology meetups.\\n* Volunteer teaching basics of website coding.\\n\\n## Tech I can't get enough of;\\n\\n* Cryptography\\n* D3.js SVG\\n* Canvas (Phaser.IO & Three.js)\\n* Streams (Kappa or Lambda)\\n* InFluxDB\\n* Elasticsearch + Kibana\\n* Trais CI\\n* Heroku\\n* AWS Compute + Lambda\\n* Redhat Openshift\\n* Docker + Swarm Mode\\n* Portainer\\n* Terraform\\n* Consul\\n* Varnish\\n* Redis\\n* Tshark / Wireshark / Sharktap\\n* dnsmasq\\n* Nginx reverse proxy and load balance\\n* InFluxDB\\n* MySQL\\n* Apache Spark\\n* MXNet\\n* Node.js\\n* Python\\n\\n## Other Tech I have practiced during employment;\\n\\n* Azure\\n* Google Cloud\\n* C++\\n* Java\\n* Groovy\\n* PHP\\n* Postgres\\n* OracleDB\\n* MS SQL Server\\n* T-SQL/PL-SQL\\n* Jenkins\\n* Nexus\\n* GitLab\\n* Codeship CI\\n* Jet CI\\n* CodeBuild\\n* CodeDeploy\\n* SVN\\n* MongoDB\\n* Squid Proxy\\n* Memcache\\n* Angular\\n* ReactNative\\n\\n## Tech I'm into lately, polishing required;\\n* Go\\n* Rust\\n* Scala\\n* Tensorflow\\n* Keras\\n\\n> I have a broad skill set but not always deep. I do however go deep into a few key areas\\n\\n**Cyber Security:** Network Hardening, incident response, network tapping and packet analysis, dnsmasq configuration, OWASP and KaliLinux penetration, attack surface analysis, and reports such as RACI, CSIR, AAR, ISTR.\\n\\n**Data Science:** new way of saying data analysis and visualisation of insights. I use H2O, Yellowfin, Yhat Rodeo, and DSS.\\n\\n**Big Data:** MS BI Suite, SSRS, Terradata, Hadoop, Pandas, Apache Beam (Google dataflow) and Flink.\\n\\n**Machine Learning:** AWS ML, IBM Watson, XGBoost, Zingtree, MXNet.\\n\\n**IoT:** the Buzzword for connected embedded systems. I've dabbled in Prototyping, GPIO sensors and devices like Raspberry Pi and ASUS Ticker.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Senior Information Security Consultant at CMD Solutions Australia, responsibile enterprise risk assessments and offering consultancy on DevSecOps projects.</p>\n<p><strong>What I do;</strong></p>\n<ul>\n<li>All things AWS and DevOps.</li>\n<li>Talks at Technology meetups.</li>\n<li>Volunteer teaching basics of website coding.</li>\n</ul>\n<h2 id=\"techicantgetenoughof\">Tech I can't get enough of;</h2>\n<ul>\n<li>Cryptography</li>\n<li>D3.js SVG</li>\n<li>Canvas (Phaser.IO &amp; Three.js)</li>\n<li>Streams (Kappa or Lambda)</li>\n<li>InFluxDB</li>\n<li>Elasticsearch + Kibana</li>\n<li>Trais CI</li>\n<li>Heroku</li>\n<li>AWS Compute + Lambda</li>\n<li>Redhat Openshift</li>\n<li>Docker + Swarm Mode</li>\n<li>Portainer</li>\n<li>Terraform</li>\n<li>Consul</li>\n<li>Varnish</li>\n<li>Redis</li>\n<li>Tshark / Wireshark / Sharktap</li>\n<li>dnsmasq</li>\n<li>Nginx reverse proxy and load balance</li>\n<li>InFluxDB</li>\n<li>MySQL</li>\n<li>Apache Spark</li>\n<li>MXNet</li>\n<li>Node.js</li>\n<li>Python</li>\n</ul>\n<h2 id=\"othertechihavepracticedduringemployment\">Other Tech I have practiced during employment;</h2>\n<ul>\n<li>Azure</li>\n<li>Google Cloud</li>\n<li>C++</li>\n<li>Java</li>\n<li>Groovy</li>\n<li>PHP</li>\n<li>Postgres</li>\n<li>OracleDB</li>\n<li>MS SQL Server</li>\n<li>T-SQL/PL-SQL</li>\n<li>Jenkins</li>\n<li>Nexus</li>\n<li>GitLab</li>\n<li>Codeship CI</li>\n<li>Jet CI</li>\n<li>CodeBuild</li>\n<li>CodeDeploy</li>\n<li>SVN</li>\n<li>MongoDB</li>\n<li>Squid Proxy</li>\n<li>Memcache</li>\n<li>Angular</li>\n<li>ReactNative</li>\n</ul>\n<h2 id=\"techimintolatelypolishingrequired\">Tech I'm into lately, polishing required;</h2>\n<ul>\n<li>Go</li>\n<li>Rust</li>\n<li>Scala</li>\n<li>Tensorflow</li>\n<li>Keras</li>\n</ul>\n<blockquote>\n<p>I have a broad skill set but not always deep. I do however go deep into a few key areas</p>\n</blockquote>\n<p><strong>Cyber Security:</strong> Network Hardening, incident response, network tapping and packet analysis, dnsmasq configuration, OWASP and KaliLinux penetration, attack surface analysis, and reports such as RACI, CSIR, AAR, ISTR.</p>\n<p><strong>Data Science:</strong> new way of saying data analysis and visualisation of insights. I use H2O, Yellowfin, Yhat Rodeo, and DSS.</p>\n<p><strong>Big Data:</strong> MS BI Suite, SSRS, Terradata, Hadoop, Pandas, Apache Beam (Google dataflow) and Flink.</p>\n<p><strong>Machine Learning:</strong> AWS ML, IBM Watson, XGBoost, Zingtree, MXNet.</p>\n<p><strong>IoT:</strong> the Buzzword for connected embedded systems. I've dabbled in Prototyping, GPIO sensors and devices like Raspberry Pi and ASUS Ticker.</p>\n<!--kg-card-end: markdown-->","comment_id":"39","plaintext":"Senior Information Security Consultant at CMD Solutions Australia, responsibile\nenterprise risk assessments and offering consultancy on DevSecOps projects.\n\nWhat I do;\n\n * All things AWS and DevOps.\n * Talks at Technology meetups.\n * Volunteer teaching basics of website coding.\n\nTech I can't get enough of;\n * Cryptography\n * D3.js SVG\n * Canvas (Phaser.IO & Three.js)\n * Streams (Kappa or Lambda)\n * InFluxDB\n * Elasticsearch + Kibana\n * Trais CI\n * Heroku\n * AWS Compute + Lambda\n * Redhat Openshift\n * Docker + Swarm Mode\n * Portainer\n * Terraform\n * Consul\n * Varnish\n * Redis\n * Tshark / Wireshark / Sharktap\n * dnsmasq\n * Nginx reverse proxy and load balance\n * InFluxDB\n * MySQL\n * Apache Spark\n * MXNet\n * Node.js\n * Python\n\nOther Tech I have practiced during employment;\n * Azure\n * Google Cloud\n * C++\n * Java\n * Groovy\n * PHP\n * Postgres\n * OracleDB\n * MS SQL Server\n * T-SQL/PL-SQL\n * Jenkins\n * Nexus\n * GitLab\n * Codeship CI\n * Jet CI\n * CodeBuild\n * CodeDeploy\n * SVN\n * MongoDB\n * Squid Proxy\n * Memcache\n * Angular\n * ReactNative\n\nTech I'm into lately, polishing required;\n * Go\n * Rust\n * Scala\n * Tensorflow\n * Keras\n\n> I have a broad skill set but not always deep. I do however go deep into a few\nkey areas\n\n\nCyber Security: Network Hardening, incident response, network tapping and packet\nanalysis, dnsmasq configuration, OWASP and KaliLinux penetration, attack surface\nanalysis, and reports such as RACI, CSIR, AAR, ISTR.\n\nData Science: new way of saying data analysis and visualisation of insights. I\nuse H2O, Yellowfin, Yhat Rodeo, and DSS.\n\nBig Data: MS BI Suite, SSRS, Terradata, Hadoop, Pandas, Apache Beam (Google\ndataflow) and Flink.\n\nMachine Learning: AWS ML, IBM Watson, XGBoost, Zingtree, MXNet.\n\nIoT: the Buzzword for connected embedded systems. I've dabbled in Prototyping,\nGPIO sensors and devices like Raspberry Pi and ASUS Ticker.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"6064789120c4c500017fbc6f","created_at":"2017-03-26 10:07:43","created_by":"1","updated_at":"2018-01-23 01:14:39","updated_by":null,"published_at":"2012-01-01 12:59:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca7","uuid":"aafd937e-dbe3-4cee-b3d7-e0d913ac124b","title":"Kaggle Competition #melbdarathon2017","slug":"week-1-kaggle-competition-melbdarathon2017","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This will be more of a play-by-play of highlights on things like **Community**, **Research**, and talking about **basics of data science** from example of what I did, not how or academically why 👨‍🎓.\\n\\nIt all started with a casual conversation that led me to the [Data Science Melbourne](http://meetu.ps/c/1RYs6/jMKxK/a) #Datathon kickoff night Thursday last week.\\n\\n### Kickoff night\\n\\nFirst thing was lining up with my laptop to copy some data. \\n![Datathon kickoff](__GHOST_URL__/content/images/2017/04/highres_460027800.jpeg)\\nBusy yes, but also very well organised.\\n![](__GHOST_URL__/content/images/2017/04/highres_459995567.jpeg)\\n\\nOnce at the table the copy was quick and I was off with a few quick `cat` and `head` greps.\\n\\n### The data\\n\\nUntil that moment I wasn't aware of the type of data we were getting. I learned that it was medical industry related, which is both scary because I've never had any work in this sector, and exciting for the same reason. \\n\\nThe event attendants all signed NDAs so other than what has been publicly made available I can't go into details.\\n\\n### Community\\n\\nI am huge on collaboration and community, I think it's my inner mentor taking over my professional self at times. Walking around the room looking for some indication of how to communicate during the event 👀, asked a few people, no one was aware.\\nSo I met the organiser Phil Brierley. He arranged a forum (how quaint) so I offered Slack. Phil is a Slack user so was pretty keen for someone to take lead on that for the event. Happy to oblige 😀\\nAt the end of last year (2016) around November I set up [Melbourne Developer Network](https://melbdev.slack.com/shared_invite/MTY4NjM1OTQxNDA4LTE0OTIxNjI4ODgtNGVjZmYzOGRiOQ) mainly to satisfy the need to combine and communicate with developers from all over the world daily while exposing them to developers from Melbourne, an awesome tech community.\\n\\n### Predict the onset of diabetes\\n\\nOn my way home I made myself familiar with the [Kaggle comp](https://inclass.kaggle.com/c/dsm2017/forums/t/31449/welcome?forumMessageId=174409#post174409) \\n\\nSo I ask my close buddy \\\"Ok Google, How to win a Kaggle comp?\\\";\\n[Blog post on: how to win a kaggle competition](https://www.import.io/post/how-to-win-a-kaggle-competition/)\\n\\nGoog knows everything 🔍 💡\\n\\nSo I noticed something interesting\\n> It used to be random forest that was the big winner, but over the last six months a new algorithm called XGboost has cropped up, and it’s winning practically every competition in the structured data category.\\n\\nNow I'm a bit of a Logistic Regression go-to kind of guy, and known to dabble in decision tree to better understand my data (well more of the overfitted regression tree is what you'll see unfortunately, but i try). So I was quite keen to learn something new for this competition\\n\\n#### What is XGBoost\\n\\nSo what is this magical silver bullet XGBoost? It's clearly an implementation of boosted tree models by its naming, this is what i found;\\n[A Gentle Introduction to XGBoost for Applied Machine Learning](http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/) which was concise though lacked specifics, so [this post](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/) on Gradient Boosting with XGBoost was perfect for my brain to consume.\\n\\n### My first Kaggle competition\\n\\nI digress, so back to the comp, I was after a bit more of an introduction as a Kaggle beginner, rather than a winning plan.\\nThis post was actually quite helpful;\\n[Titanic made easy - predict survival Competition](https://blog.dataiku.com/titanic-kaggle-made-easy)\\n\\nIt introduced the [DSS (Data Science Studio)](https://www.dataiku.com/dss/trynow/) tool which I've been exposed to before but is a great example that is full of really useful tips not just for a first time Kaggler but also as an introductory to the general topics and concepts of ML that you might like as a launch pad to go research specific things.\\n\\nOne thing that you really must do first before you can attempt to do any type os ML is calculate a baseline, it is really important and isn't talked about a great deal. You'll notice that DSS gives you different performance indicators of algorithms but you need the baseline to comprehend this at all.\\n\\nThe first thing I needed to achieve was the baseline, an essential element to any machine learning endeavour. How does one calculate a baseline? What is a baseline? \\nGoog can be your friend too 🙃\\nHonestly, you will find a well written explanation and guide in googles top results, far better then I would be able to describe here.\\n\\n### Reading the data\\n\\nAll of the data was provided in csv, obviously that was never going to be workable, and we only had a small sample of data 740MB compressed, so we're not likely to be diving into big data territory here, Python 🐍 to the rescue.\\n\\nI loaded the csv into a Spark DataFrame and then converted that to a Pandas DataFrame for querying, my colleague introduced [Rodeo](https://www.yhat.com/products/rodeo/downloads) IDE to me recently so it seems this is a great project to give that a try.\\n\\nSo, how much data do we have? Row count just under 60 million rows. Nice. \\n\\n_python_\\n```\\ntotal=0\\nfor file in os.listdir(path):\\n    df = pd.read_table(path+'/'+file)\\n    count = len(df)\\n    print(file+' '+str(count))\\n    total+=count\\nprint(total) // 59,450,785\\n```\\n\\nThe next step is to split the data into sets that will be used for model testing and training. There are a few schools of thought on this topic and in academia k-fold is overly hyped to the point it has become somewhat of a buzzword, literally a random RMIT student threw it into a conversation where we were discussing the csv file while I was grepping to see what we had \\\"have you tried k-fold to test the files?\\\", Huh!?.. I didn't bother clarifying, I just tried to be as casual as possible directing that question about k-fold to their professor or teacher.\\n\\nIn any case, while we are on that topic, I'll be using more of a \\\"holdout\\\" approach and am more comfortable doing my cross validation in this manner for this topic specifically. The one thing we should do before we even get to that is understand the portions of our data and what they are used for.\\n\\n### Test versus Train\\n\\nYou have the target feature you are going be predicting with your model, The train dataset is a portion that have already been scored, the actual results are known and included in the dataset. Your test dataset is what you test the results of your model with to make the prediction, we still have known results but this test set of data will have all of the same features as the train set.\\nBeing a Kaggle competition there will be a withheld portion of data as well, so the test set should be identified and split for your submissions. So we will end up with 1 train set and 2 test sets.\\n![visualising data sets](__GHOST_URL__/content/images/2017/04/training.test.set.png)\\n\\nI spent several hours splitting out the data into HDFS files from the Pandas DataFrames to achieve this.\\n\\nJust to be clear now we know how much data we have and how we will be using it, even though I'll be storing dataset samples as HDFS, I will not be using Hadoop\\n[Don't use Hadoop - your data isn't that big](https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html)\\n\\nGranted we have 60 million rows of data, maxing out the effectiveness of SQL almost immediately (not entirely true), don't think that just because it is hard in an SQL database that you have big data and you must resort to solutions like Hadoop, it's a myth, an ill-conceived concept, the choice for the less educated; and has a simple remedy called Pandas.\\n\\nThe HDFS format is simple enough for Pandas to deal with so it's a pure convenience for me.\\n\\n### Datathon insights competition\\n\\nOut of the 2 competitions I am far better equipped with my experience to do well on the insights portion, so I've spent a lot of time this week exploring the data in different ways.\\n\\nThe folks from [Yellowfin](https://www.yellowfinbi.com/) sponsored the datathon and were kind enough to give us free licenses to use their platform. They have several data source connectors available but none that are possible with the HDFS files i've curated but MySQL was one that I was happy to try. \\n\\n### Using Docker\\n\\nBefore doing anything you need an environment to work in. Because my computer is packed with all sorts of project I contribute to, open sourced, and the ones for my full time employment. I generally use Docker now (previously i was big on Vagrant) so here is a compose file i put together for this event; \\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/2649c267284367b53696191f09bc9735.js\\\"></script>\\n\\nAnd the python container\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/dbaa9bd4939b566b3de6f604c588ca0b.js\\\"></script>\\n\\n### Using MySQL\\n\\nSo I'm trying out Yellpwfin and one of their datasources is MySQL. Initially i was just using Pandas but that isn't as expressive as SQL for quick queries, so my go-to SQL database is usually MySQL for several reasons;\\n\\n- It's free with good documentation, loads of users, and a great community for answers if you need.\\n- MyISAM is exceptionally fast for searching\\n- You can partition data in almost any way you can imagine\\n- It has a powerful and mature cli\\n- There are solid and mature SDKs for practically any programming language\\n\\nAfter a quick `LOAD DATA INFILE` sweep to get data in and running a few views of that data in Yellowfin I quickly ascertained that I'm not going to be very productive using MySQL regardless of how Yellowfin worked (which seemed to be quite well presented with a lot of neat visualisations built in such as graphs and scatter maps which was pretty cool).\\n\\nSo although I can do some fast queries across the entire partitioned dataset with well thought-out filters, doing any sort of aggregation and grouping is really pushing the databases limits.\\n\\n### Using Elasticsearch\\n\\nBash is great for fetching and managing data as strings. Due to how much I use bash for automation and tooling it is my preferred scripting language, and although it is very powerful at string manipulation it isn't as human friendly for viewing or visualising said string results, but Elasticsearch is thanks to JSON, and I can still script everything in Bash.\\n\\nSo Elasticsearch is generally a great tool to get some quick insight with expressive queries by just using curl, it has proven to me on some massive projects in the past to be far superior than and traditional database engine, here is some quick bash to get data from mysql\\n\\n```\\n#!/usr/bin/env bash\\nuser=<u>\\npassword=<pw>\\nhost=mysql\\n\\nes_host=es:9200\\nes_index=melbdatathon2017\\nes_type=transactions\\n\\nmax=279201\\nstart=0\\nend=25\\nstep=25\\ndocument=1\\n\\necho \\\"[\\\" > ./errors.json\\nwhile [ $start -lt $max ]\\ndo\\n    end=$(( end > max ? max : end ))\\n    mysql melbdatathon2017 -h$host -u$user -p$password -B -N -s <<<\\\"\\n    SELECT\\n        cols\\n    FROM\\n    tables\\n    LIMIT start, step;\\\" | while read -r line\\n    do\\n      someCol=`echo \\\"$line\\\" | cut -f1 | tr '[:upper:]' '[:lower:]'`\\n      json=\\\"{/* stuff here */}\\\"\\n    json=`echo \\\"${json}\\\" | tr -d '\\\\r\\\\n' | tr -s ' '`\\n    es_response=`curl -s -XPOST \\\"${es_host}/${es_index}/${es_type}/?pretty\\\" -H 'Content-Type: application/json' -d \\\"${json}\\\"`\\n    failed=`echo \\\"${es_response}\\\" | jq -r \\\"._shards.failed\\\"`\\n    if [ \\\"${failed}\\\" -eq \\\"1\\\" ]; then\\n        echo \\\"${es_response},\\\" >> ./errors.json\\n    fi\\n\\n    start=`expr $end + 1`\\n    end=`expr $end + $step`\\n    document=`expr $document + 1`\\n    done\\n    echo \\\"Stored: ${document}\\\"\\ndone\\necho \\\"]\\\" >> ./errors.json\\necho \\\"Done: ${document}\\\"\\n```\\n\\n### Data mining\\n\\nFor one of the visualisations I wanted to do, I needed the lon and lat to plot a scatter map, but all we had been provided were postcodes. \\n\\nI put together this short bash script to enrich my dataset;\\n\\n<script src=\\\"https://gist.github.com/chrisdlangton/4768d0dc86d77f6638fec6ec00325111.js\\\"></script>\\n\\nOnce I had the geo data for each post code it was a simple but long running effort to script a cursor lookup by postcode and enrich my dataset.\\n\\n### Using Influxdb\\n\\nAnother useful way to store data for insights is by time series. Yes tools that read HDFS are good for this task, MySQL and Elasticsearch are also very capable as well, but Influxdb is _designed_ to achieve this specific use case.\\n\\n> I am very big on choosing the right tool for the job.\\n\\nIt was a pretty trivial task to modify the script I wrote that exports from MySQL and imports to Elasticsearch to instead achieve some inserts into Influx, and once I had the data it became clear in the data I was seeing that my goals required further research.\\n\\n### Research\\n\\nOne of the insights experiments I conducted involved cost over time, I wont go into specifics about what kind of cost we are talking about in terms of data but I can describe that it involved understanding the vague parameters of the [Pricing Pharmaceutical Benefits Scheme](https://www.humanservices.gov.au/health-professionals/enablers/pricing-pharmaceutical-benefits-scheme-medicine). My conclusion here is that there was a big black hole of information around what is classified as \\\"the cost to the pharmacist\\\" for which tears apart my goals.\\n\\n### Hack day 1\\n\\nUnable to attend in person I made myself available over Slack and helped out a few competitors throughout day on problems in their environment or understanding the data. This was only possible due to the awesome exposure of my Slack team;\\n\\n![Melbource Developer Network Slack Team on the big monitors](__GHOST_URL__/content/images/2017/04/highres_460037585.jpeg)\\n\\nI was sharing scripts and running some basic graph visualisations across the entire data, this was fun and insightful to be involved in different ways of thinkings.\\n\\nI didn't have much time invested to conduct my own work but it was well worth it to speak and meet some great people.\\n\\n### Next steps\\n\\nI still need a great isights idea, i've worked on many small pieces of insights but I'm looking for one that will have profound benefits to someone. So I spoke to a friend whom is a Diabetes sufferer for some first hand experience and he has been a great source of inspiration. \\n\\nThe insight competition doesn't actually need to be related to Diabetes, only the Kaggle comp is, so I remain open-minded to the possibilities.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>This will be more of a play-by-play of highlights on things like <strong>Community</strong>, <strong>Research</strong>, and talking about <strong>basics of data science</strong> from example of what I did, not how or academically why 👨‍🎓.</p>\n<p>It all started with a casual conversation that led me to the <a href=\"http://meetu.ps/c/1RYs6/jMKxK/a\">Data Science Melbourne</a> #Datathon kickoff night Thursday last week.</p>\n<h3 id=\"kickoffnight\">Kickoff night</h3>\n<p>First thing was lining up with my laptop to copy some data.<br>\n<img src=\"__GHOST_URL__/content/images/2017/04/highres_460027800.jpeg\" alt=\"Datathon kickoff\" loading=\"lazy\"><br>\nBusy yes, but also very well organised.<br>\n<img src=\"__GHOST_URL__/content/images/2017/04/highres_459995567.jpeg\" alt=\"\" loading=\"lazy\"></p>\n<p>Once at the table the copy was quick and I was off with a few quick <code>cat</code> and <code>head</code> greps.</p>\n<h3 id=\"thedata\">The data</h3>\n<p>Until that moment I wasn't aware of the type of data we were getting. I learned that it was medical industry related, which is both scary because I've never had any work in this sector, and exciting for the same reason.</p>\n<p>The event attendants all signed NDAs so other than what has been publicly made available I can't go into details.</p>\n<h3 id=\"community\">Community</h3>\n<p>I am huge on collaboration and community, I think it's my inner mentor taking over my professional self at times. Walking around the room looking for some indication of how to communicate during the event 👀, asked a few people, no one was aware.<br>\nSo I met the organiser Phil Brierley. He arranged a forum (how quaint) so I offered Slack. Phil is a Slack user so was pretty keen for someone to take lead on that for the event. Happy to oblige 😀<br>\nAt the end of last year (2016) around November I set up <a href=\"https://melbdev.slack.com/shared_invite/MTY4NjM1OTQxNDA4LTE0OTIxNjI4ODgtNGVjZmYzOGRiOQ\">Melbourne Developer Network</a> mainly to satisfy the need to combine and communicate with developers from all over the world daily while exposing them to developers from Melbourne, an awesome tech community.</p>\n<h3 id=\"predicttheonsetofdiabetes\">Predict the onset of diabetes</h3>\n<p>On my way home I made myself familiar with the <a href=\"https://inclass.kaggle.com/c/dsm2017/forums/t/31449/welcome?forumMessageId=174409#post174409\">Kaggle comp</a></p>\n<p>So I ask my close buddy &quot;Ok Google, How to win a Kaggle comp?&quot;;<br>\n<a href=\"https://www.import.io/post/how-to-win-a-kaggle-competition/\">Blog post on: how to win a kaggle competition</a></p>\n<p>Goog knows everything 🔍 💡</p>\n<p>So I noticed something interesting</p>\n<blockquote>\n<p>It used to be random forest that was the big winner, but over the last six months a new algorithm called XGboost has cropped up, and it’s winning practically every competition in the structured data category.</p>\n</blockquote>\n<p>Now I'm a bit of a Logistic Regression go-to kind of guy, and known to dabble in decision tree to better understand my data (well more of the overfitted regression tree is what you'll see unfortunately, but i try). So I was quite keen to learn something new for this competition</p>\n<h4 id=\"whatisxgboost\">What is XGBoost</h4>\n<p>So what is this magical silver bullet XGBoost? It's clearly an implementation of boosted tree models by its naming, this is what i found;<br>\n<a href=\"http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\">A Gentle Introduction to XGBoost for Applied Machine Learning</a> which was concise though lacked specifics, so <a href=\"http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\">this post</a> on Gradient Boosting with XGBoost was perfect for my brain to consume.</p>\n<h3 id=\"myfirstkagglecompetition\">My first Kaggle competition</h3>\n<p>I digress, so back to the comp, I was after a bit more of an introduction as a Kaggle beginner, rather than a winning plan.<br>\nThis post was actually quite helpful;<br>\n<a href=\"https://blog.dataiku.com/titanic-kaggle-made-easy\">Titanic made easy - predict survival Competition</a></p>\n<p>It introduced the <a href=\"https://www.dataiku.com/dss/trynow/\">DSS (Data Science Studio)</a> tool which I've been exposed to before but is a great example that is full of really useful tips not just for a first time Kaggler but also as an introductory to the general topics and concepts of ML that you might like as a launch pad to go research specific things.</p>\n<p>One thing that you really must do first before you can attempt to do any type os ML is calculate a baseline, it is really important and isn't talked about a great deal. You'll notice that DSS gives you different performance indicators of algorithms but you need the baseline to comprehend this at all.</p>\n<p>The first thing I needed to achieve was the baseline, an essential element to any machine learning endeavour. How does one calculate a baseline? What is a baseline?<br>\nGoog can be your friend too 🙃<br>\nHonestly, you will find a well written explanation and guide in googles top results, far better then I would be able to describe here.</p>\n<h3 id=\"readingthedata\">Reading the data</h3>\n<p>All of the data was provided in csv, obviously that was never going to be workable, and we only had a small sample of data 740MB compressed, so we're not likely to be diving into big data territory here, Python 🐍 to the rescue.</p>\n<p>I loaded the csv into a Spark DataFrame and then converted that to a Pandas DataFrame for querying, my colleague introduced <a href=\"https://www.yhat.com/products/rodeo/downloads\">Rodeo</a> IDE to me recently so it seems this is a great project to give that a try.</p>\n<p>So, how much data do we have? Row count just under 60 million rows. Nice.</p>\n<p><em>python</em></p>\n<pre><code>total=0\nfor file in os.listdir(path):\n    df = pd.read_table(path+'/'+file)\n    count = len(df)\n    print(file+' '+str(count))\n    total+=count\nprint(total) // 59,450,785\n</code></pre>\n<p>The next step is to split the data into sets that will be used for model testing and training. There are a few schools of thought on this topic and in academia k-fold is overly hyped to the point it has become somewhat of a buzzword, literally a random RMIT student threw it into a conversation where we were discussing the csv file while I was grepping to see what we had &quot;have you tried k-fold to test the files?&quot;, Huh!?.. I didn't bother clarifying, I just tried to be as casual as possible directing that question about k-fold to their professor or teacher.</p>\n<p>In any case, while we are on that topic, I'll be using more of a &quot;holdout&quot; approach and am more comfortable doing my cross validation in this manner for this topic specifically. The one thing we should do before we even get to that is understand the portions of our data and what they are used for.</p>\n<h3 id=\"testversustrain\">Test versus Train</h3>\n<p>You have the target feature you are going be predicting with your model, The train dataset is a portion that have already been scored, the actual results are known and included in the dataset. Your test dataset is what you test the results of your model with to make the prediction, we still have known results but this test set of data will have all of the same features as the train set.<br>\nBeing a Kaggle competition there will be a withheld portion of data as well, so the test set should be identified and split for your submissions. So we will end up with 1 train set and 2 test sets.<br>\n<img src=\"__GHOST_URL__/content/images/2017/04/training.test.set.png\" alt=\"visualising data sets\" loading=\"lazy\"></p>\n<p>I spent several hours splitting out the data into HDFS files from the Pandas DataFrames to achieve this.</p>\n<p>Just to be clear now we know how much data we have and how we will be using it, even though I'll be storing dataset samples as HDFS, I will not be using Hadoop<br>\n<a href=\"https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html\">Don't use Hadoop - your data isn't that big</a></p>\n<p>Granted we have 60 million rows of data, maxing out the effectiveness of SQL almost immediately (not entirely true), don't think that just because it is hard in an SQL database that you have big data and you must resort to solutions like Hadoop, it's a myth, an ill-conceived concept, the choice for the less educated; and has a simple remedy called Pandas.</p>\n<p>The HDFS format is simple enough for Pandas to deal with so it's a pure convenience for me.</p>\n<h3 id=\"datathoninsightscompetition\">Datathon insights competition</h3>\n<p>Out of the 2 competitions I am far better equipped with my experience to do well on the insights portion, so I've spent a lot of time this week exploring the data in different ways.</p>\n<p>The folks from <a href=\"https://www.yellowfinbi.com/\">Yellowfin</a> sponsored the datathon and were kind enough to give us free licenses to use their platform. They have several data source connectors available but none that are possible with the HDFS files i've curated but MySQL was one that I was happy to try.</p>\n<h3 id=\"usingdocker\">Using Docker</h3>\n<p>Before doing anything you need an environment to work in. Because my computer is packed with all sorts of project I contribute to, open sourced, and the ones for my full time employment. I generally use Docker now (previously i was big on Vagrant) so here is a compose file i put together for this event;</p>\n<script src=\"https://gist.github.com/chrisdlangton/2649c267284367b53696191f09bc9735.js\"></script>\n<p>And the python container</p>\n<script src=\"https://gist.github.com/chrisdlangton/dbaa9bd4939b566b3de6f604c588ca0b.js\"></script>\n<h3 id=\"usingmysql\">Using MySQL</h3>\n<p>So I'm trying out Yellpwfin and one of their datasources is MySQL. Initially i was just using Pandas but that isn't as expressive as SQL for quick queries, so my go-to SQL database is usually MySQL for several reasons;</p>\n<ul>\n<li>It's free with good documentation, loads of users, and a great community for answers if you need.</li>\n<li>MyISAM is exceptionally fast for searching</li>\n<li>You can partition data in almost any way you can imagine</li>\n<li>It has a powerful and mature cli</li>\n<li>There are solid and mature SDKs for practically any programming language</li>\n</ul>\n<p>After a quick <code>LOAD DATA INFILE</code> sweep to get data in and running a few views of that data in Yellowfin I quickly ascertained that I'm not going to be very productive using MySQL regardless of how Yellowfin worked (which seemed to be quite well presented with a lot of neat visualisations built in such as graphs and scatter maps which was pretty cool).</p>\n<p>So although I can do some fast queries across the entire partitioned dataset with well thought-out filters, doing any sort of aggregation and grouping is really pushing the databases limits.</p>\n<h3 id=\"usingelasticsearch\">Using Elasticsearch</h3>\n<p>Bash is great for fetching and managing data as strings. Due to how much I use bash for automation and tooling it is my preferred scripting language, and although it is very powerful at string manipulation it isn't as human friendly for viewing or visualising said string results, but Elasticsearch is thanks to JSON, and I can still script everything in Bash.</p>\n<p>So Elasticsearch is generally a great tool to get some quick insight with expressive queries by just using curl, it has proven to me on some massive projects in the past to be far superior than and traditional database engine, here is some quick bash to get data from mysql</p>\n<pre><code>#!/usr/bin/env bash\nuser=&lt;u&gt;\npassword=&lt;pw&gt;\nhost=mysql\n\nes_host=es:9200\nes_index=melbdatathon2017\nes_type=transactions\n\nmax=279201\nstart=0\nend=25\nstep=25\ndocument=1\n\necho &quot;[&quot; &gt; ./errors.json\nwhile [ $start -lt $max ]\ndo\n    end=$(( end &gt; max ? max : end ))\n    mysql melbdatathon2017 -h$host -u$user -p$password -B -N -s &lt;&lt;&lt;&quot;\n    SELECT\n        cols\n    FROM\n    tables\n    LIMIT start, step;&quot; | while read -r line\n    do\n      someCol=`echo &quot;$line&quot; | cut -f1 | tr '[:upper:]' '[:lower:]'`\n      json=&quot;{/* stuff here */}&quot;\n    json=`echo &quot;${json}&quot; | tr -d '\\r\\n' | tr -s ' '`\n    es_response=`curl -s -XPOST &quot;${es_host}/${es_index}/${es_type}/?pretty&quot; -H 'Content-Type: application/json' -d &quot;${json}&quot;`\n    failed=`echo &quot;${es_response}&quot; | jq -r &quot;._shards.failed&quot;`\n    if [ &quot;${failed}&quot; -eq &quot;1&quot; ]; then\n        echo &quot;${es_response},&quot; &gt;&gt; ./errors.json\n    fi\n\n    start=`expr $end + 1`\n    end=`expr $end + $step`\n    document=`expr $document + 1`\n    done\n    echo &quot;Stored: ${document}&quot;\ndone\necho &quot;]&quot; &gt;&gt; ./errors.json\necho &quot;Done: ${document}&quot;\n</code></pre>\n<h3 id=\"datamining\">Data mining</h3>\n<p>For one of the visualisations I wanted to do, I needed the lon and lat to plot a scatter map, but all we had been provided were postcodes.</p>\n<p>I put together this short bash script to enrich my dataset;</p>\n<script src=\"https://gist.github.com/chrisdlangton/4768d0dc86d77f6638fec6ec00325111.js\"></script>\n<p>Once I had the geo data for each post code it was a simple but long running effort to script a cursor lookup by postcode and enrich my dataset.</p>\n<h3 id=\"usinginfluxdb\">Using Influxdb</h3>\n<p>Another useful way to store data for insights is by time series. Yes tools that read HDFS are good for this task, MySQL and Elasticsearch are also very capable as well, but Influxdb is <em>designed</em> to achieve this specific use case.</p>\n<blockquote>\n<p>I am very big on choosing the right tool for the job.</p>\n</blockquote>\n<p>It was a pretty trivial task to modify the script I wrote that exports from MySQL and imports to Elasticsearch to instead achieve some inserts into Influx, and once I had the data it became clear in the data I was seeing that my goals required further research.</p>\n<h3 id=\"research\">Research</h3>\n<p>One of the insights experiments I conducted involved cost over time, I wont go into specifics about what kind of cost we are talking about in terms of data but I can describe that it involved understanding the vague parameters of the <a href=\"https://www.humanservices.gov.au/health-professionals/enablers/pricing-pharmaceutical-benefits-scheme-medicine\">Pricing Pharmaceutical Benefits Scheme</a>. My conclusion here is that there was a big black hole of information around what is classified as &quot;the cost to the pharmacist&quot; for which tears apart my goals.</p>\n<h3 id=\"hackday1\">Hack day 1</h3>\n<p>Unable to attend in person I made myself available over Slack and helped out a few competitors throughout day on problems in their environment or understanding the data. This was only possible due to the awesome exposure of my Slack team;</p>\n<p><img src=\"__GHOST_URL__/content/images/2017/04/highres_460037585.jpeg\" alt=\"Melbource Developer Network Slack Team on the big monitors\" loading=\"lazy\"></p>\n<p>I was sharing scripts and running some basic graph visualisations across the entire data, this was fun and insightful to be involved in different ways of thinkings.</p>\n<p>I didn't have much time invested to conduct my own work but it was well worth it to speak and meet some great people.</p>\n<h3 id=\"nextsteps\">Next steps</h3>\n<p>I still need a great isights idea, i've worked on many small pieces of insights but I'm looking for one that will have profound benefits to someone. So I spoke to a friend whom is a Diabetes sufferer for some first hand experience and he has been a great source of inspiration.</p>\n<p>The insight competition doesn't actually need to be related to Diabetes, only the Kaggle comp is, so I remain open-minded to the possibilities.</p>\n<!--kg-card-end: markdown-->","comment_id":"40","plaintext":"This will be more of a play-by-play of highlights on things like Community, \nResearch, and talking about basics of data science from example of what I did,\nnot how or academically why 👨‍🎓.\n\nIt all started with a casual conversation that led me to the Data Science\nMelbourne [http://meetu.ps/c/1RYs6/jMKxK/a] #Datathon kickoff night Thursday\nlast week.\n\nKickoff night\nFirst thing was lining up with my laptop to copy some data.\n\nBusy yes, but also very well organised.\n\n\nOnce at the table the copy was quick and I was off with a few quick cat and head \ngreps.\n\nThe data\nUntil that moment I wasn't aware of the type of data we were getting. I learned\nthat it was medical industry related, which is both scary because I've never had\nany work in this sector, and exciting for the same reason.\n\nThe event attendants all signed NDAs so other than what has been publicly made\navailable I can't go into details.\n\nCommunity\nI am huge on collaboration and community, I think it's my inner mentor taking\nover my professional self at times. Walking around the room looking for some\nindication of how to communicate during the event 👀, asked a few people, no one\nwas aware.\nSo I met the organiser Phil Brierley. He arranged a forum (how quaint) so I\noffered Slack. Phil is a Slack user so was pretty keen for someone to take lead\non that for the event. Happy to oblige 😀\nAt the end of last year (2016) around November I set up Melbourne Developer\nNetwork\n[https://melbdev.slack.com/shared_invite/MTY4NjM1OTQxNDA4LTE0OTIxNjI4ODgtNGVjZmYzOGRiOQ] \nmainly to satisfy the need to combine and communicate with developers from all\nover the world daily while exposing them to developers from Melbourne, an\nawesome tech community.\n\nPredict the onset of diabetes\nOn my way home I made myself familiar with the Kaggle comp\n[https://inclass.kaggle.com/c/dsm2017/forums/t/31449/welcome?forumMessageId=174409#post174409]\n\nSo I ask my close buddy \"Ok Google, How to win a Kaggle comp?\";\nBlog post on: how to win a kaggle competition\n[https://www.import.io/post/how-to-win-a-kaggle-competition/]\n\nGoog knows everything 🔍 💡\n\nSo I noticed something interesting\n\n> It used to be random forest that was the big winner, but over the last six\nmonths a new algorithm called XGboost has cropped up, and it’s winning\npractically every competition in the structured data category.\n\n\nNow I'm a bit of a Logistic Regression go-to kind of guy, and known to dabble in\ndecision tree to better understand my data (well more of the overfitted\nregression tree is what you'll see unfortunately, but i try). So I was quite\nkeen to learn something new for this competition\n\nWhat is XGBoost\nSo what is this magical silver bullet XGBoost? It's clearly an implementation of\nboosted tree models by its naming, this is what i found;\nA Gentle Introduction to XGBoost for Applied Machine Learning\n[http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/] \nwhich was concise though lacked specifics, so this post\n[http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/] \non Gradient Boosting with XGBoost was perfect for my brain to consume.\n\nMy first Kaggle competition\nI digress, so back to the comp, I was after a bit more of an introduction as a\nKaggle beginner, rather than a winning plan.\nThis post was actually quite helpful;\nTitanic made easy - predict survival Competition\n[https://blog.dataiku.com/titanic-kaggle-made-easy]\n\nIt introduced the DSS (Data Science Studio)\n[https://www.dataiku.com/dss/trynow/] tool which I've been exposed to before but\nis a great example that is full of really useful tips not just for a first time\nKaggler but also as an introductory to the general topics and concepts of ML\nthat you might like as a launch pad to go research specific things.\n\nOne thing that you really must do first before you can attempt to do any type os\nML is calculate a baseline, it is really important and isn't talked about a\ngreat deal. You'll notice that DSS gives you different performance indicators of\nalgorithms but you need the baseline to comprehend this at all.\n\nThe first thing I needed to achieve was the baseline, an essential element to\nany machine learning endeavour. How does one calculate a baseline? What is a\nbaseline?\nGoog can be your friend too 🙃\nHonestly, you will find a well written explanation and guide in googles top\nresults, far better then I would be able to describe here.\n\nReading the data\nAll of the data was provided in csv, obviously that was never going to be\nworkable, and we only had a small sample of data 740MB compressed, so we're not\nlikely to be diving into big data territory here, Python 🐍 to the rescue.\n\nI loaded the csv into a Spark DataFrame and then converted that to a Pandas\nDataFrame for querying, my colleague introduced Rodeo\n[https://www.yhat.com/products/rodeo/downloads] IDE to me recently so it seems\nthis is a great project to give that a try.\n\nSo, how much data do we have? Row count just under 60 million rows. Nice.\n\npython\n\ntotal=0\nfor file in os.listdir(path):\n    df = pd.read_table(path+'/'+file)\n    count = len(df)\n    print(file+' '+str(count))\n    total+=count\nprint(total) // 59,450,785\n\n\nThe next step is to split the data into sets that will be used for model testing\nand training. There are a few schools of thought on this topic and in academia\nk-fold is overly hyped to the point it has become somewhat of a buzzword,\nliterally a random RMIT student threw it into a conversation where we were\ndiscussing the csv file while I was grepping to see what we had \"have you tried\nk-fold to test the files?\", Huh!?.. I didn't bother clarifying, I just tried to\nbe as casual as possible directing that question about k-fold to their professor\nor teacher.\n\nIn any case, while we are on that topic, I'll be using more of a \"holdout\"\napproach and am more comfortable doing my cross validation in this manner for\nthis topic specifically. The one thing we should do before we even get to that\nis understand the portions of our data and what they are used for.\n\nTest versus Train\nYou have the target feature you are going be predicting with your model, The\ntrain dataset is a portion that have already been scored, the actual results are\nknown and included in the dataset. Your test dataset is what you test the\nresults of your model with to make the prediction, we still have known results\nbut this test set of data will have all of the same features as the train set.\nBeing a Kaggle competition there will be a withheld portion of data as well, so\nthe test set should be identified and split for your submissions. So we will end\nup with 1 train set and 2 test sets.\n\n\nI spent several hours splitting out the data into HDFS files from the Pandas\nDataFrames to achieve this.\n\nJust to be clear now we know how much data we have and how we will be using it,\neven though I'll be storing dataset samples as HDFS, I will not be using Hadoop\nDon't use Hadoop - your data isn't that big\n[https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html]\n\nGranted we have 60 million rows of data, maxing out the effectiveness of SQL\nalmost immediately (not entirely true), don't think that just because it is hard\nin an SQL database that you have big data and you must resort to solutions like\nHadoop, it's a myth, an ill-conceived concept, the choice for the less educated;\nand has a simple remedy called Pandas.\n\nThe HDFS format is simple enough for Pandas to deal with so it's a pure\nconvenience for me.\n\nDatathon insights competition\nOut of the 2 competitions I am far better equipped with my experience to do well\non the insights portion, so I've spent a lot of time this week exploring the\ndata in different ways.\n\nThe folks from Yellowfin [https://www.yellowfinbi.com/] sponsored the datathon\nand were kind enough to give us free licenses to use their platform. They have\nseveral data source connectors available but none that are possible with the\nHDFS files i've curated but MySQL was one that I was happy to try.\n\nUsing Docker\nBefore doing anything you need an environment to work in. Because my computer is\npacked with all sorts of project I contribute to, open sourced, and the ones for\nmy full time employment. I generally use Docker now (previously i was big on\nVagrant) so here is a compose file i put together for this event;\n\nAnd the python container\n\nUsing MySQL\nSo I'm trying out Yellpwfin and one of their datasources is MySQL. Initially i\nwas just using Pandas but that isn't as expressive as SQL for quick queries, so\nmy go-to SQL database is usually MySQL for several reasons;\n\n * It's free with good documentation, loads of users, and a great community for\n   answers if you need.\n * MyISAM is exceptionally fast for searching\n * You can partition data in almost any way you can imagine\n * It has a powerful and mature cli\n * There are solid and mature SDKs for practically any programming language\n\nAfter a quick LOAD DATA INFILE sweep to get data in and running a few views of\nthat data in Yellowfin I quickly ascertained that I'm not going to be very\nproductive using MySQL regardless of how Yellowfin worked (which seemed to be\nquite well presented with a lot of neat visualisations built in such as graphs\nand scatter maps which was pretty cool).\n\nSo although I can do some fast queries across the entire partitioned dataset\nwith well thought-out filters, doing any sort of aggregation and grouping is\nreally pushing the databases limits.\n\nUsing Elasticsearch\nBash is great for fetching and managing data as strings. Due to how much I use\nbash for automation and tooling it is my preferred scripting language, and\nalthough it is very powerful at string manipulation it isn't as human friendly\nfor viewing or visualising said string results, but Elasticsearch is thanks to\nJSON, and I can still script everything in Bash.\n\nSo Elasticsearch is generally a great tool to get some quick insight with\nexpressive queries by just using curl, it has proven to me on some massive\nprojects in the past to be far superior than and traditional database engine,\nhere is some quick bash to get data from mysql\n\n#!/usr/bin/env bash\nuser=<u>\npassword=<pw>\nhost=mysql\n\nes_host=es:9200\nes_index=melbdatathon2017\nes_type=transactions\n\nmax=279201\nstart=0\nend=25\nstep=25\ndocument=1\n\necho \"[\" > ./errors.json\nwhile [ $start -lt $max ]\ndo\n    end=$(( end > max ? max : end ))\n    mysql melbdatathon2017 -h$host -u$user -p$password -B -N -s <<<\"\n    SELECT\n        cols\n    FROM\n    tables\n    LIMIT start, step;\" | while read -r line\n    do\n      someCol=`echo \"$line\" | cut -f1 | tr '[:upper:]' '[:lower:]'`\n      json=\"{/* stuff here */}\"\n    json=`echo \"${json}\" | tr -d '\\r\\n' | tr -s ' '`\n    es_response=`curl -s -XPOST \"${es_host}/${es_index}/${es_type}/?pretty\" -H 'Content-Type: application/json' -d \"${json}\"`\n    failed=`echo \"${es_response}\" | jq -r \"._shards.failed\"`\n    if [ \"${failed}\" -eq \"1\" ]; then\n        echo \"${es_response},\" >> ./errors.json\n    fi\n\n    start=`expr $end + 1`\n    end=`expr $end + $step`\n    document=`expr $document + 1`\n    done\n    echo \"Stored: ${document}\"\ndone\necho \"]\" >> ./errors.json\necho \"Done: ${document}\"\n\n\nData mining\nFor one of the visualisations I wanted to do, I needed the lon and lat to plot a\nscatter map, but all we had been provided were postcodes.\n\nI put together this short bash script to enrich my dataset;\n\nOnce I had the geo data for each post code it was a simple but long running\neffort to script a cursor lookup by postcode and enrich my dataset.\n\nUsing Influxdb\nAnother useful way to store data for insights is by time series. Yes tools that\nread HDFS are good for this task, MySQL and Elasticsearch are also very capable\nas well, but Influxdb is designed to achieve this specific use case.\n\n> I am very big on choosing the right tool for the job.\n\n\nIt was a pretty trivial task to modify the script I wrote that exports from\nMySQL and imports to Elasticsearch to instead achieve some inserts into Influx,\nand once I had the data it became clear in the data I was seeing that my goals\nrequired further research.\n\nResearch\nOne of the insights experiments I conducted involved cost over time, I wont go\ninto specifics about what kind of cost we are talking about in terms of data but\nI can describe that it involved understanding the vague parameters of the \nPricing Pharmaceutical Benefits Scheme\n[https://www.humanservices.gov.au/health-professionals/enablers/pricing-pharmaceutical-benefits-scheme-medicine]\n. My conclusion here is that there was a big black hole of information around\nwhat is classified as \"the cost to the pharmacist\" for which tears apart my\ngoals.\n\nHack day 1\nUnable to attend in person I made myself available over Slack and helped out a\nfew competitors throughout day on problems in their environment or understanding\nthe data. This was only possible due to the awesome exposure of my Slack team;\n\n\n\nI was sharing scripts and running some basic graph visualisations across the\nentire data, this was fun and insightful to be involved in different ways of\nthinkings.\n\nI didn't have much time invested to conduct my own work but it was well worth it\nto speak and meet some great people.\n\nNext steps\nI still need a great isights idea, i've worked on many small pieces of insights\nbut I'm looking for one that will have profound benefits to someone. So I spoke\nto a friend whom is a Diabetes sufferer for some first hand experience and he\nhas been a great source of inspiration.\n\nThe insight competition doesn't actually need to be related to Diabetes, only\nthe Kaggle comp is, so I remain open-minded to the possibilities.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/YnaFB.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-04-20 21:47:42","created_by":"1","updated_at":"2021-03-31 14:13:35","updated_by":"1","published_at":"2017-04-21 04:08:52","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca8","uuid":"8e6a5ab3-8277-4b80-ae68-3d72c1db7024","title":"Search as a Service landscape mid 2017","slug":"search-as-a-service-landscape-mid-2017","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Search as a Service is a type of SaaS for search, externally-provided search services enable you to have a full featured search engine available within your own software offerings so you can focus on your data and search UI rather than having to create _and maintain_ all of the nuances in your own search engine.\\n\\nIn addition to discovering the tools on the market and looking at the value they offer, I'll also be doing the usual product comparison but with some of the architecture-centric concerns addressed also.\\n\\n## Summary\\n\\nThis is a non-exhaustive list of search solutions I've become familiar with that offer value in 2017;\\n\\n- [Lucene](http://lucene.apache.org/)\\n - [Solr](http://lucene.apache.org/solr/)\\n - [IndexDepot](https://www.indexdepot.com/en/) (built on Solr)\\n - [Websolr](https://www.websolr.com/) (built on Solr)\\n - Elastic.co [Elasticsearch](https://www.elastic.co/products/elasticsearch)\\n - Elastic.co [Cloud](https://www.elastic.co/cloud) (formerly Found)\\n - [Azure Search](https://docs.microsoft.com/en-us/rest/api/searchservice/)\\n - AWS [Elasticsearch](https://aws.amazon.com/elasticsearch-service/details/) (built on Elastic.co Elasticsearch)\\n- [A9](https://www.a9.com/whatwedo/product-search/)\\n - AWS [CloudSearch](http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html)\\n- [Sphinx Search](http://sphinxsearch.com/services/support/)\\n - [IndexDen](http://indexden.com/)\\n - [Infegy](http://infegy.com/)\\n- [Algolia](https://www.algolia.com/)\\n- [Swiftype](https://swiftype.com/)\\n- [Sajari](https://www.sajari.com/)\\n\\nWith so much choice of software I'm going to focus on only those that are managed services, so a refined list of SaaS providers of search looks like this;\\n\\n- [IndexDepot](https://www.indexdepot.com/en/)\\n- [Websolr](https://www.websolr.com/)\\n- Elastic.co [Cloud](https://www.elastic.co/cloud) (formerly Found)\\n- [Azure Search](https://docs.microsoft.com/en-us/rest/api/searchservice/)\\n- AWS [Elasticsearch](https://aws.amazon.com/elasticsearch-service/details/)\\n- AWS [CloudSearch](http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html)\\n- [IndexDen](http://indexden.com/)\\n- [Algolia](https://www.algolia.com/)\\n- [Swiftype](https://swiftype.com/)\\n- [Sajari](https://www.sajari.com/)\\n\\nWith this list of SaaS search offerings, I'll first highlight features and do some comparisons but keep reading to get the analysis!\\n\\n## SDKs\\n\\nThe first consideration with any SaaS is the languages they support, this will be your primary way to interact with your data so we usually want to be familiar so having the language we (or our team) knows well could be a huge consideration in terms of FTE resources and how fast we can build features.\\n\\n✅ Supported\\n❌ No SDK\\n🦄 Community SDK Available\\n\\n<table class=\\\"tg\\\">\\n  <tr>\\n    <th class=\\\"tg-yw4l\\\"></th>\\n    <th class=\\\"tg-yw4l\\\">Scala</th>\\n    <th class=\\\"tg-yw4l\\\">Ruby</th>\\n    <th class=\\\"tg-baqh\\\">Clojure</th>\\n    <th class=\\\"tg-yw4l\\\">JavaScript</th>\\n    <th class=\\\"tg-yw4l\\\">Python</th>\\n    <th class=\\\"tg-yw4l\\\">Perl</th>\\n    <th class=\\\"tg-yw4l\\\">PHP</th>\\n    <th class=\\\"tg-baqh\\\">Groovy</th>\\n    <th class=\\\"tg-baqh\\\">Haskell</th>\\n    <th class=\\\"tg-baqh\\\">Erlang</th>\\n    <th class=\\\"tg-yw4l\\\">Go</th>\\n    <th class=\\\"tg-yw4l\\\">Java</th>\\n    <th class=\\\"tg-yw4l\\\">C#</th>\\n    <th class=\\\"tg-yw4l\\\">C/C++</th>\\n    <th class=\\\"tg-yw4l\\\">Android</th>\\n    <th class=\\\"tg-yw4l\\\">iOS</th>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">IndexDepot</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Websolr</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Elastic.co Cloud</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Azure Search</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">AWS Elasticsearch</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">AWS CloudSearch</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">IndexDen</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Swiftype</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Algolia</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n  </tr>\\n  <tr>\\n    <td class=\\\"tg-yw4l\\\">Sajari</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">❌</td>\\n    <td class=\\\"tg-baqh\\\">✅</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n    <td class=\\\"tg-baqh\\\">🦄</td>\\n  </tr>\\n</table>\\n\\n> IndexDepot uses plugins for services such as Heroku as I understand, pay to learn more!\\n\\n## HTTP\\n\\nWeb services come in many forms but the buzzword _REST_ gets thrown around a lot (and almost all are not actually a RESTful API), but I'll not get into that and just demonstrate the advertised function of the product. Most web services just enable you to search over your data, some allow you to _project_ results, some offer endpoints that will ingest your data, some even offer management over your service.\\n\\n<table>\\n  <tr>\\n    <th></th>\\n    <th>Web Service</th>\\n    <th>RESTful</th>\\n    <th>Secure</th>\\n    <th>Ingestion</th>\\n    <th>Management</th>\\n    <th>JSON response</th>\\n    <th>Protocol Buffer (gRPC)</th>\\n  </tr>\\n  <tr>\\n    <td>IndexDepot</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Websolr</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Elastic.co Cloud</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Azure Search</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>AWS Elasticsearch</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>AWS CloudSearch</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>IndexDen</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Swiftype</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Algolia</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Sajari</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n  </tr>\\n</table>\\n\\n#### Key points\\n- _Web Service_ simply refers to there being some sort of HTTP endpoint available at all\\n- _Secure_ encompasses the use of securing your request data, either by HTTPS (encryption) rather than exposing your request data in the URL (which cannot be protected inherently)\\n- Confused by Proto3 / Protocol Buffers / gRPC? Basically REST is dead now and gRPC is the shiny new future. [Read about it here](http://www.grpc.io/faq/)\\n- Data ingestion is also interesting, there are several ways we get data into our database so it deserves a table;\\n\\n> REST is dead, gRPC is the shiny new future\\n\\n## Data Ingestion\\n\\nGetting data into your database for searching.\\n\\n<table>\\n  <tr>\\n    <th></th>\\n    <th>HTTP</th>\\n    <th>CLI</th>\\n    <th>SDK</th>\\n    <th>File system</th>\\n    <th>Crawl</th>\\n    <th>Inter-service</th>\\n  </tr>\\n  <tr>\\n    <td>IndexDepot</td>\\n    <td>❌</td>\\n    <td>❓</td>\\n    <td>❌</td>\\n    <td>❓</td>\\n    <td>✅</td>\\n    <td>❓</td>\\n  </tr>\\n  <tr>\\n    <td>Websolr</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Elastic.co Cloud</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Azure Search</td>\\n    <td></td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n  </tr>\\n  <tr>\\n    <td>AWS Elasticsearch</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n  </tr>\\n  <tr>\\n    <td>AWS CloudSearch</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n  </tr>\\n  <tr>\\n    <td>IndexDen</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Swiftype</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❓</td>\\n    <td>✅</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Algolia</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❓</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n  </tr>\\n  <tr>\\n    <td>Sajari</td>\\n    <td>❌</td>\\n    <td>❌</td>\\n    <td>✅</td>\\n    <td>❓</td>\\n    <td>✅</td>\\n    <td>✅</td>\\n  </tr>\\n</table>\\n\\nIt's important to mention that Inter-service ingestion refers to both _migration_ from other services like MongoDB or S3, as well as other services such as Dropbox!\\nCrawlers are the same as web spiders, so you can imagine some of these crawlers would support their own SEO-esq tagging techniques and might even be suitable to _crawl_ your structured data in forms like JSON or HTML tables from generated reports.\\n\\n## Scalability\\n\\nNow we are getting into some of the lower level concerns.\\n\\nIn terms of _scalability_, I will be looking at the service architecture design specifically, I don't want to misrepresent how we define _scalability_ in terms of our own services just to be clear.\\n\\nDistributed software cannot guarantee 100% consistency, this is best described by the CAP theorem stating a distributed system cannot be Consistent, Available, and Partitioned simultaneously. Where partitions refer to tolerance, we expect our distributed systems to be FT/HA (fault tolerant and highly available) so you'll find vendors offering 99.9% up-time to accommodate the CAP theorem case.\\n\\n> Distributed system cannot be Consistent, Available, and Partitioned simultaneously\\n\\nThe reoccurring theme when looking at how a service scales is the master/slave employed by services like the ones based Solr (via Zookeeper) versus shards/replicas we see in more modern solutions like Elasticsearch. \\n\\nThere are drawbacks with both, to be effective in horizontally scaling you want to look at sharding (shared nothing architecture) which inherently distributing data across many machines, but with this comes a common issue that we see in Elasticsearch's use of shards for writing and replicas for reading [called the _split-brain_ problem](http://blog.trifork.com/2013/10/24/how-to-avoid-the-split-brain-problem-in-elasticsearch/). Elastic.co Cloud makes certain efforts to protect its users from this issue, and Google's search openly states it has avoided this scenario altogether.\\nThere is a new implementation [Google has published](https://landing.google.com/sre/book/chapters/managing-critical-state.html) on managing critical state in distributed systems, they call it _Distributed Consensus_ but it is essentially the old-new again **Paxos** or a flavour thereof. I make a point of this because _Sajari_ is the only search provider apart from Google that does this.\\n\\nMy conclusion here is to have battle tested horizontally scaling you would want to avoid anything that isn't using shards, but try to remain cautious about how replicas are promoted to _shard_ status you might encounter the split-brain caveat and that is not fun.\\nThis leaves us with the choice between Elastic.co Cloud, Google Search, Sajari, and potentially AWS managed services due to their mammoth scale and reputation. \\n\\n## Searching\\n\\nProbably the key point, so I've left it to last to really drive it home.\\nWhether you have source data that is structured or unstructured, let's assume your data has been ingested and we are ready to start searching across it. There are several considerations here and just like scalability the comparison is a bit too complex to dump in a table, so I'll dot point now and expand upon them;\\n\\n- Indexed structured search\\n- Wildcards, often called full-text search\\n- Fuzzy-search, or approximate string matching\\n- Faceted search\\n- Geo-search, using structured lat/lon data\\n- Boosting terms\\n- AI-powered...\\n\\nFuzzy-search is like an automated wildcard search, rather uses hashes which produce the same hash result when pieces are re-hashed, which is best described as automatic tokenization of term variants. Elasticsearch does an excellent job in their documentation of [dealing with human language](https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html) to describe how plurals and variants of words can be used to query the same documents. So swim, swimmer, swimming, swims, and even related terms like pool or freestyle could return you the same single document. \\n\\nJust like fuzzy search is just good old full-text searching, so is AI-powered being just a buzzword for auto-boosting terms. While boosting will effectively demote or promote results that match a given query and still return said results, AI-powered often uses Bayesian inference (or a variant or other boosting algorithm) to find terms in plain text for you based on its own training datasets and continuous learning making it superior (or eventually) to tokenization boosting techniques alone.\\n\\nElasticsearch has made incredible advances above its rivals in terms of it out-of-the-box searching sugar, I still hold it's scan-and-scroll implementation of pagination in the highest regard, but without machine learning, I predict Elasticsearch will fail to be relevant.\\nAWS has two managed search options and neither offers modern searching functionality, they focus too heavily on service availability and very little on actual functionality users expect, and I wouldn't hope for AWS to release a third search offering to bridge this gap either.\\n\\n## Conclusion\\n\\nIf you are looking for a modern feature rich search engine that not only provides a reliable service but is capable of creating its own corpus of content for you, then Swiftype or Sajari meet the mark. For my next project, i'll be choosing Sajari because it is additionally capable of ingesting content programmatically (and it is written in Go!).\\n\\nIf you're already heavily invested in the Google, Azure, AWS cloud infrastructure you're getting reliability using their services but at a cost of functionality that ultimately your own development team will be forced to build for you, because your business and users will expect a certain level of completeness now that there are solutions that have more _battle tested_ and modern features that anything less than awesome will reflect poorly on the developers ability (poor devs) and ultimately make your own new product seem immature and dated.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Search as a Service is a type of SaaS for search, externally-provided search services enable you to have a full featured search engine available within your own software offerings so you can focus on your data and search UI rather than having to create <em>and maintain</em> all of the nuances in your own search engine.</p>\n<p>In addition to discovering the tools on the market and looking at the value they offer, I'll also be doing the usual product comparison but with some of the architecture-centric concerns addressed also.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This is a non-exhaustive list of search solutions I've become familiar with that offer value in 2017;</p>\n<ul>\n<li><a href=\"http://lucene.apache.org/\">Lucene</a></li>\n<li><a href=\"http://lucene.apache.org/solr/\">Solr</a></li>\n<li><a href=\"https://www.indexdepot.com/en/\">IndexDepot</a> (built on Solr)</li>\n<li><a href=\"https://www.websolr.com/\">Websolr</a> (built on Solr)</li>\n<li>Elastic.co <a href=\"https://www.elastic.co/products/elasticsearch\">Elasticsearch</a></li>\n<li>Elastic.co <a href=\"https://www.elastic.co/cloud\">Cloud</a> (formerly Found)</li>\n<li><a href=\"https://docs.microsoft.com/en-us/rest/api/searchservice/\">Azure Search</a></li>\n<li>AWS <a href=\"https://aws.amazon.com/elasticsearch-service/details/\">Elasticsearch</a> (built on Elastic.co Elasticsearch)</li>\n<li><a href=\"https://www.a9.com/whatwedo/product-search/\">A9</a></li>\n<li>AWS <a href=\"http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html\">CloudSearch</a></li>\n<li><a href=\"http://sphinxsearch.com/services/support/\">Sphinx Search</a></li>\n<li><a href=\"http://indexden.com/\">IndexDen</a></li>\n<li><a href=\"http://infegy.com/\">Infegy</a></li>\n<li><a href=\"https://www.algolia.com/\">Algolia</a></li>\n<li><a href=\"https://swiftype.com/\">Swiftype</a></li>\n<li><a href=\"https://www.sajari.com/\">Sajari</a></li>\n</ul>\n<p>With so much choice of software I'm going to focus on only those that are managed services, so a refined list of SaaS providers of search looks like this;</p>\n<ul>\n<li><a href=\"https://www.indexdepot.com/en/\">IndexDepot</a></li>\n<li><a href=\"https://www.websolr.com/\">Websolr</a></li>\n<li>Elastic.co <a href=\"https://www.elastic.co/cloud\">Cloud</a> (formerly Found)</li>\n<li><a href=\"https://docs.microsoft.com/en-us/rest/api/searchservice/\">Azure Search</a></li>\n<li>AWS <a href=\"https://aws.amazon.com/elasticsearch-service/details/\">Elasticsearch</a></li>\n<li>AWS <a href=\"http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html\">CloudSearch</a></li>\n<li><a href=\"http://indexden.com/\">IndexDen</a></li>\n<li><a href=\"https://www.algolia.com/\">Algolia</a></li>\n<li><a href=\"https://swiftype.com/\">Swiftype</a></li>\n<li><a href=\"https://www.sajari.com/\">Sajari</a></li>\n</ul>\n<p>With this list of SaaS search offerings, I'll first highlight features and do some comparisons but keep reading to get the analysis!</p>\n<h2 id=\"sdks\">SDKs</h2>\n<p>The first consideration with any SaaS is the languages they support, this will be your primary way to interact with your data so we usually want to be familiar so having the language we (or our team) knows well could be a huge consideration in terms of FTE resources and how fast we can build features.</p>\n<p>✅ Supported<br>\n❌ No SDK<br>\n🦄 Community SDK Available</p>\n<table class=\"tg\">\n  <tr>\n    <th class=\"tg-yw4l\"></th>\n    <th class=\"tg-yw4l\">Scala</th>\n    <th class=\"tg-yw4l\">Ruby</th>\n    <th class=\"tg-baqh\">Clojure</th>\n    <th class=\"tg-yw4l\">JavaScript</th>\n    <th class=\"tg-yw4l\">Python</th>\n    <th class=\"tg-yw4l\">Perl</th>\n    <th class=\"tg-yw4l\">PHP</th>\n    <th class=\"tg-baqh\">Groovy</th>\n    <th class=\"tg-baqh\">Haskell</th>\n    <th class=\"tg-baqh\">Erlang</th>\n    <th class=\"tg-yw4l\">Go</th>\n    <th class=\"tg-yw4l\">Java</th>\n    <th class=\"tg-yw4l\">C#</th>\n    <th class=\"tg-yw4l\">C/C++</th>\n    <th class=\"tg-yw4l\">Android</th>\n    <th class=\"tg-yw4l\">iOS</th>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">IndexDepot</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Websolr</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Elastic.co Cloud</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Azure Search</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">AWS Elasticsearch</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">AWS CloudSearch</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">IndexDen</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Swiftype</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Algolia</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">✅</td>\n  </tr>\n  <tr>\n    <td class=\"tg-yw4l\">Sajari</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">❌</td>\n    <td class=\"tg-baqh\">✅</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n    <td class=\"tg-baqh\">🦄</td>\n  </tr>\n</table>\n<blockquote>\n<p>IndexDepot uses plugins for services such as Heroku as I understand, pay to learn more!</p>\n</blockquote>\n<h2 id=\"http\">HTTP</h2>\n<p>Web services come in many forms but the buzzword <em>REST</em> gets thrown around a lot (and almost all are not actually a RESTful API), but I'll not get into that and just demonstrate the advertised function of the product. Most web services just enable you to search over your data, some allow you to <em>project</em> results, some offer endpoints that will ingest your data, some even offer management over your service.</p>\n<table>\n  <tr>\n    <th></th>\n    <th>Web Service</th>\n    <th>RESTful</th>\n    <th>Secure</th>\n    <th>Ingestion</th>\n    <th>Management</th>\n    <th>JSON response</th>\n    <th>Protocol Buffer (gRPC)</th>\n  </tr>\n  <tr>\n    <td>IndexDepot</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Websolr</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Elastic.co Cloud</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Azure Search</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>AWS Elasticsearch</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>AWS CloudSearch</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>IndexDen</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Swiftype</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Algolia</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Sajari</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>✅</td>\n  </tr>\n</table>\n<h4 id=\"keypoints\">Key points</h4>\n<ul>\n<li><em>Web Service</em> simply refers to there being some sort of HTTP endpoint available at all</li>\n<li><em>Secure</em> encompasses the use of securing your request data, either by HTTPS (encryption) rather than exposing your request data in the URL (which cannot be protected inherently)</li>\n<li>Confused by Proto3 / Protocol Buffers / gRPC? Basically REST is dead now and gRPC is the shiny new future. <a href=\"http://www.grpc.io/faq/\">Read about it here</a></li>\n<li>Data ingestion is also interesting, there are several ways we get data into our database so it deserves a table;</li>\n</ul>\n<blockquote>\n<p>REST is dead, gRPC is the shiny new future</p>\n</blockquote>\n<h2 id=\"dataingestion\">Data Ingestion</h2>\n<p>Getting data into your database for searching.</p>\n<table>\n  <tr>\n    <th></th>\n    <th>HTTP</th>\n    <th>CLI</th>\n    <th>SDK</th>\n    <th>File system</th>\n    <th>Crawl</th>\n    <th>Inter-service</th>\n  </tr>\n  <tr>\n    <td>IndexDepot</td>\n    <td>❌</td>\n    <td>❓</td>\n    <td>❌</td>\n    <td>❓</td>\n    <td>✅</td>\n    <td>❓</td>\n  </tr>\n  <tr>\n    <td>Websolr</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Elastic.co Cloud</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Azure Search</td>\n    <td></td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n  </tr>\n  <tr>\n    <td>AWS Elasticsearch</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n  </tr>\n  <tr>\n    <td>AWS CloudSearch</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n  </tr>\n  <tr>\n    <td>IndexDen</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Swiftype</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❓</td>\n    <td>✅</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Algolia</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❓</td>\n    <td>❌</td>\n    <td>❌</td>\n  </tr>\n  <tr>\n    <td>Sajari</td>\n    <td>❌</td>\n    <td>❌</td>\n    <td>✅</td>\n    <td>❓</td>\n    <td>✅</td>\n    <td>✅</td>\n  </tr>\n</table>\n<p>It's important to mention that Inter-service ingestion refers to both <em>migration</em> from other services like MongoDB or S3, as well as other services such as Dropbox!<br>\nCrawlers are the same as web spiders, so you can imagine some of these crawlers would support their own SEO-esq tagging techniques and might even be suitable to <em>crawl</em> your structured data in forms like JSON or HTML tables from generated reports.</p>\n<h2 id=\"scalability\">Scalability</h2>\n<p>Now we are getting into some of the lower level concerns.</p>\n<p>In terms of <em>scalability</em>, I will be looking at the service architecture design specifically, I don't want to misrepresent how we define <em>scalability</em> in terms of our own services just to be clear.</p>\n<p>Distributed software cannot guarantee 100% consistency, this is best described by the CAP theorem stating a distributed system cannot be Consistent, Available, and Partitioned simultaneously. Where partitions refer to tolerance, we expect our distributed systems to be FT/HA (fault tolerant and highly available) so you'll find vendors offering 99.9% up-time to accommodate the CAP theorem case.</p>\n<blockquote>\n<p>Distributed system cannot be Consistent, Available, and Partitioned simultaneously</p>\n</blockquote>\n<p>The reoccurring theme when looking at how a service scales is the master/slave employed by services like the ones based Solr (via Zookeeper) versus shards/replicas we see in more modern solutions like Elasticsearch.</p>\n<p>There are drawbacks with both, to be effective in horizontally scaling you want to look at sharding (shared nothing architecture) which inherently distributing data across many machines, but with this comes a common issue that we see in Elasticsearch's use of shards for writing and replicas for reading <a href=\"http://blog.trifork.com/2013/10/24/how-to-avoid-the-split-brain-problem-in-elasticsearch/\">called the <em>split-brain</em> problem</a>. Elastic.co Cloud makes certain efforts to protect its users from this issue, and Google's search openly states it has avoided this scenario altogether.<br>\nThere is a new implementation <a href=\"https://landing.google.com/sre/book/chapters/managing-critical-state.html\">Google has published</a> on managing critical state in distributed systems, they call it <em>Distributed Consensus</em> but it is essentially the old-new again <strong>Paxos</strong> or a flavour thereof. I make a point of this because <em>Sajari</em> is the only search provider apart from Google that does this.</p>\n<p>My conclusion here is to have battle tested horizontally scaling you would want to avoid anything that isn't using shards, but try to remain cautious about how replicas are promoted to <em>shard</em> status you might encounter the split-brain caveat and that is not fun.<br>\nThis leaves us with the choice between Elastic.co Cloud, Google Search, Sajari, and potentially AWS managed services due to their mammoth scale and reputation.</p>\n<h2 id=\"searching\">Searching</h2>\n<p>Probably the key point, so I've left it to last to really drive it home.<br>\nWhether you have source data that is structured or unstructured, let's assume your data has been ingested and we are ready to start searching across it. There are several considerations here and just like scalability the comparison is a bit too complex to dump in a table, so I'll dot point now and expand upon them;</p>\n<ul>\n<li>Indexed structured search</li>\n<li>Wildcards, often called full-text search</li>\n<li>Fuzzy-search, or approximate string matching</li>\n<li>Faceted search</li>\n<li>Geo-search, using structured lat/lon data</li>\n<li>Boosting terms</li>\n<li>AI-powered...</li>\n</ul>\n<p>Fuzzy-search is like an automated wildcard search, rather uses hashes which produce the same hash result when pieces are re-hashed, which is best described as automatic tokenization of term variants. Elasticsearch does an excellent job in their documentation of <a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html\">dealing with human language</a> to describe how plurals and variants of words can be used to query the same documents. So swim, swimmer, swimming, swims, and even related terms like pool or freestyle could return you the same single document.</p>\n<p>Just like fuzzy search is just good old full-text searching, so is AI-powered being just a buzzword for auto-boosting terms. While boosting will effectively demote or promote results that match a given query and still return said results, AI-powered often uses Bayesian inference (or a variant or other boosting algorithm) to find terms in plain text for you based on its own training datasets and continuous learning making it superior (or eventually) to tokenization boosting techniques alone.</p>\n<p>Elasticsearch has made incredible advances above its rivals in terms of it out-of-the-box searching sugar, I still hold it's scan-and-scroll implementation of pagination in the highest regard, but without machine learning, I predict Elasticsearch will fail to be relevant.<br>\nAWS has two managed search options and neither offers modern searching functionality, they focus too heavily on service availability and very little on actual functionality users expect, and I wouldn't hope for AWS to release a third search offering to bridge this gap either.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>If you are looking for a modern feature rich search engine that not only provides a reliable service but is capable of creating its own corpus of content for you, then Swiftype or Sajari meet the mark. For my next project, i'll be choosing Sajari because it is additionally capable of ingesting content programmatically (and it is written in Go!).</p>\n<p>If you're already heavily invested in the Google, Azure, AWS cloud infrastructure you're getting reliability using their services but at a cost of functionality that ultimately your own development team will be forced to build for you, because your business and users will expect a certain level of completeness now that there are solutions that have more <em>battle tested</em> and modern features that anything less than awesome will reflect poorly on the developers ability (poor devs) and ultimately make your own new product seem immature and dated.</p>\n<!--kg-card-end: markdown-->","comment_id":"41","plaintext":"Search as a Service is a type of SaaS for search, externally-provided search\nservices enable you to have a full featured search engine available within your\nown software offerings so you can focus on your data and search UI rather than\nhaving to create and maintain all of the nuances in your own search engine.\n\nIn addition to discovering the tools on the market and looking at the value they\noffer, I'll also be doing the usual product comparison but with some of the\narchitecture-centric concerns addressed also.\n\nSummary\nThis is a non-exhaustive list of search solutions I've become familiar with that\noffer value in 2017;\n\n * Lucene [http://lucene.apache.org/]\n * Solr [http://lucene.apache.org/solr/]\n * IndexDepot [https://www.indexdepot.com/en/] (built on Solr)\n * Websolr [https://www.websolr.com/] (built on Solr)\n * Elastic.co Elasticsearch [https://www.elastic.co/products/elasticsearch]\n * Elastic.co Cloud [https://www.elastic.co/cloud] (formerly Found)\n * Azure Search [https://docs.microsoft.com/en-us/rest/api/searchservice/]\n * AWS Elasticsearch [https://aws.amazon.com/elasticsearch-service/details/] \n   (built on Elastic.co Elasticsearch)\n * A9 [https://www.a9.com/whatwedo/product-search/]\n * AWS CloudSearch\n   [http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html]\n * Sphinx Search [http://sphinxsearch.com/services/support/]\n * IndexDen [http://indexden.com/]\n * Infegy [http://infegy.com/]\n * Algolia [https://www.algolia.com/]\n * Swiftype [https://swiftype.com/]\n * Sajari [https://www.sajari.com/]\n\nWith so much choice of software I'm going to focus on only those that are\nmanaged services, so a refined list of SaaS providers of search looks like this;\n\n * IndexDepot [https://www.indexdepot.com/en/]\n * Websolr [https://www.websolr.com/]\n * Elastic.co Cloud [https://www.elastic.co/cloud] (formerly Found)\n * Azure Search [https://docs.microsoft.com/en-us/rest/api/searchservice/]\n * AWS Elasticsearch [https://aws.amazon.com/elasticsearch-service/details/]\n * AWS CloudSearch\n   [http://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html]\n * IndexDen [http://indexden.com/]\n * Algolia [https://www.algolia.com/]\n * Swiftype [https://swiftype.com/]\n * Sajari [https://www.sajari.com/]\n\nWith this list of SaaS search offerings, I'll first highlight features and do\nsome comparisons but keep reading to get the analysis!\n\nSDKs\nThe first consideration with any SaaS is the languages they support, this will\nbe your primary way to interact with your data so we usually want to be familiar\nso having the language we (or our team) knows well could be a huge consideration\nin terms of FTE resources and how fast we can build features.\n\n✅ Supported\n❌ No SDK\n🦄 Community SDK Available\n\nScala Ruby Clojure JavaScript Python Perl PHP Groovy Haskell Erlang Go Java C# \nC/C++ Android iOS IndexDepot ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ Websolr ❌ ✅ ❌ ✅ ✅ ❌ \n✅ ❌ ❌ ❌ ❌ ✅ ✅ ❌ ❌ ❌ Elastic.co Cloud 🦄 ✅ 🦄 ✅ ✅ ✅ ✅ ✅ 🦄 🦄 🦄 ✅ ✅ 🦄 ❌ ❌ Azure\nSearch ❌ ✅ ❌ ✅ ✅ ❌ ✅ ❌ ❌ ❌ 🦄 ✅ ✅ 🦄 ✅ ✅ AWS Elasticsearch ❌ ✅ ❌ ✅ ✅ ❌ ✅ ❌ ❌ ❌ ✅ \n✅ ✅ ✅ ✅ ✅ AWS CloudSearch ❌ ✅ ❌ ✅ ✅ ❌ ✅ ❌ ❌ ❌ ✅ ✅ ✅ ✅ ✅ ✅ IndexDen ❌ ✅ 🦄 🦄 ✅ \n🦄 ✅ ❌ ❌ 🦄 ❌ ✅ 🦄 ❌ ❌ ❌ Swiftype ❌ ✅ ❌ ✅ ✅ ❌ 🦄 ❌ ❌ ❌ ❌ ✅ ❌ ❌ ✅ ✅ Algolia ✅ ✅ ❌ \n✅ ✅ ❌ ✅ ❌ ❌ ❌ ✅ ✅ ✅ ❌ ✅ ✅ Sajari 🦄 🦄 ❌ ✅ 🦄 ❌ ✅ ❌ ❌ ❌ ✅ 🦄 🦄 🦄 🦄 🦄 > IndexDepot uses plugins for services such as Heroku as I understand, pay to\nlearn more!\n\n\nHTTP\nWeb services come in many forms but the buzzword REST gets thrown around a lot\n(and almost all are not actually a RESTful API), but I'll not get into that and\njust demonstrate the advertised function of the product. Most web services just\nenable you to search over your data, some allow you to project results, some\noffer endpoints that will ingest your data, some even offer management over your\nservice.\n\nWeb Service RESTful Secure Ingestion Management JSON response Protocol Buffer\n(gRPC) IndexDepot ❌ ❌ ❌ ❌ ❌ ❌ ❌ Websolr ✅ ❌ ❌ ❌ ❌ ✅ ❌ Elastic.co Cloud ✅ ✅ ✅ ✅ ✅ \n✅ ❌ Azure Search ❌ ❌ ❌ ❌ ❌ ❌ ❌ AWS Elasticsearch ✅ ✅ ✅ ✅ ✅ ✅ ❌ AWS CloudSearch ✅ \n❌ ❌ ✅ ❌ ✅ ❌ IndexDen ✅ ✅ ✅ ✅ ✅ ✅ ❌ Swiftype ✅ ✅ ✅ ❌ ❌ ✅ ❌ Algolia ✅ ✅ ✅ ✅ ✅ ✅ ❌ \nSajari ✅ ❌ ✅ ❌ ❌ ✅ ✅ Key points\n * Web Service simply refers to there being some sort of HTTP endpoint available\n   at all\n * Secure encompasses the use of securing your request data, either by HTTPS\n   (encryption) rather than exposing your request data in the URL (which cannot\n   be protected inherently)\n * Confused by Proto3 / Protocol Buffers / gRPC? Basically REST is dead now and\n   gRPC is the shiny new future. Read about it here [http://www.grpc.io/faq/]\n * Data ingestion is also interesting, there are several ways we get data into\n   our database so it deserves a table;\n\n> REST is dead, gRPC is the shiny new future\n\n\nData Ingestion\nGetting data into your database for searching.\n\nHTTP CLI SDK File system Crawl Inter-service IndexDepot ❌ ❓ ❌ ❓ ✅ ❓ Websolr ❌ ❌ \n✅ ✅ ❌ ❌ Elastic.co Cloud ✅ ❌ ✅ ❌ ❌ ❌ Azure Search ❌ ✅ ❌ ❌ ✅ AWS Elasticsearch ✅ \n❌ ✅ ❌ ❌ ✅ AWS CloudSearch ✅ ✅ ✅ ❌ ❌ ✅ IndexDen ✅ ❌ ✅ ❌ ❌ ❌ Swiftype ❌ ❌ ✅ ❓ ✅ ❌ \nAlgolia ❌ ❌ ✅ ❓ ❌ ❌ Sajari ❌ ❌ ✅ ❓ ✅ ✅ It's important to mention that\nInter-service ingestion refers to both migration from other services like\nMongoDB or S3, as well as other services such as Dropbox!\nCrawlers are the same as web spiders, so you can imagine some of these crawlers\nwould support their own SEO-esq tagging techniques and might even be suitable to \ncrawl your structured data in forms like JSON or HTML tables from generated\nreports.\n\nScalability\nNow we are getting into some of the lower level concerns.\n\nIn terms of scalability, I will be looking at the service architecture design\nspecifically, I don't want to misrepresent how we define scalability in terms of\nour own services just to be clear.\n\nDistributed software cannot guarantee 100% consistency, this is best described\nby the CAP theorem stating a distributed system cannot be Consistent, Available,\nand Partitioned simultaneously. Where partitions refer to tolerance, we expect\nour distributed systems to be FT/HA (fault tolerant and highly available) so\nyou'll find vendors offering 99.9% up-time to accommodate the CAP theorem case.\n\n> Distributed system cannot be Consistent, Available, and Partitioned\nsimultaneously\n\n\nThe reoccurring theme when looking at how a service scales is the master/slave\nemployed by services like the ones based Solr (via Zookeeper) versus\nshards/replicas we see in more modern solutions like Elasticsearch.\n\nThere are drawbacks with both, to be effective in horizontally scaling you want\nto look at sharding (shared nothing architecture) which inherently distributing\ndata across many machines, but with this comes a common issue that we see in\nElasticsearch's use of shards for writing and replicas for reading called the\nsplit-brain problem\n[http://blog.trifork.com/2013/10/24/how-to-avoid-the-split-brain-problem-in-elasticsearch/]\n. Elastic.co Cloud makes certain efforts to protect its users from this issue,\nand Google's search openly states it has avoided this scenario altogether.\nThere is a new implementation Google has published\n[https://landing.google.com/sre/book/chapters/managing-critical-state.html] on\nmanaging critical state in distributed systems, they call it Distributed\nConsensus but it is essentially the old-new again Paxos or a flavour thereof. I\nmake a point of this because Sajari is the only search provider apart from\nGoogle that does this.\n\nMy conclusion here is to have battle tested horizontally scaling you would want\nto avoid anything that isn't using shards, but try to remain cautious about how\nreplicas are promoted to shard status you might encounter the split-brain caveat\nand that is not fun.\nThis leaves us with the choice between Elastic.co Cloud, Google Search, Sajari,\nand potentially AWS managed services due to their mammoth scale and reputation.\n\nSearching\nProbably the key point, so I've left it to last to really drive it home.\nWhether you have source data that is structured or unstructured, let's assume\nyour data has been ingested and we are ready to start searching across it. There\nare several considerations here and just like scalability the comparison is a\nbit too complex to dump in a table, so I'll dot point now and expand upon them;\n\n * Indexed structured search\n * Wildcards, often called full-text search\n * Fuzzy-search, or approximate string matching\n * Faceted search\n * Geo-search, using structured lat/lon data\n * Boosting terms\n * AI-powered...\n\nFuzzy-search is like an automated wildcard search, rather uses hashes which\nproduce the same hash result when pieces are re-hashed, which is best described\nas automatic tokenization of term variants. Elasticsearch does an excellent job\nin their documentation of dealing with human language\n[https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html] to\ndescribe how plurals and variants of words can be used to query the same\ndocuments. So swim, swimmer, swimming, swims, and even related terms like pool\nor freestyle could return you the same single document.\n\nJust like fuzzy search is just good old full-text searching, so is AI-powered\nbeing just a buzzword for auto-boosting terms. While boosting will effectively\ndemote or promote results that match a given query and still return said\nresults, AI-powered often uses Bayesian inference (or a variant or other\nboosting algorithm) to find terms in plain text for you based on its own\ntraining datasets and continuous learning making it superior (or eventually) to\ntokenization boosting techniques alone.\n\nElasticsearch has made incredible advances above its rivals in terms of it\nout-of-the-box searching sugar, I still hold it's scan-and-scroll implementation\nof pagination in the highest regard, but without machine learning, I predict\nElasticsearch will fail to be relevant.\nAWS has two managed search options and neither offers modern searching\nfunctionality, they focus too heavily on service availability and very little on\nactual functionality users expect, and I wouldn't hope for AWS to release a\nthird search offering to bridge this gap either.\n\nConclusion\nIf you are looking for a modern feature rich search engine that not only\nprovides a reliable service but is capable of creating its own corpus of content\nfor you, then Swiftype or Sajari meet the mark. For my next project, i'll be\nchoosing Sajari because it is additionally capable of ingesting content\nprogrammatically (and it is written in Go!).\n\nIf you're already heavily invested in the Google, Azure, AWS cloud\ninfrastructure you're getting reliability using their services but at a cost of\nfunctionality that ultimately your own development team will be forced to build\nfor you, because your business and users will expect a certain level of\ncompleteness now that there are solutions that have more battle tested and\nmodern features that anything less than awesome will reflect poorly on the\ndevelopers ability (poor devs) and ultimately make your own new product seem\nimmature and dated.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/azure-search-image.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-06-13 22:53:26","created_by":"1","updated_at":"2021-03-31 14:13:25","updated_by":"1","published_at":"2017-06-13 11:24:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbca9","uuid":"c7b819da-b52c-4efa-b967-db1dbe1e9fa5","title":"Only inexperienced developers use infinite loops","slug":"only-inexperienced-developers-use-infinite-loops","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A language agnostic look at this common programming pattern. My hope is that once you've read my post you'll never write an infinite loop again.\\n\\n> My favourite number: 8\\n\\n## What are infinite loops\\n\\nI've chosen Python as the language to demonstrate due to its readability.\\n\\n#### 1. Loops\\n\\nThe simplest loop is the while loop with a condition that doesn't change, to demonstrate;\\n\\n```python\\nwhile True:\\n  \\\"\\\"\\\"do code things\\\"\\\"\\\"\\n  pass\\n```\\n\\nIn C or C-like languages you may see many syntax variants like `for (;;) {}` and `do {} while` but if they're infinite there's no notable difference.\\n\\nThe issue with such infinite loops is we are trained to see these as a pattern and not a bug, and few developers are aware of any caveats or only know of a few. All of the associated caveats I'll soon explain are actually software defects, and the way to solve the defect is to refactor away from the infinite loop.\\n\\n> Infinite loops are software defects\\n\\n#### 2. Iterators\\n\\nThe next common technique is mutating an iterable, like so;\\n\\n```python\\nl = [1]\\nfor x in l:\\n    l.append(x + 1)\\n```\\n\\nThe iterable being iterated inevitably grows in each iterations preventing the program to complete. The obvious problem with this is the iterable memory space will grow and may even exceed memory constraints causing a panic or exception. So when this technique is observed we often associate it with being a bug immediately, appropriately.\\n\\n#### 3. Generators\\n\\nA new technique has come about with the growing popularity of generators. \\n\\n```python\\ndef infinity():\\n    while True:\\n        yield\\nfor _ in infinity():\\n    pass\\n```\\n\\nThis is particularly dangerous, we turn to generators due to their efficiency and often they are amazing for this. But when we turn that into an infinite loop it becomes the source of the worst of both examples 1 and 2 above.\\n\\n> Generators as loops leak memory and are defects\\n\\n## Use cases that work\\n\\nSo why do we use a _infinite loop_ in any language? What benefits do we get using this and what caveats does this pattern bring to our programs?\\n\\nThere are only 2 cases where an infinite loop might _arguably_ make sense.\\n\\n#### 1. Limited scope\\n\\nIf the entire scope of the program is to operate the loop, and only the loop itself, then there is reason to argue that the program can do that well. But what a silly program that would be.\\n\\nThere are successful programs that have used infinite loops. Such as the Node.js event loop, and a game engine (like Phaser.io), but developers shouldn't choose to write an infinite loop unless we are ready to accept the responsibility and consequences.\\n\\n> \\\"Infinite loops\\\" with great power comes great responsibility\\n\\nMozilla has created circusd to fill the place of the now dead supervisord. Its purpose is to keep your program alive or give you finer control of a long-running workload often implemented with an infinite loop. With such programs available, developers don't need to write their own infinite loops anymore.\\n\\n#### 2. Advanced Programming Languages\\n\\nRust to my knowledge (and I don't know everything) is the only memory safe programming language available to mere mortals.\\n\\nIn a language that gives us control over memory management (like  Rust) and the ability to manage fully any external file descriptors, we may avoid the caveats associated with infinity loops.\\n\\n> Rust; memory safe programming for mere mortals\\n\\n## Caveats\\n\\nThese are common use cases for developers to resort to infinity loops and the associated software defects they introduce. \\n\\n#### Daemons\\n\\nWhenever we are given a requirement that leads us to use a daemon pattern in our programs we immediately and amateurishly turn to infinity loops. \\nAt the risk of stating the obvious to those more experienced, a _daemon_ is not actually a pattern, it isn't even intended as a noun, it's an adjective to describe a process that has no ability to interact with or be interacted with its executor. In no way is a daemon directly associated with long-running processes, so why do many developers make this immediate association? If you fall into this misconception you're just hacking together some code example you stumbled across probably written by someone equally or less knowledgeable as yourself in what they're doing.\\n\\n> The misconception that infinity loops are daemons\\n\\nIf you have a use case for a program to be a _daemon_ you would need to first defer that program execution to a child process which becomes its own session leader allowing the original process to exit without killing the child containing the program you want to continue executing, thus the program is now detached from its executor and is, therefore, a _daemon_. Notice there is no mention of infinite loops?\\n\\nIf you don't understand process, session, and leaders yet [start here](http://www.win.tue.nl/~aeb/linux/lk/lk-10.html#ss10.3)\\n\\n> To win software development; understand processes\\n\\n#### Many termination conditions\\n\\nWhat is a termination condition? Here are a few from various languages;\\n\\n- return\\n- exit\\n- die\\n- break \\n- continue\\n- goto\\n\\nWith so many exit statements available it is often that we change a conditional loop to an infinite loop for readability or simplicity sake, which is quite possibly the most ill-conceived approach. Try to convince me that it is easier to search a block of code for any of the listed exit statements over looking at the single loop condition and I'll happily eat my words, literally, I'll print the whole post and eat it for your entertainment.\\n\\nUnderstand your exit condition/s, then depending on that analysis you can produce a _single conditional_ to track and use for your loop condition so it is no longer an infinite loop. If that sounds challenging here is an example;\\n\\n```python\\ncondition = None\\nbreak_condition = [42, 67, 12]\\nwhile condition not in break_condition:\\n    \\\"\\\"\\\"Run some task and assign a value to represent that outcome\\\"\\\"\\\"\\n    condition = random.randint(0, 100)\\n    \\\"\\\"\\\"As long as the outcomes are appropriate to continue the loop\\\"\\\"\\\"\\n    pass\\n```\\n\\nNow all you need to do is track one variable for all of your complex exit statements while avoiding using infinite loops!\\n\\n> Understand your exit conditions for clean code\\n\\n## Defects introduced by infinite loops\\n\\nEarlier I promised to explain that infinite loop caveats are actually software defects, which shouldn't have ever been introduced.\\n\\n#### Memory leaks\\n\\nMany reported bugs that get traced back to a memory leak are often addressed by optimising code to work more efficiently with the languages GC. \\nIf this occurs in an infinite loop you've not fixed the defect but rather resolved the bug report symptom only. For the memory leak to accumulate data over time the root cause is the infinite loop, you might of today found one leak and patched it, good for you, but the root cause (the infinite loop) remained and the defect prevails to strike again when new code is introduced by an unsuspecting junior developer who inevitably will get blamed. \\n\\n> A junior developer's regression bug is your fault\\n\\nIf you consider yourself a senior developer and you find a memory leak within an infinite loop it is your responsibility to refactor to remove the infinite loop, you don't need it, or that junior developer's regression bug is your fault.\\n\\n#### Incomplete execution\\n\\nWhen the executing program is killed or forced to terminate by an external source (like a load balancer or a terminal session being interrupted) your infinite loop and all of the logic inside is immediately abandoned. \\nYou may reach this conclusion through debugging a number of bug reports, and when you reach this conclusion hopefully you learn that it can be managed using [signals](http://man7.org/linux/man-pages/man7/signal.7.html). \\nI've seen this happen in almost every using containers like Docker, but it has nothing to do with containers or Docker, it's only come into the view of developers since ops engineers are becoming less concerned with program development.\\nIf signals are not already managed and you're program is not designed or effective when terminated at any stage, you should implement signal handling immediately. It's uncommon for APIs or web server scripts to need this so coming from a web developer background you'd be excused for not knowing about signals, but if you're not from a web background shame on you.\\n\\n> Implement signal handling immediately, shame on you\\n\\nIn an infinite loop, signals become more difficult to manage and I often see a loop short-circuit condition only at the top of the loop, and only handles one of the signals (usually Interrupt or Terminate).\\nAlthough this common and most developers wouldn't see the problem with it, here is a quick question you should consider; how do you handle unknown state?\\n\\nBy not handling all signals, or not responding to the signal promptly, the program enters an unknown state which is not good, to say the least.\\nYou may be addressing another symptom but again not the root cause. Unless you react to all signals promptly, not just at the top of your infinite loop, you create a different race condition to the one you tried to solve _and_ introduce an unknown state condition that didn't exist before you _fixed_ the bug that led you to this.\\n\\nExperienced developers either found themselves doing this and learned not to do it ever, or were thorough enough to learn about signals, daemons, and infinite loops in all of their hellishness before falling for their allures.\\n\\n#### Inconsistent state\\n\\nWe just explored _unknown state_, where the program received a signal and suppressed that signal, an inconsistent state is not that. \\n\\nAn inconsistent state is part of your program logic, it's where you have an expectation of the program state to be one thing but you've been given bug reports that somehow it has entered something else entirely. The most common inconsistent state defect with infinity loops is where the program was not able to complete all logic within its loop. \\n\\nInconsistencies may be related to data in your database/s, files (local or remote), or even termination of some remote resource.\\n\\n> Ops engineers can help with causes for inconsistent state\\n\\nThis may sound similar to the problem faced with signals, but the root cause of inconsistent state is not signals. \\n\\nIf you've had an experienced ops engineer lead you to solve this symptom by looking into connection timeouts or locked files you'd normally resolve the bug reports by working around a connection timeout - reconnecting or something similar, again addressing a symptom without understanding it.\\n\\n[File descriptors](https://en.wikipedia.org/wiki/File_descriptor) are the things that a program obtains from the system to open files and remote connections, but due to infinite loops being long-running processes the program expects a file or remote connection to be in a certain state when it reuses the same resource it had opened earlier. But as you'll learn from your debugging, things change, get locked or deleted, and connections timeout. Simply reconnecting doesn't _stop_ the problem happening because things like database connections and file handling are done via a library or some other abstraction it is often impossible for programs to manage their own file descriptors for these resources, leaving you with only an option to reconnect or some other useless abstraction that doesn't actually prevent the problem occurring only react to the symptoms.\\n\\n> Learn about file descriptors to achieve greatness\\n\\nNow you've learned about file descriptors you should resolve your bug at the root cause, refactor your code to remove the infinite loop so your program never enters an inconsistent state where it would ever have any stale file descriptor references in memory. \\n\\n## Refactor your loop\\n\\nHow to refactor away from infinite loops? \\n\\nAbove I gave an example of using a single conditional with multiple exit statements, this method takes all of the exit conditions within the loop and replaces them with a variable that takes a unique value corresponding to that exit condition. You can then turn your infinite loop into a loop with one conditional, therefore one place to track your exit conditions. \\nYou'll find that now you have an easy way to reason with your loop and it's exit conditions, you'll start identifying previously hard to locate exit conditions in your code. What you previously had was a pseudo-daemon with a big defect commonly called an infinite loop, and now you have a program you can reason with.\\n\\nAnother way to refactor might be using an application like [circusd](https://github.com/circus-tent/circus) to take the place of the infinite loop altogether making it redundant to have one in your code at all. All your program will need to do is execute the program itself (the code you had in your loop).\\n\\nYou'll find that something like circusd will also make many (or all) your previous infinite loop exit condition redundant in your code, as it provides you ways to run your program, recover or keep it running, delay start, delay executions, and many many more programming complexities so you can just write your program and forget you ever heard the word _daemon_.\\n\\n> Mozilla's circus saves lives\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>A language agnostic look at this common programming pattern. My hope is that once you've read my post you'll never write an infinite loop again.</p>\n<blockquote>\n<p>My favourite number: 8</p>\n</blockquote>\n<h2 id=\"whatareinfiniteloops\">What are infinite loops</h2>\n<p>I've chosen Python as the language to demonstrate due to its readability.</p>\n<h4 id=\"1loops\">1. Loops</h4>\n<p>The simplest loop is the while loop with a condition that doesn't change, to demonstrate;</p>\n<pre><code class=\"language-python\">while True:\n  &quot;&quot;&quot;do code things&quot;&quot;&quot;\n  pass\n</code></pre>\n<p>In C or C-like languages you may see many syntax variants like <code>for (;;) {}</code> and <code>do {} while</code> but if they're infinite there's no notable difference.</p>\n<p>The issue with such infinite loops is we are trained to see these as a pattern and not a bug, and few developers are aware of any caveats or only know of a few. All of the associated caveats I'll soon explain are actually software defects, and the way to solve the defect is to refactor away from the infinite loop.</p>\n<blockquote>\n<p>Infinite loops are software defects</p>\n</blockquote>\n<h4 id=\"2iterators\">2. Iterators</h4>\n<p>The next common technique is mutating an iterable, like so;</p>\n<pre><code class=\"language-python\">l = [1]\nfor x in l:\n    l.append(x + 1)\n</code></pre>\n<p>The iterable being iterated inevitably grows in each iterations preventing the program to complete. The obvious problem with this is the iterable memory space will grow and may even exceed memory constraints causing a panic or exception. So when this technique is observed we often associate it with being a bug immediately, appropriately.</p>\n<h4 id=\"3generators\">3. Generators</h4>\n<p>A new technique has come about with the growing popularity of generators.</p>\n<pre><code class=\"language-python\">def infinity():\n    while True:\n        yield\nfor _ in infinity():\n    pass\n</code></pre>\n<p>This is particularly dangerous, we turn to generators due to their efficiency and often they are amazing for this. But when we turn that into an infinite loop it becomes the source of the worst of both examples 1 and 2 above.</p>\n<blockquote>\n<p>Generators as loops leak memory and are defects</p>\n</blockquote>\n<h2 id=\"usecasesthatwork\">Use cases that work</h2>\n<p>So why do we use a <em>infinite loop</em> in any language? What benefits do we get using this and what caveats does this pattern bring to our programs?</p>\n<p>There are only 2 cases where an infinite loop might <em>arguably</em> make sense.</p>\n<h4 id=\"1limitedscope\">1. Limited scope</h4>\n<p>If the entire scope of the program is to operate the loop, and only the loop itself, then there is reason to argue that the program can do that well. But what a silly program that would be.</p>\n<p>There are successful programs that have used infinite loops. Such as the Node.js event loop, and a game engine (like Phaser.io), but developers shouldn't choose to write an infinite loop unless we are ready to accept the responsibility and consequences.</p>\n<blockquote>\n<p>&quot;Infinite loops&quot; with great power comes great responsibility</p>\n</blockquote>\n<p>Mozilla has created circusd to fill the place of the now dead supervisord. Its purpose is to keep your program alive or give you finer control of a long-running workload often implemented with an infinite loop. With such programs available, developers don't need to write their own infinite loops anymore.</p>\n<h4 id=\"2advancedprogramminglanguages\">2. Advanced Programming Languages</h4>\n<p>Rust to my knowledge (and I don't know everything) is the only memory safe programming language available to mere mortals.</p>\n<p>In a language that gives us control over memory management (like  Rust) and the ability to manage fully any external file descriptors, we may avoid the caveats associated with infinity loops.</p>\n<blockquote>\n<p>Rust; memory safe programming for mere mortals</p>\n</blockquote>\n<h2 id=\"caveats\">Caveats</h2>\n<p>These are common use cases for developers to resort to infinity loops and the associated software defects they introduce.</p>\n<h4 id=\"daemons\">Daemons</h4>\n<p>Whenever we are given a requirement that leads us to use a daemon pattern in our programs we immediately and amateurishly turn to infinity loops.<br>\nAt the risk of stating the obvious to those more experienced, a <em>daemon</em> is not actually a pattern, it isn't even intended as a noun, it's an adjective to describe a process that has no ability to interact with or be interacted with its executor. In no way is a daemon directly associated with long-running processes, so why do many developers make this immediate association? If you fall into this misconception you're just hacking together some code example you stumbled across probably written by someone equally or less knowledgeable as yourself in what they're doing.</p>\n<blockquote>\n<p>The misconception that infinity loops are daemons</p>\n</blockquote>\n<p>If you have a use case for a program to be a <em>daemon</em> you would need to first defer that program execution to a child process which becomes its own session leader allowing the original process to exit without killing the child containing the program you want to continue executing, thus the program is now detached from its executor and is, therefore, a <em>daemon</em>. Notice there is no mention of infinite loops?</p>\n<p>If you don't understand process, session, and leaders yet <a href=\"http://www.win.tue.nl/~aeb/linux/lk/lk-10.html#ss10.3\">start here</a></p>\n<blockquote>\n<p>To win software development; understand processes</p>\n</blockquote>\n<h4 id=\"manyterminationconditions\">Many termination conditions</h4>\n<p>What is a termination condition? Here are a few from various languages;</p>\n<ul>\n<li>return</li>\n<li>exit</li>\n<li>die</li>\n<li>break</li>\n<li>continue</li>\n<li>goto</li>\n</ul>\n<p>With so many exit statements available it is often that we change a conditional loop to an infinite loop for readability or simplicity sake, which is quite possibly the most ill-conceived approach. Try to convince me that it is easier to search a block of code for any of the listed exit statements over looking at the single loop condition and I'll happily eat my words, literally, I'll print the whole post and eat it for your entertainment.</p>\n<p>Understand your exit condition/s, then depending on that analysis you can produce a <em>single conditional</em> to track and use for your loop condition so it is no longer an infinite loop. If that sounds challenging here is an example;</p>\n<pre><code class=\"language-python\">condition = None\nbreak_condition = [42, 67, 12]\nwhile condition not in break_condition:\n    &quot;&quot;&quot;Run some task and assign a value to represent that outcome&quot;&quot;&quot;\n    condition = random.randint(0, 100)\n    &quot;&quot;&quot;As long as the outcomes are appropriate to continue the loop&quot;&quot;&quot;\n    pass\n</code></pre>\n<p>Now all you need to do is track one variable for all of your complex exit statements while avoiding using infinite loops!</p>\n<blockquote>\n<p>Understand your exit conditions for clean code</p>\n</blockquote>\n<h2 id=\"defectsintroducedbyinfiniteloops\">Defects introduced by infinite loops</h2>\n<p>Earlier I promised to explain that infinite loop caveats are actually software defects, which shouldn't have ever been introduced.</p>\n<h4 id=\"memoryleaks\">Memory leaks</h4>\n<p>Many reported bugs that get traced back to a memory leak are often addressed by optimising code to work more efficiently with the languages GC.<br>\nIf this occurs in an infinite loop you've not fixed the defect but rather resolved the bug report symptom only. For the memory leak to accumulate data over time the root cause is the infinite loop, you might of today found one leak and patched it, good for you, but the root cause (the infinite loop) remained and the defect prevails to strike again when new code is introduced by an unsuspecting junior developer who inevitably will get blamed.</p>\n<blockquote>\n<p>A junior developer's regression bug is your fault</p>\n</blockquote>\n<p>If you consider yourself a senior developer and you find a memory leak within an infinite loop it is your responsibility to refactor to remove the infinite loop, you don't need it, or that junior developer's regression bug is your fault.</p>\n<h4 id=\"incompleteexecution\">Incomplete execution</h4>\n<p>When the executing program is killed or forced to terminate by an external source (like a load balancer or a terminal session being interrupted) your infinite loop and all of the logic inside is immediately abandoned.<br>\nYou may reach this conclusion through debugging a number of bug reports, and when you reach this conclusion hopefully you learn that it can be managed using <a href=\"http://man7.org/linux/man-pages/man7/signal.7.html\">signals</a>.<br>\nI've seen this happen in almost every using containers like Docker, but it has nothing to do with containers or Docker, it's only come into the view of developers since ops engineers are becoming less concerned with program development.<br>\nIf signals are not already managed and you're program is not designed or effective when terminated at any stage, you should implement signal handling immediately. It's uncommon for APIs or web server scripts to need this so coming from a web developer background you'd be excused for not knowing about signals, but if you're not from a web background shame on you.</p>\n<blockquote>\n<p>Implement signal handling immediately, shame on you</p>\n</blockquote>\n<p>In an infinite loop, signals become more difficult to manage and I often see a loop short-circuit condition only at the top of the loop, and only handles one of the signals (usually Interrupt or Terminate).<br>\nAlthough this common and most developers wouldn't see the problem with it, here is a quick question you should consider; how do you handle unknown state?</p>\n<p>By not handling all signals, or not responding to the signal promptly, the program enters an unknown state which is not good, to say the least.<br>\nYou may be addressing another symptom but again not the root cause. Unless you react to all signals promptly, not just at the top of your infinite loop, you create a different race condition to the one you tried to solve <em>and</em> introduce an unknown state condition that didn't exist before you <em>fixed</em> the bug that led you to this.</p>\n<p>Experienced developers either found themselves doing this and learned not to do it ever, or were thorough enough to learn about signals, daemons, and infinite loops in all of their hellishness before falling for their allures.</p>\n<h4 id=\"inconsistentstate\">Inconsistent state</h4>\n<p>We just explored <em>unknown state</em>, where the program received a signal and suppressed that signal, an inconsistent state is not that.</p>\n<p>An inconsistent state is part of your program logic, it's where you have an expectation of the program state to be one thing but you've been given bug reports that somehow it has entered something else entirely. The most common inconsistent state defect with infinity loops is where the program was not able to complete all logic within its loop.</p>\n<p>Inconsistencies may be related to data in your database/s, files (local or remote), or even termination of some remote resource.</p>\n<blockquote>\n<p>Ops engineers can help with causes for inconsistent state</p>\n</blockquote>\n<p>This may sound similar to the problem faced with signals, but the root cause of inconsistent state is not signals.</p>\n<p>If you've had an experienced ops engineer lead you to solve this symptom by looking into connection timeouts or locked files you'd normally resolve the bug reports by working around a connection timeout - reconnecting or something similar, again addressing a symptom without understanding it.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/File_descriptor\">File descriptors</a> are the things that a program obtains from the system to open files and remote connections, but due to infinite loops being long-running processes the program expects a file or remote connection to be in a certain state when it reuses the same resource it had opened earlier. But as you'll learn from your debugging, things change, get locked or deleted, and connections timeout. Simply reconnecting doesn't <em>stop</em> the problem happening because things like database connections and file handling are done via a library or some other abstraction it is often impossible for programs to manage their own file descriptors for these resources, leaving you with only an option to reconnect or some other useless abstraction that doesn't actually prevent the problem occurring only react to the symptoms.</p>\n<blockquote>\n<p>Learn about file descriptors to achieve greatness</p>\n</blockquote>\n<p>Now you've learned about file descriptors you should resolve your bug at the root cause, refactor your code to remove the infinite loop so your program never enters an inconsistent state where it would ever have any stale file descriptor references in memory.</p>\n<h2 id=\"refactoryourloop\">Refactor your loop</h2>\n<p>How to refactor away from infinite loops?</p>\n<p>Above I gave an example of using a single conditional with multiple exit statements, this method takes all of the exit conditions within the loop and replaces them with a variable that takes a unique value corresponding to that exit condition. You can then turn your infinite loop into a loop with one conditional, therefore one place to track your exit conditions.<br>\nYou'll find that now you have an easy way to reason with your loop and it's exit conditions, you'll start identifying previously hard to locate exit conditions in your code. What you previously had was a pseudo-daemon with a big defect commonly called an infinite loop, and now you have a program you can reason with.</p>\n<p>Another way to refactor might be using an application like <a href=\"https://github.com/circus-tent/circus\">circusd</a> to take the place of the infinite loop altogether making it redundant to have one in your code at all. All your program will need to do is execute the program itself (the code you had in your loop).</p>\n<p>You'll find that something like circusd will also make many (or all) your previous infinite loop exit condition redundant in your code, as it provides you ways to run your program, recover or keep it running, delay start, delay executions, and many many more programming complexities so you can just write your program and forget you ever heard the word <em>daemon</em>.</p>\n<blockquote>\n<p>Mozilla's circus saves lives</p>\n</blockquote>\n<!--kg-card-end: markdown-->","comment_id":"42","plaintext":"A language agnostic look at this common programming pattern. My hope is that\nonce you've read my post you'll never write an infinite loop again.\n\n> My favourite number: 8\n\n\nWhat are infinite loops\nI've chosen Python as the language to demonstrate due to its readability.\n\n1. Loops\nThe simplest loop is the while loop with a condition that doesn't change, to\ndemonstrate;\n\nwhile True:\n  \"\"\"do code things\"\"\"\n  pass\n\n\nIn C or C-like languages you may see many syntax variants like for (;;) {} and \ndo {} while but if they're infinite there's no notable difference.\n\nThe issue with such infinite loops is we are trained to see these as a pattern\nand not a bug, and few developers are aware of any caveats or only know of a\nfew. All of the associated caveats I'll soon explain are actually software\ndefects, and the way to solve the defect is to refactor away from the infinite\nloop.\n\n> Infinite loops are software defects\n\n\n2. Iterators\nThe next common technique is mutating an iterable, like so;\n\nl = [1]\nfor x in l:\n    l.append(x + 1)\n\n\nThe iterable being iterated inevitably grows in each iterations preventing the\nprogram to complete. The obvious problem with this is the iterable memory space\nwill grow and may even exceed memory constraints causing a panic or exception.\nSo when this technique is observed we often associate it with being a bug\nimmediately, appropriately.\n\n3. Generators\nA new technique has come about with the growing popularity of generators.\n\ndef infinity():\n    while True:\n        yield\nfor _ in infinity():\n    pass\n\n\nThis is particularly dangerous, we turn to generators due to their efficiency\nand often they are amazing for this. But when we turn that into an infinite loop\nit becomes the source of the worst of both examples 1 and 2 above.\n\n> Generators as loops leak memory and are defects\n\n\nUse cases that work\nSo why do we use a infinite loop in any language? What benefits do we get using\nthis and what caveats does this pattern bring to our programs?\n\nThere are only 2 cases where an infinite loop might arguably make sense.\n\n1. Limited scope\nIf the entire scope of the program is to operate the loop, and only the loop\nitself, then there is reason to argue that the program can do that well. But\nwhat a silly program that would be.\n\nThere are successful programs that have used infinite loops. Such as the Node.js\nevent loop, and a game engine (like Phaser.io), but developers shouldn't choose\nto write an infinite loop unless we are ready to accept the responsibility and\nconsequences.\n\n> \"Infinite loops\" with great power comes great responsibility\n\n\nMozilla has created circusd to fill the place of the now dead supervisord. Its\npurpose is to keep your program alive or give you finer control of a\nlong-running workload often implemented with an infinite loop. With such\nprograms available, developers don't need to write their own infinite loops\nanymore.\n\n2. Advanced Programming Languages\nRust to my knowledge (and I don't know everything) is the only memory safe\nprogramming language available to mere mortals.\n\nIn a language that gives us control over memory management (like Rust) and the\nability to manage fully any external file descriptors, we may avoid the caveats\nassociated with infinity loops.\n\n> Rust; memory safe programming for mere mortals\n\n\nCaveats\nThese are common use cases for developers to resort to infinity loops and the\nassociated software defects they introduce.\n\nDaemons\nWhenever we are given a requirement that leads us to use a daemon pattern in our\nprograms we immediately and amateurishly turn to infinity loops.\nAt the risk of stating the obvious to those more experienced, a daemon is not\nactually a pattern, it isn't even intended as a noun, it's an adjective to\ndescribe a process that has no ability to interact with or be interacted with\nits executor. In no way is a daemon directly associated with long-running\nprocesses, so why do many developers make this immediate association? If you\nfall into this misconception you're just hacking together some code example you\nstumbled across probably written by someone equally or less knowledgeable as\nyourself in what they're doing.\n\n> The misconception that infinity loops are daemons\n\n\nIf you have a use case for a program to be a daemon you would need to first\ndefer that program execution to a child process which becomes its own session\nleader allowing the original process to exit without killing the child\ncontaining the program you want to continue executing, thus the program is now\ndetached from its executor and is, therefore, a daemon. Notice there is no\nmention of infinite loops?\n\nIf you don't understand process, session, and leaders yet start here\n[http://www.win.tue.nl/~aeb/linux/lk/lk-10.html#ss10.3]\n\n> To win software development; understand processes\n\n\nMany termination conditions\nWhat is a termination condition? Here are a few from various languages;\n\n * return\n * exit\n * die\n * break\n * continue\n * goto\n\nWith so many exit statements available it is often that we change a conditional\nloop to an infinite loop for readability or simplicity sake, which is quite\npossibly the most ill-conceived approach. Try to convince me that it is easier\nto search a block of code for any of the listed exit statements over looking at\nthe single loop condition and I'll happily eat my words, literally, I'll print\nthe whole post and eat it for your entertainment.\n\nUnderstand your exit condition/s, then depending on that analysis you can\nproduce a single conditional to track and use for your loop condition so it is\nno longer an infinite loop. If that sounds challenging here is an example;\n\ncondition = None\nbreak_condition = [42, 67, 12]\nwhile condition not in break_condition:\n    \"\"\"Run some task and assign a value to represent that outcome\"\"\"\n    condition = random.randint(0, 100)\n    \"\"\"As long as the outcomes are appropriate to continue the loop\"\"\"\n    pass\n\n\nNow all you need to do is track one variable for all of your complex exit\nstatements while avoiding using infinite loops!\n\n> Understand your exit conditions for clean code\n\n\nDefects introduced by infinite loops\nEarlier I promised to explain that infinite loop caveats are actually software\ndefects, which shouldn't have ever been introduced.\n\nMemory leaks\nMany reported bugs that get traced back to a memory leak are often addressed by\noptimising code to work more efficiently with the languages GC.\nIf this occurs in an infinite loop you've not fixed the defect but rather\nresolved the bug report symptom only. For the memory leak to accumulate data\nover time the root cause is the infinite loop, you might of today found one leak\nand patched it, good for you, but the root cause (the infinite loop) remained\nand the defect prevails to strike again when new code is introduced by an\nunsuspecting junior developer who inevitably will get blamed.\n\n> A junior developer's regression bug is your fault\n\n\nIf you consider yourself a senior developer and you find a memory leak within an\ninfinite loop it is your responsibility to refactor to remove the infinite loop,\nyou don't need it, or that junior developer's regression bug is your fault.\n\nIncomplete execution\nWhen the executing program is killed or forced to terminate by an external\nsource (like a load balancer or a terminal session being interrupted) your\ninfinite loop and all of the logic inside is immediately abandoned.\nYou may reach this conclusion through debugging a number of bug reports, and\nwhen you reach this conclusion hopefully you learn that it can be managed using \nsignals [http://man7.org/linux/man-pages/man7/signal.7.html].\nI've seen this happen in almost every using containers like Docker, but it has\nnothing to do with containers or Docker, it's only come into the view of\ndevelopers since ops engineers are becoming less concerned with program\ndevelopment.\nIf signals are not already managed and you're program is not designed or\neffective when terminated at any stage, you should implement signal handling\nimmediately. It's uncommon for APIs or web server scripts to need this so coming\nfrom a web developer background you'd be excused for not knowing about signals,\nbut if you're not from a web background shame on you.\n\n> Implement signal handling immediately, shame on you\n\n\nIn an infinite loop, signals become more difficult to manage and I often see a\nloop short-circuit condition only at the top of the loop, and only handles one\nof the signals (usually Interrupt or Terminate).\nAlthough this common and most developers wouldn't see the problem with it, here\nis a quick question you should consider; how do you handle unknown state?\n\nBy not handling all signals, or not responding to the signal promptly, the\nprogram enters an unknown state which is not good, to say the least.\nYou may be addressing another symptom but again not the root cause. Unless you\nreact to all signals promptly, not just at the top of your infinite loop, you\ncreate a different race condition to the one you tried to solve and introduce an\nunknown state condition that didn't exist before you fixed the bug that led you\nto this.\n\nExperienced developers either found themselves doing this and learned not to do\nit ever, or were thorough enough to learn about signals, daemons, and infinite\nloops in all of their hellishness before falling for their allures.\n\nInconsistent state\nWe just explored unknown state, where the program received a signal and\nsuppressed that signal, an inconsistent state is not that.\n\nAn inconsistent state is part of your program logic, it's where you have an\nexpectation of the program state to be one thing but you've been given bug\nreports that somehow it has entered something else entirely. The most common\ninconsistent state defect with infinity loops is where the program was not able\nto complete all logic within its loop.\n\nInconsistencies may be related to data in your database/s, files (local or\nremote), or even termination of some remote resource.\n\n> Ops engineers can help with causes for inconsistent state\n\n\nThis may sound similar to the problem faced with signals, but the root cause of\ninconsistent state is not signals.\n\nIf you've had an experienced ops engineer lead you to solve this symptom by\nlooking into connection timeouts or locked files you'd normally resolve the bug\nreports by working around a connection timeout - reconnecting or something\nsimilar, again addressing a symptom without understanding it.\n\nFile descriptors [https://en.wikipedia.org/wiki/File_descriptor] are the things\nthat a program obtains from the system to open files and remote connections, but\ndue to infinite loops being long-running processes the program expects a file or\nremote connection to be in a certain state when it reuses the same resource it\nhad opened earlier. But as you'll learn from your debugging, things change, get\nlocked or deleted, and connections timeout. Simply reconnecting doesn't stop the\nproblem happening because things like database connections and file handling are\ndone via a library or some other abstraction it is often impossible for programs\nto manage their own file descriptors for these resources, leaving you with only\nan option to reconnect or some other useless abstraction that doesn't actually\nprevent the problem occurring only react to the symptoms.\n\n> Learn about file descriptors to achieve greatness\n\n\nNow you've learned about file descriptors you should resolve your bug at the\nroot cause, refactor your code to remove the infinite loop so your program never\nenters an inconsistent state where it would ever have any stale file descriptor\nreferences in memory.\n\nRefactor your loop\nHow to refactor away from infinite loops?\n\nAbove I gave an example of using a single conditional with multiple exit\nstatements, this method takes all of the exit conditions within the loop and\nreplaces them with a variable that takes a unique value corresponding to that\nexit condition. You can then turn your infinite loop into a loop with one\nconditional, therefore one place to track your exit conditions.\nYou'll find that now you have an easy way to reason with your loop and it's exit\nconditions, you'll start identifying previously hard to locate exit conditions\nin your code. What you previously had was a pseudo-daemon with a big defect\ncommonly called an infinite loop, and now you have a program you can reason\nwith.\n\nAnother way to refactor might be using an application like circusd\n[https://github.com/circus-tent/circus] to take the place of the infinite loop\naltogether making it redundant to have one in your code at all. All your program\nwill need to do is execute the program itself (the code you had in your loop).\n\nYou'll find that something like circusd will also make many (or all) your\nprevious infinite loop exit condition redundant in your code, as it provides you\nways to run your program, recover or keep it running, delay start, delay\nexecutions, and many many more programming complexities so you can just write\nyour program and forget you ever heard the word daemon.\n\n> Mozilla's circus saves lives","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/hx87xFWN.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-06-17 03:36:21","created_by":"1","updated_at":"2021-03-31 14:13:04","updated_by":"1","published_at":"2017-06-17 08:58:34","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcaa","uuid":"54fdb155-3ec1-477e-8bcb-c33650cd1f97","title":"Precise vs Accurate on Arbitrary-precision Arithmetic","slug":"arbitrary-precision-arithmetic-precise-vs-accurate","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"When Math isn't accurate in code\\n\\n## Precise vs Accurate\\n\\nSo here's a simple example to get you started, punch a simple calculation `0.1 + 0.2` into any calculator, scientific, google, whatever.\\nWhat result do you get?\\n\\nYou should see `0.3` right?\\n\\n### The code issues with arithmetic\\n\\nNow go into your favourite programming language and do the same thing, what do you get?\\n\\nYou'll be pretty pleased if you're using Go, Haskell, Groovy, Lua, Swift, or even PHP and MySQL!\\n\\nSurprising to me, at first, I assumed R was accurate until I double checked;\\n\\n```r\\nprint(.1+.2, digits=18)\\n// 0.30000000000000004\\n```\\n\\n> Even R gets numbers wrong, and it's used heavily by data _scientists_\\n\\nA contract I am working on currently is doing some big data analytics using Python 3 and R, so lets look at some python 3;\\n\\n```python\\nprint(.1 + .2)\\n// 0.30000000000000004\\n```\\n\\nNot cool snakey...\\n\\nSo this is a pretty common issue right? no. Common assumes it's commonly identified. It is almost never known but it is most definitely a wide spread practice.\\n \\nThe business (i.e. a Ph.D scientist) was completely unaware that their developers are using programming languages that will not produce the same results that their calculators produce when they proof the result the software produces...\\n\\nSo several years back I recall a conversation i had with a tech lead, the below is a very basic version of a Node.js solution I saw in a B2B financial transaction software;\\n\\n```javascript\\nNumber.prototype.addTo = function(n) {\\n  return this + n\\n}\\nvar number = 0.1\\nnumber.addTo(0.2)\\n// 0.30000000000000004\\n```\\n\\nSo what's a few cents you say? Consider we are running millions of calculations over millions of dollar transactions and then consider using multiplication instead of addition, yeah\\n\\n> Scary.\\n\\n### What is this problem called?\\n\\nCaused when using any floating points in any calculation, this isn't a \\\"bug\\\" but rather an example of **Arbitrary-precision arithmetic**.\\n\\nThere is one very hard way to teach yourself, read [What Every Computer Scientist Should Know About Floating-Point Arithmetic](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) but a much better way is to take one step back and become familiar with all of the relevant material from [IEEE 754: Standard for Binary Floating-Point Arithmetic](https://standards.ieee.org/standard/754-2019.html) so you've first found a baseline of what should be expected in your programming of Arithmetic.\\n\\n### Solutions?\\n\\nFor the **python** code you can try just using Python 2 and keep values as decimals and not floats\\n\\nFor **JavaScript** there are libraries such as [BigDecimal](https://github.com/dtrebbien/BigDecimal.js) to help fix the issues, **Ruby** also has a library option called [BigDecimal](http://ruby-doc.org/stdlib/libdoc/bigdecimal/rdoc/index.html) but of course developers still use language native operators and cannot be forced to use a math library..\\n\\nSome languages like **Java** are much easier, just ensure you're being explicit (should be disciplined enough by now)\\n\\n```java\\nSystem.out.println(.1F + .2F);\\n// 0.3\\n```\\n\\nBut be careful because some languages like **Go** will go wrong when forcing the use of floats;\\n\\n```go\\nvar a float64 = .1\\nvar b float64 = .2\\nfmt.Println(a + b)\\n// 0.30000000000000004\\n```\\n\\nThe key is to understand your language of choice, and mentor team members that are not well informed.\\n\\n### Fixing defects in existing software\\n\\n> Don't fix the bugs!\\n\\nReally, I'm not kidding. Don't go refactor your entire code base dot fix bugs. The software has a particular reliance on the faulty functionality and therefore it's likely the faultiness has forced the end-users or integrated systems to work around the defect (usually by treating values as strings or truncating values).\\n\\nIf you change the source of the defect now and the dependants work-arounds downstream are not removed you will just be facing a wider range of defects later.\\n\\nAmazingly you don't need to take my word for it, According to Edaena Salinas on [SE-Radio #295](http://www.se-radio.net/2017/06/se-radio-episode-295-michael-feathers-on-legacy-code/), Microsoft Excel rounding issues in Excel that will never be fixed and they even went so far as to give the defects a name so as to legitimise them, they call the Mathematical precision defects of excel a \\\"domain expectation\\\" now.\\n\\n### The take away\\n\\nIf you've been reading my dribble you might be expecting an opinion around this point, sorry to disappoint, this is an education issue now. If even the language developers, well most of them, weren't aware of the IEEE standards or willing to be thorough enough to educate themselves and be good developers, what hope do companies even as large as Microsoft have to be precise?\\n\\n> We as individuals can only try our best.\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>When Math isn't accurate in code</p>\n<h2 id=\"precisevsaccurate\">Precise vs Accurate</h2>\n<p>So here's a simple example to get you started, punch a simple calculation <code>0.1 + 0.2</code> into any calculator, scientific, google, whatever.<br>\nWhat result do you get?</p>\n<p>You should see <code>0.3</code> right?</p>\n<h3 id=\"thecodeissueswitharithmetic\">The code issues with arithmetic</h3>\n<p>Now go into your favourite programming language and do the same thing, what do you get?</p>\n<p>You'll be pretty pleased if you're using Go, Haskell, Groovy, Lua, Swift, or even PHP and MySQL!</p>\n<p>Surprising to me, at first, I assumed R was accurate until I double checked;</p>\n<pre><code class=\"language-r\">print(.1+.2, digits=18)\n// 0.30000000000000004\n</code></pre>\n<blockquote>\n<p>Even R gets numbers wrong, and it's used heavily by data <em>scientists</em></p>\n</blockquote>\n<p>A contract I am working on currently is doing some big data analytics using Python 3 and R, so lets look at some python 3;</p>\n<pre><code class=\"language-python\">print(.1 + .2)\n// 0.30000000000000004\n</code></pre>\n<p>Not cool snakey...</p>\n<p>So this is a pretty common issue right? no. Common assumes it's commonly identified. It is almost never known but it is most definitely a wide spread practice.</p>\n<p>The business (i.e. a Ph.D scientist) was completely unaware that their developers are using programming languages that will not produce the same results that their calculators produce when they proof the result the software produces...</p>\n<p>So several years back I recall a conversation i had with a tech lead, the below is a very basic version of a Node.js solution I saw in a B2B financial transaction software;</p>\n<pre><code class=\"language-javascript\">Number.prototype.addTo = function(n) {\n  return this + n\n}\nvar number = 0.1\nnumber.addTo(0.2)\n// 0.30000000000000004\n</code></pre>\n<p>So what's a few cents you say? Consider we are running millions of calculations over millions of dollar transactions and then consider using multiplication instead of addition, yeah</p>\n<blockquote>\n<p>Scary.</p>\n</blockquote>\n<h3 id=\"whatisthisproblemcalled\">What is this problem called?</h3>\n<p>Caused when using any floating points in any calculation, this isn't a &quot;bug&quot; but rather an example of <strong>Arbitrary-precision arithmetic</strong>.</p>\n<p>There is one very hard way to teach yourself, read <a href=\"http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a> but a much better way is to take one step back and become familiar with all of the relevant material from <a href=\"https://standards.ieee.org/standard/754-2019.html\">IEEE 754: Standard for Binary Floating-Point Arithmetic</a> so you've first found a baseline of what should be expected in your programming of Arithmetic.</p>\n<h3 id=\"solutions\">Solutions?</h3>\n<p>For the <strong>python</strong> code you can try just using Python 2 and keep values as decimals and not floats</p>\n<p>For <strong>JavaScript</strong> there are libraries such as <a href=\"https://github.com/dtrebbien/BigDecimal.js\">BigDecimal</a> to help fix the issues, <strong>Ruby</strong> also has a library option called <a href=\"http://ruby-doc.org/stdlib/libdoc/bigdecimal/rdoc/index.html\">BigDecimal</a> but of course developers still use language native operators and cannot be forced to use a math library..</p>\n<p>Some languages like <strong>Java</strong> are much easier, just ensure you're being explicit (should be disciplined enough by now)</p>\n<pre><code class=\"language-java\">System.out.println(.1F + .2F);\n// 0.3\n</code></pre>\n<p>But be careful because some languages like <strong>Go</strong> will go wrong when forcing the use of floats;</p>\n<pre><code class=\"language-go\">var a float64 = .1\nvar b float64 = .2\nfmt.Println(a + b)\n// 0.30000000000000004\n</code></pre>\n<p>The key is to understand your language of choice, and mentor team members that are not well informed.</p>\n<h3 id=\"fixingdefectsinexistingsoftware\">Fixing defects in existing software</h3>\n<blockquote>\n<p>Don't fix the bugs!</p>\n</blockquote>\n<p>Really, I'm not kidding. Don't go refactor your entire code base dot fix bugs. The software has a particular reliance on the faulty functionality and therefore it's likely the faultiness has forced the end-users or integrated systems to work around the defect (usually by treating values as strings or truncating values).</p>\n<p>If you change the source of the defect now and the dependants work-arounds downstream are not removed you will just be facing a wider range of defects later.</p>\n<p>Amazingly you don't need to take my word for it, According to Edaena Salinas on <a href=\"http://www.se-radio.net/2017/06/se-radio-episode-295-michael-feathers-on-legacy-code/\">SE-Radio #295</a>, Microsoft Excel rounding issues in Excel that will never be fixed and they even went so far as to give the defects a name so as to legitimise them, they call the Mathematical precision defects of excel a &quot;domain expectation&quot; now.</p>\n<h3 id=\"thetakeaway\">The take away</h3>\n<p>If you've been reading my dribble you might be expecting an opinion around this point, sorry to disappoint, this is an education issue now. If even the language developers, well most of them, weren't aware of the IEEE standards or willing to be thorough enough to educate themselves and be good developers, what hope do companies even as large as Microsoft have to be precise?</p>\n<blockquote>\n<p>We as individuals can only try our best.</p>\n</blockquote>\n<!--kg-card-end: markdown-->","comment_id":"44","plaintext":"When Math isn't accurate in code\n\nPrecise vs Accurate\nSo here's a simple example to get you started, punch a simple calculation 0.1 +\n0.2 into any calculator, scientific, google, whatever.\nWhat result do you get?\n\nYou should see 0.3 right?\n\nThe code issues with arithmetic\nNow go into your favourite programming language and do the same thing, what do\nyou get?\n\nYou'll be pretty pleased if you're using Go, Haskell, Groovy, Lua, Swift, or\neven PHP and MySQL!\n\nSurprising to me, at first, I assumed R was accurate until I double checked;\n\nprint(.1+.2, digits=18)\n// 0.30000000000000004\n\n\n> Even R gets numbers wrong, and it's used heavily by data scientists\n\n\nA contract I am working on currently is doing some big data analytics using\nPython 3 and R, so lets look at some python 3;\n\nprint(.1 + .2)\n// 0.30000000000000004\n\n\nNot cool snakey...\n\nSo this is a pretty common issue right? no. Common assumes it's commonly\nidentified. It is almost never known but it is most definitely a wide spread\npractice.\n\nThe business (i.e. a Ph.D scientist) was completely unaware that their\ndevelopers are using programming languages that will not produce the same\nresults that their calculators produce when they proof the result the software\nproduces...\n\nSo several years back I recall a conversation i had with a tech lead, the below\nis a very basic version of a Node.js solution I saw in a B2B financial\ntransaction software;\n\nNumber.prototype.addTo = function(n) {\n  return this + n\n}\nvar number = 0.1\nnumber.addTo(0.2)\n// 0.30000000000000004\n\n\nSo what's a few cents you say? Consider we are running millions of calculations\nover millions of dollar transactions and then consider using multiplication\ninstead of addition, yeah\n\n> Scary.\n\n\nWhat is this problem called?\nCaused when using any floating points in any calculation, this isn't a \"bug\" but\nrather an example of Arbitrary-precision arithmetic.\n\nThere is one very hard way to teach yourself, read What Every Computer\nScientist\nShould Know About Floating-Point Arithmetic\n[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html] but a much\nbetter way is to take one step back and become familiar with all of the relevant\nmaterial from IEEE 754: Standard for Binary Floating-Point Arithmetic\n[https://standards.ieee.org/standard/754-2019.html] so you've first found a\nbaseline of what should be expected in your programming of Arithmetic.\n\nSolutions?\nFor the python code you can try just using Python 2 and keep values as decimals\nand not floats\n\nFor JavaScript there are libraries such as BigDecimal\n[https://github.com/dtrebbien/BigDecimal.js] to help fix the issues, Ruby also\nhas a library option called BigDecimal\n[http://ruby-doc.org/stdlib/libdoc/bigdecimal/rdoc/index.html] but of course\ndevelopers still use language native operators and cannot be forced to use a\nmath library..\n\nSome languages like Java are much easier, just ensure you're being explicit\n(should be disciplined enough by now)\n\nSystem.out.println(.1F + .2F);\n// 0.3\n\n\nBut be careful because some languages like Go will go wrong when forcing the use\nof floats;\n\nvar a float64 = .1\nvar b float64 = .2\nfmt.Println(a + b)\n// 0.30000000000000004\n\n\nThe key is to understand your language of choice, and mentor team members that\nare not well informed.\n\nFixing defects in existing software\n> Don't fix the bugs!\n\n\nReally, I'm not kidding. Don't go refactor your entire code base dot fix bugs.\nThe software has a particular reliance on the faulty functionality and therefore\nit's likely the faultiness has forced the end-users or integrated systems to\nwork around the defect (usually by treating values as strings or truncating\nvalues).\n\nIf you change the source of the defect now and the dependants work-arounds\ndownstream are not removed you will just be facing a wider range of defects\nlater.\n\nAmazingly you don't need to take my word for it, According to Edaena Salinas on \nSE-Radio #295\n[http://www.se-radio.net/2017/06/se-radio-episode-295-michael-feathers-on-legacy-code/]\n, Microsoft Excel rounding issues in Excel that will never be fixed and they\neven went so far as to give the defects a name so as to legitimise them, they\ncall the Mathematical precision defects of excel a \"domain expectation\" now.\n\nThe take away\nIf you've been reading my dribble you might be expecting an opinion around this\npoint, sorry to disappoint, this is an education issue now. If even the language\ndevelopers, well most of them, weren't aware of the IEEE standards or willing to\nbe thorough enough to educate themselves and be good developers, what hope do\ncompanies even as large as Microsoft have to be precise?\n\n> We as individuals can only try our best.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/2000px-IEEE_754r_Half_Floating_Point_Format.svg.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-06-28 08:30:39","created_by":"1","updated_at":"2021-03-31 14:11:50","updated_by":"1","published_at":"2017-06-28 12:19:44","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcab","uuid":"69d2c9ec-6866-4d43-8227-f36eceffbb2f","title":"Password disclosure redundancy","slug":"password-disclosure-redundancy","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"How to set yourself up so that a data breach doesn’t mean bad actors have a chance to recover data.\\n\\n> Password disclosures are bad, they will happen to you — count on it.\\n\\nInstead of being fearful or looking for blame after the fact, accept that your systems will be penetrated it’s just a matter of time.\\n\\n## Vendors\\n\\nThere is no vendor that you should trust with protecting your data for you, and no matter how perfectly your contract is worded there is no piece of paper that will change the nature of humans. It’s you’re own dysfunction if you think someone else will solve your concerns for you, it’s ignorant of you and futile to try.\\n\\n> Like you, they too will be compromised. It is a matter of time.\\n\\nWhen they are compromised it will be insurance that is used to resolve any contractual requirements of the breach, or legal actions. In any case they were stupid enough to presume they were invulnerable to be contracted in a way that doesn’t allow an inevitable data breach.\\n\\nSo when there is a data breach the sad reality is that vendors will reuse techniques that apply to all of their customers, called a product, which means your threat landscape before you had the vendor was just contained to just your own business operations. Now the threat landscape covers the vendors and their customers too! If an employee of the 3rd party company is compromised it is highly likely that the compromise extends to your business too using the same compromise.\\n\\nIf you had taken ownership of your own data security in the first place you’d read about the vendor’s problems in the news and your business is isolated from the issue.\\n\\n> Don’t increase your threat landscape — it only works against you, not for you.\\n\\nEach of your security decisions must reduce the threat landscape without compromise, otherwise you are deciding to be less secure and that is simply immature and can only mean disaster to you when bad actors find you.\\n\\n## Threat vectors\\n\\nKnow your threat vectors, which are routes that malicious attackers may take to compromise you. Look to your physical vectors like Network, Servers, Devices, and Hardware which can be compromised internally or remotely but don’t ignore the human vectors such as user password habits, uncertain personal devices, and social engineering.\\n\\n> Education is more valuable then technology\\n\\nEducate people that their attention to security can be more important and impactful if compromised then any of the technology used by the company.\\n\\nDevising a strategy around threat vectors with consideration of your attack surfaces using the following techniques;\\n\\n**Principle of least privilege**: ensure users and applications do not have privileges to do more then they are designed or permitted to do. Create RACI reports to identify where you can improve on this.\\n\\n**Operate with minimums needed**: reducing the attack surface area. An example would be if your admin area is only used by employees consider blocking any other IP then the office one, which might mean you’d need VPN access when not working from the office.\\n\\n**Establishing a default secure posture**: start with no risk and risk increases as features are added that are acceptable compromises by design not by mistake. The best example is AWS S3 buckets, the default policy enforces only the owner access, you need to deliberately make S3 less secure to provide certain functionality.\\n\\n**Avoid security by obscurity**: which is universally accepted as weak security. The security of systems and data should not be reliant on keeping a secret hidden. For example encryption is by design intended to be decrypted, therefore if the key is stolen there is zero security remaining. A better technique is to apply similar techniques used to protect passwords that result in a resilient SHA1, but SHA1 has a recognizable set of characters so XOR these characters and encrypt that rather then encrypting the raw data.\\n\\n**Defense in depth**: is another common way used to describe the technique described above. What this does is make vulnerabilities extensively difficult to exploit therefore less likely to occur.\\n\\n## Attack Methods\\n\\nWe’re pretty familiar with who attackers might be; drive-bys, state actors, ex-employees, organised crime, youngsters, ect. but more important then who is how.\\n\\nYou can read about common methods such as Advanced Persistent Threat (APT), Distributed Denial of Service (DDoS), Cross-Platform Malware (CPM), Phishing, or the more unpredictable ones that come under Metamorphic and Polymorphic Malware which may include a Command and Control component. Most security software will protect you from these known methods and therefore the actors that utilise them.\\n\\nWhat I’d like to bring to your attention is unknown methods and the opportunist actors that security software cannot protect you from at all because these are the threats that are the known unknowns that will eventually penetrate your defenses.\\n\\n> Let the threat actors take your data\\n\\nIf your security strategy diligently follows all of the principles its likely that you’re in a position to not actually care that your data is exposed, let the threat take the data because it is rendered useless to them, but be careful to still prevent the threat actors to cause you harm after they’re established a beach-head.\\n\\n> Monitor, Monitor, Monitor\\n\\nMake sure you are set up to constantly monitor you’re normal activity, because without knowing the normal you cannot detect the anomalous.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>How to set yourself up so that a data breach doesn’t mean bad actors have a chance to recover data.</p>\n<blockquote>\n<p>Password disclosures are bad, they will happen to you — count on it.</p>\n</blockquote>\n<p>Instead of being fearful or looking for blame after the fact, accept that your systems will be penetrated it’s just a matter of time.</p>\n<h2 id=\"vendors\">Vendors</h2>\n<p>There is no vendor that you should trust with protecting your data for you, and no matter how perfectly your contract is worded there is no piece of paper that will change the nature of humans. It’s you’re own dysfunction if you think someone else will solve your concerns for you, it’s ignorant of you and futile to try.</p>\n<blockquote>\n<p>Like you, they too will be compromised. It is a matter of time.</p>\n</blockquote>\n<p>When they are compromised it will be insurance that is used to resolve any contractual requirements of the breach, or legal actions. In any case they were stupid enough to presume they were invulnerable to be contracted in a way that doesn’t allow an inevitable data breach.</p>\n<p>So when there is a data breach the sad reality is that vendors will reuse techniques that apply to all of their customers, called a product, which means your threat landscape before you had the vendor was just contained to just your own business operations. Now the threat landscape covers the vendors and their customers too! If an employee of the 3rd party company is compromised it is highly likely that the compromise extends to your business too using the same compromise.</p>\n<p>If you had taken ownership of your own data security in the first place you’d read about the vendor’s problems in the news and your business is isolated from the issue.</p>\n<blockquote>\n<p>Don’t increase your threat landscape — it only works against you, not for you.</p>\n</blockquote>\n<p>Each of your security decisions must reduce the threat landscape without compromise, otherwise you are deciding to be less secure and that is simply immature and can only mean disaster to you when bad actors find you.</p>\n<h2 id=\"threatvectors\">Threat vectors</h2>\n<p>Know your threat vectors, which are routes that malicious attackers may take to compromise you. Look to your physical vectors like Network, Servers, Devices, and Hardware which can be compromised internally or remotely but don’t ignore the human vectors such as user password habits, uncertain personal devices, and social engineering.</p>\n<blockquote>\n<p>Education is more valuable then technology</p>\n</blockquote>\n<p>Educate people that their attention to security can be more important and impactful if compromised then any of the technology used by the company.</p>\n<p>Devising a strategy around threat vectors with consideration of your attack surfaces using the following techniques;</p>\n<p><strong>Principle of least privilege</strong>: ensure users and applications do not have privileges to do more then they are designed or permitted to do. Create RACI reports to identify where you can improve on this.</p>\n<p><strong>Operate with minimums needed</strong>: reducing the attack surface area. An example would be if your admin area is only used by employees consider blocking any other IP then the office one, which might mean you’d need VPN access when not working from the office.</p>\n<p><strong>Establishing a default secure posture</strong>: start with no risk and risk increases as features are added that are acceptable compromises by design not by mistake. The best example is AWS S3 buckets, the default policy enforces only the owner access, you need to deliberately make S3 less secure to provide certain functionality.</p>\n<p><strong>Avoid security by obscurity</strong>: which is universally accepted as weak security. The security of systems and data should not be reliant on keeping a secret hidden. For example encryption is by design intended to be decrypted, therefore if the key is stolen there is zero security remaining. A better technique is to apply similar techniques used to protect passwords that result in a resilient SHA1, but SHA1 has a recognizable set of characters so XOR these characters and encrypt that rather then encrypting the raw data.</p>\n<p><strong>Defense in depth</strong>: is another common way used to describe the technique described above. What this does is make vulnerabilities extensively difficult to exploit therefore less likely to occur.</p>\n<h2 id=\"attackmethods\">Attack Methods</h2>\n<p>We’re pretty familiar with who attackers might be; drive-bys, state actors, ex-employees, organised crime, youngsters, ect. but more important then who is how.</p>\n<p>You can read about common methods such as Advanced Persistent Threat (APT), Distributed Denial of Service (DDoS), Cross-Platform Malware (CPM), Phishing, or the more unpredictable ones that come under Metamorphic and Polymorphic Malware which may include a Command and Control component. Most security software will protect you from these known methods and therefore the actors that utilise them.</p>\n<p>What I’d like to bring to your attention is unknown methods and the opportunist actors that security software cannot protect you from at all because these are the threats that are the known unknowns that will eventually penetrate your defenses.</p>\n<blockquote>\n<p>Let the threat actors take your data</p>\n</blockquote>\n<p>If your security strategy diligently follows all of the principles its likely that you’re in a position to not actually care that your data is exposed, let the threat take the data because it is rendered useless to them, but be careful to still prevent the threat actors to cause you harm after they’re established a beach-head.</p>\n<blockquote>\n<p>Monitor, Monitor, Monitor</p>\n</blockquote>\n<p>Make sure you are set up to constantly monitor you’re normal activity, because without knowing the normal you cannot detect the anomalous.</p>\n<!--kg-card-end: markdown-->","comment_id":"5a04361d0c87e714556f627d","plaintext":"How to set yourself up so that a data breach doesn’t mean bad actors have a\nchance to recover data.\n\n> Password disclosures are bad, they will happen to you — count on it.\n\n\nInstead of being fearful or looking for blame after the fact, accept that your\nsystems will be penetrated it’s just a matter of time.\n\nVendors\nThere is no vendor that you should trust with protecting your data for you, and\nno matter how perfectly your contract is worded there is no piece of paper that\nwill change the nature of humans. It’s you’re own dysfunction if you think\nsomeone else will solve your concerns for you, it’s ignorant of you and futile\nto try.\n\n> Like you, they too will be compromised. It is a matter of time.\n\n\nWhen they are compromised it will be insurance that is used to resolve any\ncontractual requirements of the breach, or legal actions. In any case they were\nstupid enough to presume they were invulnerable to be contracted in a way that\ndoesn’t allow an inevitable data breach.\n\nSo when there is a data breach the sad reality is that vendors will reuse\ntechniques that apply to all of their customers, called a product, which means\nyour threat landscape before you had the vendor was just contained to just your\nown business operations. Now the threat landscape covers the vendors and their\ncustomers too! If an employee of the 3rd party company is compromised it is\nhighly likely that the compromise extends to your business too using the same\ncompromise.\n\nIf you had taken ownership of your own data security in the first place you’d\nread about the vendor’s problems in the news and your business is isolated from\nthe issue.\n\n> Don’t increase your threat landscape — it only works against you, not for you.\n\n\nEach of your security decisions must reduce the threat landscape without\ncompromise, otherwise you are deciding to be less secure and that is simply\nimmature and can only mean disaster to you when bad actors find you.\n\nThreat vectors\nKnow your threat vectors, which are routes that malicious attackers may take to\ncompromise you. Look to your physical vectors like Network, Servers, Devices,\nand Hardware which can be compromised internally or remotely but don’t ignore\nthe human vectors such as user password habits, uncertain personal devices, and\nsocial engineering.\n\n> Education is more valuable then technology\n\n\nEducate people that their attention to security can be more important and\nimpactful if compromised then any of the technology used by the company.\n\nDevising a strategy around threat vectors with consideration of your attack\nsurfaces using the following techniques;\n\nPrinciple of least privilege: ensure users and applications do not have\nprivileges to do more then they are designed or permitted to do. Create RACI\nreports to identify where you can improve on this.\n\nOperate with minimums needed: reducing the attack surface area. An example would\nbe if your admin area is only used by employees consider blocking any other IP\nthen the office one, which might mean you’d need VPN access when not working\nfrom the office.\n\nEstablishing a default secure posture: start with no risk and risk increases as\nfeatures are added that are acceptable compromises by design not by mistake. The\nbest example is AWS S3 buckets, the default policy enforces only the owner\naccess, you need to deliberately make S3 less secure to provide certain\nfunctionality.\n\nAvoid security by obscurity: which is universally accepted as weak security. The\nsecurity of systems and data should not be reliant on keeping a secret hidden.\nFor example encryption is by design intended to be decrypted, therefore if the\nkey is stolen there is zero security remaining. A better technique is to apply\nsimilar techniques used to protect passwords that result in a resilient SHA1,\nbut SHA1 has a recognizable set of characters so XOR these characters and\nencrypt that rather then encrypting the raw data.\n\nDefense in depth: is another common way used to describe the technique described\nabove. What this does is make vulnerabilities extensively difficult to exploit\ntherefore less likely to occur.\n\nAttack Methods\nWe’re pretty familiar with who attackers might be; drive-bys, state actors,\nex-employees, organised crime, youngsters, ect. but more important then who is\nhow.\n\nYou can read about common methods such as Advanced Persistent Threat (APT),\nDistributed Denial of Service (DDoS), Cross-Platform Malware (CPM), Phishing, or\nthe more unpredictable ones that come under Metamorphic and Polymorphic Malware\nwhich may include a Command and Control component. Most security software will\nprotect you from these known methods and therefore the actors that utilise them.\n\nWhat I’d like to bring to your attention is unknown methods and the opportunist\nactors that security software cannot protect you from at all because these are\nthe threats that are the known unknowns that will eventually penetrate your\ndefenses.\n\n> Let the threat actors take your data\n\n\nIf your security strategy diligently follows all of the principles its likely\nthat you’re in a position to not actually care that your data is exposed, let\nthe threat take the data because it is rendered useless to them, but be careful\nto still prevent the threat actors to cause you harm after they’re established a\nbeach-head.\n\n> Monitor, Monitor, Monitor\n\n\nMake sure you are set up to constantly monitor you’re normal activity, because\nwithout knowing the normal you cannot detect the anomalous.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/snip20170808_16_0.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-09 11:03:57","created_by":"1","updated_at":"2021-03-31 14:12:38","updated_by":"1","published_at":"2017-10-25 11:04:00","published_by":"1","custom_excerpt":"Let the threat actors take your data. Data breaches are bad, they will happen to you — count on it. Education is more valuable then technology. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcac","uuid":"484d546a-eebe-4e63-b4d6-16d6696ab470","title":"Removing that single-point-of-failure Password Managers pose","slug":"removing-that-single-point-of-failure-password-managers-pose","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"You’ve heard that your password has already been compromised by LinkedIn, Yahoo, Vodafone, Sony, Minecraft, and Snapchat to name a few of the [more then 200 ](https://feeds.feedburner.com/HaveIBeenPwnedLatestBreaches) breaches I know about, but do you know that the Password Managers you put trust in for **all of your passwords** are just as vulnerable?\\n\\nWith the revelations of security slip-ups in password managers we’re still told by security experts that everything’s fine now. Patch. Keep using them. Move along, but I say that’s not good enough and it’s time to get smarter and question whether our security experts are keeping up with the times and listening to their younger more in touch peers.\\n\\n> Password Managers are no more secure from hackers then any other product\\n\\nIf we take only 5 password managers that first come to mind today we can see a rich history\\n\\n* February 2011 LastPass XSS discovered\\n* May 2011 LastPass announced possible breach\\n* June 2012 KeePass remote attack exploit\\n* June 2015 LastPass compromised servers\\n* October 2015 1PasswordAnywhere exposed PPI & user reset password links\\n* November 2015 KeePass vulnerable to KeeFarce hacking tool\\n* June 2016 KeePass Automatic Update Vulnerability\\n* July 2016 LastPass users plaintext passwords exposed\\n* August 2016 KeePass Header Authentication\\n* August 2016 OneLogin application cleartext logs exposed\\n* August 2016 Dashlane Universal XSS\\n* September 2016 1Password user passwords exposed to subdomains\\n* February 2017 1Password encrypted data exposed via Cloudflare issue\\n* March 2017 LastPass multiple security vulnerabilities in browser extensions\\n* May 2017 OneLogin unauthorized full server side access security breach\\n* May 2017 KeePass master passwords cracked with Hashcat tool\\n\\nI spent less than an hour to come up with that timeline, this is not a research exercise so it is likely i uncovered only half the total list out there and its 90% more then you knew.\\n\\nI read security experts publications and listen to security podcasts daily and I knew of only a handful in that list, you’re probably reading that to yourself on the train to work and passengers are thinking if they should call an ambulance for you before you pass out!\\n\\n> Where’s the mainstream media coverage on this?\\n\\nPassword managers are the single-point-of-failure when it comes to your passwords, they can expose all of your passwords to all of your programs, sites, secure notes, and encryption keys in just 1 breach, and I’ve just shown you 16 vulnerabilities for the leading 5 tools and you’re probably using one of these daily.\\n\\n> This should scare you\\n\\nSo my message is **treat Password Managers as a convenience tool and *not* give them your passwords anymore**.\\n\\nHang on, what? That defeats the purpose you might ask, well no it doesn’t. Password managers are still useful as convenience tools even if they don’t know your whole password but rather they just need to manage a portion of the password you can’t remember yourself.\\n\\nHere’s the technique;\\n\\n* Tell the password manager to generate the long jumble of characters as usual, and store it for you for any particular program\\n* Choose a PIN or password you will remember — DO NOT write it down anywhere, ever! So make it memorable (even if it is easy).\\nFor demonstration we’ll say it is 123456 (do not use 123456 yourself..)\\n* Now combine the memorable password with the generated one your password manager keeps, prefix or suffix it’s your choice, but tell the program the password is the combination of your memorable one and the one generated and stored by the password manager.\\n    To demonstrate, your new password might be;\\n    $tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ*123456\\n    or\\n    123456$tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ*\\n* Never give the password manager your memorable PIN or password\\n* Now when you use the program your password manager will fill in the password it has, it is incomplete and wont let you log into the program but all you have to do is add your memorable portion to the one pre-filled and you’re good to go.\\n* [even better] instead of append or prepend (which might be intuative to an attacher) choose a location somewhere in the middle to place your PIN and do so for every password just like a prefix or suffix but rather it is somewhere in the middle. An attacker with both the PIN and the long random string password might try suffix and prefix but to crack this they would need to try the PIN shifting by 1 character. Now, what if you have 2 PINs in there! That is not just easy for you to type but it is far more difficault to guess and harder to code an algorithm to try all the variations. \\n\\nNow if the password vault of the Password manager is compromised all the bad actor has is a collection of unusable parts of your passwords.\\n\\nIf they are aware you have a prefix or suffix PIN or password, and they have compromised another 3rd party application that you used the PIN or password combined with the generated portion they obtained from your password vault AND they are able to access the secret memorable portion as cleartext by somehow breaking the encryption used by the 3rd party application (or it was stored cleartext in the first place, which I might wonder how they earned your trust).\\n\\nIn this chain of low likelihood scenarios you arrive back at the current level of security you have with your chosen Password manager.\\n\\nNow it might be obvious to some of you, but those that know that length is the only thing that really influences password strength would agree that adding a suffix or prefix can only make passwords stronger, not weaker.\\n\\nBy any standards, you have to admit that removing any single-point-of-failure to the access of all of your passwords in one breach can only be a good thing.\\n\\nOn the topic of keystroke capture risks to someone learning your memorable secret — this is the case where there is a running compromise on the computer you are using, i.e. the computer is already compromised.\\n\\nMost, if not all, Password Manager’s openly state they cannot protect your passwords from an already compromised system. I’d suggest this includes systems that are operating a keystroke capture vulnerability where the Password Manager statement is less specific. Why? Because the Password Manager’s master passphrase can be compromised unlocking the password vault!\\n\\nWhere the keystroke capture might unlock the Password Manager vault as-well-as learn your memorable PIN or password, they’d have to also figure out that you use these in combination, trying the prefix or suffix techniques or any other technique you’ve chosen to use to combine them (I use my own secret technique) and that is a lot of assumptions, it’s not a simple hack or breach and its more likely that the bad actor will simply try one or 2 things and move onto a more vulnerable victim whereas if you just store whole passwords in your Password Manager you are that more vulnerable victim.\\n\\nBut of course, critique is expressly invited — bring the pain\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>You’ve heard that your password has already been compromised by LinkedIn, Yahoo, Vodafone, Sony, Minecraft, and Snapchat to name a few of the <a href=\"https://feeds.feedburner.com/HaveIBeenPwnedLatestBreaches\">more then 200 </a> breaches I know about, but do you know that the Password Managers you put trust in for <strong>all of your passwords</strong> are just as vulnerable?</p>\n<p>With the revelations of security slip-ups in password managers we’re still told by security experts that everything’s fine now. Patch. Keep using them. Move along, but I say that’s not good enough and it’s time to get smarter and question whether our security experts are keeping up with the times and listening to their younger more in touch peers.</p>\n<blockquote>\n<p>Password Managers are no more secure from hackers then any other product</p>\n</blockquote>\n<p>If we take only 5 password managers that first come to mind today we can see a rich history</p>\n<ul>\n<li>February 2011 LastPass XSS discovered</li>\n<li>May 2011 LastPass announced possible breach</li>\n<li>June 2012 KeePass remote attack exploit</li>\n<li>June 2015 LastPass compromised servers</li>\n<li>October 2015 1PasswordAnywhere exposed PPI &amp; user reset password links</li>\n<li>November 2015 KeePass vulnerable to KeeFarce hacking tool</li>\n<li>June 2016 KeePass Automatic Update Vulnerability</li>\n<li>July 2016 LastPass users plaintext passwords exposed</li>\n<li>August 2016 KeePass Header Authentication</li>\n<li>August 2016 OneLogin application cleartext logs exposed</li>\n<li>August 2016 Dashlane Universal XSS</li>\n<li>September 2016 1Password user passwords exposed to subdomains</li>\n<li>February 2017 1Password encrypted data exposed via Cloudflare issue</li>\n<li>March 2017 LastPass multiple security vulnerabilities in browser extensions</li>\n<li>May 2017 OneLogin unauthorized full server side access security breach</li>\n<li>May 2017 KeePass master passwords cracked with Hashcat tool</li>\n</ul>\n<p>I spent less than an hour to come up with that timeline, this is not a research exercise so it is likely i uncovered only half the total list out there and its 90% more then you knew.</p>\n<p>I read security experts publications and listen to security podcasts daily and I knew of only a handful in that list, you’re probably reading that to yourself on the train to work and passengers are thinking if they should call an ambulance for you before you pass out!</p>\n<blockquote>\n<p>Where’s the mainstream media coverage on this?</p>\n</blockquote>\n<p>Password managers are the single-point-of-failure when it comes to your passwords, they can expose all of your passwords to all of your programs, sites, secure notes, and encryption keys in just 1 breach, and I’ve just shown you 16 vulnerabilities for the leading 5 tools and you’re probably using one of these daily.</p>\n<blockquote>\n<p>This should scare you</p>\n</blockquote>\n<p>So my message is <strong>treat Password Managers as a convenience tool and <em>not</em> give them your passwords anymore</strong>.</p>\n<p>Hang on, what? That defeats the purpose you might ask, well no it doesn’t. Password managers are still useful as convenience tools even if they don’t know your whole password but rather they just need to manage a portion of the password you can’t remember yourself.</p>\n<p>Here’s the technique;</p>\n<ul>\n<li>Tell the password manager to generate the long jumble of characters as usual, and store it for you for any particular program</li>\n<li>Choose a PIN or password you will remember — DO NOT write it down anywhere, ever! So make it memorable (even if it is easy).<br>\nFor demonstration we’ll say it is 123456 (do not use 123456 yourself..)</li>\n<li>Now combine the memorable password with the generated one your password manager keeps, prefix or suffix it’s your choice, but tell the program the password is the combination of your memorable one and the one generated and stored by the password manager.<br>\nTo demonstrate, your new password might be;<br>\n$tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ<em>123456<br>\nor<br>\n123456$tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ</em></li>\n<li>Never give the password manager your memorable PIN or password</li>\n<li>Now when you use the program your password manager will fill in the password it has, it is incomplete and wont let you log into the program but all you have to do is add your memorable portion to the one pre-filled and you’re good to go.</li>\n<li>[even better] instead of append or prepend (which might be intuative to an attacher) choose a location somewhere in the middle to place your PIN and do so for every password just like a prefix or suffix but rather it is somewhere in the middle. An attacker with both the PIN and the long random string password might try suffix and prefix but to crack this they would need to try the PIN shifting by 1 character. Now, what if you have 2 PINs in there! That is not just easy for you to type but it is far more difficault to guess and harder to code an algorithm to try all the variations.</li>\n</ul>\n<p>Now if the password vault of the Password manager is compromised all the bad actor has is a collection of unusable parts of your passwords.</p>\n<p>If they are aware you have a prefix or suffix PIN or password, and they have compromised another 3rd party application that you used the PIN or password combined with the generated portion they obtained from your password vault AND they are able to access the secret memorable portion as cleartext by somehow breaking the encryption used by the 3rd party application (or it was stored cleartext in the first place, which I might wonder how they earned your trust).</p>\n<p>In this chain of low likelihood scenarios you arrive back at the current level of security you have with your chosen Password manager.</p>\n<p>Now it might be obvious to some of you, but those that know that length is the only thing that really influences password strength would agree that adding a suffix or prefix can only make passwords stronger, not weaker.</p>\n<p>By any standards, you have to admit that removing any single-point-of-failure to the access of all of your passwords in one breach can only be a good thing.</p>\n<p>On the topic of keystroke capture risks to someone learning your memorable secret — this is the case where there is a running compromise on the computer you are using, i.e. the computer is already compromised.</p>\n<p>Most, if not all, Password Manager’s openly state they cannot protect your passwords from an already compromised system. I’d suggest this includes systems that are operating a keystroke capture vulnerability where the Password Manager statement is less specific. Why? Because the Password Manager’s master passphrase can be compromised unlocking the password vault!</p>\n<p>Where the keystroke capture might unlock the Password Manager vault as-well-as learn your memorable PIN or password, they’d have to also figure out that you use these in combination, trying the prefix or suffix techniques or any other technique you’ve chosen to use to combine them (I use my own secret technique) and that is a lot of assumptions, it’s not a simple hack or breach and its more likely that the bad actor will simply try one or 2 things and move onto a more vulnerable victim whereas if you just store whole passwords in your Password Manager you are that more vulnerable victim.</p>\n<p>But of course, critique is expressly invited — bring the pain</p>\n<!--kg-card-end: markdown-->","comment_id":"5a0517e6a535cb4b5581cd78","plaintext":"You’ve heard that your password has already been compromised by LinkedIn, Yahoo,\nVodafone, Sony, Minecraft, and Snapchat to name a few of the more then 200\n[https://feeds.feedburner.com/HaveIBeenPwnedLatestBreaches] breaches I know\nabout, but do you know that the Password Managers you put trust in for all of\nyour passwords are just as vulnerable?\n\nWith the revelations of security slip-ups in password managers we’re still told\nby security experts that everything’s fine now. Patch. Keep using them. Move\nalong, but I say that’s not good enough and it’s time to get smarter and\nquestion whether our security experts are keeping up with the times and\nlistening to their younger more in touch peers.\n\n> Password Managers are no more secure from hackers then any other product\n\n\nIf we take only 5 password managers that first come to mind today we can see a\nrich history\n\n * February 2011 LastPass XSS discovered\n * May 2011 LastPass announced possible breach\n * June 2012 KeePass remote attack exploit\n * June 2015 LastPass compromised servers\n * October 2015 1PasswordAnywhere exposed PPI & user reset password links\n * November 2015 KeePass vulnerable to KeeFarce hacking tool\n * June 2016 KeePass Automatic Update Vulnerability\n * July 2016 LastPass users plaintext passwords exposed\n * August 2016 KeePass Header Authentication\n * August 2016 OneLogin application cleartext logs exposed\n * August 2016 Dashlane Universal XSS\n * September 2016 1Password user passwords exposed to subdomains\n * February 2017 1Password encrypted data exposed via Cloudflare issue\n * March 2017 LastPass multiple security vulnerabilities in browser extensions\n * May 2017 OneLogin unauthorized full server side access security breach\n * May 2017 KeePass master passwords cracked with Hashcat tool\n\nI spent less than an hour to come up with that timeline, this is not a research\nexercise so it is likely i uncovered only half the total list out there and its\n90% more then you knew.\n\nI read security experts publications and listen to security podcasts daily and I\nknew of only a handful in that list, you’re probably reading that to yourself on\nthe train to work and passengers are thinking if they should call an ambulance\nfor you before you pass out!\n\n> Where’s the mainstream media coverage on this?\n\n\nPassword managers are the single-point-of-failure when it comes to your\npasswords, they can expose all of your passwords to all of your programs, sites,\nsecure notes, and encryption keys in just 1 breach, and I’ve just shown you 16\nvulnerabilities for the leading 5 tools and you’re probably using one of these\ndaily.\n\n> This should scare you\n\n\nSo my message is treat Password Managers as a convenience tool and not give them\nyour passwords anymore.\n\nHang on, what? That defeats the purpose you might ask, well no it doesn’t.\nPassword managers are still useful as convenience tools even if they don’t know\nyour whole password but rather they just need to manage a portion of the\npassword you can’t remember yourself.\n\nHere’s the technique;\n\n * Tell the password manager to generate the long jumble of characters as usual,\n   and store it for you for any particular program\n * Choose a PIN or password you will remember — DO NOT write it down anywhere,\n   ever! So make it memorable (even if it is easy).\n   For demonstration we’ll say it is 123456 (do not use 123456 yourself..)\n * Now combine the memorable password with the generated one your password\n   manager keeps, prefix or suffix it’s your choice, but tell the program the\n   password is the combination of your memorable one and the one generated and\n   stored by the password manager.\n   To demonstrate, your new password might be;\n   $tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ123456\n   or\n   123456$tgV9YTJ6@47WqJ6dcnVtSy4^eMageJ\n * Never give the password manager your memorable PIN or password\n * Now when you use the program your password manager will fill in the password\n   it has, it is incomplete and wont let you log into the program but all you\n   have to do is add your memorable portion to the one pre-filled and you’re\n   good to go.\n * [even better] instead of append or prepend (which might be intuative to an\n   attacher) choose a location somewhere in the middle to place your PIN and do\n   so for every password just like a prefix or suffix but rather it is somewhere\n   in the middle. An attacker with both the PIN and the long random string\n   password might try suffix and prefix but to crack this they would need to try\n   the PIN shifting by 1 character. Now, what if you have 2 PINs in there! That\n   is not just easy for you to type but it is far more difficault to guess and\n   harder to code an algorithm to try all the variations.\n\nNow if the password vault of the Password manager is compromised all the bad\nactor has is a collection of unusable parts of your passwords.\n\nIf they are aware you have a prefix or suffix PIN or password, and they have\ncompromised another 3rd party application that you used the PIN or password\ncombined with the generated portion they obtained from your password vault AND\nthey are able to access the secret memorable portion as cleartext by somehow\nbreaking the encryption used by the 3rd party application (or it was stored\ncleartext in the first place, which I might wonder how they earned your trust).\n\nIn this chain of low likelihood scenarios you arrive back at the current level\nof security you have with your chosen Password manager.\n\nNow it might be obvious to some of you, but those that know that length is the\nonly thing that really influences password strength would agree that adding a\nsuffix or prefix can only make passwords stronger, not weaker.\n\nBy any standards, you have to admit that removing any single-point-of-failure to\nthe access of all of your passwords in one breach can only be a good thing.\n\nOn the topic of keystroke capture risks to someone learning your memorable\nsecret — this is the case where there is a running compromise on the computer\nyou are using, i.e. the computer is already compromised.\n\nMost, if not all, Password Manager’s openly state they cannot protect your\npasswords from an already compromised system. I’d suggest this includes systems\nthat are operating a keystroke capture vulnerability where the Password Manager\nstatement is less specific. Why? Because the Password Manager’s master\npassphrase can be compromised unlocking the password vault!\n\nWhere the keystroke capture might unlock the Password Manager vault as-well-as\nlearn your memorable PIN or password, they’d have to also figure out that you\nuse these in combination, trying the prefix or suffix techniques or any other\ntechnique you’ve chosen to use to combine them (I use my own secret technique)\nand that is a lot of assumptions, it’s not a simple hack or breach and its more\nlikely that the bad actor will simply try one or 2 things and move onto a more\nvulnerable victim whereas if you just store whole passwords in your Password\nManager you are that more vulnerable victim.\n\nBut of course, critique is expressly invited — bring the pain","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/1_cqoGlmKjtNGexibG1Xg_3w.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-10 03:07:18","created_by":"1","updated_at":"2021-03-31 14:11:59","updated_by":"1","published_at":"2017-10-15 03:16:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcad","uuid":"7b8fd67b-d32d-430d-a631-6712fed56971","title":"Chrome out of control breaking the web","slug":"chrome-out-of-control-breaking-the-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"> Standards, who needs them?\\nGoogle Chrome certainly doesn't..\\n\\nIt is blatently obvious Google's disdain of w3c, web developers, and just simple web users expecting sites they visist to continue to work while they upgrade browsers to stay secure.\\n\\nYes, disdane, Google proves their superiority complex knows no bound when they [call breaking the web an intervention](https://developers.google.com/web/updates/2017/01/scrolling-intervention#the_intervention) like its justified, basically we all want it but not realise.\\n\\nBack in July 2016 [I wrote about a new feature](__GHOST_URL__/use-passive-event-listeners-to-prevent-scroll-interruptions/) called \\\"Passive Event Listeners\\\" that were an optional opt-in nice new feature to speed up your web pages but it was **woefully immature riddled with implementation problems and gotcha's**.\\n\\nA year later and without fixing any of the usability or breaking changes, Google see's fit to make the feature [on by default](https://www.chromestatus.com/features/5093566007214080).\\nI know what you're thinking, no big deal right? Wrong, it's a huge deal, check this out. \\nTo have a working web page that uses browser events (a core feature all sites use) We now need to;\\n\\n* Feature Detect a new native browser feature detection can be used\\n* (no) Use a side-effect method of feature detection for browser events\\n* If passive event is on, `addEventListener` 3rd argument is a dictionary\\n* (no) passive events not available `addEventListener` 3rd argument is a flag for `capture`\\n* Feature detection for `defaultPrevented` (try steps 1 and 2 above)\\n* (no) freely use `e.preventDefault()` as we have for decades\\n* (yes) `e.preventDefault()` is ignored when `defaultPrevented` is available, this is Chrome's **intervention mode** with forced use of passive event listeners enabled\\n\\n> Take that web developers!\\n\\nNow what about other browsers like Firefox, IE, Edge, Safari, Mobile versions of each?\\n\\nNow what about transpilers? You like TypeScript, CoffeeScript, React, Dart, [insert other transpiled to JavaScript source code here] right? Can you even support Chrome any more?\\n\\nTranspilers alone give JavaScript the reputation of being more complex and high entry barrier then even Assembly or C. That's just transpilers alone that makes the entry barrier so high.. No one writes Prototypal JavaScript anymore, but if they did Browser vendors are now making the JavaScript ecosystem even more complex even for the lowest entry barrier then the most complex languages there is, not easier!\\n\\nIs it a good time to abandon web development altogether?\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>Standards, who needs them?<br>\nGoogle Chrome certainly doesn't..</p>\n</blockquote>\n<p>It is blatently obvious Google's disdain of w3c, web developers, and just simple web users expecting sites they visist to continue to work while they upgrade browsers to stay secure.</p>\n<p>Yes, disdane, Google proves their superiority complex knows no bound when they <a href=\"https://developers.google.com/web/updates/2017/01/scrolling-intervention#the_intervention\">call breaking the web an intervention</a> like its justified, basically we all want it but not realise.</p>\n<p>Back in July 2016 <a href=\"__GHOST_URL__/use-passive-event-listeners-to-prevent-scroll-interruptions/\">I wrote about a new feature</a> called &quot;Passive Event Listeners&quot; that were an optional opt-in nice new feature to speed up your web pages but it was <strong>woefully immature riddled with implementation problems and gotcha's</strong>.</p>\n<p>A year later and without fixing any of the usability or breaking changes, Google see's fit to make the feature <a href=\"https://www.chromestatus.com/features/5093566007214080\">on by default</a>.<br>\nI know what you're thinking, no big deal right? Wrong, it's a huge deal, check this out.<br>\nTo have a working web page that uses browser events (a core feature all sites use) We now need to;</p>\n<ul>\n<li>Feature Detect a new native browser feature detection can be used</li>\n<li>(no) Use a side-effect method of feature detection for browser events</li>\n<li>If passive event is on, <code>addEventListener</code> 3rd argument is a dictionary</li>\n<li>(no) passive events not available <code>addEventListener</code> 3rd argument is a flag for <code>capture</code></li>\n<li>Feature detection for <code>defaultPrevented</code> (try steps 1 and 2 above)</li>\n<li>(no) freely use <code>e.preventDefault()</code> as we have for decades</li>\n<li>(yes) <code>e.preventDefault()</code> is ignored when <code>defaultPrevented</code> is available, this is Chrome's <strong>intervention mode</strong> with forced use of passive event listeners enabled</li>\n</ul>\n<blockquote>\n<p>Take that web developers!</p>\n</blockquote>\n<p>Now what about other browsers like Firefox, IE, Edge, Safari, Mobile versions of each?</p>\n<p>Now what about transpilers? You like TypeScript, CoffeeScript, React, Dart, [insert other transpiled to JavaScript source code here] right? Can you even support Chrome any more?</p>\n<p>Transpilers alone give JavaScript the reputation of being more complex and high entry barrier then even Assembly or C. That's just transpilers alone that makes the entry barrier so high.. No one writes Prototypal JavaScript anymore, but if they did Browser vendors are now making the JavaScript ecosystem even more complex even for the lowest entry barrier then the most complex languages there is, not easier!</p>\n<p>Is it a good time to abandon web development altogether?</p>\n<!--kg-card-end: markdown-->","comment_id":"5a052821fc03850577937220","plaintext":"> Standards, who needs them?\nGoogle Chrome certainly doesn't..\n\n\nIt is blatently obvious Google's disdain of w3c, web developers, and just simple\nweb users expecting sites they visist to continue to work while they upgrade\nbrowsers to stay secure.\n\nYes, disdane, Google proves their superiority complex knows no bound when they \ncall breaking the web an intervention\n[https://developers.google.com/web/updates/2017/01/scrolling-intervention#the_intervention] \nlike its justified, basically we all want it but not realise.\n\nBack in July 2016 I wrote about a new feature\n[/use-passive-event-listeners-to-prevent-scroll-interruptions/] called \"Passive\nEvent Listeners\" that were an optional opt-in nice new feature to speed up your\nweb pages but it was woefully immature riddled with implementation problems and\ngotcha's.\n\nA year later and without fixing any of the usability or breaking changes, Google\nsee's fit to make the feature on by default\n[https://www.chromestatus.com/features/5093566007214080].\nI know what you're thinking, no big deal right? Wrong, it's a huge deal, check\nthis out.\nTo have a working web page that uses browser events (a core feature all sites\nuse) We now need to;\n\n * Feature Detect a new native browser feature detection can be used\n * (no) Use a side-effect method of feature detection for browser events\n * If passive event is on, addEventListener 3rd argument is a dictionary\n * (no) passive events not available addEventListener 3rd argument is a flag for \n   capture\n * Feature detection for defaultPrevented (try steps 1 and 2 above)\n * (no) freely use e.preventDefault() as we have for decades\n * (yes) e.preventDefault() is ignored when defaultPrevented is available, this\n   is Chrome's intervention mode with forced use of passive event listeners\n   enabled\n\n> Take that web developers!\n\n\nNow what about other browsers like Firefox, IE, Edge, Safari, Mobile versions of\neach?\n\nNow what about transpilers? You like TypeScript, CoffeeScript, React, Dart,\n[insert other transpiled to JavaScript source code here] right? Can you even\nsupport Chrome any more?\n\nTranspilers alone give JavaScript the reputation of being more complex and high\nentry barrier then even Assembly or C. That's just transpilers alone that makes\nthe entry barrier so high.. No one writes Prototypal JavaScript anymore, but if\nthey did Browser vendors are now making the JavaScript ecosystem even more\ncomplex even for the lowest entry barrier then the most complex languages there\nis, not easier!\n\nIs it a good time to abandon web development altogether?","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/google_dont_be_evil.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-10 04:16:33","created_by":"1","updated_at":"2021-03-31 14:12:08","updated_by":"1","published_at":"2017-11-09 08:50:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcae","uuid":"84165c25-175d-41b1-b9fa-4087d3fe50cb","title":"Perfecting the Dockerfile","slug":"perfecting-the-dockerfile","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"There are plenty of Docker best practices articles out there, this is not one, exactly.. I will instead talk to maintainability, efficiency, and security covering best practices where relevant.\\n\\nI've been using Docker for a couple of years now, it seems like a lifetime ago I'd jump into a new project and setup Vagrant.\\n\\nJust like we had to perfect the `Vagrantfile` with its complicated network and volume sharing, so too does the `Dockerfile` present certain challenges (but we finally have nice shared networks and volumes).\\n\\nWhat does a basic Dockerfile look like?\\n\\n```\\nFROM python:2.7\\n\\nENV APP_ROOT=/usr/src/app\\n\\nADD requirements.txt .\\nRUN pip install -r requirements.txt\\n\\nADD ./src $APP_ROOT/src\\n\\nCMD [\\\"python\\\", \\\"command.py\\\", \\\"start\\\"]\\n```\\n\\nLooks simple enough, what can go wrong?\\n\\n- Basics; FROM, WORKDIR, ENV, ARG, and LABEL\\n- Requiring file resources; side-effects of ADD\\n- Reducing image size\\n  - Caching (layers)\\n  - Caching (combined commands)\\n  - Caching (optimisation)\\n  - Ignoring files\\n- Change the executable; using ENTRYPOINT and CMD\\n- Mapping volumes or mounts\\n- Security\\n  - Root access prevention\\n  - Lateral movement\\n- Monitoring; using HEALTHCHECK\\n\\nThat's a lot to cover.\\n\\n## Basics\\n\\nThe `FROM` keyword takes a base container and extends it with layer for your purpose.\\n\\nA base is usually a operating system such as `FROM ubuntu:17.10` but it is becoming more common to choose a combination of programming language and O/S `FROM python:2.7-debian`.\\n\\nIn the example Dockerfile we didn't have `WORKDIR` which is a useful basic and does what it says on the box, sets the containers following commands to be executed in the working directory defined `WORKDIR /usr/src`. When you start a session into a running container you will be in this directory.\\n\\nNext we look at `ENV` which is a helpful tool when building a container but comes with side-effects.\\n\\n```\\nFROM python:2.7\\nENV APP_ROOT=/usr/src/app\\nENV PG_MAJOR 9.3\\nENV PG_VERSION 9.3.4\\n\\nWORKDIR $APP_ROOT\\nRUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && …\\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\\n\\nADD requirements.txt .\\nRUN pip install -r requirements.txt\\n\\nCMD [\\\"python\\\", \\\"command.py\\\", \\\"start\\\"]\\n```\\n\\nHere we use `ENV` to set the WORKDIR and define software versions that get installed and the side-effect is in the `ENV PATH` declaration as this will change the existing O/S environment variable. You can pass in new values at build time using `--env APP_ROOT=/usr/src`\\n\\nTo avoid side-effects and still have the benefits of parameterising our build we can use `ARG` instead of `ENV`.\\n\\nWith ARG we can take a build argument from the executor `ARG PG_VERSION` used as `--build-arg PG_VERSION=9.3.4` and then in the Dockerfile `$PG_VERSION` has the value `9.3.4`. We can use a default in the Dockerfile `ARG PG_VERSION 9.3.4` so that the `--build-arg` is not mandatory.\\n\\nYou can combine `ENV` and `ARG` also;\\n\\n```\\nENV PG_MAJOR 9.3\\nENV PG_VERSION ${PG_MAJOR}.0\\n```\\n\\nNow the value of `$PG_VERSION` is `9.3.0` but when we pass `--env PG_MAJOR=9.2` then the ARG `$PG_VERSION` is now `9.2.0` from that ENV change.\\n\\nLastly we look at `LABEL` which simply adds metadata to the final image. Any key value pair can be given without side-effects `LABEL version=\\\"1.0\\\"` will not effect any other versions unless there was a LABEL called version already.\\n\\n## Requiring file resources\\n\\nThe no side-effects way to require files is using `COPY`, simply give it source on host and destination in container values;\\n\\n```\\nCOPY ./requirements.txt /usr/src/app/requirements.txt\\nCOPY ./src /usr/src/app\\n```\\n\\nBoth relative and absolute works here, and copy in sequence over-writes so if there was a `requirements.txt` fine in `/src` also, it will over-write the one in root `./requirements.txt`.\\n\\nBe careful when using `ADD`, and avoid it unless you actually know why you are using it. ADD takes 2 arguments, remote source file and local destination directory.\\n\\n```\\nADD http://example.com/foobar.tar.gz /tmp/\\n```\\n\\nIt will resolve, download, and auto-extract files in the archive to the destination `/tmp`.\\n\\n```\\nRUN curl -SL http://example.com/foobar.tar.gz && \\\\\\n  tar -xfz foobar.tar.gz -C /tmp/\\n```\\n\\nBut works for all operating systems as it's executed by the host not inside the container however the layer itself is quite large.\\n\\n## Reducing image size \\n\\nDocker containers are made up of layers each contributing to the final image size itself, so if you are not careful a simple app can become larger then 1gb easily.\\n\\n### Caching (layers)\\n\\nReducing layers is a great way to achieve smaller image sizes, lets take this example\\n\\n```\\nFROM alpine:latest\\n\\nRUN apk update\\nRUN apk add --update ca-certificates wget\\nRUN update-ca-certificates\\nRUN wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- | tar xzC /tmp/\\nRUN cp /tmp/docker/* /usr/bin/\\n```\\n\\nOutputs 6 layers at 197MB\\n\\n```\\n~$ docker image list\\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\\nadd-test            latest             106ef90bfa53        Just now   197MB\\n```\\n\\nNow combine into a single RUN command\\n\\n```\\nFROM alpine:latest\\n\\nRUN apk update \\\\\\n    && apk --no-cache add --update --virtual build-deps ca-certificates wget \\\\\\n    && update-ca-certificates \\\\\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\\\\n    | tar xzC /tmp/ \\\\\\n    && cp /tmp/docker/* /usr/bin/ \\\\\\n    && rm -rf /tmp/ \\\\\\n    && apk del build-deps \\\\\\n    && rm -rf /var/cache/apk/*\\n```\\n\\nOutputs 2 layers though still with a total 197MB we now speed up subsequent builds as there are less layers to compute and fetch from cache or build anew.\\n\\n### Caching (combined commands)\\n\\nIn the last example we saw how combining commands works to improve build times leveraging cached layers better, now we will look at how we can combine even more commands to reduce the image size;\\n\\n```\\nFROM alpine:latest\\n\\nRUN apk update \\\\\\n    && apk add --update ca-certificates wget \\\\\\n    && update-ca-certificates \\\\\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\\\\n    | tar xzC /tmp/ \\\\\\n    && cp /tmp/docker/* /usr/bin/ \\\\\\n    && rm -rf /tmp/ \\\\\\n    && apk del ca-certificates wget \\\\\\n    && rm -rf /var/cache/apk/*\\n```\\n\\nOutputs 2 layers still but now only 99.2MB\\n\\n```\\n~$ docker image list\\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\\nadd-test            latest             7b6ba76d4fc3        Just now   99.2MB\\n```\\n\\nI utilised the package manager to clean up our build only programs that aren't needed to run our app, as well as removing any temp files for the build.\\n\\nAnother useful feature in Alpine package manager we don't have in many others is the ability to tell apk that these packages are build only packages so they can be cleaned up later with only 1 command argument\\n\\n```\\nFROM alpine:latest\\n\\nRUN apk update \\\\\\n    && apk add --update --virtual build-deps ca-certificates wget \\\\\\n    && update-ca-certificates \\\\\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\\\\n    | tar xzC /tmp/ \\\\\\n    && cp /tmp/docker/* /usr/bin/ \\\\\\n    && apk del build-deps \\\\\\n    && rm -rf /var/cache/apk/*\\n```\\n\\nThe size and build time is unchanged, but it is now easier to manage our build only programs as our app grows.\\n\\n### Caching (optimisation)\\n\\nWe've seen how less layers speeds up our build times when layers are built from cache, but sometimes things change between builds forcing the layers to skip cache.\\n\\nIn Docker, if any layer is changed all layers after it will be rebuilt too, so ensure you order your layers not only by logical build steps, but also by least changed steps first.\\n\\n```\\nFROM alpine:latest\\n\\nCOPY ./src /usr/src\\n\\nRUN apk update \\\\\\n    && apk add --update --virtual build-deps ca-certificates wget \\\\\\n    && update-ca-certificates \\\\\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\\\\n    | tar xzC /tmp/ \\\\\\n    && cp /tmp/docker/* /usr/bin/ \\\\\\n    && apk del build-deps \\\\\\n    && rm -rf /var/cache/apk/*\\n```\\n\\nAbove, the app source files will change between builds, so having it at the top forces Docker to rebuild all layers after the COPY. Move most likely to change often steps the the bottom to utilise cached layers better.\\n\\n### Ignoring files\\n\\nIf you're familiar with how `.gitignore` is used to keep your git repo small, tidy, and free of large files such as unwanted archives and binaries in your tree - the same applies to the `.dockerignore` file.\\n\\nPlace `.dockerignore` in your project root (where you run docker cli) and during builds the docker cli will ignore unwanted files.\\n\\nIf you are developing in one O/S and deploying to another (even another based on Linux) it is important to consider that binaries will compile differently on your host then they should be for the container so ensure you ignore all your dependency directories so they are built inside the container.\\n\\n## Change the executable\\n\\nUsing `ENTRYPOINT` and `CMD` can be confusing.\\n\\nYou might need to be familiar with Linux process and sessions to fully grasp this, so I'll do my best to keep this high level enough for Docker users unfamiliar with Linux.\\n\\nThink of ENTRYPOINT as the base program you want the container to run, because a Docker container can only run 1 program while it is running. When you start an interactive shell (terminal session) you generally tell Docker to use `/bin/sh` or `/bin/bash` depending on O/S as the base program and during that interactive session you type out commands like `ls`, `cd`, `java` for example, these are the `CMD` in your Dockerfile!\\n\\nSo if you were to run `docker run -it alpine` to start a session and then `ls /usr` (or just one line `docker run -it alpine ls /usr`) it is the same as;\\n\\n```\\nFROM apline\\nENTRYPOINT /bin/sh\\nCMD [\\\"ls\\\", \\\"/usr\\\"]\\n```\\n\\nSo you can see how this might be useful to change to something like (if you have a Nodejs app);\\n\\n```\\nFROM node\\nENTRYPOINT /usr/local/bin/npm\\nCMD [\\\"run\\\", \\\"start-server\\\"]\\n```\\n\\n## Mapping volumes or mounts\\n\\nYou need to make a decision whether you want to have a mount or a volume.\\n\\nBoth have all of the same arguments available on the cli but if you're intending to run the docker container as a service you can only use mounts whereas if you intend to use a plugin or driver such as `vieux/sshfs` which is a file system over ssh, you must use a volume to make use of volume drivers.\\n    \\nIn a Dockerfile you can define what the expected `VOLUMES` should be, but it's not possible to mount a host directory using a Dockerfile by convention because images should be portable.\\nYou must map the volume when you run the container, e.g. `docker run -v \\\"$(pwd)\\\"/src:/usr/src/app:ro` or `docker run --mount type=bind,source=\\\"$(pwd)\\\"/src,target=/usr/src/app,readonly`.\\n\\n\\n## Security\\n\\nWe talked about volumes and mounts, regardless if you choose mounts or volumes you should always make them read-only for security reasons.\\nNever choose `shared` bind propagation on a mount point and Docker is secure by default meaning the most secure option `rprivate` is also the default but it is best to be explicit on this so `docker run --mount type=bind,source=\\\"$(pwd)\\\"/src,target=/usr/src/app,readonly,bind-propagation=rprivate`.\\n\\nSome other points;\\n\\n- Do not docker exec commands with privileged option\\n- Configure centralized and remote logging\\n- Do not disable default seccomp profile\\n- Use either apparmor or selinux\\n- Set a non-host user with `useradd`\\n- Set `--ulimit` and `--memory` when running a Docker container\\n- Use docker secrets instead of environment or configuration alternatives\\n- Strictly no SSH access into containers\\n- Scan Dockerfiles with Docker Bench (or chosen proprietary alternative)\\n\\n### Root access prevention\\n\\nThe SUID flag on binaries has a vulnerability where intruders have a vector for assuming root access to the host. It's best to just remove these as it's unlikely you'll be sing them from your app.\\n\\n```\\nRUN for i in `find / -perm +6000 -type f`; do rm -f $i; done\\n```\\n\\nIf you're unsure and removing them breaks stuff you can also unset the flag on each file with\\n\\n```\\nRUN for i in `find / -perm +6000 -type f`; do chmod a-s $i; done\\n```\\n\\nBut if you want finer control while keeping the binaries and know your way around Linux capability controls you can use something like this\\n\\n```\\nRUN setcap cap_net_raw+p /bin/ping\\n```\\n\\nWhich unsets SUID and allows the use of RAW and PACKET sockets.\\n\\nIt was mentioned above, but another control is having a restricted user that is not a host user, you can create one by running\\n\\n```\\nRUN adduser --system --no-create-home --disabled-password --disabled-login --shell /bin/sh myappuser\\nUSER myappuser\\n```\\n\\nEach command after `USER` is executed as the new restricted user, so do this at the top of the Dockerfile.\\n\\n### Lateral movement\\n\\nUsing build arguments `--icc`, `--link`, and `--iptables` flags control which containers are allowed inter-container communication which is the risk mechanism used for moving from one hacked system to another.\\n\\nDocker prides itself that it is security first, but this is one case where convenience was chosen at the compromise of security because by default all containers may communicate with one another freely.\\n\\n## Monitoring\\n\\nI recommend using HEALTHCHECK on every container without exception because Docker must be running a process to stay active so inherently there is something to monitor.\\n\\nHEALTHCHECK uses non-zero exit codes to detect a processes health, so a simply check for a web server would be;\\n\\n```\\nHEALTHCHECK --interval=12s --timeout=12s --start-period=30s \\\\\\n    CMD curl --silent --fail https://localhost:36000/ || exit 1\\n```\\n\\nOf course if you're on Windows curl is either not there or it's alias to powershell Invoke-WebRequest is there but the arguments aren't the same, therefore it is best not to use any host O/S specific programs and write one in python, go, java, nodejs, whatever language you feel comfortable using - just make sure it emits proper exit codes.\\n\\n## Thank you for reading and don't forget to share this if you found it interesting.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>There are plenty of Docker best practices articles out there, this is not one, exactly.. I will instead talk to maintainability, efficiency, and security covering best practices where relevant.</p>\n<p>I've been using Docker for a couple of years now, it seems like a lifetime ago I'd jump into a new project and setup Vagrant.</p>\n<p>Just like we had to perfect the <code>Vagrantfile</code> with its complicated network and volume sharing, so too does the <code>Dockerfile</code> present certain challenges (but we finally have nice shared networks and volumes).</p>\n<p>What does a basic Dockerfile look like?</p>\n<pre><code>FROM python:2.7\n\nENV APP_ROOT=/usr/src/app\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nADD ./src $APP_ROOT/src\n\nCMD [&quot;python&quot;, &quot;command.py&quot;, &quot;start&quot;]\n</code></pre>\n<p>Looks simple enough, what can go wrong?</p>\n<ul>\n<li>Basics; FROM, WORKDIR, ENV, ARG, and LABEL</li>\n<li>Requiring file resources; side-effects of ADD</li>\n<li>Reducing image size\n<ul>\n<li>Caching (layers)</li>\n<li>Caching (combined commands)</li>\n<li>Caching (optimisation)</li>\n<li>Ignoring files</li>\n</ul>\n</li>\n<li>Change the executable; using ENTRYPOINT and CMD</li>\n<li>Mapping volumes or mounts</li>\n<li>Security\n<ul>\n<li>Root access prevention</li>\n<li>Lateral movement</li>\n</ul>\n</li>\n<li>Monitoring; using HEALTHCHECK</li>\n</ul>\n<p>That's a lot to cover.</p>\n<h2 id=\"basics\">Basics</h2>\n<p>The <code>FROM</code> keyword takes a base container and extends it with layer for your purpose.</p>\n<p>A base is usually a operating system such as <code>FROM ubuntu:17.10</code> but it is becoming more common to choose a combination of programming language and O/S <code>FROM python:2.7-debian</code>.</p>\n<p>In the example Dockerfile we didn't have <code>WORKDIR</code> which is a useful basic and does what it says on the box, sets the containers following commands to be executed in the working directory defined <code>WORKDIR /usr/src</code>. When you start a session into a running container you will be in this directory.</p>\n<p>Next we look at <code>ENV</code> which is a helpful tool when building a container but comes with side-effects.</p>\n<pre><code>FROM python:2.7\nENV APP_ROOT=/usr/src/app\nENV PG_MAJOR 9.3\nENV PG_VERSION 9.3.4\n\nWORKDIR $APP_ROOT\nRUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [&quot;python&quot;, &quot;command.py&quot;, &quot;start&quot;]\n</code></pre>\n<p>Here we use <code>ENV</code> to set the WORKDIR and define software versions that get installed and the side-effect is in the <code>ENV PATH</code> declaration as this will change the existing O/S environment variable. You can pass in new values at build time using <code>--env APP_ROOT=/usr/src</code></p>\n<p>To avoid side-effects and still have the benefits of parameterising our build we can use <code>ARG</code> instead of <code>ENV</code>.</p>\n<p>With ARG we can take a build argument from the executor <code>ARG PG_VERSION</code> used as <code>--build-arg PG_VERSION=9.3.4</code> and then in the Dockerfile <code>$PG_VERSION</code> has the value <code>9.3.4</code>. We can use a default in the Dockerfile <code>ARG PG_VERSION 9.3.4</code> so that the <code>--build-arg</code> is not mandatory.</p>\n<p>You can combine <code>ENV</code> and <code>ARG</code> also;</p>\n<pre><code>ENV PG_MAJOR 9.3\nENV PG_VERSION ${PG_MAJOR}.0\n</code></pre>\n<p>Now the value of <code>$PG_VERSION</code> is <code>9.3.0</code> but when we pass <code>--env PG_MAJOR=9.2</code> then the ARG <code>$PG_VERSION</code> is now <code>9.2.0</code> from that ENV change.</p>\n<p>Lastly we look at <code>LABEL</code> which simply adds metadata to the final image. Any key value pair can be given without side-effects <code>LABEL version=&quot;1.0&quot;</code> will not effect any other versions unless there was a LABEL called version already.</p>\n<h2 id=\"requiringfileresources\">Requiring file resources</h2>\n<p>The no side-effects way to require files is using <code>COPY</code>, simply give it source on host and destination in container values;</p>\n<pre><code>COPY ./requirements.txt /usr/src/app/requirements.txt\nCOPY ./src /usr/src/app\n</code></pre>\n<p>Both relative and absolute works here, and copy in sequence over-writes so if there was a <code>requirements.txt</code> fine in <code>/src</code> also, it will over-write the one in root <code>./requirements.txt</code>.</p>\n<p>Be careful when using <code>ADD</code>, and avoid it unless you actually know why you are using it. ADD takes 2 arguments, remote source file and local destination directory.</p>\n<pre><code>ADD http://example.com/foobar.tar.gz /tmp/\n</code></pre>\n<p>It will resolve, download, and auto-extract files in the archive to the destination <code>/tmp</code>.</p>\n<pre><code>RUN curl -SL http://example.com/foobar.tar.gz &amp;&amp; \\\n  tar -xfz foobar.tar.gz -C /tmp/\n</code></pre>\n<p>But works for all operating systems as it's executed by the host not inside the container however the layer itself is quite large.</p>\n<h2 id=\"reducingimagesize\">Reducing image size</h2>\n<p>Docker containers are made up of layers each contributing to the final image size itself, so if you are not careful a simple app can become larger then 1gb easily.</p>\n<h3 id=\"cachinglayers\">Caching (layers)</h3>\n<p>Reducing layers is a great way to achieve smaller image sizes, lets take this example</p>\n<pre><code>FROM alpine:latest\n\nRUN apk update\nRUN apk add --update ca-certificates wget\nRUN update-ca-certificates\nRUN wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- | tar xzC /tmp/\nRUN cp /tmp/docker/* /usr/bin/\n</code></pre>\n<p>Outputs 6 layers at 197MB</p>\n<pre><code>~$ docker image list\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\nadd-test            latest             106ef90bfa53        Just now   197MB\n</code></pre>\n<p>Now combine into a single RUN command</p>\n<pre><code>FROM alpine:latest\n\nRUN apk update \\\n    &amp;&amp; apk --no-cache add --update --virtual build-deps ca-certificates wget \\\n    &amp;&amp; update-ca-certificates \\\n    &amp;&amp; wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    &amp;&amp; cp /tmp/docker/* /usr/bin/ \\\n    &amp;&amp; rm -rf /tmp/ \\\n    &amp;&amp; apk del build-deps \\\n    &amp;&amp; rm -rf /var/cache/apk/*\n</code></pre>\n<p>Outputs 2 layers though still with a total 197MB we now speed up subsequent builds as there are less layers to compute and fetch from cache or build anew.</p>\n<h3 id=\"cachingcombinedcommands\">Caching (combined commands)</h3>\n<p>In the last example we saw how combining commands works to improve build times leveraging cached layers better, now we will look at how we can combine even more commands to reduce the image size;</p>\n<pre><code>FROM alpine:latest\n\nRUN apk update \\\n    &amp;&amp; apk add --update ca-certificates wget \\\n    &amp;&amp; update-ca-certificates \\\n    &amp;&amp; wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    &amp;&amp; cp /tmp/docker/* /usr/bin/ \\\n    &amp;&amp; rm -rf /tmp/ \\\n    &amp;&amp; apk del ca-certificates wget \\\n    &amp;&amp; rm -rf /var/cache/apk/*\n</code></pre>\n<p>Outputs 2 layers still but now only 99.2MB</p>\n<pre><code>~$ docker image list\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\nadd-test            latest             7b6ba76d4fc3        Just now   99.2MB\n</code></pre>\n<p>I utilised the package manager to clean up our build only programs that aren't needed to run our app, as well as removing any temp files for the build.</p>\n<p>Another useful feature in Alpine package manager we don't have in many others is the ability to tell apk that these packages are build only packages so they can be cleaned up later with only 1 command argument</p>\n<pre><code>FROM alpine:latest\n\nRUN apk update \\\n    &amp;&amp; apk add --update --virtual build-deps ca-certificates wget \\\n    &amp;&amp; update-ca-certificates \\\n    &amp;&amp; wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    &amp;&amp; cp /tmp/docker/* /usr/bin/ \\\n    &amp;&amp; apk del build-deps \\\n    &amp;&amp; rm -rf /var/cache/apk/*\n</code></pre>\n<p>The size and build time is unchanged, but it is now easier to manage our build only programs as our app grows.</p>\n<h3 id=\"cachingoptimisation\">Caching (optimisation)</h3>\n<p>We've seen how less layers speeds up our build times when layers are built from cache, but sometimes things change between builds forcing the layers to skip cache.</p>\n<p>In Docker, if any layer is changed all layers after it will be rebuilt too, so ensure you order your layers not only by logical build steps, but also by least changed steps first.</p>\n<pre><code>FROM alpine:latest\n\nCOPY ./src /usr/src\n\nRUN apk update \\\n    &amp;&amp; apk add --update --virtual build-deps ca-certificates wget \\\n    &amp;&amp; update-ca-certificates \\\n    &amp;&amp; wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    &amp;&amp; cp /tmp/docker/* /usr/bin/ \\\n    &amp;&amp; apk del build-deps \\\n    &amp;&amp; rm -rf /var/cache/apk/*\n</code></pre>\n<p>Above, the app source files will change between builds, so having it at the top forces Docker to rebuild all layers after the COPY. Move most likely to change often steps the the bottom to utilise cached layers better.</p>\n<h3 id=\"ignoringfiles\">Ignoring files</h3>\n<p>If you're familiar with how <code>.gitignore</code> is used to keep your git repo small, tidy, and free of large files such as unwanted archives and binaries in your tree - the same applies to the <code>.dockerignore</code> file.</p>\n<p>Place <code>.dockerignore</code> in your project root (where you run docker cli) and during builds the docker cli will ignore unwanted files.</p>\n<p>If you are developing in one O/S and deploying to another (even another based on Linux) it is important to consider that binaries will compile differently on your host then they should be for the container so ensure you ignore all your dependency directories so they are built inside the container.</p>\n<h2 id=\"changetheexecutable\">Change the executable</h2>\n<p>Using <code>ENTRYPOINT</code> and <code>CMD</code> can be confusing.</p>\n<p>You might need to be familiar with Linux process and sessions to fully grasp this, so I'll do my best to keep this high level enough for Docker users unfamiliar with Linux.</p>\n<p>Think of ENTRYPOINT as the base program you want the container to run, because a Docker container can only run 1 program while it is running. When you start an interactive shell (terminal session) you generally tell Docker to use <code>/bin/sh</code> or <code>/bin/bash</code> depending on O/S as the base program and during that interactive session you type out commands like <code>ls</code>, <code>cd</code>, <code>java</code> for example, these are the <code>CMD</code> in your Dockerfile!</p>\n<p>So if you were to run <code>docker run -it alpine</code> to start a session and then <code>ls /usr</code> (or just one line <code>docker run -it alpine ls /usr</code>) it is the same as;</p>\n<pre><code>FROM apline\nENTRYPOINT /bin/sh\nCMD [&quot;ls&quot;, &quot;/usr&quot;]\n</code></pre>\n<p>So you can see how this might be useful to change to something like (if you have a Nodejs app);</p>\n<pre><code>FROM node\nENTRYPOINT /usr/local/bin/npm\nCMD [&quot;run&quot;, &quot;start-server&quot;]\n</code></pre>\n<h2 id=\"mappingvolumesormounts\">Mapping volumes or mounts</h2>\n<p>You need to make a decision whether you want to have a mount or a volume.</p>\n<p>Both have all of the same arguments available on the cli but if you're intending to run the docker container as a service you can only use mounts whereas if you intend to use a plugin or driver such as <code>vieux/sshfs</code> which is a file system over ssh, you must use a volume to make use of volume drivers.</p>\n<p>In a Dockerfile you can define what the expected <code>VOLUMES</code> should be, but it's not possible to mount a host directory using a Dockerfile by convention because images should be portable.<br>\nYou must map the volume when you run the container, e.g. <code>docker run -v &quot;$(pwd)&quot;/src:/usr/src/app:ro</code> or <code>docker run --mount type=bind,source=&quot;$(pwd)&quot;/src,target=/usr/src/app,readonly</code>.</p>\n<h2 id=\"security\">Security</h2>\n<p>We talked about volumes and mounts, regardless if you choose mounts or volumes you should always make them read-only for security reasons.<br>\nNever choose <code>shared</code> bind propagation on a mount point and Docker is secure by default meaning the most secure option <code>rprivate</code> is also the default but it is best to be explicit on this so <code>docker run --mount type=bind,source=&quot;$(pwd)&quot;/src,target=/usr/src/app,readonly,bind-propagation=rprivate</code>.</p>\n<p>Some other points;</p>\n<ul>\n<li>Do not docker exec commands with privileged option</li>\n<li>Configure centralized and remote logging</li>\n<li>Do not disable default seccomp profile</li>\n<li>Use either apparmor or selinux</li>\n<li>Set a non-host user with <code>useradd</code></li>\n<li>Set <code>--ulimit</code> and <code>--memory</code> when running a Docker container</li>\n<li>Use docker secrets instead of environment or configuration alternatives</li>\n<li>Strictly no SSH access into containers</li>\n<li>Scan Dockerfiles with Docker Bench (or chosen proprietary alternative)</li>\n</ul>\n<h3 id=\"rootaccessprevention\">Root access prevention</h3>\n<p>The SUID flag on binaries has a vulnerability where intruders have a vector for assuming root access to the host. It's best to just remove these as it's unlikely you'll be sing them from your app.</p>\n<pre><code>RUN for i in `find / -perm +6000 -type f`; do rm -f $i; done\n</code></pre>\n<p>If you're unsure and removing them breaks stuff you can also unset the flag on each file with</p>\n<pre><code>RUN for i in `find / -perm +6000 -type f`; do chmod a-s $i; done\n</code></pre>\n<p>But if you want finer control while keeping the binaries and know your way around Linux capability controls you can use something like this</p>\n<pre><code>RUN setcap cap_net_raw+p /bin/ping\n</code></pre>\n<p>Which unsets SUID and allows the use of RAW and PACKET sockets.</p>\n<p>It was mentioned above, but another control is having a restricted user that is not a host user, you can create one by running</p>\n<pre><code>RUN adduser --system --no-create-home --disabled-password --disabled-login --shell /bin/sh myappuser\nUSER myappuser\n</code></pre>\n<p>Each command after <code>USER</code> is executed as the new restricted user, so do this at the top of the Dockerfile.</p>\n<h3 id=\"lateralmovement\">Lateral movement</h3>\n<p>Using build arguments <code>--icc</code>, <code>--link</code>, and <code>--iptables</code> flags control which containers are allowed inter-container communication which is the risk mechanism used for moving from one hacked system to another.</p>\n<p>Docker prides itself that it is security first, but this is one case where convenience was chosen at the compromise of security because by default all containers may communicate with one another freely.</p>\n<h2 id=\"monitoring\">Monitoring</h2>\n<p>I recommend using HEALTHCHECK on every container without exception because Docker must be running a process to stay active so inherently there is something to monitor.</p>\n<p>HEALTHCHECK uses non-zero exit codes to detect a processes health, so a simply check for a web server would be;</p>\n<pre><code>HEALTHCHECK --interval=12s --timeout=12s --start-period=30s \\\n    CMD curl --silent --fail https://localhost:36000/ || exit 1\n</code></pre>\n<p>Of course if you're on Windows curl is either not there or it's alias to powershell Invoke-WebRequest is there but the arguments aren't the same, therefore it is best not to use any host O/S specific programs and write one in python, go, java, nodejs, whatever language you feel comfortable using - just make sure it emits proper exit codes.</p>\n<h2 id=\"thankyouforreadinganddontforgettosharethisifyoufounditinteresting\">Thank you for reading and don't forget to share this if you found it interesting.</h2>\n<!--kg-card-end: markdown-->","comment_id":"5a0975ecb5acfb056b46d584","plaintext":"There are plenty of Docker best practices articles out there, this is not one,\nexactly.. I will instead talk to maintainability, efficiency, and security\ncovering best practices where relevant.\n\nI've been using Docker for a couple of years now, it seems like a lifetime ago\nI'd jump into a new project and setup Vagrant.\n\nJust like we had to perfect the Vagrantfile with its complicated network and\nvolume sharing, so too does the Dockerfile present certain challenges (but we\nfinally have nice shared networks and volumes).\n\nWhat does a basic Dockerfile look like?\n\nFROM python:2.7\n\nENV APP_ROOT=/usr/src/app\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nADD ./src $APP_ROOT/src\n\nCMD [\"python\", \"command.py\", \"start\"]\n\n\nLooks simple enough, what can go wrong?\n\n * Basics; FROM, WORKDIR, ENV, ARG, and LABEL\n * Requiring file resources; side-effects of ADD\n * Reducing image size * Caching (layers)\n    * Caching (combined commands)\n    * Caching (optimisation)\n    * Ignoring files\n   \n   \n * Change the executable; using ENTRYPOINT and CMD\n * Mapping volumes or mounts\n * Security * Root access prevention\n    * Lateral movement\n   \n   \n * Monitoring; using HEALTHCHECK\n\nThat's a lot to cover.\n\nBasics\nThe FROM keyword takes a base container and extends it with layer for your\npurpose.\n\nA base is usually a operating system such as FROM ubuntu:17.10 but it is\nbecoming more common to choose a combination of programming language and O/S \nFROM python:2.7-debian.\n\nIn the example Dockerfile we didn't have WORKDIR which is a useful basic and\ndoes what it says on the box, sets the containers following commands to be\nexecuted in the working directory defined WORKDIR /usr/src. When you start a\nsession into a running container you will be in this directory.\n\nNext we look at ENV which is a helpful tool when building a container but comes\nwith side-effects.\n\nFROM python:2.7\nENV APP_ROOT=/usr/src/app\nENV PG_MAJOR 9.3\nENV PG_VERSION 9.3.4\n\nWORKDIR $APP_ROOT\nRUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && …\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python\", \"command.py\", \"start\"]\n\n\nHere we use ENV to set the WORKDIR and define software versions that get\ninstalled and the side-effect is in the ENV PATH declaration as this will change\nthe existing O/S environment variable. You can pass in new values at build time\nusing --env APP_ROOT=/usr/src\n\nTo avoid side-effects and still have the benefits of parameterising our build we\ncan use ARG instead of ENV.\n\nWith ARG we can take a build argument from the executor ARG PG_VERSION used as \n--build-arg PG_VERSION=9.3.4 and then in the Dockerfile $PG_VERSION has the\nvalue 9.3.4. We can use a default in the Dockerfile ARG PG_VERSION 9.3.4 so that\nthe --build-arg is not mandatory.\n\nYou can combine ENV and ARG also;\n\nENV PG_MAJOR 9.3\nENV PG_VERSION ${PG_MAJOR}.0\n\n\nNow the value of $PG_VERSION is 9.3.0 but when we pass --env PG_MAJOR=9.2 then\nthe ARG $PG_VERSION is now 9.2.0 from that ENV change.\n\nLastly we look at LABEL which simply adds metadata to the final image. Any key\nvalue pair can be given without side-effects LABEL version=\"1.0\" will not effect\nany other versions unless there was a LABEL called version already.\n\nRequiring file resources\nThe no side-effects way to require files is using COPY, simply give it source on\nhost and destination in container values;\n\nCOPY ./requirements.txt /usr/src/app/requirements.txt\nCOPY ./src /usr/src/app\n\n\nBoth relative and absolute works here, and copy in sequence over-writes so if\nthere was a requirements.txt fine in /src also, it will over-write the one in\nroot ./requirements.txt.\n\nBe careful when using ADD, and avoid it unless you actually know why you are\nusing it. ADD takes 2 arguments, remote source file and local destination\ndirectory.\n\nADD http://example.com/foobar.tar.gz /tmp/\n\n\nIt will resolve, download, and auto-extract files in the archive to the\ndestination /tmp.\n\nRUN curl -SL http://example.com/foobar.tar.gz && \\\n  tar -xfz foobar.tar.gz -C /tmp/\n\n\nBut works for all operating systems as it's executed by the host not inside the\ncontainer however the layer itself is quite large.\n\nReducing image size\nDocker containers are made up of layers each contributing to the final image\nsize itself, so if you are not careful a simple app can become larger then 1gb\neasily.\n\nCaching (layers)\nReducing layers is a great way to achieve smaller image sizes, lets take this\nexample\n\nFROM alpine:latest\n\nRUN apk update\nRUN apk add --update ca-certificates wget\nRUN update-ca-certificates\nRUN wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- | tar xzC /tmp/\nRUN cp /tmp/docker/* /usr/bin/\n\n\nOutputs 6 layers at 197MB\n\n~$ docker image list\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\nadd-test            latest             106ef90bfa53        Just now   197MB\n\n\nNow combine into a single RUN command\n\nFROM alpine:latest\n\nRUN apk update \\\n    && apk --no-cache add --update --virtual build-deps ca-certificates wget \\\n    && update-ca-certificates \\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    && cp /tmp/docker/* /usr/bin/ \\\n    && rm -rf /tmp/ \\\n    && apk del build-deps \\\n    && rm -rf /var/cache/apk/*\n\n\nOutputs 2 layers though still with a total 197MB we now speed up subsequent\nbuilds as there are less layers to compute and fetch from cache or build anew.\n\nCaching (combined commands)\nIn the last example we saw how combining commands works to improve build times\nleveraging cached layers better, now we will look at how we can combine even\nmore commands to reduce the image size;\n\nFROM alpine:latest\n\nRUN apk update \\\n    && apk add --update ca-certificates wget \\\n    && update-ca-certificates \\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    && cp /tmp/docker/* /usr/bin/ \\\n    && rm -rf /tmp/ \\\n    && apk del ca-certificates wget \\\n    && rm -rf /var/cache/apk/*\n\n\nOutputs 2 layers still but now only 99.2MB\n\n~$ docker image list\nREPOSITORY          TAG                IMAGE ID            CREATED    SIZE\nadd-test            latest             7b6ba76d4fc3        Just now   99.2MB\n\n\nI utilised the package manager to clean up our build only programs that aren't\nneeded to run our app, as well as removing any temp files for the build.\n\nAnother useful feature in Alpine package manager we don't have in many others is\nthe ability to tell apk that these packages are build only packages so they can\nbe cleaned up later with only 1 command argument\n\nFROM alpine:latest\n\nRUN apk update \\\n    && apk add --update --virtual build-deps ca-certificates wget \\\n    && update-ca-certificates \\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    && cp /tmp/docker/* /usr/bin/ \\\n    && apk del build-deps \\\n    && rm -rf /var/cache/apk/*\n\n\nThe size and build time is unchanged, but it is now easier to manage our build\nonly programs as our app grows.\n\nCaching (optimisation)\nWe've seen how less layers speeds up our build times when layers are built from\ncache, but sometimes things change between builds forcing the layers to skip\ncache.\n\nIn Docker, if any layer is changed all layers after it will be rebuilt too, so\nensure you order your layers not only by logical build steps, but also by least\nchanged steps first.\n\nFROM alpine:latest\n\nCOPY ./src /usr/src\n\nRUN apk update \\\n    && apk add --update --virtual build-deps ca-certificates wget \\\n    && update-ca-certificates \\\n    && wget -q --no-check-certificate https://download.docker.com/linux/static/stable/x86_64/docker-17.09.0-ce.tgz -O- \\\n    | tar xzC /tmp/ \\\n    && cp /tmp/docker/* /usr/bin/ \\\n    && apk del build-deps \\\n    && rm -rf /var/cache/apk/*\n\n\nAbove, the app source files will change between builds, so having it at the top\nforces Docker to rebuild all layers after the COPY. Move most likely to change\noften steps the the bottom to utilise cached layers better.\n\nIgnoring files\nIf you're familiar with how .gitignore is used to keep your git repo small,\ntidy, and free of large files such as unwanted archives and binaries in your\ntree - the same applies to the .dockerignore file.\n\nPlace .dockerignore in your project root (where you run docker cli) and during\nbuilds the docker cli will ignore unwanted files.\n\nIf you are developing in one O/S and deploying to another (even another based on\nLinux) it is important to consider that binaries will compile differently on\nyour host then they should be for the container so ensure you ignore all your\ndependency directories so they are built inside the container.\n\nChange the executable\nUsing ENTRYPOINT and CMD can be confusing.\n\nYou might need to be familiar with Linux process and sessions to fully grasp\nthis, so I'll do my best to keep this high level enough for Docker users\nunfamiliar with Linux.\n\nThink of ENTRYPOINT as the base program you want the container to run, because a\nDocker container can only run 1 program while it is running. When you start an\ninteractive shell (terminal session) you generally tell Docker to use /bin/sh or \n/bin/bash depending on O/S as the base program and during that interactive\nsession you type out commands like ls, cd, java for example, these are the CMD \nin your Dockerfile!\n\nSo if you were to run docker run -it alpine to start a session and then ls /usr \n(or just one line docker run -it alpine ls /usr) it is the same as;\n\nFROM apline\nENTRYPOINT /bin/sh\nCMD [\"ls\", \"/usr\"]\n\n\nSo you can see how this might be useful to change to something like (if you have\na Nodejs app);\n\nFROM node\nENTRYPOINT /usr/local/bin/npm\nCMD [\"run\", \"start-server\"]\n\n\nMapping volumes or mounts\nYou need to make a decision whether you want to have a mount or a volume.\n\nBoth have all of the same arguments available on the cli but if you're intending\nto run the docker container as a service you can only use mounts whereas if you\nintend to use a plugin or driver such as vieux/sshfs which is a file system over\nssh, you must use a volume to make use of volume drivers.\n\nIn a Dockerfile you can define what the expected VOLUMES should be, but it's not\npossible to mount a host directory using a Dockerfile by convention because\nimages should be portable.\nYou must map the volume when you run the container, e.g. docker run -v\n\"$(pwd)\"/src:/usr/src/app:ro or docker run --mount\ntype=bind,source=\"$(pwd)\"/src,target=/usr/src/app,readonly.\n\nSecurity\nWe talked about volumes and mounts, regardless if you choose mounts or volumes\nyou should always make them read-only for security reasons.\nNever choose shared bind propagation on a mount point and Docker is secure by\ndefault meaning the most secure option rprivate is also the default but it is\nbest to be explicit on this so docker run --mount\ntype=bind,source=\"$(pwd)\"/src,target=/usr/src/app,readonly,bind-propagation=rprivate\n.\n\nSome other points;\n\n * Do not docker exec commands with privileged option\n * Configure centralized and remote logging\n * Do not disable default seccomp profile\n * Use either apparmor or selinux\n * Set a non-host user with useradd\n * Set --ulimit and --memory when running a Docker container\n * Use docker secrets instead of environment or configuration alternatives\n * Strictly no SSH access into containers\n * Scan Dockerfiles with Docker Bench (or chosen proprietary alternative)\n\nRoot access prevention\nThe SUID flag on binaries has a vulnerability where intruders have a vector for\nassuming root access to the host. It's best to just remove these as it's\nunlikely you'll be sing them from your app.\n\nRUN for i in `find / -perm +6000 -type f`; do rm -f $i; done\n\n\nIf you're unsure and removing them breaks stuff you can also unset the flag on\neach file with\n\nRUN for i in `find / -perm +6000 -type f`; do chmod a-s $i; done\n\n\nBut if you want finer control while keeping the binaries and know your way\naround Linux capability controls you can use something like this\n\nRUN setcap cap_net_raw+p /bin/ping\n\n\nWhich unsets SUID and allows the use of RAW and PACKET sockets.\n\nIt was mentioned above, but another control is having a restricted user that is\nnot a host user, you can create one by running\n\nRUN adduser --system --no-create-home --disabled-password --disabled-login --shell /bin/sh myappuser\nUSER myappuser\n\n\nEach command after USER is executed as the new restricted user, so do this at\nthe top of the Dockerfile.\n\nLateral movement\nUsing build arguments --icc, --link, and --iptables flags control which\ncontainers are allowed inter-container communication which is the risk mechanism\nused for moving from one hacked system to another.\n\nDocker prides itself that it is security first, but this is one case where\nconvenience was chosen at the compromise of security because by default all\ncontainers may communicate with one another freely.\n\nMonitoring\nI recommend using HEALTHCHECK on every container without exception because\nDocker must be running a process to stay active so inherently there is something\nto monitor.\n\nHEALTHCHECK uses non-zero exit codes to detect a processes health, so a simply\ncheck for a web server would be;\n\nHEALTHCHECK --interval=12s --timeout=12s --start-period=30s \\\n    CMD curl --silent --fail https://localhost:36000/ || exit 1\n\n\nOf course if you're on Windows curl is either not there or it's alias to\npowershell Invoke-WebRequest is there but the arguments aren't the same,\ntherefore it is best not to use any host O/S specific programs and write one in\npython, go, java, nodejs, whatever language you feel comfortable using - just\nmake sure it emits proper exit codes.\n\nThank you for reading and don't forget to share this if you found it\ninteresting.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/docker-redo-4.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-13 10:37:32","created_by":"1","updated_at":"2021-03-31 14:12:19","updated_by":"1","published_at":"2017-11-14 10:23:13","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcaf","uuid":"ab8f11c7-4ac1-4d9b-9dfb-a5ad626baf55","title":"Problems with AWS API Gateway stemmed from CloudFront","slug":"problems-with-aws-api-gateway-stemmed-from-cloudfront","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"API Gateway is a service offered by AWS and was established originally as a code fork of thier CloudFront service and has evolved separately ever since.\\n\\nAlthough an interesting fact there has been little evidence to show any sort of relationship between the 2 services since API Gateway was released.\\n\\nAPI Gateway today abstracts the concept of CloudFront's distributions and extends on its behaviors and integrations. We can see residual effects of the CloudFront code fork in certain limitiations such as;\\n- Like earlier versions of CloudFront, API Gateway today suffers from the maximum timeout period of 30 seconds for integrations to respond whereas CloudFront today offers advanced configuration of its behaviors and integrations lacking in API Gateway.\\n- Caching of any Resource that is not a `GET` Method is stored statically with the cache key being only the Resource and Method combination, regardless if the input body changes the response will remain the same when served from cache. This is not surprising as earlier versions of CloudFront had caching enabled only for GET requests and limited ability to white-list request features such as headers, query parameters, and request body.\\n- API Gateway endpoints are located in multiple regions, with latency-based routing. Just like CloudFront.\\n- It is impossible to set the same custom domain on multiple API Gateway configurations because it is still basically CloudFront under the hood.\\n\\nCloudFront has continued to evolve and receive feature enhancements and patches while API Gateway seems to remain mostly unchanged with its feature offerings and limitations imposed by it's fork of an earlier version of CloudFront.\\nIt is therefore obvious that the parallels we draw between CloudFront and API Gateway must end at that they both receive internet facing traffic and offer business rule based routing of that traffic to other AWS service offerings as integrations. I point this out simply because the CloudFront that once was is not the same CloudFront today and API Gateway by all appearances has taken an entirely different release path.\\n\\n## API Gateway re-design?\\n\\nWill there be an API Gateway re-design?\\n\\nI have heard mumblings at AWS events and meetups that there is ongoing developments with the above limitations and more being worked on, but I won't be holding my breath on this mainly due to it's stability, adoption, and release track record.\\n\\nWhat would I hope to see in a API Gateway re-design? Easy;\\n\\n- Regional API Gateway service\\n- Support being put behind a VPC\\n- A more intuitive way to use custom domains\\n    - Certificate Manager integration for HTTPS\\n- Perhaps TLS 1.3 / HTTP2 support\\n- Exposed behaviour configurations similar to modern CloudFront\\n- Full availability of Load Balancer options from EC2\\n- Be available in CloudFront as an origin for better caching\\n    - remove all cache from API Gateway or make clear distinctions that it's a static request cache only.\\n    - As an origin setting, you would want API Gateway to be region based and have CloudFront route requesters from certain regions onto appropriately mapped API Gateway ID's in each region\\n\\nI'm fairly certain I could continue this list focusing on areas of validation, integrations, stages, quota's, and authentication particularly - but I wanted to keep this a short post. \\n\\nThank you for reading and don't forget to share this if you found it interesting.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>API Gateway is a service offered by AWS and was established originally as a code fork of thier CloudFront service and has evolved separately ever since.</p>\n<p>Although an interesting fact there has been little evidence to show any sort of relationship between the 2 services since API Gateway was released.</p>\n<p>API Gateway today abstracts the concept of CloudFront's distributions and extends on its behaviors and integrations. We can see residual effects of the CloudFront code fork in certain limitiations such as;</p>\n<ul>\n<li>Like earlier versions of CloudFront, API Gateway today suffers from the maximum timeout period of 30 seconds for integrations to respond whereas CloudFront today offers advanced configuration of its behaviors and integrations lacking in API Gateway.</li>\n<li>Caching of any Resource that is not a <code>GET</code> Method is stored statically with the cache key being only the Resource and Method combination, regardless if the input body changes the response will remain the same when served from cache. This is not surprising as earlier versions of CloudFront had caching enabled only for GET requests and limited ability to white-list request features such as headers, query parameters, and request body.</li>\n<li>API Gateway endpoints are located in multiple regions, with latency-based routing. Just like CloudFront.</li>\n<li>It is impossible to set the same custom domain on multiple API Gateway configurations because it is still basically CloudFront under the hood.</li>\n</ul>\n<p>CloudFront has continued to evolve and receive feature enhancements and patches while API Gateway seems to remain mostly unchanged with its feature offerings and limitations imposed by it's fork of an earlier version of CloudFront.<br>\nIt is therefore obvious that the parallels we draw between CloudFront and API Gateway must end at that they both receive internet facing traffic and offer business rule based routing of that traffic to other AWS service offerings as integrations. I point this out simply because the CloudFront that once was is not the same CloudFront today and API Gateway by all appearances has taken an entirely different release path.</p>\n<h2 id=\"apigatewayredesign\">API Gateway re-design?</h2>\n<p>Will there be an API Gateway re-design?</p>\n<p>I have heard mumblings at AWS events and meetups that there is ongoing developments with the above limitations and more being worked on, but I won't be holding my breath on this mainly due to it's stability, adoption, and release track record.</p>\n<p>What would I hope to see in a API Gateway re-design? Easy;</p>\n<ul>\n<li>Regional API Gateway service</li>\n<li>Support being put behind a VPC</li>\n<li>A more intuitive way to use custom domains\n<ul>\n<li>Certificate Manager integration for HTTPS</li>\n</ul>\n</li>\n<li>Perhaps TLS 1.3 / HTTP2 support</li>\n<li>Exposed behaviour configurations similar to modern CloudFront</li>\n<li>Full availability of Load Balancer options from EC2</li>\n<li>Be available in CloudFront as an origin for better caching\n<ul>\n<li>remove all cache from API Gateway or make clear distinctions that it's a static request cache only.</li>\n<li>As an origin setting, you would want API Gateway to be region based and have CloudFront route requesters from certain regions onto appropriately mapped API Gateway ID's in each region</li>\n</ul>\n</li>\n</ul>\n<p>I'm fairly certain I could continue this list focusing on areas of validation, integrations, stages, quota's, and authentication particularly - but I wanted to keep this a short post.</p>\n<p>Thank you for reading and don't forget to share this if you found it interesting.</p>\n<!--kg-card-end: markdown-->","comment_id":"5a0e686ef9e9dc0594bdb69f","plaintext":"API Gateway is a service offered by AWS and was established originally as a code\nfork of thier CloudFront service and has evolved separately ever since.\n\nAlthough an interesting fact there has been little evidence to show any sort of\nrelationship between the 2 services since API Gateway was released.\n\nAPI Gateway today abstracts the concept of CloudFront's distributions and\nextends on its behaviors and integrations. We can see residual effects of the\nCloudFront code fork in certain limitiations such as;\n\n * Like earlier versions of CloudFront, API Gateway today suffers from the\n   maximum timeout period of 30 seconds for integrations to respond whereas\n   CloudFront today offers advanced configuration of its behaviors and\n   integrations lacking in API Gateway.\n * Caching of any Resource that is not a GET Method is stored statically with\n   the cache key being only the Resource and Method combination, regardless if\n   the input body changes the response will remain the same when served from\n   cache. This is not surprising as earlier versions of CloudFront had caching\n   enabled only for GET requests and limited ability to white-list request\n   features such as headers, query parameters, and request body.\n * API Gateway endpoints are located in multiple regions, with latency-based\n   routing. Just like CloudFront.\n * It is impossible to set the same custom domain on multiple API Gateway\n   configurations because it is still basically CloudFront under the hood.\n\nCloudFront has continued to evolve and receive feature enhancements and patches\nwhile API Gateway seems to remain mostly unchanged with its feature offerings\nand limitations imposed by it's fork of an earlier version of CloudFront.\nIt is therefore obvious that the parallels we draw between CloudFront and API\nGateway must end at that they both receive internet facing traffic and offer\nbusiness rule based routing of that traffic to other AWS service offerings as\nintegrations. I point this out simply because the CloudFront that once was is\nnot the same CloudFront today and API Gateway by all appearances has taken an\nentirely different release path.\n\nAPI Gateway re-design?\nWill there be an API Gateway re-design?\n\nI have heard mumblings at AWS events and meetups that there is ongoing\ndevelopments with the above limitations and more being worked on, but I won't be\nholding my breath on this mainly due to it's stability, adoption, and release\ntrack record.\n\nWhat would I hope to see in a API Gateway re-design? Easy;\n\n * Regional API Gateway service\n * Support being put behind a VPC\n * A more intuitive way to use custom domains * Certificate Manager integration\n      for HTTPS\n   \n   \n * Perhaps TLS 1.3 / HTTP2 support\n * Exposed behaviour configurations similar to modern CloudFront\n * Full availability of Load Balancer options from EC2\n * Be available in CloudFront as an origin for better caching * remove all cache\n      from API Gateway or make clear distinctions that it's a static request\n      cache only.\n    * As an origin\n      setting, you would want API Gateway to be region based and have CloudFront\n      route requesters from certain regions onto appropriately mapped API\n      Gateway ID's in each region\n   \n   \n\nI'm fairly certain I could continue this list focusing on areas of validation,\nintegrations, stages, quota's, and authentication particularly - but I wanted to\nkeep this a short post.\n\nThank you for reading and don't forget to share this if you found it\ninteresting.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/aws-api-gateway-cloudfront.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-17 04:41:18","created_by":"1","updated_at":"2021-03-31 14:12:49","updated_by":"1","published_at":"2017-11-17 09:25:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb0","uuid":"9ec541a7-b515-468f-b2eb-8e99dfed743e","title":"Learn everything about every internet connected device with Python","slug":"learn-everything-about-every-internet-connected-device-with-python","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"> Warning: do not run these scripts unless you understand the repercussions\\n\\nThere are 2 sections here, defensive and offensive. \\n\\n# Why did I do this?\\n\\nTaking my own network security seriously I started monitoring all incoming and unknown outgoing packets using a passive tap which is a network packet capturing device (SharkTap) that intercepts and duplicates traffic behind my firewall and in front of my vlan where devices and WiFi are placed.\\n\\n# Preparing\\n\\nTo know the anomalous you first need to understand the normal, so I first turned off all my devices and turned them on one by one while collecting mac addresses, setting static ip addresses, and understanding which port and protocol they communicate over.\\nWith this spreadsheet of information I created a network topology diagram and several UML diagrams for my connected devices or applications in operation.\\n\\n# Collection of data\\n\\nWe're going to first need some data for our exercise.\\n\\n## tshark\\n\\nFor best speed using a language i know well, we'll use Bash with tshark (command line utility of wireshark);\\n\\n```\\n#!/usr/bin/env bash\\nDATE=`date \\\"+%Y%m%d-%H%M\\\"`\\nOUTPUT_DIR=<change me>\\nLOG_DIR=`pwd`/\\nWORKDIR=`pwd`/\\nFILENAME=\\\"${DATE}_suffix\\\"\\nEXT=\\\".csv\\\"\\nC_EXT=\\\".tar.gz\\\"\\nLOGNAME=\\\"capture.log\\\"\\nDURATION=$1\\nINTERFACE=$2\\n\\nif [[ -z ${DURATION} ]]; then\\n  DURATION=60\\nfi\\nif [[ -z ${INTERFACE} ]]; then\\n  INTERFACE=any\\nfi\\necho -e \\\"Capturing interface [${INTERFACE}] for [${DURATION}] seconds\\\"\\necho -e \\\"Output: ${OUTPUT_DIR}${FILENAME}${EXT}\\\"\\necho -e \\\"Log: ${LOG_DIR}${LOGNAME}\\\"\\nmkdir -p ${OUTPUT_DIR}\\ntshark -a duration:${DURATION} \\\\\\n    -i ${INTERFACE} \\\\\\n    -u s \\\\\\n    -E separator=, \\\\\\n    -E quote=d \\\\\\n    -E occurrence=f \\\\\\n    -T fields \\\\\\n\\t\\t-e frame.time_epoch \\\\\\n\\t\\t-e ip.src \\\\\\n\\t\\t-e ip.dst \\\\\\n\\t\\t-e http.host \\\\\\n\\t\\t-e http.request.uri \\\\\\n\\t\\t-e dns.qry.name \\\\\\n\\t\\t-e tcp.srcport \\\\\\n\\t\\t-e tcp.dstport \\\\\\n\\t\\t-e udp.srcport \\\\\\n\\t\\t-e udp.dstport \\\\\\n\\t\\t-e _ws.col.Protocol \\\\\\n\\t\\t-e _ws.col.Info \\\\\\n\\t\\t-e arp.src.hw_mac \\\\\\n\\t\\t-e arp.dst.hw_mac \\\\\\n\\t\\t-e http.user_agent \\\\\\n\\t\\t-e eth.src \\\\\\n\\t\\t-e eth.dst \\\\\\n\\t\\t-e eth.src_resolved \\\\\\n\\t\\t-e eth.dst_resolved \\\\\\n\\t1>>${WORKDIR}${FILENAME}${EXT} \\\\\\n\\t2>>${LOG_DIR}${LOGNAME}\\n\\nenv GZIP=-9 tar cvzf ${WORKDIR}${FILENAME}${C_EXT} ${WORKDIR}${FILENAME}${EXT} && \\\\\\n    cp ${WORKDIR}${FILENAME}${C_EXT} ${OUTPUT_DIR}${FILENAME}${C_EXT} && \\\\\\n    rm ${WORKDIR}${FILENAME}${EXT} && \\\\\\n    rm ${WORKDIR}${FILENAME}${C_EXT}\\n```\\n\\nComplete the self-explanatory variables with relevant values and call the script with `./capture.sh 60 all` to capture on all interfaces for 1 minute.\\n\\nThe resulting file will look like this\\n\\n```\\n\\\"1503238803.452442307\\\",\\\"10.1.1.126\\\",\\\"10.1.1.16\\\",,,,\\\"59435\\\",\\\"20008\\\",,,\\\"TCP\\\",\\\"59435 → 20008 [ACK] Seq=1 Ack=1 Win=4094 Len=0 TSval=1005594454 TSecr=2175465899\\\",,,,\\\"28:xx:xx:xx:xx:bf\\\",\\\"f4:xx:xx:xx:xx:54\\\",\\\"Apple_17:02:bf\\\",\\\"Elitegro_6a:7f:54\\\"\\n\\\"1503238803.455425666\\\",\\\"77.243.22.33\\\",\\\"10.1.1.127\\\",,,,\\\"17548\\\",\\\"51413\\\",,,\\\"TCP\\\",\\\"17548 → 51413 [ACK] Seq=1 Ack=1 Win=16685 Len=0 SLE=1421 SRE=5681\\\",,,,\\\"e0:xx:xx:xx:xx:14\\\",\\\"90:xx:xx:xx:xx:b9\\\",\\\"Technico_d3:6a:14\\\",\\\"AsustekC_c8:f5:b9\\\"\\n```\\n\\nFirst is some internal traffic, and the next is incoming from the Internet.\\n\\n## Backup to S3\\n\\nCompletely optional, but I like to back up my files to S3, here is my hourly script to do that;\\n\\n```\\n#!/usr/bin/env bash\\nAWS=`which aws`\\nNOW=`date \\\"+%Y%m%d-%H\\\"`\\nBUCKET=<change me>\\nAWS_PROFILE=<change me>\\n\\nBACKUPDIR=/mnt/backup/tshark/\\nTMPDIR=/tmp/${NOW}/\\n\\nLOG_DIR=`pwd`/\\nLOGNAME=\\\"capture.log\\\"\\n\\necho -e \\\"Creating ${TMPDIR}\\\"\\nmkdir -p ${TMPDIR} && \\\\\\n    echo -e \\\"ok\\\"\\n\\necho -e \\\"Copy from NAS ${BACKUPDIR}\\\"\\ncp ${BACKUPDIR}`date -d \\\"1 hour ago\\\" \\\"+%Y%m%d-%H\\\"`*.tar.gz ${TMPDIR} && \\\\\\n    echo -e \\\"ok\\\"\\necho -e \\\"AWS S3 upload starting\\\"\\n$AWS s3 cp \\\\\\n    ${TMPDIR} \\\\\\n    s3://${BUCKET} \\\\\\n    --profile=${AWS_PROFILE} \\\\\\n    --exclude \\\"*\\\" \\\\\\n    --include \\\"*.tar.gz\\\" \\\\\\n    --recursive \\\\\\n        >> ${LOG_DIR}/${LOGNAME} && \\\\\\n    echo -e \\\"ok\\\"\\necho -e \\\"Cleanup ${TMPDIR}\\\"\\nrm -rf ${TMPDIR} && \\\\\\n    echo -e \\\"done\\\"\\n```\\n\\nAgain you just need to change the variables.\\n\\nUsing the aws cli tool you can also easily sync a full directory to an S3 bucket folder if your local source has capacity to keep a copy of all files that are backed up, just replace `cp` with `sync`.\\n\\n## Analyse with Python Pandas\\n\\nI like to use Pandas mostly for my analysis, it's not the fastest or most efficient with large datasets but it serves me well until i encounter such limitations then i pivot to the right tool for what I am trying to do. So to get you started, here is my script to download csv files from S3 and compile a DataFrame in Pandas.\\n\\n```\\nimport sys\\nimport threading\\nimport pandas as pd\\nimport boto3\\nimport ntpath\\nfrom datetime import datetime\\nfrom pprint import pprint\\n\\nBUCKET = '< change me >'\\nPROFILE = '<.change me >'\\n\\n\\nclass ProgressPercentage(object):\\n    def __init__(self, filename):\\n        self._filename = filename\\n        self._seen_so_far = 0\\n        self._lock = threading.Lock()\\n    def __call__(self, bytes_amount):\\n        # To simplify we'll assume this is hooked up\\n        # to a single filename.\\n        with self._lock:\\n            self._seen_so_far += bytes_amount\\n            sys.stdout.write(\\n                \\\"\\\\r%s --> %s bytes transferred\\\" % (\\n                    self._filename, self._seen_so_far))\\n            sys.stdout.flush()\\n\\ndef download(object):\\n  fp = object.get('Key')\\n  filename = ntpath.basename(fp)\\n  obj = client.download_file(BUCKET, fp, filename, Callback=ProgressPercentage(filename))\\n  print('Done')\\n  return obj, filename\\n\\ndev = boto3.session.Session(profile_name=PROFILE)\\nclient = dev.client('s3')\\n\\nchunksize = 10 ** 8\\nobjects = client.list_objects(Bucket=BUCKET)\\ndf = pd.DataFrame()\\nfor object in objects.get('Contents'):\\n    obj, filename = download(object)\\n    for df in pd.read_csv(filename, na_values=['nan'], keep_default_na=False, compression='gzip', error_bad_lines=False, chunksize=chunksize):\\n        df.append(df, ignore_index=True)\\n        break\\n\\n\\\"\\\"\\\"\\nUse df data frame, maybe save for Hadoop with to_hdf()\\n\\\"\\\"\\\"\\n```\\n\\nTypically I just interrogate pcap data using tshark alone, but the above becomes more useful once you've gone beyond the data in pcap which we'll be gathering next.\\n\\n## Enrich your data\\n\\nWhen it comes to IP the first thing that comes to mind is WHOIS. You won't always get useful data here because you can pay extra to protect it, but criminals are mostly looking to get paid not pay to protect themselves so I find this to be particularly useful for investigation;\\n\\n```\\ndef whois(ip):\\n  countries = get_countries()\\n  obj = IPWhois(ip)\\n  result = obj.lookup_rdap(depth=1, asn_methods=['dns', 'whois', 'http'])\\n  country = countries[result['asn_country_code']]\\n  type = result['network']['type']\\n  name = result['network']['name']\\n  description = result['asn_description']\\n  registry = result['asn_registry']\\n  entities = ', '.join(result['entities'])\\n\\n  return country, type, name, description, registry, entities\\n```\\n\\nThere are also some free services to use the IP to get some geo-location data;\\n\\n```\\ndef get_coords(ip):\\n  url = \\\"https://freegeoip.net/json/%s\\\" % ip\\n  r = requests.get(url)\\n\\n  return r.json()\\n```\\n\\nThis will produce something like this\\n\\n\\n```\\n{\\\"ip\\\":\\\"xxx.xxx.xxx.xxx\\\",\\\"country_code\\\":\\\"AU\\\",\\\"country_name\\\":\\\"Australia\\\",\\\"region_code\\\":\\\"VIC\\\",\\\"region_name\\\":\\\"Victoria\\\",\\\"city\\\":\\\"Brunswick East\\\",\\\"zip_code\\\":\\\"3057\\\",\\\"time_zone\\\":\\\"Australia/Melbourne\\\",\\\"latitude\\\":-37.7725,\\\"longitude\\\":144.9724,\\\"metro_code\\\":0}\\n```\\n\\nWe can now use Google Maps to find out even more useful information using the latitude and longitude (you'll need your own API key to perform this query)\\n\\n```\\ndef get_geo(lat, lon):\\n  url = \\\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?key={}&radius=1&location={},{}\\\".format(API_KEY, lat, lon)\\n  r = requests.get(url)\\n\\n  return r.json()\\n```\\n\\nGoogle responds with a bunch of info, but you can get even more by taking the Google Place ID and performing another query;\\n\\n```\\ndef get_place(id):\\n  url = \\\"https://maps.googleapis.com/maps/api/place/details/json?key={}&placeid={}\\\".format(API_KEY, id)\\n  r = requests.get(url)\\n\\n  return r.json()\\n```\\n\\nWhich gives you a rich dataset for the location.\\n\\n## Retaliation\\n\\nOn the offensive side, you may discover some nefarious activity and the evidence shows you it's from somewhere law enforcement can't help you so you decide to take action into your own hands.\\n\\n> I recommend you report to authorities rather than go on offense\\n\\nFirst clue to find is if there is a website at this IP address, you can easily identify a hostname from IP using `python -c \\\"import socket;print socket.gethostbyaddr('xxx.xxx.xxx.xxx')\\\"`.\\n\\nTo test if a webserver responds on an alternative port you'll first will need to identify all exposed ports;\\n\\n```\\n#!/usr/bin/env python\\n###########################\\n# pip install python-nmap\\n# usage:\\n#    ./scanner.py -i 59.x.x.x\\nimport nmap\\nimport argparse\\nimport termios, fcntl, sys, os\\nfrom datetime import datetime\\n\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('-i --ip', help='host ip to scan', dest='ip')\\nargs = parser.parse_args()\\nip = args.ip\\nif not ip:\\n  print 'host ip must be provided'\\n  sys.exit(1)\\n\\ndef get_items(dict_object):\\n  for key in dict_object:\\n    yield key, dict_object[key]\\n\\ndef scan_cb(host, result):\\n  state = result['scan'][host]['status']['state']\\n  hostname = result['scan'][host]['hostnames'][0]['name']\\n  print \\\"[%s] Host: %s (%s)\\\" % (state, hostname, host)\\n\\n  for port, p in get_items(result['scan'][host]['tcp']):\\n    port_state = p['state']\\n    port_name = p['product']\\n    port_info = \\\"%s %s\\\" % (p['reason'], p['extrainfo'])\\n    print('[%s] Port %d: %s (%s)' % (port_state, port, port_name, port_info))\\n\\n\\nfd = sys.stdin.fileno()\\n\\noldterm = termios.tcgetattr(fd)\\nnewattr = termios.tcgetattr(fd)\\nnewattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO\\ntermios.tcsetattr(fd, termios.TCSANOW, newattr)\\n\\noldflags = fcntl.fcntl(fd, fcntl.F_GETFL)\\nfcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)\\n\\ntry:\\n  nm = nmap.PortScannerAsync()\\n  nm.scan(ip, '20-23,25,53,57,67-69,80,81,82,107-113,115,118-119,135,137-139,143,153,156,170,177,179,194,209,213,218,220,300,311,366,369-371,383,384,387,389,399,427,433,434,443-445,464,465,475,491,514,515,517,518,520,521,524,530-533,540,546-548,556,560,561,563-585,587,591,593,601,604,623,625,631,635,636,639,641,646-648,653-655,657,660,666,674,688,690,691,706,711,712,749-754,760,782,783,808,832,843,847,848,873,953-61000', callback=scan_cb)\\n  start_time = datetime.now()\\n  while nm.still_scanning():\\n    stop_time = datetime.now()\\n    elapsed = stop_time - start_time\\n    try:\\n      c = sys.stdin.read(1)\\n      if str(c) == 's':\\n        print(\\\"elapsed %ds\\\" % elapsed.seconds)\\n    except IOError: pass\\n    nm.wait(1)\\n\\nfinally:\\n    termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm)\\n    fcntl.fcntl(fd, fcntl.F_SETFL, oldflags)\\n```\\n\\nThis will take a while, press `s` on the keyboard to see the elapsed time (and know it's still running the scan).\\n\\nOnce you have some ports to test you can simply use a HTTP head request in python to see if a web server responds `python -c \\\"from requests import request;print request('head', 'http://xxx.xxx.xxx.xxx:8080').headers\\\"`.\\n\\nKnowing that there is a web server you can use some tools like sqlmap to attempt SQL Injection, or ZAP to identify XSS or other known un-patched web vulnerabilities.\\n\\nIf you are really keen or the attacker was an internet connected device rather than a server you could go 1 step further, with that list of open ports you can [cross reference](https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers) or Google search the port number to try and identify what program is listening on that port.\\nKnowing the application is just the first step, you'll have to research to find out how to carry out an exploit on anything you find.\\n\\n## For the researchers and really bad guys\\n\\nIt's kind of terrifying how easy all of the above was to figure out, I was always under the impression this stuff was difficult. It really worried me when I realised just how easy it would be to turn these defensive and retaliatory techniques and use them purely offensively with just 16 lines of code.\\n\\n```\\n#!/usr/bin/env python\\n##################################\\n# pip install netaddr\\nfrom netaddr import IPAddress, IPNetwork\\n\\nfile_out = \\\"./internet-connected.csv\\\"\\nwith open(file_out, mode='a+') as f:\\n    for class_a in range(0, 255):\\n        a = IPAddress(\\\"%s.0.0.0\\\" % class_a)\\n        if not a.is_reserved():\\n            for class_b in range(0, 255):\\n                b = IPAddress(\\\"%d.%d.0.0\\\" % (class_a, class_b))\\n                if not b.is_reserved():\\n                    for class_c in range(0, 255):\\n                        c = IPAddress(\\\"%d.%d.%d.0\\\" % (class_a, class_b, class_c))\\n                        if not c.is_reserved() and not c.is_private():\\n                            ip = \\\"%s/24\\\" % c\\n                            for host_ip in IPNetwork(ip).iter_hosts():\\n                                if host_ip.is_unicast():\\n                                    f.write(str(host_ip) + '\\\\n')\\n```\\n\\nThe above snippet will compile you a list of IPv4 addresses that are potentially internet connected devices, all of them..\\n\\nWith such a list you avoid raising too much suspicion by hitting reserved addresses and with constant scanning a researcher might be able to identify hosts that are new using a diff on past scans, learn which hosts are tor exit nodes, compile analysis on usage of certain application usage like web servers or operating systems - the possibilities are pretty endless.\\n\\nThank you for reading and don't forget to share this if you found it interesting.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>Warning: do not run these scripts unless you understand the repercussions</p>\n</blockquote>\n<p>There are 2 sections here, defensive and offensive.</p>\n<h1 id=\"whydididothis\">Why did I do this?</h1>\n<p>Taking my own network security seriously I started monitoring all incoming and unknown outgoing packets using a passive tap which is a network packet capturing device (SharkTap) that intercepts and duplicates traffic behind my firewall and in front of my vlan where devices and WiFi are placed.</p>\n<h1 id=\"preparing\">Preparing</h1>\n<p>To know the anomalous you first need to understand the normal, so I first turned off all my devices and turned them on one by one while collecting mac addresses, setting static ip addresses, and understanding which port and protocol they communicate over.<br>\nWith this spreadsheet of information I created a network topology diagram and several UML diagrams for my connected devices or applications in operation.</p>\n<h1 id=\"collectionofdata\">Collection of data</h1>\n<p>We're going to first need some data for our exercise.</p>\n<h2 id=\"tshark\">tshark</h2>\n<p>For best speed using a language i know well, we'll use Bash with tshark (command line utility of wireshark);</p>\n<pre><code>#!/usr/bin/env bash\nDATE=`date &quot;+%Y%m%d-%H%M&quot;`\nOUTPUT_DIR=&lt;change me&gt;\nLOG_DIR=`pwd`/\nWORKDIR=`pwd`/\nFILENAME=&quot;${DATE}_suffix&quot;\nEXT=&quot;.csv&quot;\nC_EXT=&quot;.tar.gz&quot;\nLOGNAME=&quot;capture.log&quot;\nDURATION=$1\nINTERFACE=$2\n\nif [[ -z ${DURATION} ]]; then\n  DURATION=60\nfi\nif [[ -z ${INTERFACE} ]]; then\n  INTERFACE=any\nfi\necho -e &quot;Capturing interface [${INTERFACE}] for [${DURATION}] seconds&quot;\necho -e &quot;Output: ${OUTPUT_DIR}${FILENAME}${EXT}&quot;\necho -e &quot;Log: ${LOG_DIR}${LOGNAME}&quot;\nmkdir -p ${OUTPUT_DIR}\ntshark -a duration:${DURATION} \\\n    -i ${INTERFACE} \\\n    -u s \\\n    -E separator=, \\\n    -E quote=d \\\n    -E occurrence=f \\\n    -T fields \\\n\t\t-e frame.time_epoch \\\n\t\t-e ip.src \\\n\t\t-e ip.dst \\\n\t\t-e http.host \\\n\t\t-e http.request.uri \\\n\t\t-e dns.qry.name \\\n\t\t-e tcp.srcport \\\n\t\t-e tcp.dstport \\\n\t\t-e udp.srcport \\\n\t\t-e udp.dstport \\\n\t\t-e _ws.col.Protocol \\\n\t\t-e _ws.col.Info \\\n\t\t-e arp.src.hw_mac \\\n\t\t-e arp.dst.hw_mac \\\n\t\t-e http.user_agent \\\n\t\t-e eth.src \\\n\t\t-e eth.dst \\\n\t\t-e eth.src_resolved \\\n\t\t-e eth.dst_resolved \\\n\t1&gt;&gt;${WORKDIR}${FILENAME}${EXT} \\\n\t2&gt;&gt;${LOG_DIR}${LOGNAME}\n\nenv GZIP=-9 tar cvzf ${WORKDIR}${FILENAME}${C_EXT} ${WORKDIR}${FILENAME}${EXT} &amp;&amp; \\\n    cp ${WORKDIR}${FILENAME}${C_EXT} ${OUTPUT_DIR}${FILENAME}${C_EXT} &amp;&amp; \\\n    rm ${WORKDIR}${FILENAME}${EXT} &amp;&amp; \\\n    rm ${WORKDIR}${FILENAME}${C_EXT}\n</code></pre>\n<p>Complete the self-explanatory variables with relevant values and call the script with <code>./capture.sh 60 all</code> to capture on all interfaces for 1 minute.</p>\n<p>The resulting file will look like this</p>\n<pre><code>&quot;1503238803.452442307&quot;,&quot;10.1.1.126&quot;,&quot;10.1.1.16&quot;,,,,&quot;59435&quot;,&quot;20008&quot;,,,&quot;TCP&quot;,&quot;59435 → 20008 [ACK] Seq=1 Ack=1 Win=4094 Len=0 TSval=1005594454 TSecr=2175465899&quot;,,,,&quot;28:xx:xx:xx:xx:bf&quot;,&quot;f4:xx:xx:xx:xx:54&quot;,&quot;Apple_17:02:bf&quot;,&quot;Elitegro_6a:7f:54&quot;\n&quot;1503238803.455425666&quot;,&quot;77.243.22.33&quot;,&quot;10.1.1.127&quot;,,,,&quot;17548&quot;,&quot;51413&quot;,,,&quot;TCP&quot;,&quot;17548 → 51413 [ACK] Seq=1 Ack=1 Win=16685 Len=0 SLE=1421 SRE=5681&quot;,,,,&quot;e0:xx:xx:xx:xx:14&quot;,&quot;90:xx:xx:xx:xx:b9&quot;,&quot;Technico_d3:6a:14&quot;,&quot;AsustekC_c8:f5:b9&quot;\n</code></pre>\n<p>First is some internal traffic, and the next is incoming from the Internet.</p>\n<h2 id=\"backuptos3\">Backup to S3</h2>\n<p>Completely optional, but I like to back up my files to S3, here is my hourly script to do that;</p>\n<pre><code>#!/usr/bin/env bash\nAWS=`which aws`\nNOW=`date &quot;+%Y%m%d-%H&quot;`\nBUCKET=&lt;change me&gt;\nAWS_PROFILE=&lt;change me&gt;\n\nBACKUPDIR=/mnt/backup/tshark/\nTMPDIR=/tmp/${NOW}/\n\nLOG_DIR=`pwd`/\nLOGNAME=&quot;capture.log&quot;\n\necho -e &quot;Creating ${TMPDIR}&quot;\nmkdir -p ${TMPDIR} &amp;&amp; \\\n    echo -e &quot;ok&quot;\n\necho -e &quot;Copy from NAS ${BACKUPDIR}&quot;\ncp ${BACKUPDIR}`date -d &quot;1 hour ago&quot; &quot;+%Y%m%d-%H&quot;`*.tar.gz ${TMPDIR} &amp;&amp; \\\n    echo -e &quot;ok&quot;\necho -e &quot;AWS S3 upload starting&quot;\n$AWS s3 cp \\\n    ${TMPDIR} \\\n    s3://${BUCKET} \\\n    --profile=${AWS_PROFILE} \\\n    --exclude &quot;*&quot; \\\n    --include &quot;*.tar.gz&quot; \\\n    --recursive \\\n        &gt;&gt; ${LOG_DIR}/${LOGNAME} &amp;&amp; \\\n    echo -e &quot;ok&quot;\necho -e &quot;Cleanup ${TMPDIR}&quot;\nrm -rf ${TMPDIR} &amp;&amp; \\\n    echo -e &quot;done&quot;\n</code></pre>\n<p>Again you just need to change the variables.</p>\n<p>Using the aws cli tool you can also easily sync a full directory to an S3 bucket folder if your local source has capacity to keep a copy of all files that are backed up, just replace <code>cp</code> with <code>sync</code>.</p>\n<h2 id=\"analysewithpythonpandas\">Analyse with Python Pandas</h2>\n<p>I like to use Pandas mostly for my analysis, it's not the fastest or most efficient with large datasets but it serves me well until i encounter such limitations then i pivot to the right tool for what I am trying to do. So to get you started, here is my script to download csv files from S3 and compile a DataFrame in Pandas.</p>\n<pre><code>import sys\nimport threading\nimport pandas as pd\nimport boto3\nimport ntpath\nfrom datetime import datetime\nfrom pprint import pprint\n\nBUCKET = '&lt; change me &gt;'\nPROFILE = '&lt;.change me &gt;'\n\n\nclass ProgressPercentage(object):\n    def __init__(self, filename):\n        self._filename = filename\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n    def __call__(self, bytes_amount):\n        # To simplify we'll assume this is hooked up\n        # to a single filename.\n        with self._lock:\n            self._seen_so_far += bytes_amount\n            sys.stdout.write(\n                &quot;\\r%s --&gt; %s bytes transferred&quot; % (\n                    self._filename, self._seen_so_far))\n            sys.stdout.flush()\n\ndef download(object):\n  fp = object.get('Key')\n  filename = ntpath.basename(fp)\n  obj = client.download_file(BUCKET, fp, filename, Callback=ProgressPercentage(filename))\n  print('Done')\n  return obj, filename\n\ndev = boto3.session.Session(profile_name=PROFILE)\nclient = dev.client('s3')\n\nchunksize = 10 ** 8\nobjects = client.list_objects(Bucket=BUCKET)\ndf = pd.DataFrame()\nfor object in objects.get('Contents'):\n    obj, filename = download(object)\n    for df in pd.read_csv(filename, na_values=['nan'], keep_default_na=False, compression='gzip', error_bad_lines=False, chunksize=chunksize):\n        df.append(df, ignore_index=True)\n        break\n\n&quot;&quot;&quot;\nUse df data frame, maybe save for Hadoop with to_hdf()\n&quot;&quot;&quot;\n</code></pre>\n<p>Typically I just interrogate pcap data using tshark alone, but the above becomes more useful once you've gone beyond the data in pcap which we'll be gathering next.</p>\n<h2 id=\"enrichyourdata\">Enrich your data</h2>\n<p>When it comes to IP the first thing that comes to mind is WHOIS. You won't always get useful data here because you can pay extra to protect it, but criminals are mostly looking to get paid not pay to protect themselves so I find this to be particularly useful for investigation;</p>\n<pre><code>def whois(ip):\n  countries = get_countries()\n  obj = IPWhois(ip)\n  result = obj.lookup_rdap(depth=1, asn_methods=['dns', 'whois', 'http'])\n  country = countries[result['asn_country_code']]\n  type = result['network']['type']\n  name = result['network']['name']\n  description = result['asn_description']\n  registry = result['asn_registry']\n  entities = ', '.join(result['entities'])\n\n  return country, type, name, description, registry, entities\n</code></pre>\n<p>There are also some free services to use the IP to get some geo-location data;</p>\n<pre><code>def get_coords(ip):\n  url = &quot;https://freegeoip.net/json/%s&quot; % ip\n  r = requests.get(url)\n\n  return r.json()\n</code></pre>\n<p>This will produce something like this</p>\n<pre><code>{&quot;ip&quot;:&quot;xxx.xxx.xxx.xxx&quot;,&quot;country_code&quot;:&quot;AU&quot;,&quot;country_name&quot;:&quot;Australia&quot;,&quot;region_code&quot;:&quot;VIC&quot;,&quot;region_name&quot;:&quot;Victoria&quot;,&quot;city&quot;:&quot;Brunswick East&quot;,&quot;zip_code&quot;:&quot;3057&quot;,&quot;time_zone&quot;:&quot;Australia/Melbourne&quot;,&quot;latitude&quot;:-37.7725,&quot;longitude&quot;:144.9724,&quot;metro_code&quot;:0}\n</code></pre>\n<p>We can now use Google Maps to find out even more useful information using the latitude and longitude (you'll need your own API key to perform this query)</p>\n<pre><code>def get_geo(lat, lon):\n  url = &quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?key={}&amp;radius=1&amp;location={},{}&quot;.format(API_KEY, lat, lon)\n  r = requests.get(url)\n\n  return r.json()\n</code></pre>\n<p>Google responds with a bunch of info, but you can get even more by taking the Google Place ID and performing another query;</p>\n<pre><code>def get_place(id):\n  url = &quot;https://maps.googleapis.com/maps/api/place/details/json?key={}&amp;placeid={}&quot;.format(API_KEY, id)\n  r = requests.get(url)\n\n  return r.json()\n</code></pre>\n<p>Which gives you a rich dataset for the location.</p>\n<h2 id=\"retaliation\">Retaliation</h2>\n<p>On the offensive side, you may discover some nefarious activity and the evidence shows you it's from somewhere law enforcement can't help you so you decide to take action into your own hands.</p>\n<blockquote>\n<p>I recommend you report to authorities rather than go on offense</p>\n</blockquote>\n<p>First clue to find is if there is a website at this IP address, you can easily identify a hostname from IP using <code>python -c &quot;import socket;print socket.gethostbyaddr('xxx.xxx.xxx.xxx')&quot;</code>.</p>\n<p>To test if a webserver responds on an alternative port you'll first will need to identify all exposed ports;</p>\n<pre><code>#!/usr/bin/env python\n###########################\n# pip install python-nmap\n# usage:\n#    ./scanner.py -i 59.x.x.x\nimport nmap\nimport argparse\nimport termios, fcntl, sys, os\nfrom datetime import datetime\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-i --ip', help='host ip to scan', dest='ip')\nargs = parser.parse_args()\nip = args.ip\nif not ip:\n  print 'host ip must be provided'\n  sys.exit(1)\n\ndef get_items(dict_object):\n  for key in dict_object:\n    yield key, dict_object[key]\n\ndef scan_cb(host, result):\n  state = result['scan'][host]['status']['state']\n  hostname = result['scan'][host]['hostnames'][0]['name']\n  print &quot;[%s] Host: %s (%s)&quot; % (state, hostname, host)\n\n  for port, p in get_items(result['scan'][host]['tcp']):\n    port_state = p['state']\n    port_name = p['product']\n    port_info = &quot;%s %s&quot; % (p['reason'], p['extrainfo'])\n    print('[%s] Port %d: %s (%s)' % (port_state, port, port_name, port_info))\n\n\nfd = sys.stdin.fileno()\n\noldterm = termios.tcgetattr(fd)\nnewattr = termios.tcgetattr(fd)\nnewattr[3] = newattr[3] &amp; ~termios.ICANON &amp; ~termios.ECHO\ntermios.tcsetattr(fd, termios.TCSANOW, newattr)\n\noldflags = fcntl.fcntl(fd, fcntl.F_GETFL)\nfcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)\n\ntry:\n  nm = nmap.PortScannerAsync()\n  nm.scan(ip, '20-23,25,53,57,67-69,80,81,82,107-113,115,118-119,135,137-139,143,153,156,170,177,179,194,209,213,218,220,300,311,366,369-371,383,384,387,389,399,427,433,434,443-445,464,465,475,491,514,515,517,518,520,521,524,530-533,540,546-548,556,560,561,563-585,587,591,593,601,604,623,625,631,635,636,639,641,646-648,653-655,657,660,666,674,688,690,691,706,711,712,749-754,760,782,783,808,832,843,847,848,873,953-61000', callback=scan_cb)\n  start_time = datetime.now()\n  while nm.still_scanning():\n    stop_time = datetime.now()\n    elapsed = stop_time - start_time\n    try:\n      c = sys.stdin.read(1)\n      if str(c) == 's':\n        print(&quot;elapsed %ds&quot; % elapsed.seconds)\n    except IOError: pass\n    nm.wait(1)\n\nfinally:\n    termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm)\n    fcntl.fcntl(fd, fcntl.F_SETFL, oldflags)\n</code></pre>\n<p>This will take a while, press <code>s</code> on the keyboard to see the elapsed time (and know it's still running the scan).</p>\n<p>Once you have some ports to test you can simply use a HTTP head request in python to see if a web server responds <code>python -c &quot;from requests import request;print request('head', 'http://xxx.xxx.xxx.xxx:8080').headers&quot;</code>.</p>\n<p>Knowing that there is a web server you can use some tools like sqlmap to attempt SQL Injection, or ZAP to identify XSS or other known un-patched web vulnerabilities.</p>\n<p>If you are really keen or the attacker was an internet connected device rather than a server you could go 1 step further, with that list of open ports you can <a href=\"https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers\">cross reference</a> or Google search the port number to try and identify what program is listening on that port.<br>\nKnowing the application is just the first step, you'll have to research to find out how to carry out an exploit on anything you find.</p>\n<h2 id=\"fortheresearchersandreallybadguys\">For the researchers and really bad guys</h2>\n<p>It's kind of terrifying how easy all of the above was to figure out, I was always under the impression this stuff was difficult. It really worried me when I realised just how easy it would be to turn these defensive and retaliatory techniques and use them purely offensively with just 16 lines of code.</p>\n<pre><code>#!/usr/bin/env python\n##################################\n# pip install netaddr\nfrom netaddr import IPAddress, IPNetwork\n\nfile_out = &quot;./internet-connected.csv&quot;\nwith open(file_out, mode='a+') as f:\n    for class_a in range(0, 255):\n        a = IPAddress(&quot;%s.0.0.0&quot; % class_a)\n        if not a.is_reserved():\n            for class_b in range(0, 255):\n                b = IPAddress(&quot;%d.%d.0.0&quot; % (class_a, class_b))\n                if not b.is_reserved():\n                    for class_c in range(0, 255):\n                        c = IPAddress(&quot;%d.%d.%d.0&quot; % (class_a, class_b, class_c))\n                        if not c.is_reserved() and not c.is_private():\n                            ip = &quot;%s/24&quot; % c\n                            for host_ip in IPNetwork(ip).iter_hosts():\n                                if host_ip.is_unicast():\n                                    f.write(str(host_ip) + '\\n')\n</code></pre>\n<p>The above snippet will compile you a list of IPv4 addresses that are potentially internet connected devices, all of them..</p>\n<p>With such a list you avoid raising too much suspicion by hitting reserved addresses and with constant scanning a researcher might be able to identify hosts that are new using a diff on past scans, learn which hosts are tor exit nodes, compile analysis on usage of certain application usage like web servers or operating systems - the possibilities are pretty endless.</p>\n<p>Thank you for reading and don't forget to share this if you found it interesting.</p>\n<!--kg-card-end: markdown-->","comment_id":"5a0fd9fcf9e9dc0594bdb6a6","plaintext":"> Warning: do not run these scripts unless you understand the repercussions\n\n\nThere are 2 sections here, defensive and offensive.\n\nWhy did I do this?\nTaking my own network security seriously I started monitoring all incoming and\nunknown outgoing packets using a passive tap which is a network packet capturing\ndevice (SharkTap) that intercepts and duplicates traffic behind my firewall and\nin front of my vlan where devices and WiFi are placed.\n\nPreparing\nTo know the anomalous you first need to understand the normal, so I first turned\noff all my devices and turned them on one by one while collecting mac addresses,\nsetting static ip addresses, and understanding which port and protocol they\ncommunicate over.\nWith this spreadsheet of information I created a network topology diagram and\nseveral UML diagrams for my connected devices or applications in operation.\n\nCollection of data\nWe're going to first need some data for our exercise.\n\ntshark\nFor best speed using a language i know well, we'll use Bash with tshark (command\nline utility of wireshark);\n\n#!/usr/bin/env bash\nDATE=`date \"+%Y%m%d-%H%M\"`\nOUTPUT_DIR=<change me>\nLOG_DIR=`pwd`/\nWORKDIR=`pwd`/\nFILENAME=\"${DATE}_suffix\"\nEXT=\".csv\"\nC_EXT=\".tar.gz\"\nLOGNAME=\"capture.log\"\nDURATION=$1\nINTERFACE=$2\n\nif [[ -z ${DURATION} ]]; then\n  DURATION=60\nfi\nif [[ -z ${INTERFACE} ]]; then\n  INTERFACE=any\nfi\necho -e \"Capturing interface [${INTERFACE}] for [${DURATION}] seconds\"\necho -e \"Output: ${OUTPUT_DIR}${FILENAME}${EXT}\"\necho -e \"Log: ${LOG_DIR}${LOGNAME}\"\nmkdir -p ${OUTPUT_DIR}\ntshark -a duration:${DURATION} \\\n    -i ${INTERFACE} \\\n    -u s \\\n    -E separator=, \\\n    -E quote=d \\\n    -E occurrence=f \\\n    -T fields \\\n\t\t-e frame.time_epoch \\\n\t\t-e ip.src \\\n\t\t-e ip.dst \\\n\t\t-e http.host \\\n\t\t-e http.request.uri \\\n\t\t-e dns.qry.name \\\n\t\t-e tcp.srcport \\\n\t\t-e tcp.dstport \\\n\t\t-e udp.srcport \\\n\t\t-e udp.dstport \\\n\t\t-e _ws.col.Protocol \\\n\t\t-e _ws.col.Info \\\n\t\t-e arp.src.hw_mac \\\n\t\t-e arp.dst.hw_mac \\\n\t\t-e http.user_agent \\\n\t\t-e eth.src \\\n\t\t-e eth.dst \\\n\t\t-e eth.src_resolved \\\n\t\t-e eth.dst_resolved \\\n\t1>>${WORKDIR}${FILENAME}${EXT} \\\n\t2>>${LOG_DIR}${LOGNAME}\n\nenv GZIP=-9 tar cvzf ${WORKDIR}${FILENAME}${C_EXT} ${WORKDIR}${FILENAME}${EXT} && \\\n    cp ${WORKDIR}${FILENAME}${C_EXT} ${OUTPUT_DIR}${FILENAME}${C_EXT} && \\\n    rm ${WORKDIR}${FILENAME}${EXT} && \\\n    rm ${WORKDIR}${FILENAME}${C_EXT}\n\n\nComplete the self-explanatory variables with relevant values and call the script\nwith ./capture.sh 60 all to capture on all interfaces for 1 minute.\n\nThe resulting file will look like this\n\n\"1503238803.452442307\",\"10.1.1.126\",\"10.1.1.16\",,,,\"59435\",\"20008\",,,\"TCP\",\"59435 → 20008 [ACK] Seq=1 Ack=1 Win=4094 Len=0 TSval=1005594454 TSecr=2175465899\",,,,\"28:xx:xx:xx:xx:bf\",\"f4:xx:xx:xx:xx:54\",\"Apple_17:02:bf\",\"Elitegro_6a:7f:54\"\n\"1503238803.455425666\",\"77.243.22.33\",\"10.1.1.127\",,,,\"17548\",\"51413\",,,\"TCP\",\"17548 → 51413 [ACK] Seq=1 Ack=1 Win=16685 Len=0 SLE=1421 SRE=5681\",,,,\"e0:xx:xx:xx:xx:14\",\"90:xx:xx:xx:xx:b9\",\"Technico_d3:6a:14\",\"AsustekC_c8:f5:b9\"\n\n\nFirst is some internal traffic, and the next is incoming from the Internet.\n\nBackup to S3\nCompletely optional, but I like to back up my files to S3, here is my hourly\nscript to do that;\n\n#!/usr/bin/env bash\nAWS=`which aws`\nNOW=`date \"+%Y%m%d-%H\"`\nBUCKET=<change me>\nAWS_PROFILE=<change me>\n\nBACKUPDIR=/mnt/backup/tshark/\nTMPDIR=/tmp/${NOW}/\n\nLOG_DIR=`pwd`/\nLOGNAME=\"capture.log\"\n\necho -e \"Creating ${TMPDIR}\"\nmkdir -p ${TMPDIR} && \\\n    echo -e \"ok\"\n\necho -e \"Copy from NAS ${BACKUPDIR}\"\ncp ${BACKUPDIR}`date -d \"1 hour ago\" \"+%Y%m%d-%H\"`*.tar.gz ${TMPDIR} && \\\n    echo -e \"ok\"\necho -e \"AWS S3 upload starting\"\n$AWS s3 cp \\\n    ${TMPDIR} \\\n    s3://${BUCKET} \\\n    --profile=${AWS_PROFILE} \\\n    --exclude \"*\" \\\n    --include \"*.tar.gz\" \\\n    --recursive \\\n        >> ${LOG_DIR}/${LOGNAME} && \\\n    echo -e \"ok\"\necho -e \"Cleanup ${TMPDIR}\"\nrm -rf ${TMPDIR} && \\\n    echo -e \"done\"\n\n\nAgain you just need to change the variables.\n\nUsing the aws cli tool you can also easily sync a full directory to an S3 bucket\nfolder if your local source has capacity to keep a copy of all files that are\nbacked up, just replace cp with sync.\n\nAnalyse with Python Pandas\nI like to use Pandas mostly for my analysis, it's not the fastest or most\nefficient with large datasets but it serves me well until i encounter such\nlimitations then i pivot to the right tool for what I am trying to do. So to get\nyou started, here is my script to download csv files from S3 and compile a\nDataFrame in Pandas.\n\nimport sys\nimport threading\nimport pandas as pd\nimport boto3\nimport ntpath\nfrom datetime import datetime\nfrom pprint import pprint\n\nBUCKET = '< change me >'\nPROFILE = '<.change me >'\n\n\nclass ProgressPercentage(object):\n    def __init__(self, filename):\n        self._filename = filename\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n    def __call__(self, bytes_amount):\n        # To simplify we'll assume this is hooked up\n        # to a single filename.\n        with self._lock:\n            self._seen_so_far += bytes_amount\n            sys.stdout.write(\n                \"\\r%s --> %s bytes transferred\" % (\n                    self._filename, self._seen_so_far))\n            sys.stdout.flush()\n\ndef download(object):\n  fp = object.get('Key')\n  filename = ntpath.basename(fp)\n  obj = client.download_file(BUCKET, fp, filename, Callback=ProgressPercentage(filename))\n  print('Done')\n  return obj, filename\n\ndev = boto3.session.Session(profile_name=PROFILE)\nclient = dev.client('s3')\n\nchunksize = 10 ** 8\nobjects = client.list_objects(Bucket=BUCKET)\ndf = pd.DataFrame()\nfor object in objects.get('Contents'):\n    obj, filename = download(object)\n    for df in pd.read_csv(filename, na_values=['nan'], keep_default_na=False, compression='gzip', error_bad_lines=False, chunksize=chunksize):\n        df.append(df, ignore_index=True)\n        break\n\n\"\"\"\nUse df data frame, maybe save for Hadoop with to_hdf()\n\"\"\"\n\n\nTypically I just interrogate pcap data using tshark alone, but the above becomes\nmore useful once you've gone beyond the data in pcap which we'll be gathering\nnext.\n\nEnrich your data\nWhen it comes to IP the first thing that comes to mind is WHOIS. You won't\nalways get useful data here because you can pay extra to protect it, but\ncriminals are mostly looking to get paid not pay to protect themselves so I find\nthis to be particularly useful for investigation;\n\ndef whois(ip):\n  countries = get_countries()\n  obj = IPWhois(ip)\n  result = obj.lookup_rdap(depth=1, asn_methods=['dns', 'whois', 'http'])\n  country = countries[result['asn_country_code']]\n  type = result['network']['type']\n  name = result['network']['name']\n  description = result['asn_description']\n  registry = result['asn_registry']\n  entities = ', '.join(result['entities'])\n\n  return country, type, name, description, registry, entities\n\n\nThere are also some free services to use the IP to get some geo-location data;\n\ndef get_coords(ip):\n  url = \"https://freegeoip.net/json/%s\" % ip\n  r = requests.get(url)\n\n  return r.json()\n\n\nThis will produce something like this\n\n{\"ip\":\"xxx.xxx.xxx.xxx\",\"country_code\":\"AU\",\"country_name\":\"Australia\",\"region_code\":\"VIC\",\"region_name\":\"Victoria\",\"city\":\"Brunswick East\",\"zip_code\":\"3057\",\"time_zone\":\"Australia/Melbourne\",\"latitude\":-37.7725,\"longitude\":144.9724,\"metro_code\":0}\n\n\nWe can now use Google Maps to find out even more useful information using the\nlatitude and longitude (you'll need your own API key to perform this query)\n\ndef get_geo(lat, lon):\n  url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json?key={}&radius=1&location={},{}\".format(API_KEY, lat, lon)\n  r = requests.get(url)\n\n  return r.json()\n\n\nGoogle responds with a bunch of info, but you can get even more by taking the\nGoogle Place ID and performing another query;\n\ndef get_place(id):\n  url = \"https://maps.googleapis.com/maps/api/place/details/json?key={}&placeid={}\".format(API_KEY, id)\n  r = requests.get(url)\n\n  return r.json()\n\n\nWhich gives you a rich dataset for the location.\n\nRetaliation\nOn the offensive side, you may discover some nefarious activity and the evidence\nshows you it's from somewhere law enforcement can't help you so you decide to\ntake action into your own hands.\n\n> I recommend you report to authorities rather than go on offense\n\n\nFirst clue to find is if there is a website at this IP address, you can easily\nidentify a hostname from IP using python -c \"import socket;print\nsocket.gethostbyaddr('xxx.xxx.xxx.xxx')\".\n\nTo test if a webserver responds on an alternative port you'll first will need to\nidentify all exposed ports;\n\n#!/usr/bin/env python\n###########################\n# pip install python-nmap\n# usage:\n#    ./scanner.py -i 59.x.x.x\nimport nmap\nimport argparse\nimport termios, fcntl, sys, os\nfrom datetime import datetime\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-i --ip', help='host ip to scan', dest='ip')\nargs = parser.parse_args()\nip = args.ip\nif not ip:\n  print 'host ip must be provided'\n  sys.exit(1)\n\ndef get_items(dict_object):\n  for key in dict_object:\n    yield key, dict_object[key]\n\ndef scan_cb(host, result):\n  state = result['scan'][host]['status']['state']\n  hostname = result['scan'][host]['hostnames'][0]['name']\n  print \"[%s] Host: %s (%s)\" % (state, hostname, host)\n\n  for port, p in get_items(result['scan'][host]['tcp']):\n    port_state = p['state']\n    port_name = p['product']\n    port_info = \"%s %s\" % (p['reason'], p['extrainfo'])\n    print('[%s] Port %d: %s (%s)' % (port_state, port, port_name, port_info))\n\n\nfd = sys.stdin.fileno()\n\noldterm = termios.tcgetattr(fd)\nnewattr = termios.tcgetattr(fd)\nnewattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO\ntermios.tcsetattr(fd, termios.TCSANOW, newattr)\n\noldflags = fcntl.fcntl(fd, fcntl.F_GETFL)\nfcntl.fcntl(fd, fcntl.F_SETFL, oldflags | os.O_NONBLOCK)\n\ntry:\n  nm = nmap.PortScannerAsync()\n  nm.scan(ip, '20-23,25,53,57,67-69,80,81,82,107-113,115,118-119,135,137-139,143,153,156,170,177,179,194,209,213,218,220,300,311,366,369-371,383,384,387,389,399,427,433,434,443-445,464,465,475,491,514,515,517,518,520,521,524,530-533,540,546-548,556,560,561,563-585,587,591,593,601,604,623,625,631,635,636,639,641,646-648,653-655,657,660,666,674,688,690,691,706,711,712,749-754,760,782,783,808,832,843,847,848,873,953-61000', callback=scan_cb)\n  start_time = datetime.now()\n  while nm.still_scanning():\n    stop_time = datetime.now()\n    elapsed = stop_time - start_time\n    try:\n      c = sys.stdin.read(1)\n      if str(c) == 's':\n        print(\"elapsed %ds\" % elapsed.seconds)\n    except IOError: pass\n    nm.wait(1)\n\nfinally:\n    termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm)\n    fcntl.fcntl(fd, fcntl.F_SETFL, oldflags)\n\n\nThis will take a while, press s on the keyboard to see the elapsed time (and\nknow it's still running the scan).\n\nOnce you have some ports to test you can simply use a HTTP head request in\npython to see if a web server responds python -c \"from requests import\nrequest;print request('head', 'http://xxx.xxx.xxx.xxx:8080').headers\".\n\nKnowing that there is a web server you can use some tools like sqlmap to attempt\nSQL Injection, or ZAP to identify XSS or other known un-patched web\nvulnerabilities.\n\nIf you are really keen or the attacker was an internet connected device rather\nthan a server you could go 1 step further, with that list of open ports you can \ncross reference [https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers] \nor Google search the port number to try and identify what program is listening\non that port.\nKnowing the application is just the first step, you'll have to research to find\nout how to carry out an exploit on anything you find.\n\nFor the researchers and really bad guys\nIt's kind of terrifying how easy all of the above was to figure out, I was\nalways under the impression this stuff was difficult. It really worried me when\nI realised just how easy it would be to turn these defensive and retaliatory\ntechniques and use them purely offensively with just 16 lines of code.\n\n#!/usr/bin/env python\n##################################\n# pip install netaddr\nfrom netaddr import IPAddress, IPNetwork\n\nfile_out = \"./internet-connected.csv\"\nwith open(file_out, mode='a+') as f:\n    for class_a in range(0, 255):\n        a = IPAddress(\"%s.0.0.0\" % class_a)\n        if not a.is_reserved():\n            for class_b in range(0, 255):\n                b = IPAddress(\"%d.%d.0.0\" % (class_a, class_b))\n                if not b.is_reserved():\n                    for class_c in range(0, 255):\n                        c = IPAddress(\"%d.%d.%d.0\" % (class_a, class_b, class_c))\n                        if not c.is_reserved() and not c.is_private():\n                            ip = \"%s/24\" % c\n                            for host_ip in IPNetwork(ip).iter_hosts():\n                                if host_ip.is_unicast():\n                                    f.write(str(host_ip) + '\\n')\n\n\nThe above snippet will compile you a list of IPv4 addresses that are potentially\ninternet connected devices, all of them..\n\nWith such a list you avoid raising too much suspicion by hitting reserved\naddresses and with constant scanning a researcher might be able to identify\nhosts that are new using a diff on past scans, learn which hosts are tor exit\nnodes, compile analysis on usage of certain application usage like web servers\nor operating systems - the possibilities are pretty endless.\n\nThank you for reading and don't forget to share this if you found it\ninteresting.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/hacker.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-11-18 06:58:04","created_by":"1","updated_at":"2021-03-31 14:12:28","updated_by":"1","published_at":"2017-11-18 09:21:37","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb1","uuid":"3e485c8a-094a-4aac-8844-67e92a7d2432","title":"CISSP Study Material","slug":"cissp-study-material","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As I am currently studying to sit the CISSP exam in 2018 and because I've taken over 25,000 words in notes so far I thought I'd share what I have so that others might be able to study a bit easier.\\n\\nThe relevant CISSP material is difficult to search for mainly I believe is due to the exam changes often or as people pass they tend to only pass on notes to friends, family, or colleagues.\\n\\nI've done my best to group relevant notes together in a coherent way to follow if you're just starting out, so enjoy this brain numbing content, and don't forget to share the content if you found it useful so others may also benefit by finding it too.\\n\\n# Acronyms\\n**GRC** = Governance, Risk Management and Compliance\\n**BIA** = Business Impact Analysis\\n**BCP** = Business Continuity Plan\\n**IDS** = Intrusion detection system\\n**IPS** = Intrusion prevention system\\n**SIEM** = Security information and event management\\n**DAC** = Discretionary Access Control\\n**DRP** = Disaster Recovery Plan\\n**RPO** = Recovery Point Objective\\n**RTO** = Recovery Time Objective\\n**MTD** = Max Tolerable Downtime\\n**MOE** = Measures of Effectiveness\\n**IPS** = Voice Intrusion Prevention System\\n\\n# Fundamental Principles of Security\\n- Availability - Reliable and timely access to data and resources is provided to\\nauthorized individuals\\n- Integrity - Accuracy and reliability of the information and systems are provided\\nand any unauthorized modification is prevented\\n- Confidentiality - Necessary level of secrecy is enforced and unauthorized\\ndisclosure is prevented\\n\\n# Security Definitions\\n- Threat agent - Entity that can exploit a vulnerability, something (individual, \\nmachine, software, etc) that can exploit vulnerabilities\\n- Threat - The danger of a threat agent exploiting a vulnerability\\n- Vulnerability\\t- Weakness or a lack of countermeasure, something a threat agent\\ncan act upon, a hole that can be exploited\\n- Risk - The probability of a threat agent exploiting a vulnerability and the\\nassociated impact, threat plus the impact (asset - something important\\nto business)\\n- Exposure - Presence of a vulnerability, which exposes the organization \\nto a threat, possibility of happening\\n- Control - Safeguard that is put in place to reduce a risk, also \\ncalled a countermeasure\\n\\n# Security Framework\\n- act as reference points\\n- provide common language for communications\\n- allow us to share information and create relevancy\\n- ITIL\\n- COBIT\\n\\n# Enterprise frameworks\\n- TOGAF\\n- DoDAF\\n- MODAF\\n- SABSA\\n- COSO\\n\\n## ISO Standards\\n- ISO27000 - built on BS7799\\n\\n# Risk\\n- we have assets that threat agents may want to take advantage of \\nthrough vulnerabilities\\n- identify assets and their importance\\n- understand risk in the context of the business\\n\\n# Risk Management\\n- have a risk management policy\\n- have a risk management team\\n- start by doing risk assessment\\n\\n# Risk Assessment\\n- identifying vulnerabilities our assets face\\n- create risk profile\\n- identify as much as possible\\n- cannot find them all\\n- must continually perform assessment\\n- Four main goals\\n- identify assets and their value to the organization\\n- identify vulnerabilities and threats\\n- quantify or measure the business impact\\n- balance economically the application of a countermeasure against the cost of the countermeasure, develop cost analysis\\n- has to be supported by the upper management\\n- risk analysis team\\n- made up of specialists\\n- risk management specialists\\n- change management specialists\\n- IT knowledge specialists\\n## NIST\\n- SP800-30 rev.1\\n- http://csrc.nist.gov/publications/drafts/800-30-rev1/SP800-30-Rev1-ipd.pdf\\n## FRAP\\n- Facilitated Risk Analysis Process\\n## Octave\\nThe Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE) approach defines a risk-based strategic assessment and planning technique for security. OCTAVE is a self-directed approach, meaning that people from an organization assume responsibility for setting the organization's security strategy.\\n\\n## ISO27005\\nThe ISO27k standards are deliberately risk-aligned, meaning that organizations are encouraged to assess the security risks to their information (called “information security risks” in the standards, but in reality they are simply information risks) as a prelude to treating them in various ways. Dealing with the highest risks first makes sense from the practical implementation and management perspectives.\\n\\n# Risk Analysis Approaches\\n- Quantitative risk analysis\\n- hard measures, numbers, dollar value, fact based\\n- Qualitative risk analysis\\n- soft measure, not easily defined, opinion based\\n\\n# Quantitative Risk Measurement (Number Based)\\n- SLE Single Loss Expectancy\\n- ARO Annualized Rate of Occurrence\\n- ALE Annualized Loss Expectancy (SLE x ARO)\\n\\nFor example: purchasing a firewall. What is its purpose and capabilities on average, three times a year there is a breach, and data is compromised liability of restoring data in the end the cost of incident is $5000 this would be the single loss expectancy occurs three times, annual occurrence $15000 - 5000 * 3 = 15,000 this is ALE is the cost to mitigate greater or less than ALE.\\n\\n# Quantitative Assessment\\n- systems\\n- training\\n- vulnerabilities\\n\\n## Qualitative Risk\\n- not so much hard numbers\\n- should be contextual to business policies and compliance requirements\\n\\nQualitative risk analysis is a project management technique concerned with discovering the probability of a risk event occurring and the impact the risk will have if it does occur. All risks have both probability and impact.\\n\\n![risk-matrix](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/risk-matrix.jpg)\\n\\n### Very High\\nRequires the prompt attention of management. The stakeholder (executive or operational owner) must undertake detailed research, identify risk reduction options and prepare a detailed risk management plan. Reporting these exposures and mitigations through relevant forums is essential. Monitoring the risk mitigation strategies is essential.\\n\\n### High\\nHigh inherent risk requires the attention of the relevant manager so that appropriate controls can be set in place. The Risk & Reputation Committee monitor the implementation of key enterprise risk controls.\\n\\nHigh residual risk is to be monitored to ensure the associated controls are working. Detailed research to identify additional risk reduction options and preparation of a detailed risk management plan is required.\\n\\nReporting these exposures and mitigations through relevant forums is required. Monitoring the risk mitigation strategies is required \\n\\n### Medium\\n\\nThis is the threshold that delineates the higher-level risks from those of less concern. After considering the nature of the likelihood and consequence values in relation to whether the risk could increase in value, consider additional cost-effective mitigations to reduce the risk.\\n\\nResponsibility would fall on the relevant manager and specific monitoring of response procedures would occur.\\n\\n### Low\\nLook to accept, monitor and report on the risk or manage it through the implementation/enhancement of procedures\\n\\n### Very Low\\nThese risks would be not considered any further unless they contribute to a scenario that could pose a serious event, or they escalate in either likelihood and/or consequence\\n\\n## Likelihood\\nFind relevant definitions for each category within your business. An example of rare could just mean it could occur but only in exceptional circumstances and decades apart where almost certain risks are known to occur frequently (monthly or bi-quarterly depending on business thresholds).\\n\\n## Impact Consequence\\nFind relevant definitions for each category within your business. An example of Insignificant is an impact can be absorbed within the day-to-day business running costs where-as extreme might be contractual non-compliance or breach of legislation with penalties or fines over a considerably higher amount then perhaps major would have had and may also come with extreme brand, operational, financial or reputational impacts.\\n\\n# Implementation\\n\\n## Policies\\nOverall general statement produced by senior management\\n- high level statements of intent\\n- what expectations of correct usage\\n\\n## Standards\\nRefer to mandatory activities or rules\\n- regulatory compliance\\n- mandated based on compliance regime\\n- in US, Sarbanes-Oxley, HIPPA\\n- Basel Accords\\n- Montreal Protocol\\n\\n## Baselines\\nTemplate for comparison, measure deviation from normal\\n- standardized solution\\n- allows us to find and measure deviations\\n\\n## Guidelines\\nRecommendations\\n- not mandatory, optional\\n- best practices\\n\\n## Procedures\\nStep by step activities that need to be performed in a certain order, detailed instructions.\\n\\n# Business Continuity and Disaster Recovery\\n \\nThe goal of disaster recovery is to minimize the effects of a disaster\\nThe goal of business continuity is to resume normal business operations as quickly as possible with the least amount of resources necessary to do so\\n\\n## Disaster Recovery Plan (DRP)\\nCarried out when everything is going to still be suffering from the effects of the disaster\\n## Business Continuity Plan (BCP)\\nUsed to return the business to normal operations\\n## Business Continuity Management (BCM)\\nThe process that is responsible for DRP and BCP.\\n\\n### Over-arching process\\n\\n[NIST SP800-34](http://csrc.nist.gov/publications/PubsSPs.htm) Continuity Planning Guide for IT\\n1) Develop the continuity planning policy statement\\n2) Conduct the business impact analysis (BIA)\\n3) Identify preventive controls\\n4) Develop recovery strategies\\n5) Develop the contingency plan\\n6) Test the plan and conduct training and exercises\\n7) Maintain the plan\\n\\n[BS25999](http://www.itgovernance.co.uk/bs25999.aspx) British Standard for Business Continuity Management\\n[ISO 27031:2011](http://www.iso.org/iso/catalogue_detail?csnumber=44374) Business Continuity Planning\\n[ISO 22301:2012](http://www.iso.org/iso/catalogue_detail?csnumber=50038) Business Continuity Management Systems\\n\\nDepartment of Homeland Security BCP Kit [BCP Policy](http://www.ready.gov/business-continuity-planning-suite) Supplies the framework and governance for the BCP effort.\\n\\nContains:\\n- Scope\\n- Mission Statement\\n- Principles\\n- Guidelines\\n- Standards\\n\\nProcess to draft the policy\\n1) Identify and document the components of the policy\\n2) Identify and define existing policies that the BCP may affect\\n3) Identify pertinent laws and standards\\n4) Identify best practices\\n5) Perform a GAP analysis\\n6) Compose a draft of the new policy\\n7) Internal review of the draft\\n8) Incorporate feedback into draft\\n9) Get approval of senior management\\n10) Publish a final draft and distribute throughout organization\\n\\n## SWOT Analysis\\n- Strength\\n- Weakness\\n- Opportunity\\n- Threat\\n\\n## Business Continuity Planning Requirements\\n- Senior management support\\n- Management should be involved in setting the overall goals in continuity planning\\n\\t\\n## Business Impact Analysis (BIA)\\n- Functional analysis\\n- Identifies which of the company's critical systems are needed for survival\\n- Estimates the down-time that can be tolerated by the system\\n\\t- Maximum Tolerable Down-time (MTD)\\n\\t- Maximum Period Time of Disruption (MPTD)\\n\\n### Steps:\\n1) Select individuals to interview for data gathering\\n2) Create data gathering tools\\n\\t- Surveys\\n\\t- Questionnaires\\n3) Identify company's critical business functions\\n4) Identify the resources these functions depend on\\n5) Calculate how long these functions can survive without these resources\\n6) Identify vulnerabilities and threats to these functions\\n7) Calculate the risk for each different business function\\n8) Document findings and report them to senior management\\n\\n### Goals:\\n- Determine criticality\\n- Estimate max downtime\\n- Evaluate internal and external resource requirements\\n\\n### Process\\n- Gather information\\n- Analyse information\\n- Perform threat analysis\\n- Document results and present recommendations\\n\\n## Disaster Recovery Planning\\n\\nStage 1 - Business as usual\\nStage 2 - Disaster occurs\\nStage 3 - Recovery\\nStage 4 - Resume production\\n\\n![disaster-recovery-planning](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/disaster-recovery-planning.jpg)\\n\\n### Recovery Time Objective (RTO)\\n- The earliest time period and a service level within which a business process must be restored after a disaster\\n- RTO value is smaller than the MTD  because the MTD represents the time after which an inability to recover will mean severe damage to the business's reputation and/or the bottom line. \\n- The RTO assumes there is a period of acceptable down-time\\n\\n### Work Recovery Time (WRT)\\n- Remainder of the overall MTD value\\n\\n### Recovery Point Objective (RPO)\\n- Acceptable amount of data loss measured in time\\n\\n> MTD, RTO and RPO values are derived during the BIA\\n\\n## Risk Assessment\\nLooks at the impact and likely-hood to various threats to the business\\nGoals:\\n- Identify and document single points of failure\\n- Make a prioritized list of threats\\n- Gather information to develop risk control strategies\\n- Document acceptance of identified risks\\n- Document acknowledgment of risks that may not be addressed\\n\\n### Formula:\\nRisk = Threat * Impact * Probability\\n\\nMain components:\\n1) Review existing strategies for risk management\\n2) Construct numerical scoring system for probability and impact\\n3) Use numerical scoring to gauge effect of threats\\n4) Estimate probability of threats\\n5) Weigh each threat through the scoring system\\n6) Calculate the risk by combining the scores of likelihood and impact of each threat\\n7) Secure sponsor sign off on risk priorities\\n8) Weigh appropriate measures\\n9) Make sure planned measures do not heighten other risks\\n10) Present assessment's findings to executive management\\n\\n# Certification vs Accreditation\\n\\nCertification is the comprehensive technical evaluation of the security components and their compliance for the purpose of accreditation.\\nThe goal of a certification process is to ensure that a system, product, or network is right for the customer’s purposes.\\n\\nAccreditation is the formal acceptance of the adequacy of a system’s overall security and functionality by management.\\n\\n# Access Control\\n## Access Control Systems\\n- RADIUS (not vendor specific)\\n- AAA\\n- UDP\\n- Encrypts only the password from client to server\\n- works over PPP\\n- TACACS/xTACACS/TACACS+\\n- TCP\\n- Encrypts all traffic\\n- works over multiple protocols\\n- Diameter\\n- supports PAP, CHAP, EAP\\n- replay protection\\n- end to end protection\\n- enhanced accounting\\n\\n### Content dependent access control\\n- focused on content of data\\n- data may be labeled, like HR or Finances\\n- dependent on the context of the usage of the data\\n\\n![CISSP-access-controls](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/CISSP-access-controls.jpg)\\n> Y = Control, X = Control Category, Color key = Control types\\n\\n## Threats to systems:\\n\\nMaintenance hooks are a type of back door used by developers to get \\\"back into\\\" their code if necessary.\\n\\n### Preventive measures against back doors:\\n- Use a host intrusion detection system to watch for any attackers using back doors into the system.\\n- Use file system encryption to protect sensitive information.\\n- Implement auditing to detect any type of back door use.\\n\\t\\nA time-of-check/time-of-use (TOC/TOU) attack deals with the sequence of steps a system uses to complete a task. This type of attack takes advantage of the dependency on the timing of events that take place in a multitasking operating system. A TOC/TOU attack is when an attacker jumps in between two tasks and modifies something to control the result.\\nTo protect against TOC/TOU attacks, the operating system can apply software locks to the items it will use when it is carrying out its “checking” tasks.\\n\\nA race condition is when two different processes need to carry out their tasks on one resource. A race condition is an attack in which an attacker makes processes execute out of sequence to control the result.\\nTo protect against race condition attacks, do not split up critical tasks that can have their sequence altered.\\n\\n### TCP/IP OSI Layer\\n![layers](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/layers.JPG)\\n\\n\\n## Threats to Access Control\\n\\nDefeating Access Controls\\n* Credentials can be mirrored\\n* Passwords can be brute forced\\n* Locks can be picked\\n* You must assess the weaknesses of your solution\\n\\n## Keystroke monitoring\\n\\n### Object reuse (data remnants)\\n- object disposal\\n\\n### Tempest Shielding\\n- Faraday Cage\\n\\n### White Noise\\n- Voice assistants (Alexa, Siri, ect) can be activated by white noise\\n\\n### Control Zone\\n- SCIF: A Sensitive Compartmented Information Facility which is a defense term for a secure room.\\n\\n## NIDS\\n\\nNetwork Intrusion Detection Systems\\n\\n- Real time traffic analysis\\n- Monitor though upload to logging server\\n- passive monitoring\\n- usually per subnet\\n\\n## HIDS/HIPS\\nHost-based Intrusion Detection/Prevention System\\n\\n- Software based agent installed on machine\\n- not real time\\n- aggregated on schedule\\n\\n### How do they work\\n\\nHIDS: Passive, do not stop attack, maybe send alert\\nHIPS: Active, can stop attack as well as send alert, respond and stop attack\\n\\n- Signature Based\\n- Pattern Matching\\n- Must be kept up to date\\n- antivirus\\n- Stateful Matching\\n- Looks at sequences across traffic\\n- Building pattern based on traffic\\n- Must be kept up to date\\n- Anomaly Based\\n- looks at stream of traffic\\n- Look at expected behavior (rule based)\\n- Detects Abnormalities\\n- Could be statistical\\n- Look at raw traffic\\n- Could look at protocol Anomalies\\n- Heuristics\\n- Create virtual sandbox in memory\\n- Looks for abnormalities\\n\\n## Honeypot (honeynets)\\n- hacking magnet\\n- decoy\\n- honeynet.org\\n- enticement\\n### Enticement\\n- legal\\n- let them make the decision to attack the system\\n### Entrapment\\n- illegal\\n- trick them into attacking the system\\n\\n## Attacks against Honeypots\\n- Network Sniffer\\n    - used to capture traffic\\n    - Network monitor, etc.\\n- Dictionary attack\\n- Brute force attack\\n- Phishing\\n- Pharming\\n- Whaling\\n- Spear Phishing\\n- Vishing\\n\\n## Emanations Security\\n- Data radiating out via electrical/wireless signals\\n- TEMPEST shielding can help contain these signals\\n- Faraday cage\\n\\n## Intrusion Detection\\nIDS Types\\n- Signature Based\\n    - Stateful\\n        - Matches traffic patterns to activities\\n    - Signature\\n        - Matches individual packets to predefined definitions\\n    - Regular updates required\\n    - Cannot detect previously unknown attacks\\n- Anomaly Based\\n    - Learns the normal traffic on the network and then spots exceptions\\n    - Requires a \\\"training\\\" period to reduce false positives\\n    - Also known as heuristic scanning\\n    - Statistical detection\\n        - Looks for traffic that is outside of the statistical norm\\n    - Protocol detection\\n        - Looks for protocols that are not typically in use on the network\\n    - Traffic detection\\n        - Looks for unusual activity inside of the traffic\\n- Rule Based\\n    - Detects attack traffic through pre-defined rules\\n    - Can be coupled with expert systems to be dynamic\\n    - Cannot detect previously unknown attacks\\n\\n# Software Development Security\\n\\nA system development life cycle (SDLC) is made up of the following basic components of each phase:\\n\\n## Initiation \\n- Need for a new system is defined; This phase addresses the questions, “What do we need and why do we need it?”\\n- A preliminary risk assessment should be carried out to develop an initial description of the confidentiality, integrity, and availability requirements of the system.\\n\\t\\n## Acquisition/development\\n\\nNew system is either created or purchased\\n\\nActivities during this phase will include:\\n\\n- Requirements analysis\\n\\t- In-depth study of what functions the company needs the desired system to carry out.\\n- Formal risk assessment\\n\\t- Identifies vulnerabilities and threats in the proposed system and the potential risk levels as they pertain to confidentiality, integrity, and availability. This builds upon the initial risk assessment carried out in the previous phase. The results of this assessment help the team build the system’s security plan.\\n- Security functional requirements analysis\\n\\t- Identifies the protection levels that must be provided by the system to meet all regulatory, legal, and policy compliance needs.\\n- Security assurance requirements analysis\\n\\t- Identifies the assurance levels the system must provide. The activities that need to be carried out to ensure the desired level of confidence in the system are determined, which are usually specific types of tests and evaluations.\\n- Third-party evaluations\\n\\t- Reviewing the level of service and quality a specific vendor will provide if the system is to be purchased.\\n- Security plan\\n\\t- Documented security controls the system must contain to ensure compliance with the company’s security needs. This plan provides a complete description of the system and ties them to key company documents, as in configuration management, test and evaluation plans, system interconnection agreements, security accreditations, etc.\\n- Security test and evaluation plan\\n\\t- Outlines how security controls should be evaluated before the system is approved and deployed.\\n\\t\\n### Implementation\\n- New system is installed into production environment.\\n- A system should have baselines set pertaining to the system’s hardware, software, and firmware configuration during the implementation phase.\\n\\n### Operation/maintenance\\n- System is used and cared for.\\n- Continuous monitoring needs to take place to ensure that baselines are always met.\\n- Defined configuration management and change control procedures help to ensure that a system’s baseline is always met.\\n- Vulnerability assessments and penetration testing should also take place in this phase.\\n### Disposal\\n- System is removed from production environment.\\n- Disposal activities need to ensure that an orderly termination of the system takes place and that all necessary data are preserved.\\n\\n## Software Development Life Cycle (SDLC)\\n\\nThe life cycle of software development deals with putting repeatable and predictable processes in place that help ensure functionality, cost, quality, and delivery schedule requirements are met.\\n\\n## Phases:\\n\\n### Requirements gathering\\n- Determine the why create this software, the what the software will do, and the for whom the software will be created.\\n- This is the phase when everyone involved attempts to understand why the project is needed and what the scope of the project entails.\\n- Security requirements\\n- Security risk assessment\\n- Privacy risk assessment\\n- Risk-level acceptance\\n### Design\\n- Deals with how the software will accomplish the goals identified, which are encapsulated into a functional design.\\n- Attack surface analysis - identify and reduce the amount of code and functionality accessible to untrusted users.\\n- Threat modeling - systematic approach used to understand how different threats could be realized and how a successful compromise could take place.\\n### Development\\n- Programming software code to meet specifications laid out in the design phase.\\n- The software design that was created in the previous phase is broken down into defined deliverables, and programmers develop code to meet the deliverable requirements.\\n### Testing/Validation\\n- Validating software to ensure that goals are met and the software works as planned.\\n- It is important to map security risks to test cases and code.\\n- Tests are conducted in an environment that should mirror the production environment.\\n- Security attacks and penetration tests usually take place during this phase to identify any missed vulnerabilities.\\n- The most common testing approaches:\\n\\t- Unit testing - Individual component is in a controlled environment where programmers validate data structure, logic, and boundary conditions.\\n\\t- Integration testing - Verifying that components work together as outlined in design specifications.\\n\\t- Acceptance testing - Ensuring that the code meets customer requirements.\\n\\t- Regression testing - After a change to a system takes place, retesting to ensure functionality, performance, and protection.\\n- Fuzzing is a technique used to discover flaws and vulnerabilities in software. Fuzzing is the act of sending random data to the target program in \\torder to trigger failures.\\n- Dynamic analysis refers to the evaluation of a program in real time, i.e., when it is running. Dynamic analysis is carried out once a program has cleared the static analysis stage and basic programming flaws have been rectified offline.\\n- Static analysis a debugging technique that is carried out by examining the code without executing the program, and therefore is carried out before the program is compiled.\\n### Release/Maintenance\\n- Deploying the software and then ensuring that it is properly configured, patched, and monitored.\\n\\n## Statement of Work (SOW)\\n\\nDescribes the product and customer requirements. A detailed-oriented SOW will help ensure that these requirements are properly understood and assumptions are not made.\\n\\n### Work breakdown structure (WBS)\\nA project management tool used to define and group a project’s individual work elements in an organized manner.\\n\\n### Privacy Impact Rating\\nIndicates the sensitivity level of the data that will be processed or made accessible.\\n\\n### Computer-aided software engineering (CASE)\\nRefers to software that allows for the automated development of software, which can come in the form of program editors, debuggers, code analyzers, version-control mechanisms, and more.\\n\\n### Verification\\nDetermines if the product accurately represents and meets the specifications.\\n\\n### Validation\\nDetermines if the product provides the necessary solution for the intended real-world problem.\\n\\n## Software Development Models\\n\\n### Build and Fix Model\\nDevelopment takes place immediately with little or no planning involved. Problems are dealt with as they occur, which is usually after the software product is released to the customer.\\n\\n### Waterfall Model\\nA linear-sequential life-cycle approach. Each phase must be completed in its entirety before the next phase can begin. At the end of each phase, a review takes place to make sure the project is on the correct path and if the project should continue.\\nIn this model all requirements are gathered in the initial phase and there is no formal way to integrate changes as more information becomes available or requirements change.\\n\\n### V-Shaped Model (V-Model)\\nFollows steps that are laid out in a V format. \\nThis model emphasizes the verification and validation of the product at each phase and provides a formal method of developing testing plans as each coding phase is executed.\\nEach phase must be completed before the next phase begins.\\n\\n### Prototyping\\nA sample of software code or a model (prototype) can be developed to explore a specific approach to a problem before investing expensive time and resources.\\nRapid prototyping is an approach that allows the development team to quickly create a prototype (sample) to test the validity of the current understanding of the project requirements.\\nEvolutionary prototypes are developed, with the goal of incremental improvement.\\n\\n### Incremental Model\\nEach incremental phase results in a deliverable that is an operational product. This means that a working version of the software is produced after the first iteration and that version is improved upon in each of the subsequent iterations.\\n\\n### Spiral Model\\nUses an iterative approach to software development and places emphasis on risk analysis. The model is made up of four main phases: planning, risk analysis, development and test, and evaluation.\\n\\n### Rapid Application Development\\nRelies more on the use of rapid prototyping instead of extensive upfront planning.\\nCombines the use of prototyping and iterative development procedures with the goal of accelerating the software development process.\\n\\n### Agile Model\\nFocuses on incremental and iterative development methods that promote cross-functional teamwork and continuous feedback mechanisms.\\nThe Agile model does not use prototypes to represent the full product, but breaks the product down into individual features.\\n\\n### Capability Maturity Model Integration (CMMI)\\n- Initial Development process is ad hoc or even chaotic. The company does not use effective management procedures and plans. There is no assurance of consistency, and quality is unpredictable.\\n- Repeatable A formal management structure, change control, and quality assurance are in place. The company can properly repeat processes throughout each project. The company does not have formal process models defined.\\n- Defined Formal procedures are in place that outline and define processes carried out in each project. The organization has a way to allow for quantitative process improvement.\\n- Managed The company has formal processes in place to collect and analyze quantitative data, and metrics are defined and fed into the process improvement program.\\n- Optimizing The company has budgeted and integrated plans for continuous process improvement.\\n\\n## Software escrow\\nStoring of the source code of software with a third-party escrow agent. The software source code is released to the licensee if the licensor (software vendor) files for bankruptcy or fails to maintain and update the software product as promised in the software license agreement.\\n\\n### Programming Languages and Concepts:\\n\\n- (1st generation programming language) - Machine language is in a format that the computer’s processor can understand and work with directly.\\n- (2nd generation programming language) - Assembly language is considered a low-level programming language and is the symbolic representation of machine-level instructions. It is “one step above” machine language. It uses symbols (called mnemonics) to represent complicated binary codes. Programs written in assembly language are also hardware specific.\\n- (3rd generation programming language) - High-level languages use abstract statements. Abstraction naturalized multiple assembly language instructions into a single high-level statement, e.g., the IF – THEN – ELSE. High-level languages are processor independent. Code written in a high-level language can be converted to machine language for different processor architectures using compilers and interpreters.\\n- (4th generation programming language) - Very high-level languages focus on highly abstract algorithms that allow straightforward programming implementation in specific environments.\\n- (5th generation programming language) - Natural languages have the ultimate target of eliminating the need for programming expertise and instead use advanced knowledge-based processing and artificial intelligence.\\n\\n### Assemblers\\nTools that convert assembly language source code into machine code.\\n### Compilers\\nTools that convert high-level language statements into the necessary machine-level format (.exe, .dll, etc.) for specific processors to understand.\\n### Interpreters\\nTools that convert code written in interpreted languages to the machine-level format for processing.\\n### Garbage collector\\nIdentifies blocks of memory that were once allocated but are no longer in use and deallocates the blocks and marks them as free.\\n### Object Oriented Programming (OOP)\\nWorks with classes and objects.\\nA method is the functionality or procedure an object can carry out.\\nData hiding is provided by encapsulation, which protects an object’s private data from outside access. No object should be allowed to, or have the need to, access another object’s internal data or processes.\\n### Abstraction\\nThe capability to suppress unnecessary details so the important, inherent properties can be examined and reviewed.\\n### Polymorphism\\nTwo objects can receive the same input and have different outputs.\\n### Data modeling\\nConsiders data independently of the way the data are processed and of the components that process the data. A process used to define and analyze data requirements needed to support the business processes.\\n### Cohesion\\nA measurement that indicates how many different types of tasks a module needs to carry out.\\n### Coupling\\nA measurement that indicates how much interaction one module requires for carrying out its tasks.\\n### Data structure\\nA representation of the logical relationship between elements of data.\\n### Distributed Computing Environment (DCE)\\nThe first framework and development toolkit for developing client/server applications to allow for distributed computing.\\n### Common Object Request Broker Architecture (CORBA)\\nOpen objectoriented standard architecture developed by the Object Management Group (OMG). The standards enable software components written in different computer languages and running on different systems to communicate.\\n### Object request broker (ORB)\\nManages all communications between components and enables them to interact in a heterogeneous and distributed environment. The ORB acts as a “broker” between a client request for a service from a distributed object and the completion of that request.\\n### Component Object Model (COM)\\nA model developed by Microsoft that allows for interprocess communication between applications potentially written in different programming languages on the same computer system.\\n### Object linking and embedding (OLE)\\nProvides a way for objects to be shared on a local computer and to use COM as their foundation. It is a technology developed by Microsoft that allows embedding and linking to documents and other objects.\\n### Java Platform, Enterprise Edition (J2EE)\\nIs based upon the Java programming language, which allows a modular approach to programming code with the goal of interoperability. J2EE defines a client/server model that is object oriented and platform independent.\\n### Service-oriented architecture (SOA)\\nProvides standardized access to the most needed services to many different applications at one time. Service interactions are self-contained and loosely coupled, so that each interaction is independent of any other interaction.\\n### Simple Object Access Protocol (SOAP)\\nAn XML-based protocol that encodes messages in a web service environment.\\n### Mashup\\nThe combination of functionality, data, and presentation capabilities of two or more sources to provide some type of new service or functionality.\\n### Software as a Service (SAAS)\\nA software delivery model that allows applications and data to be centrally hosted and accessed by thin clients, commonly web browsers. A common delivery method of cloud computing.\\n### Cloud computing\\nA method of providing computing as a service rather than as a physical product. It is Internet-based computing, whereby shared resources and software are provided to computers and other devices on demand.\\n### Mobile code\\nCode that can be transmitted across a network, to be executed by a system or device on the other end.\\n### Java applets\\nSmall components (applets) that provide various functionalities and are delivered to users in the form of Java bytecode. Java applets can run in a web browser using a Java Virtual Machine (JVM). Java is platform independent; thus, Java applets can be executed by browsers for many platforms.\\n### Sandbox\\nA virtual environment that allows for very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources.\\n### ActiveX\\nA Microsoft technology composed of a set of OOP technologies and tools based on COM and DCOM. It is a framework for defining reusable software components in a programming language–independent manner.\\n### Authenticode\\nA type of code signing, which is the process of digitally signing software components and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was digitally signed. Authenticode is Microsoft’s implementation of code signing.\\n\\n## Threats for Web Environments\\n\\t\\n### Information gathering\\nUsually the first step in an attacker’s methodology, in which the information gathered may allow an attacker to infer additional information that can be used to compromise systems.\\n### Server side includes (SSI)\\nAn interpreted server-side scripting language used almost exclusively for web-based communication. It is commonly used to include the contents of one or more files into a web page on a web server. Allows web developers to reuse content by inserting the same content into multiple web documents.\\n### Client-side validation\\n- Input validation is done at the client before it is even sent back to the server to process.\\n- Path or directory traversal - Attack is also known as the “dot dot slash” because it is perpetrated by inserting the characters “../” several times into a URL to back up or traverse into directories that were not supposed to be accessible from the Web.\\n- Unicode encoding - An attacker using Unicode could effectively make the same directory traversal request without using “/” but with any of the Unicode representations of that character (three exist: %c1%1c, %c0%9v, and %c0%af)\\n- URL Encoding - %20% = a space\\n### Cross-site scripting (XSS)\\n- An attack where a vulnerability is found on a web site that allows an attacker to inject malicious code into a web application.\\n- There are three different XSS vulnerabilities:\\n\\t- Nonpersistent XSS vulnerabilities, or reflected vulnerabilities, occur when an attacker tricks the victim into processing a URL programmed with a rogue script to steal the victim’s sensitive information (cookie, session ID, etc.). The principle behind this attack lies in exploiting the lack of proper input or output validation on dynamic web sites.\\n\\t- Persistent XSS vulnerabilities, also known as stored or second order vulnerabilities, are generally targeted at web sites that allow users to input data which are stored in a database or any other such location, e.g., forums, message boards, guest books, etc. The attacker posts some text that contains some malicious JavaScript, and when other users later view the posts, their browsers render the page and execute the attackers JavaScript. \\n\\t- DOM (Document Object Model)–based XSS vulnerabilities are also referred to as local cross-site scripting. DOM is the standard structure layout to represent HTML and XML documents in the browser. In such attacks the document components such as form fields and cookies can be referenced through JavaScript. The attacker uses the DOM environment to modify the original client-side JavaScript. This causes the victim’s browser to execute the resulting abusive JavaScript code.\\n### Parameter validation\\nThe values that are being received by the application are validated to be within defined limits before the server application processes them within the system.\\n### Web proxy\\nA piece of software installed on a system that is designed to intercept all traffic between the local web browser and the web server.\\n### Replay attack\\nAn attacker capturing the traffic from a legitimate session and replaying it with the goal of masquerading an authenticated user.\\n\\t\\n## Database Management Software\\nA database is a collection of data stored in a meaningful way that enables multiple users and applications to access, view, and modify data as needed.\\n\\nAny type of database should have the following characteristics:\\n- It centralizes by not having data held on several different servers throughout the network.\\n- It allows for easier backup procedures.\\n- It provides transaction persistence.\\n- It allows for more consistency since all the data are held and maintained in one central location.\\n- It provides recovery and fault tolerance.\\n- It allows the sharing of data with multiple users.\\n- It provides security controls that implement integrity checking, access control, and the necessary level of confidentiality.\\n\\nTransaction persistence means the database procedures carrying out transactions are durable and reliable. The state of the database’s security should be the same after a transaction has occurred, and the integrity of the transaction needs to be ensured.\\t\\n\\n### Database Models\\n- **Relational**: uses attributes (columns) and tuples (rows) to contain and organize information. It presents information in the form of tables.\\n- **Hierarchical**: combines records and fields that are related in a logical tree structure. The structure and relationship between the data elements are\\tdifferent from those in a relational database. In the hierarchical database the parents can have one child, many children, or no children. The most commonly used implementation of the hierarchical model is in the Lightweight Directory Access Protocol (LDAP) model.\\n- **Network**: allows each data element to have multiple parent and child records. This forms a redundant network-like structure instead of a strict tree structure.\\n- **Object-oriented**: is designed to handle a variety of data types (images, audio, documents, video).\\n- **Object-relational**: - a relational database with a software front end that is written in an object-oriented programming language.\\n- **Record**: A collection of related data items.\\n- **File**: A collection of records of the same type.\\n- **Database**: A cross-referenced collection of data.\\n- **DBMS**: Manages and controls the database.\\n- **Tuple**: A row in a two-dimensional database.\\n- **Attribute**: A column in a two-dimensional database.\\n- **Primary key**: Columns that make each row unique. (Every row of a table must include a primary key.)\\n- **View**: A virtual relation defined by the database administrator in order to keep subjects from viewing certain data.\\n- **Foreign key**: An attribute of one table that is related to the primary key of another table.\\n- **Cell**:  An intersection of a row and a column.\\n- **Schema**: Defines the structure of the database.\\n- **Data dictionary**: Central repository of data elements and their relationships.\\n- **Rollback**: An operation that ends a current transaction and cancels all the recent changes to the database until the previous checkpoint/commit point.\\n- **Two-phase commit**: A mechanism that is another control used in databases to ensure the integrity of the data held within the database.\\n- **Cell suppression**: A technique used to hide specific cells that contain sensitive information.\\n- **Noise and perturbation**: A technique of inserting bogus information in the hopes of misdirecting an attacker or confusing the matter enough that the actual attack will not be fruitful.\\n- **Data warehousing**: Combines data from multiple databases or data sources into a large database for the purpose of providing more extensive information retrieval and data analysis.\\n- **Data mining**: The process of massaging the data held in the data warehouse into more useful information.\\n\\n### Database Programming Interfaces\\n- Open Database Connectivity (ODBC) An API that allows an application to communicate with a database, either locally or remotely\\n- Object Linking and Embedding Database (OLE DB) Separates data into components that run as middleware on a client or server. It provides a lowlevel interface to link information across different databases, and provides access to data no matter where they are located or how they are formatted.\\n- ActiveX Data Objects (ADO) An API that allows applications to access back-end database systems. It is a set of ODBC interfaces that exposes the functionality of data sources through accessible objects. ADO uses the OLE DB interface to connect with the database, and can be developed with many different scripting languages.\\n- Java Database Connectivity (JDBC) An API that allows a Java application to communicate with a database. The application can bridge through ODBC or directly to the database.\\n\\n### Data definition language (DDL)\\nDefines the structure and schema of the database.\\n### Data manipulation language (DML)\\nContains all the commands that enable a user to view, manipulate, and use the database (view, add, modify, sort, and delete commands).\\n### Query language (QL)\\nEnables users to make requests of the database.\\n\\n### Integrity:\\nDatabase software performs three main types of integrity services: semantic, referential, and entity. \\n- A semantic integrity mechanism makes sure structural and semantic rules are enforced. These rules pertain to data types, logical values, uniqueness constraints, and operations that could adversely affect the structure of the database.\\n- A database has referential integrity if all foreign keys reference existing primary keys. There should be a mechanism in place that ensures no foreign key contains a reference to a primary key of a nonexisting record, or a null value.\\n- Entity integrity guarantees that the tuples are uniquely identified by primary key values.\\n\\n### Polyinstantiation\\nThis enables a table that contains multiple tuples with the same primary keys, with each instance distinguished by a security level.\\n\\n### Online transaction processing (OLTP)\\nUsed when databases are clustered to provide fault tolerance and higher performance. The main goal of OLTP is to ensure that transactions happen properly or they don’t happen at all.\\n\\n### The ACID test:\\n- **Atomicity**: Divides transactions into units of work and ensures that all modifications take effect or none takes effect. Either the changes are committed or the database is rolled back.\\n- **Consistency**: A transaction must follow the integrity policy developed for that particular database and ensure all data are consistent in the different databases.\\n- **Isolation**: Transactions execute in isolation until completed, without interacting with other transactions. The results of the modification are not available until the transaction is completed.\\n- **Durability**: Once the transaction is verified as accurate on all systems, it is committed and the databases cannot be rolled back.\\n\\n## Expert systems\\nUse artificial intelligence (AI) to solve complex problems. They are systems that emulate the decision-making ability of a human expert. \\n\\n## Inference engine\\nA computer program that tries to derive answers from a knowledge base. It is the “brain” that expert systems use to reason about the data in the knowledge base for the ultimate purpose of formulating new conclusions.\\n\\n## Rule-based programming\\nA common way of developing expert systems, with rules based on if-then logic units, and specifying a set of actions to be performed for a given situation.\\n\\n## Artificial neural network (ANN)\\nA mathematical or computational model based on the neural structure of the brain.\\n\\n## Malware Types:\\n\\t\\n### Virus\\nA small application, or string of code, that infects host applications. It is a programming code that can replicate itself and spread from one system to another.\\n### Macro virus\\nA virus written in a macro language and that is platform independent. Since many applications allow macro programs to be embedded in documents, the programs may be run automatically when the document is opened. This provides a distinct mechanism by which viruses can be spread.\\n### Compression viruses\\nAnother type of virus that appends itself to executables on the system and compresses them by using the user’s permissions.\\n### Stealth virus\\nA virus that hides the modifications it has made. The virus tries to trick antivirus software by intercepting its requests to the operating system and providing false and bogus information.\\n### Polymorphic virus\\nProduces varied but operational copies of itself. A polymorphic virus may have no parts that remain identical between infections, making it very difficult to detect directly using signatures.\\n### Multipart virus\\nAlso called a multipartite virus, this has several components to it and can be distributed to different parts of the system. It infects and spreads in multiple ways, which makes it harder to eradicate when identified.\\n### Self-garbling virus\\nAttempts to hide from antivirus software by modifying its own code so that it does not match predefined signatures.\\n### Meme viruses\\nThese are not actual computer viruses, but types of e-mail messages that are continually forwarded around the Internet.\\n### Bots\\nSoftware applications that run automated tasks over the Internet, which perform tasks that are both simple and structurally repetitive. Malicious use of bots is the coordination and operation of an automated attack by a botnet (centrally controlled collection of bots).\\n### Worms\\nThese are different from viruses in that they can reproduce on their own without a host application and are self-contained programs.\\n### Logic bomb\\nExecutes a program, or string of code, when a certain event happens or a date and time arrives.\\n### Rootkit\\nSet of malicious tools that are loaded on a compromised system through stealthy techniques. The tools are used to carry out more attacks either on the infected systems or surrounding systems.\\n### Trojan horse\\nA program that is disguised as another program with the goal of carrying out malicious activities in the background without the user knowing.\\n### Remote access Trojans (RATs)\\nMalicious programs that run on systems and allow intruders to access and use a system remotely.\\n### Immunizer\\nAttaches code to the file or application, which would fool a virus into “thinking” it was already infected.\\n### Behavior blocking\\nAllowing the suspicious code to execute within the operating system and watches its interactions with the operating system, looking for suspicious activities.\\n\\n# Security Architecture and Design\\n\\nStakeholders for a system are the users, operators, maintainers, developers, and suppliers.\\n\\nComputer architecture encompasses all of the parts of a computer system that are necessary for it to function, including the operating system, memory chips, logic circuits, storage devices, input and output devices, security components, buses, and networking interfaces.\\n\\n## ISO/IEC 42010:2007\\nInternational standard that provides guidelines on how to create and maintain system architectures.\\n\\n## Central processing unit (CPU)\\nCarries out the execution of instructions within a computer.\\n\\n## Arithmetic logic unit (ALU)\\nComponent of the CPU that carries out logic and mathematical functions as they are laid out in the programming code being processed by the CPU.\\n\\n## Register\\nSmall, temporary memory storage units integrated and used by the CPU during its processing functions.\\n\\n## Control unit\\nPart of the CPU that oversees the collection of instructions and data from memory and how they are passed to the processing components of the CPU.\\n\\n## General registers\\nTemporary memory location the CPU uses during its processes of executing instructions. The ALU’s “scratch pad” it uses while carrying out logic and math functions.\\n\\n## Special registers\\nTemporary memory location that holds critical processing parameters. They hold values as in the program counter, stack pointer, and program status word.\\n\\n## Program counter\\nHolds the memory address for the following instructions the CPU needs to act upon.\\n\\n## Stack\\nMemory segment used by processes to communicate instructions and data to each other.\\n\\t\\n## Program status word\\nCondition variable that indicates to the CPU what mode (kernel or user) instructions need to be carried out in.\\n\\t\\n## User mode (problem state)\\nProtection mode that a CPU works within when carrying out less trusted process instructions.\\n\\t\\n## Kernel mode (supervisory state, privilege mode)\\nMode that a CPU works within when carrying out more trusted process instructions. The process has access to more computer resources when working in kernel versus user mode.\\n\\n## Address bus\\nPhysical connections between processing components and memory segments used to communicate the physical memory addresses being used during processing procedures.\\n\\n## Data bus\\nPhysical connections between processing components and memory segments used to transmit data being used during processing procedures.\\n\\t\\n## Symmetric mode multiprocessing\\nWhen a computer has two or more CPUs and each CPU is being used in a load-balancing method.\\n\\t\\n## Asymmetric mode multiprocessing\\nWhen a computer has two or more CPUs and one CPU is dedicated to a specific program while the other CPUs carry out general processing procedures.\\n\\t\\n## Process\\nProgram loaded in memory within an operating system.\\n\\n## Multiprogramming \\nInterleaved execution of more than one program (process) or task by a single operating system.\\n\\n## Multitasking\\nSimultaneous execution of more than one program (process) or task by a single operating system.\\n\\t\\n## Cooperative multitasking\\nMultitasking scheduling scheme used by older operating systems to allow for computer resource time slicing. Processes had too much control over resources, which would allow for the programs or systems to “hang.”\\n\\n## Preemptive multitasking\\nMultitasking scheduling scheme used by operating systems to allow for computer resource time slicing. Used in newer, more stable operating systems.\\n\\n## Process states (ready, running, blocked)\\nProcesses can be in various activity levels. Ready = waiting for input. Running = instructions being executed by CPU. Blocked = process is “suspended.”\\n\\n## Interrupts\\nValues assigned to computer components (hardware and software) to allow for efficient computer resource time slicing.\\n\\n## Maskable interrupt\\nInterrupt value assigned to a noncritical operating system activity.\\n\\n## Nonmaskable interrupt\\nInterrupt value assigned to a critical operating system activity.\\n\\n## Thread\\nInstruction set generated by a process when it has a specific activity that needs to be carried out by an operating system. When the activity is finished, the thread is destroyed.\\n\\n## Multithreading\\nApplications that can carry out multiple activities simultaneously by generating different instruction sets (threads).\\n\\t\\n## Software deadlock\\nTwo processes cannot complete their activities because they are both waiting for system resources to be released.\\n\\n## Process isolation\\nProtection mechanism provided by operating systems that can be implemented as encapsulation, time multiplexing of shared resources, naming distinctions, and virtual memory mapping.\\n\\nWhen a process is encapsulated, no other process understands or interacts with its internal programming code.\\n\\nEncapsulation provides data hiding, which means that outside software components will not know how a process works and will not be able to manipulate the process’s internal code. This is an integrity mechanism and enforces modularity in programming code.\\n\\n## Time multiplexing \\nA technology that allows processes to use the same resources.\\n\\n## Virtual address mapping \\nAllows the different processes to have their own memory space. The memory manager ensures no processes improperly interact with another process’s memory. This provides integrity and confidentiality for the individual processes and their data and an overall stable processing environment for the operating system.\\n\\nThe goals of memory management are to:\\n- Provide an abstraction level for programmers\\n- Maximize performance with the limited amount of memory available\\n- Protect the operating system and applications loaded into memory\\n\\nThe memory manager has five basic responsibilities:\\n\\n- Relocation\\n\\t- Swap contents from RAM to the hard drive as needed\\n\\t- Provide pointers for applications if their instructions and memory segment have been moved to a different location in main memory\\n\\n- Protection\\n\\t- Limit processes to interact only with the memory segments assigned to them\\n\\t- Provide access control to memory segments\\n\\n- Sharing\\n\\t- Use complex controls to ensure integrity and confidentiality when processes need to use the same shared memory segments\\n\\t- Allow many users with different levels of access to interact with the same application running in one memory segment\\n\\n- Logical organization\\n\\t- Segment all memory types and provide an addressing scheme for each at an abstraction level\\n\\t- Allow for the sharing of specific software modules, such as dynamic link library (DLL) procedures\\n\\n- Physical organization\\n\\t- Segment the physical memory space for application and operating system processes\\t\\n\\n## Dynamic link libraries (DLLs)\\nA set of subroutines that are shared by different applications and operating system processes.\\n\\t\\n## Base registers\\nBeginning of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.\\n\\t\\n## Limit registers\\nEnding of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.\\n\\t\\n## Memory Protection Issues:\\n- Every address reference is validated for protection.\\n- Two or more processes can share access to the same segment with potentially different access rights.\\n- Different instruction and data types can be assigned different levels of protection.\\n- Processes cannot generate an unpermitted address or gain access to an unpermitted segment\\n\\t\\n## RAM\\nMemory sticks that are plugged into a computer’s motherboard and work as volatile memory space for an operating system.\\n\\nAdditional types of RAM you should be familiar with:\\n\\n- Synchronous DRAM (SDRAM) Synchronizes itself with the system’s CPU and synchronizes signal input and output on the RAM chip. It coordinates its activities with the CPU clock so the timing of the CPU and the timing of the memory activities are synchronized. This increases the speed of transmitting and executing data.\\n- Extended data out DRAM (EDO DRAM) This is faster than DRAM because DRAM can access only one block of data at a time, whereas EDO DRAM can capture the next block of data while the first block is being sent to the CPU for processing. It has a type of “look ahead” feature that speeds up memory access.\\n- Burst EDO DRAM (BEDO DRAM) Works like (and builds upon) EDO DRAM in that it can transmit data to the CPU as it carries out a read option, but it can send more data at once (burst). It reads and sends up to four memory addresses in a small number of clock cycles.\\n- Double data rate SDRAM (DDR SDRAM) Carries out read operations on the rising and falling cycles of a clock pulse. So instead of carrying out one operation per clock cycle, it carries out two and thus can deliver twice the throughput of SDRAM. Basically, it doubles the speed of memory activities, when compared to SDRAM, with a smaller number of clock cycles.\\n\\n## Thrashing\\nWhen a computer spends more time moving data from one small portion of memory to another than actually processing the data.\\n\\t\\t\\n## ROM\\nNonvolatile memory that is used on motherboards for BIOS functionality and various device controllers to allow for operating system-to-device communication. Sometimes used for off-loading graphic rendering or cryptographic functionality.\\n\\nTypes of ROM:\\n- Programmable read-only memory (PROM) is a form of ROM that can be modified after it has been manufactured. PROM can be programmed only one time because the voltage that is used to write bits into the memory cells actually burns out the fuses that connect the individual memory cells.\\n- Erasable programmable read-only memory (EPROM) can be erased, modified, and upgraded. EPROM holds data that can be electrically erased or written to.\\n- Flash memory is a special type of memory that is used in digital cameras, BIOS chips, memory cards, and video game consoles. It is a solid-state technology, meaning it does not have moving parts and is used more as a type of hard drive than memory.\\n\\n## Hardware segmentation\\nPhysically mapping software to individual memory segments.\\n\\n## Cache memory\\nFast and expensive memory type that is used by a CPU to increase read and write operations.\\n\\n## Absolute addresses\\nHardware addresses used by the CPU.\\n\\t\\n## Logical addresses\\nIndirect addressing used by processes within an operating system. The memory manager carries out logical-to-absolute address mapping.\\n\\t\\n## Stack\\nMemory construct that is made up of individually addressable buffers. Process-to-process communication takes place through the use of stacks.\\n\\n## Buffer overflow\\nToo much data is put into the buffers that make up a stack. Common attack vector used by hackers to run malicious code on a target system.\\n\\t\\n## Bounds checking\\nEnsuring the inputted data are of an acceptable length.\\n\\n## Address space layout randomization (ASLR)\\nMemory protection mechanism used by some operating systems. The addresses used by components of a process are randomized so that it is harder for an attacker to exploit specific memory vulnerabilities.\\n\\n## Data execution prevention (DEP)\\nMemory protection mechanism used by some operating systems. Memory segments may be marked as nonexecutable so that they cannot be misused by malicious software.\\n\\n## Garbage collector\\nTool that marks unused memory segments as usable to ensure that an operating system does not run out of memory. Used to protect against memory leaks.\\n\\n## Virtual memory\\nCombination of main memory (RAM) and secondary memory within an operating system.\\t\\n\\n## Interrupt\\nSoftware or hardware signal that indicates that system resources (i.e., CPU) are needed for instruction processing.\\n\\t\\n## Programmable I/O\\nThe CPU sends data to an I/O device and polls the device to see if it is ready to accept more data.\\n\\n## Interrupt-driven I/O\\nThe CPU sends a character over to the printer and then goes and works on another process’s request.\\n\\n## I/O using Direct memory access (DMA)\\nA way of transferring data between I/O devices and the system’s memory without using the CPU.\\n\\n## Premapped I/O\\nThe CPU sends the physical memory address of the requesting process to the I/O device, and the I/O device is trusted enough to interact with the contents of memory directly, so the CPU does not control the interactions between the I/O device and memory.\\n\\n## Fully Mapped I/O\\nUnder fully mapped I/O, the operating system does not trust the I/O device.\\n\\t\\t\\n## Instruction set\\nSet of operations and commands that can be implemented by a particular processor (CPU).\\n\\t\\n## Microarchitecture\\nSpecific design of a microprocessor, which includes physical components (registers, logic gates, ALU, cache, etc.) that support a specific instruction set.\\n\\n## Ring-based architecture (Protection Rings)\\nMechanisms to protect data and functionality from faults (by improving fault tolerance) and malicious behaviour through  two or more hierarchical levels or layers of privilege within the architecture of a computer system\\n\\n## Application programming interface\\nSoftware interface that enables process-to-process interaction. Common way to provide access to standard routines to a set of software programs.\\n\\n## Monolithic operating system architecture\\nAll of the code of the operating system working in kernel mode in an ad hoc and nonmodularized manner.\\n\\n## Layered operating system architecture\\nArchitecture that separates system functionality into hierarchical layers.\\n\\t\\n## Data hiding\\nUse of segregation in design decisions to protect software components from negatively interacting with each other. Commonly enforced through strict interfaces.\\n\\n## Microkernel architecture\\nReduced amount of code running in kernel mode carrying out critical operating system functionality. Only the absolutely necessary code runs in kernel mode, and the remaining operating system code runs in user mode.\\n\\n## Hybrid microkernel architecture\\nCombination of monolithic and microkernel architectures. The microkernel carries out critical operating system functionality, and the remaining functionality is carried out in a client\\\\server model within kernel mode.\\n\\n## Mode transition\\nWhen the CPU has to change from processing code in user mode to kernel mode. This is a protection measure, but it causes a performance hit.\\n\\n## Virtualization\\nCreation of a simulated environment (hardware platform, operating system, storage, etc.) that allows for central control and scalability.\\n\\n## Hypervisor\\nCentral program used to manage virtual machines (guests) within a simulated environment (host).\\n\\n## Security policy\\nStrategic tool used to dictate how sensitive information and resources are to be managed and protected.\\n\\n## Trusted computing base\\nA collection of all the hardware, software, and firmware components within a system that provide security and enforce the system’s security policy.\\n\\n## Trusted path\\nTrustworthy software channel that is used for communication between two processes that cannot be circumvented.\\n\\t\\n## Security perimeter\\nMechanism used to delineate between the components within and outside of the trusted computing base.\\n\\n## Reference monitor\\nConcept that defines a set of design requirements of a reference validation mechanism (security kernel), which enforces an access control policy over subjects’ (processes, users) ability to perform operations (read, write, execute) on objects (files, resources) on a system.\\n\\t\\n## Security kernel\\nHardware, software, and firmware components that fall within the TCB and implement and enforce the reference monitor concept.\\n\\t\\nThe security kernel has three main requirements:\\n- It must provide isolation for the processes carrying out the reference monitor concept, and the processes must be tamperproof.\\n- It must be invoked for every access attempt and must be impossible to circumvent. Thus, the security kernel must be implemented in a complete and foolproof way.\\n- It must be small enough to be tested and verified in a complete and comprehensive manner.\\n\\n## Multilevel security policies\\nOutlines how a system can simultaneously process information at different classifications for users with different clearance levels.\\n\\t\\n## State machine models\\n- To verify the security of a system, the state is used, which means that all current permissions and all current instances of subjects accessing objects must be captured.\\n- A system that has employed a state machine model will be in a secure state in each and every instance of its existence.\\n\\n## Security Models\\n### Bell-LaPadula model: \\nThis is the first mathematical model of a multilevel security policy that defines the concept of a secure state and necessary modes of access. It ensures that information only flows in a manner that does not violate the system policy and is confidentiality focused.\\n- The simple security rule\\n\\t- A subject cannot read data at a higher security level (no read up).\\n- The *-property rule\\n\\t- A subject cannot write to an object at a lower security level (no write down).\\n- The strong star property rule\\n\\t- A subject can perform read and write functions only to the objects at its same security level.\\n\\t\\t\\n### Biba model\\nA formal state transition model that describes a set of access control rules designed to ensure data integrity.\\n- The simple integrity axiom A subject cannot read data at a lower integrity level (no read down).\\n- The *-integrity axiom A subject cannot modify an object in a higher integrity level (no write up).\\n\\t\\n### Clark-Wilson model \\nThis integrity model is implemented to protect the integrity of data and to ensure that properly formatted transactions take place. It addresses all three goals of integrity:\\n- Subjects can access objects only through authorized programs (access triple).\\n- Separation of duties is enforced.\\n- Auditing is required.\\n\\t\\n### Brewer and Nash model\\nThis model allows for dynamically changing access controls that protect against conflicts of interest. Also known as the Chinese Wall model.\\n\\n## Security Modes\\n\\nDedicated Security Mode All users must have:\\n- Proper clearance for all information on the system\\n- Formal access approval for all information on the system\\n- A signed NDA for all information on the system\\n- A valid need-to-know for all information on the system\\n- All users can access all data.\\n\\nSystem High-Security Mode All users must have:\\n- Proper clearance for all information on the system\\n- Formal access approval for all information on the system\\n- A signed NDA for all information on the system\\n- A valid need-to-know for some information on the system\\n- All users can access some data, based on their need-to-know.\\n\\nCompartmented Security Mode All users must have:\\n- Proper clearance for the highest level of data classification on the system\\n- Formal access approval for some information on the system\\n- A signed NDA for all information they will access on the system\\n- A valid need-to-know for some of the information on the system\\n- All users can access some data, based on their need-to-know and formal access approval.\\n\\nMultilevel Security Mode All users must have:\\n- Proper clearance for some of the information on the system\\n- Formal access approval for some of the information on the system\\n- A signed NDA for all information on the system\\n- A valid need-to-know for some of the information on the system\\n- All users can access some data, based on their need-to-know, clearance, and formal access approval.\\n\\n## The Common Criteria\\nUnder the Common Criteria model, an evaluation is carried out on a product and it is assigned an Evaluation Assurance Level (EAL).\\n- EAL1 Functionally tested\\n- EAL2 Structurally tested\\n- EAL3 Methodically tested and checked\\n- EAL4 Methodically designed, tested, and reviewed\\n- EAL5 Semiformally designed and tested\\n- EAL6 Semiformally verified design and tested\\n- EAL7 Formally verified design and tested\\n\\nThe Common Criteria uses protection profiles in its evaluation process.\\n\\nA protection profile contains the following five sections:\\n- Descriptive elements\\n\\t- Provides the name of the profile and a description of the security problem to be solved.\\n- Rationale\\n\\t- Justifies the profile and gives a more detailed description of the real-world problem to be solved. The environment, usage assumptions, and threats are illustrated along with guidance on the security policies that can be supported by products and systems that conform to this profile.\\n- Functional requirements\\n\\t- Establishes a protection boundary, meaning the threats or compromises within this boundary to be countered. The product or system must enforce the boundary established in this section.\\n- Development assurance requirements\\n\\t- Identifies the specific requirements the product or system must meet during the development phases, from design to implementation.\\n- Evaluation assurance requirements\\n\\t- Establishes the type and intensity of the evaluation.\\n\\n## Systems Evaluation Methods\\n- In the Trusted Computer System Evaluation Criteria (TCSEC), commonly known as the Orange Book,  the lower assurance level ratings look at a system’s protection mechanisms and testing results to produce an assurance rating, but the higher assurance level ratings look more at the system design, specifications, development procedures, supporting documentation, and testing results.\\n- An assurance evaluation examines the security-relevant parts of a system, meaning the TCB, access control mechanisms, reference monitor, kernel, and protection mechanisms. The relationship and interaction between these components are also evaluated.\\n- The Orange Book was used to evaluate whether a product contained the security properties the vendor claimed it did and whether the product was appropriate for a specific application or function.\\n- TCSEC addresses confidentiality, but not integrity. Functionality of the security mechanisms and the assurance of those mechanisms are not evaluated separately, but rather are combined and rated as a whole\\n- TCSEC provides a classification system that is divided into hierarchical divisions of assurance levels:\\n\\t- A. Verified protection\\n\\t- B. Mandatory protection\\n\\t- B1: Labeled Security\\n\\t\\t- Each data object must contain a classification label and each subject must have a clearance label. When a subject attempts to access an object,\\n\\t\\tthe system must compare the subject’s and object’s security labels to ensure the requested actions are acceptable. Data leaving the system must also contain an accurate\\n\\t\\tsecurity label. The security policy is based on an informal statement, and the design specifications are reviewed and verified.\\n\\tB2: Structured Protection\\n\\t\\t- The security policy is clearly defined and documented, and the system design and implementation are subjected to more thorough review and testing procedures. This class requires more stringent authentication mechanisms and well-defined interfaces among layers. Subjects and devices require labels, and the system must not allow covert channels.\\n\\tB3: Security Domains\\n\\t\\t- In this class, more granularity is provided in each protection mechanism, and the programming code that is not necessary to support the security policy is excluded.\\n\\t- C. Discretionary protection\\n\\t- C1: Discretionary Security Protection \\n\\t\\t- Discretionary access control is based on individuals and/or groups. It requires a separation of users and information, and identification and authentication of individual entities.\\n\\t- C2: Controlled Access Protection\\n\\t\\t- Users need to be identified individually to provide more precise access control and auditing functionality. Logical access control mechanisms are used to enforce authentication and the uniqueness of each individual’s identification.\\n\\t- D. Minimal security\\n\\t- D1: The classes with higher numbers offer a greater degree of trust and assurance. The criteria breaks down into seven different areas:\\n\\t\\t- Security policy The policy must be explicit and well defined and enforced by the mechanisms within the system.\\n\\t\\t- Identification Individual subjects must be uniquely identified.\\n\\t\\t- Labels Access control labels must be associated properly with objects.\\n\\t\\t- Documentation Documentation must be provided, including test, design, and specification documents, user guides, and manuals.\\n\\t\\t- Accountability Audit data must be captured and protected to enforce accountability.\\n\\t\\t- Life-cycle assurance Software, hardware, and firmware must be able to be tested individually to ensure that each enforces the security policy in an effective manner throughout their lifetimes.\\n\\t\\t- Continuous protection The security mechanisms and the system as a whole must perform predictably and acceptably in different situations continuously.\\n- The Red Book is referred to as the Trusted Network Interpretation (TNI). The following is a brief overview of the security items addressed in the Red Book:\\n\\t- Communication integrity\\n\\t- Authentication Protects against masquerading and playback attacks. Mechanisms include digital signatures, encryption, timestamp, and passwords.\\n\\t- Message integrity Protects the protocol header, routing information, and packet payload from being modified. Mechanisms include message authentication and encryption.\\n\\t- Nonrepudiation Ensures that a sender cannot deny sending a message. Mechanisms include encryption, digital signatures, and notarization.\\n\\t- Denial-of-service prevention\\n\\t- Continuity of operations Ensures that the network is available even if attacked. Mechanisms include fault-tolerant and redundant systems and the capability to reconfigure network parameters in case of an emergency.\\n\\t- Network management Monitors network performance and identifies attacks and failures. Mechanisms include components that enable network administrators to monitor and restrict resource access.\\n\\t- Compromise protection\\n\\t- Data confidentiality Protects data from being accessed in an unauthorized method during transmission. Mechanisms include access controls, encryption, and physical protection of cables.\\n\\t- Traffic flow confidentiality Ensures that unauthorized entities are not aware of routing information or frequency of communication via traffic analysis. Mechanisms include padding messages, sending noise, or sending false messages.\\n\\t- Selective routing Routes messages in a way to avoid specific threats. Mechanisms include network configuration and routing tables.\\n- The Information Technology Security Evaluation Criteria (ITSEC) was the first attempt at establishing a single standard for evaluating security attributes of computer systems and products by many European countries.\\n- ITSEC evaluates two main attributes of a system’s protection mechanisms: functionality and assurance.\\n- A difference between ITSEC and TCSEC is that TCSEC bundles functionality and assurance into one rating, whereas ITSEC evaluates these two attributes separately.\\n\\n\\n# Penetration testing\\n## Steps for vulnerability assessment\\n1.\\tVulnerability scanning\\n2.\\tAnalysis\\n3.\\tCommunicate results\\n\\n## Penetration test strategies\\n•\\tExternal testing\\n•\\tInternal testing\\n•\\tBlind testing\\n•\\tDouble-blind testing\\n\\n## Categories of penetration testing\\n•\\tZero knowledge\\n•\\tPartial knowledge\\n•\\tFull knowledge\\n\\n## Penetration test methodology\\n1.\\tReconnaissance\\n2.\\tEnumeration\\n3.\\tVulnerability analysis\\n4.\\tExecution / Exploitation\\n5.\\tDocument findings\\n\\n# Access Controls\\n## Primary Concerns\\n- Who owns the data?\\n- Who consumes/uses the data?\\n- Who shouldn't have access to the data?\\n\\n## Security Concepts\\n- Confidentiality\\n- Integrity\\n- Availability\\n\\n## Data control terms\\n- Subject - A person / end user\\n- Object  - The data a subject is accessing\\n\\n## Degrees of access control\\n- Read-only   - Can see, but cannot edit the data.\\n- Contributor - Can read, and also modify/add data.\\n\\n## Operational Terms\\n1. Identification - Determine the identity of a subject\\n2. Authentication - Validate a subject's identity\\n3. Authorization  - Validate their access against a directory LDAP - Lightweight Directory Access Protocol x.500 - LDAP standard\\n4. Accountability - Ensuring that the access controls are applied Logging / Recording\\n\\n### Race Condition\\nWhen processes try to carry out whatever activity they are set to perform in an incorrect order.\\ne.g. Authorizing before authenticating\\n\\n## Cookie\\n- Stored in a text file\\n- Credential information stored for repeated use\\n- Allows web sites to track a user session across multiple pages. \\n\\n## Single-Sign-On (SSO)\\n- Login once access many\\n- Subjects login once and are then able to access objects without having to re-authenticate\\n\\n## User provisioning\\n- Creation of user accounts / subjects\\n- Assignment of permissions and access at the time of creation\\n- Can be automated (Especially for large organizations)\\n\\n## Federated Identity\\n- Trust between multiple systems\\n- Subjects can access objects in separate systems\\n\\n## Markup Languages\\n- XML  - Extensible Markup Language\\n- SPML - Service Provisioning Markup Language\\n- SAML - Security Assertion Markup Language\\n- SOAP - Simple Object Access Protocol\\n\\n## Biometrics\\nSomething we \\\"are\\\"\\n- Fingerprints\\n- Handprint\\n- Signature Dynamic\\n\\n## Errors\\nType 1 - False Reject\\n    - Subject is incorrectly denied\\n\\nType 2 - False Accept\\n    - Subject is incorrectly authenticated\\n\\n### Crossover Error Rate (CER)\\n    - Where the rate of Type 1 and Type 2 intersect\\n\\n## Biometric Types\\n### Fingerprints\\n- Most common\\n- Easily captured\\n- Stored digitally, can be large\\n- Easy to fool\\n### Palm scanning\\n### Retina scanning\\n- Pattern of blood vessels in the eye\\n### Iris scan\\n- Color pattern surrounding the pupil\\n- The most accurate\\n- The most secure\\n### Signature Dynamics\\n- Force\\n- Inclination\\n- Letter characteristics\\n### Keystroke Dynamics\\n- Patterns used while typing\\n### Voice print\\n### Facial scan\\n### Hand topography\\n- Peaks and valleys on the hand\\n### Hand geometry\\n- Width of fingers\\n\\n## Passwords\\nSomething we \\\"know\\\"\\n- Various character lengths\\n- Alpha-numeric\\n- Special character\\n- String of characters identify subject\\n- The most common method of authentication\\n\\n## Password Weaknesses\\n### Electronic monitoring\\n- Sniffers\\n- Password capture devices\\n- Cain and Abel\\n- Kismet\\n### Accessing the password file\\n- Targeting the authentication server\\n- Linux\\n    - /etc/shadow\\n    - /etc/passwd\\n- Windows\\n    - SAM Hive in the registry\\n    - C:\\\\Windows\\\\Security\\n    - C:\\\\Windows\\\\Security\\\\Database\\n- Brute force attack\\n    - Attempting to \\\"guess\\\" the password\\n    - l0pht Crack\\n- Dictionary Attacks\\n    - Attempts passwords from a pre-compiled list of passwords\\n- Social Engineering\\n    - Gather information by tricking employees\\n- Rainbow tables\\n    - Collection of hash results\\n### Clipping Level\\n- Threshold\\n- Measurement of a certain amount of an activity\\n### Password Checker\\n- Checks the strength and compliance of existing passwords\\n### Password Cracker\\n- Attempts to break/decrypt a password\\n### Cognitive Password\\n- Fact or Opinion Based\\n- Mother's maiden name\\n- First pet's name\\n\\n## One-time Password (Dynamic Password)\\nPasswords that can only be used once\\n\\n## Token Devices\\n- Synchronous\\n    - Counter-synchronization\\n    - Time-synchronization\\n- Asynchronous\\n    - Challenge / Response\\n\\n## Passphrase\\n\\nSentence instead of a word\\n> \\\"I like the number 4!\\\"\\n\\n## Memory Cards\\n\\nHold information with no processing capabilities\\n\\n## Smartcards\\n- Hold information and can process\\n- Contact\\n- Contactless\\n- Attacks\\n    - Fault generation\\n    - Side-channel Attacks\\n    - Micro-probing\\n\\n## Kerberos\\t\\n\\nSee [MSDN Microsoft](http://msdn.microsoft.com/en-us/library/ff649429.aspx) and [Kerberos (RFC 1510)](ftp://ftp.isi.edu/in-notes/rfc1510.txt) and be aware it is susceptible to [time attacks](http://msdn.microsoft.com/en-us/library/ff647076.aspx ).\\n\\n## Models\\nDiscretionary Access Control (DAC): Permissions through inheritance\\n- User gains access through group membership Mandatory Access Control (MAC): Owner of data mandates who has access. \\n- Data must be classified (secret, top secret, SBU, etc). \\n- Classified by owner\\n- Sensitivity labels\\n- Role-based Access Control (RBAC)\\n- Backup administrator\\n- Task based\\n- Rule-based Access Control\\n- in gateway or border device like a router\\t\\n\\n## Constrained user interface\\n- Menu\\n- Shell\\n- Database views\\n\\n# Security & Risk Management Glossary\\n## Key Terms\\n- **Availability**: Reliable and timely access to data and resources is provided to authorized individuals.\\n- **Integrity**: Accuracy and reliability of the information and systems are provided and any unauthorized modification is prevented.\\n- **Confidentiality**: Necessary level of secrecy is enforced and unauthorized disclosure is prevented.\\n- **Shoulder surfing**: Viewing information in an unauthorized manner by looking over the shoulder of someone else.\\n- **Social engineering**: Gaining unauthorized access by tricking someone into divulging sensitive information.\\n- **Vulnerability**: Weakness or a lack of a countermeasure.\\n- **Threat agent**: Entity that can exploit a vulnerability.\\n- **Threat**: The danger of a threat agent exploiting a vulnerability.\\n- **Risk**: The probability of a threat agent exploiting a vulnerability and the associated impact.\\n- **Control**: Safeguard that is put in place to reduce a risk, also called a countermeasure.\\n- **Exposure**: Presence of a vulnerability, which exposes the organization to a threat.\\n- **Control types**: \\n\\tAdministrative, technical (logical), and physical\\n- **Control functions**:\\n\\t- **Deterrent**: Discourage a potential attacker\\n\\t- **Preventive**: Stop an incident from occurring\\n\\t- **Corrective**: Fix items after an incident has occurred\\n\\t- **Recovery**: Restore necessary components to return to normal operations\\n\\t- **Detective**: Identify an incident’s activities after it took place\\n\\t- **Compensating**: Alternative control that provides similar protection as the original control\\n- **Defense-in-depth**: Implementation of multiple controls so that successful penetration and compromise is more difficult to attain.\\n- **Security through obscurity**: Relying upon the secrecy or complexity of an item as its security, instead of practicing solid security practices.\\n- **ISO/IEC 27000 series**: Industry-recognized best practices for the development and management of an information security management system.\\n- **Zachman framework**: Enterprise architecture framework used to define and understand a business environment developed by John Zachman.\\n- **TOGAF**: Enterprise architecture framework used to define and understand a business environment developed by The Open Group.\\n- **SABSA**: framework Risk-driven enterprise security architecture that maps to business initiatives, similar to the Zachman framework.\\n- **DoDAF**: U.S. Department of Defense architecture framework that ensures interoperability of systems to meet military mission goals.\\n- **MODAF**: Architecture framework used mainly in military support missions developed by the British Ministry of Defence.\\n- **CobiT**: Set of control objectives used as a framework for IT governance developed by Information Systems Audit and Control Association (ISACA) and the IT Governance Institute (ITGI).\\n- **NIST SP 800-53**: Set of controls that are used to secure U.S. federal systems developed by NIST.\\n- **COSO**: Internal control model used for corporate governance to help prevent fraud developed by the Committee of Sponsoring Organizations (COSO) of the Treadway Commission.\\n- **ITIL**: Best practices for information technology services management processes developed by the United Kingdom’s Office of Government Commerce.\\n- **Six Sigma**: Business management strategy developed by Motorola with the goal of improving business processes.\\n- **Capability Maturity Model Integration (CMMI)**: Process improvement model developed by Carnegie Mellon.\\n- **NIST SP 800-30**: Risk Management Guide for Information Technology Systems A U.S. federal standard that is focused on IT risks.\\n- **Facilitated Risk Analysis Process (FRAP)**: A focused, qualitative approach that carries out pre-screening to save time and money.\\n- **Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE)**: Team-oriented approach that assesses organizational and IT risks through facilitated workshops.\\n- **AS/NZS 4360**: Australia and New Zealand business risk management assessment approach.\\n- **ISO/IEC 27005**: International standard for the implementation of a risk management program that integrates into an information security management system (ISMS).\\n- **Failure Modes and Effect Analysis (FMEA)**: Approach that dissects a component into its basic functions to identify flaws and those flaw's effects.\\n- **Fault tree analysis**: Approach to map specific flaws to root causes in complex systems.\\n- **CRAMM**: Central Computing and Telecommunications Agency Risk Analysis and Management Method.\\n- **Quantitative risk analysis**: Assigning monetary and numeric values to all the data elements of a risk assessment.\\n- **Qualitative risk analysis**: Opinion-based method of analyzing risk with the use of scenarios and ratings.\\n- **Single loss expectancy (SLE)**: One instance of an expected loss if a specific vulnerability is exploited and how it affects a single asset. Asset Value × Exposure Factor = SLE.\\n- **Annualized loss expectancy (ALE)**: Annual expected loss if a specific vulnerability is exploited and how it affects a single asset. SLE × ARO = ALE.\\n- **Uncertainty analysis**: Assigning confidence level values to data elements.\\n- **Delphi method**: Data collection method that happens in an anonymous fashion.\\n- **Cost/benefit analysis**: Calculating the value of a control. (ALE before implementing a control) – (ALE after implementing a control) – (annual cost of control) = value of control.\\n- **Functionality versus effectiveness of control**: Functionality is what a control does, and its effectiveness is how well the control does it.\\n- **Total risk**: Full risk amount before a control is put into place. Threats × vulnerabilities × assets = total risk.\\n- **Residual risk**: Risk that remains after implementing a control. Threats × vulnerabilities × assets × (control gap) = residual risk.\\n- **Accepted ways for handling risk**: Accept, transfer, mitigate, avoid.\\n- **Policy**: High-level document that outlines senior management’s security directives.\\n- **Standard**: Compulsory rules that support the security policies.\\n- **Guideline**: Suggestions and best practices.\\n- **Procedures**: Step-by-step implementation instructions.\\n- **Data owner**: Individual responsible for the protection and classification of a specific data set.\\n- **Data custodian**: Individual responsible for implementing and maintaining security controls to meet security requirements outlined by data owner.\\n- **Separation of duties**: Preventive administrative control used to ensure one person cannot carry out a critical task alone.\\n- **Collusion**: Two or more people working together to carry out fraudulent activities.\\n- **Rotation of duties**: Detective administrative control used to uncover potential fraudulent activities.\\n- **Mandatory vacation**: Detective administrative control used to uncover potential fraudulent activities by requiring a person to be away from the organization for a period of time.\\n- **Access controls**: are security features that control how users and systems communicate and interact with other systems and resources.\\n- **Access**: The flow of information between a subject and an object. \\n- **Subject**: An active entity that requests access to an object or the data within an object.\\n- **Object**: Can be a computer, database, file, computer program, directory, or field contained in a table within a database.\\n- **Race condition**: When processes carry out their tasks on a shared resource in an incorrect order.\\n- **User provisioning**: The creation, maintenance, and deactivation of user objects and attributes as they exist in one or more systems, directories, or applications, in response to business processes.\\n- **Federated identity**: A portable identity, and its associated entitlements, that can be used across business boundaries.\\n- **Security Assertion Markup Language (SAML)**: An XML standard that allows the exchange of authentication and authorization data to be shared between security domains.\\n- **Service Provisioning Markup Language (SPML)**: Allows for the automation of user management (account creation, amendments, revocation) and access entitlement configuration related to electronically published services across multiple provisioning systems.\\n- **Simple Object Access Protocol (SOAP)**: SOAP is a specification that outlines how information pertaining to web services is exchanged in a structured manner.\\n- **Type I error**: When a biometric system rejects an authorized individual (false rejection rate). \\n- **Type II error**: When the system accepts impostors who should be rejected(false acceptance rate).\\n- **Clipping Level**: A threshold.\\n- **Cognitive passwords**: Fact or opinion based information used to verify an individual’s identity.\\n- **Asynchronous token–generating method**: Employs a challenge/response scheme to authenticate the user.\\n- **Synchronous token device**: Synchronizes with the authentication service by using time or a counter as the core piece of the authentication process. If the\\nsynchronization is time-based, the token device and the authentication service must hold the same time within their internal clocks.\\n- **Memory card**: Holds information but cannot process information. \\n- **Smart card**: Holds information and has the necessary hardware and software to actually process that information.\\n- **Side-channel attacks**: Non-intrusive and are used to uncover sensitive information about how a component works, without trying to compromise any type of flaw or\\nweakness.\\n- **Security domain**: Resources within this logical structure (domain) are working under the same security policy and managed by the same group.\\n- **Access Control Model**: An access control model is a framework that dictates how subjects access objects.\\n- **Access Control Matrix**: A table of subjects and objects indicating what actions individual subjects can take upon individual objects.\\n- **Capability Table**: A capability table specifies the access rights a certain subject possesses pertaining to specific objects. A capability table is different from an ACL because the subject is bound to the capability table, whereas the object is bound to the ACL.\\n- **Content-based access**: Bases access decisions on the sensitivity of the data, not solely on subject identity.\\n- **Context-based access**: Bases access decisions on the state of the situation, not solely on identity or content sensitivity.\\n- **Restricted interface**: Limits the user’s environment within the system, thus limiting access to objects.\\n- **Rule-based access**: Restricts subject's access attempts by predefined rules.\\n- **Remote Authentication Dial-In User Service (RADIUS)**: A network protocol that provides client/server authentication and authorization, and audits remote users.\\n- **Central processing unit (CPU)**: A silicon component made up of integrated chips with millions of transistors that carry out the execution of instructions within a computer.\\n- **Arithmetic logic unit (ALU)**: Component of the CPU that carries out logic and mathematical functions as they are laid out in the programming code being processed by the CPU.\\n- **Register**: Small, temporary memory storage units integrated and used by the CPU during its processing functions.\\n- **Control unit**: Part of the CPU that oversees the collection of instructions and data from memory and how they are passed to the processing components of the CPU.\\n- **General registers**: Temporary memory location the CPU uses during its processes of executing instructions. The ALU’s “scratch pad” it uses while carrying out logic and math functions.\\n- **Special registers**: Temporary memory location that holds critical processing parameters. They hold values as in the program counter, stack pointer, and program status word.\\n- **Program counter**: Holds the memory address for the following instructions the CPU needs to act upon.\\n- **Stack Memory**: Segment used by processes to communicate instructions and data to each other.\\n- **Program status word (PSW)**: Condition variable that indicates to the CPU what mode (kernel or user) instructions need to be carried out in.\\n- **User mode (problem state)**: Protection mode that a CPU works within when carrying out less trusted process instructions.\\n- **Kernel mode (supervisory state, privilege mode)**: Mode that a CPU works within when carrying out more trusted process instructions. The process has access to more computer resources when working in kernel versus user mode.\\n- **Address bus**: Physical connections between processing components and memory segments used to communicate the physical memory addresses being used during processing procedures.\\n- **Data bus**: Physical connections between processing components and memory segments used to transmit data being used during processing\\nprocedures.\\n- **Symmetric mode multiprocessing**: When a computer has two or more CPUs and each CPU is being used in a load-balancing method.\\n- **Asymmetric mode multiprocessing**: When a computer has two or more CPUs and one CPU is dedicated to a specific program while the other CPUs carry out general processing procedures.\\n- **Process**: Program loaded in memory within an operating system.\\n- **Multiprogramming**: Interleaved execution of more than one program (process) or task by a single operating system.\\n- **Multitasking**: Simultaneous execution of more than one program (process) or task by a single operating system.\\n- **Cooperative multitasking**: Multitasking scheduling scheme used by older operating systems to allow for computer resource time slicing.\\n- **Preemptive multitasking**: Multitasking scheduling scheme used by operating systems to allow for computer resource time slicing. Used in newer, more stable operating systems.\\n- **Process states (ready, running, blocked)**: Processes can be in various activity levels. Ready = waiting for input. Running = instructions being executed by CPU. Blocked = process is “suspended.”\\n- **Interrupts**: Values assigned to computer components (hardware and software) to allow for efficient computer resource time slicing.\\n- **Maskable interrupt**: Interrupt value assigned to a non-critical operating system activity.\\n- **Non-maskable interrupt**: Interrupt value assigned to a critical operating system activity.\\n- **Thread**: Instruction set generated by a process when it has a specific activity that needs to be carried out by an operating system. When the activity is finished, the thread is destroyed.\\n- **Multi-threading**: Applications that can carry out multiple activities simultaneously by generating different instruction sets (threads).\\n- **Software deadlock**: Two processes cannot complete their activities because they are both waiting for system resources to be released.\\n- **Process isolation**: Protection mechanism provided by operating systems that can be implemented as encapsulation, time multiplexing of shared resources, naming distinctions, and virtual memory mapping.\\n- **Dynamic link libraries (DLLs)**: A set of subroutines that are shared by different applications and operating system processes.\\n- **Base registers**: Beginning of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.\\n- **Limit registers**: Ending of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.\\n- **RAM**: Memory sticks that are plugged into a computer’s motherboard and work as volatile memory space for an operating system.\\n- **ROM**: Non-volatile memory that is used on motherboards for BIOS functionality and various device controllers to allow for operating system-to-device communication. Sometimes used for off-loading graphic rendering or cryptographic functionality.\\n- **Hardware segmentation**: Physically mapping software to individual memory segments.\\n- **Cache memory**: Fast memory type that is used by a CPU to increase read and write operations.\\n- **Absolute addresses**: Hardware addresses used by the CPU.\\n- **Logical addresses**: Indirect addressing used by processes within an operating system. The memory manager carries out logical-to-absolute address mapping.\\n- **Stack Memory**: Construct that is made up of individually addressable buffers. Process-to-process communication takes place through the use of stacks.\\n- **Buffer overflow**: Too much data is put into the buffers that make up a stack. Common attack vector used by hackers to run malicious code on a target system.\\n- **Address space layout randomization (ASLR)**: Memory protection mechanism used by some operating systems. The addresses used by components of a process are randomized so that it is harder for an attacker to exploit specific memory vulnerabilities.\\n- **Data execution prevention (DEP)**: Memory protection mechanism used by some operating systems. Memory segments may be marked as non-executable so that they cannot be misused by malicious software.\\n- **Garbage collector**: Tool that marks unused memory segments as usable to ensure that an operating system does not run out of memory.\\n- **Virtual memory**: Combination of main memory (RAM) and secondary memory within an operating system.\\n- **Time multiplexing**: Technology that allows processes to use the same resources.\\n- **Interrupt**: Software or hardware signal that indicates that system resources (i.e., CPU) are needed for instruction processing.\\n- **Instruction set**: Set of operations and commands that can be implemented by a particular processor (CPU).\\n- **Microarchitecture**: Specific design of a microprocessor, which includes physical components (registers, logic gates, ALU, cache, etc.) that support a specific instruction set.\\n- **Application programming interface (API)**: Software interface that enables process-to-process interaction. Common way to provide access to standard routines to a set of software programs.\\n- **Monolithic operating system architecture**: All of the code of the operating system working in kernel mode in an ad-hoc and non-modularized manner.\\n- **Layered operating system architecture**: Architecture that separates system functionality into hierarchical layers.\\n- **Data hiding**: Use of segregation in design decisions to protect software components from negatively interacting with each other. Commonly enforced through strict interfaces.\\n- **Microkernel architecture**: Reduced amount of code running in kernel mode carrying out critical operating system functionality. Only the absolutely necessary code runs in kernel mode, and the remaining operating system code runs in user mode.\\n- **Hybrid microkernel architecture**: Combination of monolithic and microkernel architectures. The microkernel carries out critical operating system  functionality, and the remaining functionality is carried out in a client\\\\server model within kernel mode.\\n- **Mode transition**: When the CPU has to change from processing code in user mode to kernel mode.\\n- **Virtualization**: Creation of a simulated environment (hardware platform, operating system, storage, etc.) that allows for central control and scalability.\\n- **Hypervisor**: Central program used to manage virtual machines (guests) within a simulated environment (host). \\n- **Security policy**: Strategic tool used to dictate how sensitive information and resources are to be managed and protected.\\n- **Trusted computing base**: A collection of all the hardware, software, and firmware components within a system that provide security and enforce the system’s security policy.\\n- **Trusted path**: Trustworthy software channel that is used for communication between two processes that cannot be circumvented.\\n- **Security perimeter**: Mechanism used to delineate between the components within and outside of the trusted computing base.\\n- **Reference monitor**: Concept that defines a set of design requirements of a reference validation mechanism (security kernel), which enforces an access control policy over subject's (processes, users) ability to perform operations (read, write, execute) on objects (files, resources) on a system.\\n- **Security kernel**: Hardware, software, and firmware components that fall within the TCB and implement and enforce the reference monitor concept.\\n- **Multilevel security policies**: Outlines how a system can simultaneously process information at different classifications for users with different\\nclearance levels.\\n- **Protection profile**: Description of a needed security solution.\\n- **Target of evaluation (TOE)**: Product proposed to provide a needed security solution.\\n- **Security target**: Vendor’s written explanation of the security functionality and assurance mechanisms that meet the needed security solution.\\n- **Security functional requirements**: Individual security functions which must be provided by a product.\\n- **Security assurance requirements**: Measures taken during development and evaluation of the product to assure compliance with the claimed security functionality.\\n- **Packages—EALs**: Functional and assurance requirements are bundled into packages for reuse. This component describes what must be met to achieve specific EAL ratings.\\n- **Assurance evaluation criteria**: Check-list and process of examining the security-relevant parts of a system (TCB, reference monitor, security kernel) and assigning the system an assurance rating.\\n- **Trusted Computer System Evaluation Criteria (TCSEC) (aka Orange Book)**: U.S. DoD standard used to assess the effectiveness of the security controls built into a system. Replaced by the Common Criteria.\\n- **Information Technology Security Evaluation Criteria (ITSEC)**: European standard used to assess the effectiveness of the security controls built into a system.\\n- **Common Criteria**: International standard used to assess the effectiveness of the security controls built into a system from functional and assurance perspectives.\\n- **Certification**: Technical evaluation of the security components and their compliance to a predefined security policy for the purpose of accreditation.\\n- **Accreditation**: Formal acceptance of the adequacy of a system’s overall security by management.\\n- **Open system**: Designs are built upon accepted standards to allow for interoperability.\\n- **Closed system**: Designs are built upon proprietary procedures, which inhibit interoperability capabilities.\\n- **Maintenance hooks**: Code within software that provides a back door entry capability.\\n- **Time-of-check/time-of-use (TOC/TOU) attack**: Attacker manipulates the “condition check” step and the “use” step within software to allow for unauthorized activity.\\n- **Race condition**: Two or more processes attempt to carry out their activity on one resource at the same time. Unexpected behaviour can result if the sequence of execution does not take place in the proper order.\\n- **Open Systems Interconnection (OSI) model**: International standardization of system-based network communication through a modular seven-layer architecture.\\n- **TCP/IP model**: Standardization of device-based network communication through a modular four-layer architecture. Specific to the IP suite, created in 1970 by an agency of the U.S. Department of Defense (DoD).\\n- **Transmission Control Protocol (TCP)**: Core protocol of the TCP/IP suite, which provides connection-oriented, end-to-end, reliable network connectivity.\\n- **Internet Protocol (IP)**: Core protocol of the TCP/IP suite. Provides packet construction, addressing, and routing functionality.\\n- **User Datagram Protocol (UDP)**: Connectionless, unreliable transport layer protocol, which is considered a “best effort” protocol.\\n- **Ports**: Software construct that allows for application- **or service-specific communication between systems on a network. Ports are broken down into categories**: well known (0–1023), registered (1024–49151), and dynamic (49152–65535).\\n- **SYN flood**: DoS attack where an attacker sends a succession of SYN packets with the goal of overwhelming the victim system so that it is unresponsive to legitimate traffic.\\n- **Session hijacking**: Attack method that allows an attacker to overtake and control a communication session between two systems.\\n- **IPv6**: IP version 6 is the successor to IP version 4 and provides 128-bit addressing, integrated IPSec security protocol, simplified header formats, and some automated configuration.\\n- **Subnet**: Logical subdivision of a network that improves network administration and helps reduce network traffic congestion. Process of segmenting a network into smaller networks through the use of an addressing scheme made up of network and host portions.\\n- **Classless Interdomain Routing (CIDR)**: Variable-length subnet masking, which allows a network to be divided into different-sized subnets. The goal is to increase the efficiency of the use of IP addresses since classful addressing schemes commonly end up in unused addresses.\\n- **6to4**: Transition mechanism for migrating from IPv4 to IPv6. It allows systems to use IPv6 to communicate if their traffic has to transverse an IPv4 network.\\n- **Teredo**: Transition mechanism for migrating from IPv4 to IPv6. It allows systems to use IPv6 to communicate if their traffic has to transverse an IPv4 network, but also performs its function behind NAT devices.\\n- **Intra-Site Automatic Tunnel Addressing Protocol (ISATAP)**: An IPv6 transition mechanism meant to transmit IPv6 packets between dual-stack nodes on top of an IPv4 network.\\n- **IEEE 802.1AE (MACSec)**: Standard that specifies a set of protocols to meet the security requirements for protecting data traversing Ethernet LANs.\\n- **IEEE 802.1AR**: Standard that specifies unique per-device identifiers (DevID) and the management and cryptographic binding of a device (router, switch, access point) to its identifiers.\\n- **Digital signals**: Binary digits are represented and transmitted as discrete electrical pulses.\\n- **Analog signals**: Continuously varying electromagnetic wave that represents and transmits data.\\n- **Asynchronous communication**: Transmission sequencing technology that uses start and stop bits or similar encoding mechanism. Used in environments that transmit a variable amount of data in a periodic fashion.\\n- **Synchronous communication**: Transmission sequencing technology that uses a clocking pulse or timing scheme for data transfer synchronization.\\n- **Baseband transmission**: Uses the full bandwidth for only one communication channel and has a low data transfer rate compared to broadband.\\n- **Broadband transmission**: Divides the bandwidth of a communication channel into many channels, enabling different types of data to be transmitted at one time.\\n- **Unshielded twisted pair (UTP)**: Cabling in which copper wires are twisted together for the purposes of canceling out EMI from external sources. UTP cables are found in many Ethernet networks and telephone systems.\\n- **Shielded twisted pair (STP)**: Twisted-pair cables are often shielded in an attempt to prevent RFI and EMI. This shielding can be applied to individual pairs or to the collection of pairs.\\n- **Attenuation**: Gradual loss in intensity of any kind of flux through a medium. As an electrical signal travels down a cable, the signal can degrade and distort or corrupt the data it is carrying.\\n- **Crosstalk**: A signal on one channel of a transmission creates an undesired effect in another channel by interacting with it. The signal from one cable “spills over” into another cable.\\n- **Plenum cables**: Cable is jacketed with a fire-retardant plastic cover that does not release toxic chemicals when burned.\\n- **Ring topology**: Each system connects to two other systems, forming a single, unidirectional network pathway for signals, thus forming a ring.\\n- **Bus topology**: Systems are connected to a single transmission channel (i.e., network cable), forming a linear construct.\\n- **Star topology**: Network consists of one central device, which acts as a conduit to transmit messages. The central device, to which all other nodes are connected, provides a common connection point for all nodes.\\n- **Mesh topology**: Network where each system must not only capture and disseminate its own data, but also serve as a relay for other systems; that is, it must collaborate to propagate the data in the network.\\n- **Ethernet**: Common LAN media access technology standardized by IEEE 802.3. Uses 48-bit MAC addressing, works in contention-based networks, and has extended outside of just LAN environments.\\n- **Token ring**: LAN medium access technology that controls network communication traffic through the use of token frames. This technology has been mostly replaced by Ethernet.\\n- **Fiber Distributed Data Interface (FDDI)**: Ring-based token network protocol that was derived from the IEEE 802.4 token bus timed token protocol.\\nIt can work in LAN or MAN environments and provides fault tolerance through dual-ring architecture.\\n- **Carrier sense multiple access with collision detection (CSMA/CD)**: A media access control method that uses a carrier sensing scheme. When a transmitting system detects another signal while transmitting a frame, it stops transmitting that frame, transmits a jam signal, and then waits for a random time interval before trying to resend the frame. This reduces collisions on a network.\\n- **Carrier sense multiple access with collision avoidance (CSMA/CA)**: A media access control method that uses a carrier sensing scheme. A system wishing to transmit data has to first listen to the channel for a predetermined amount of time to determine whether or not another system is transmitting on the channel. If the channel is sensed as “idle,” then the system is permitted to begin the transmission process. If the channel is sensed as “busy,” the system defers its transmission for a random period of time.\\n- **Internet Group Management Protocol (IGMP)**: Used by systems and adjacent routers on IP networks to establish and maintain multicast group memberships.\\n- **Media access control (MAC)**: Data communication protocol sub-layer of the data link layer specified in the OSI model. It provides hardware addressing and channel access control mechanisms that make it possible for several nodes to communicate within a multiple-access network that incorporates a shared medium.\\n- **Address Resolution Protocol (ARP)**: A networking protocol used for resolution of network layer IP addresses into link layer MAC addresses.\\n- **Dynamic Host Configuration Protocol (DHCP)**: A network configuration service for hosts on IP networks. It provides IP addressing, DNS server, subnet mask, and other important network configuration data to each host through automation.\\n- **Internet Control Message Protocol (ICMP)**: A core protocol of the IP suite used to send status and error messages.\\n- **Ping of Death**: A DoS attack type on a computer that involves sending malformed or oversized ICMP packets to a target.\\n- **Smurf attack**: A DDoS attack type on a computer that floods the target system with spoofed broadcast ICMP packets.\\n- **Fraggle attack**: A DDoS attack type on a computer that floods the target system with a large amount of UDP echo traffic to IP broadcast addresses.\\n- **Simple Network Management Protocol (SNMP)**: A protocol within the IP suite that is used for network device management activities through the use of a structure that uses managers, agents, and Management Information Bases.\\n- **Domain Name System (DNS)**: A hierarchical distributed naming system for computers, services, or any resource connected to an IP based network. It associates various pieces of information with domain names assigned to each of the participating entities.\\n- **DNS zone transfer**: The process of replicating the databases containing the DNS data across a set of DNS servers.\\n- **DNSSEC**: A set of extensions to DNS that provide to DNS clients (resolvers) origin authentication of DNS data to reduce the threat of DNS poisoning, spoofing, and similar attack types.\\n- **Simple Mail Transfer Protocol (SMTP)**: An Internet standard protocol for electronic mail (e-mail) transmission across IP-based networks.\\n- **Post Office Protocol (POP)**: An Internet standard protocol used by e-mail clients to retrieve e-mail from a remote server and supports simple download-and-delete requirements for access to remote mailboxes.\\n- **Internet Message Access Protocol (IMAP)**: An Internet standard protocol used by e-mail clients to retrieve e-mail from a remote server. E-mail clients using IMAP generally leave messages on the server until the user explicitly deletes them.\\n- **Open mail relay**: An SMTP server configured in such a way that it allows anyone on the Internet to send e-mail through it, not just mail destined to or originating from known users.\\n- **E-mail spoofing**: Activity in which the sender address and other arts of the e-mail header are altered to appear as though the e-mail originated from a different source. Since SMTP does not provide any\\nauthentication, it is easy to impersonate and forge e-mails.\\n- **Sender Policy Framework (SPF)**: An e-mail validation system designed to prevent e-mail spam by detecting e-mail spoofing, a common vulnerability, by verifying sender IP addresses.\\n- **Phishing**: A way of attempting to obtain data such as usernames, passwords, credit card information, and other sensitive data by masquerading as an authenticated entity in an electronic communication. Spear phishing targets individuals, and whaling targets people with high authorization (C-Level Executives).\\n- **Network address translation (NAT)**: The process of modifying IP address information in packet headers while in transit across a traffic routing device, with the goal of reducing the demand for public IP addresses.\\n- **Distance-vector routing protocol**: A routing protocol that calculates paths based on the distance (or number of hops) and a vector (a direction).\\n- **Link-state routing protocol**: A routing protocol used in packet-switching networks where each router constructs a map of the connectivity within the network and calculates the best logical paths, which form its routing table.\\n- **Border Gateway Protocol (BGP)**: The protocol that carries out core routing decisions on the Internet. It maintains a table of IP networks, or “prefixes,” which designate network reachability among autonomous systems.\\n- **Wormhole attack**: This takes place when an attacker captures packets at one location in the network and tunnels them to another location in the network for a second attacker to use against a target system.\\n- **Spanning Tree Protocol (STP)**: A network protocol that ensures a loop-free topology for any bridged Ethernet LAN and allows redundant links to be available in case connection links go down.\\n- **Source routing**: Allows a sender of a packet to specify the route the packet takes through the network versus routers determining the path.\\n- **Multi-protocol Label Switching (MPLS)**: A networking technology that directs data from one network node to the next based on short path labels rather than long network addresses, avoiding complex lookups in a routing table.\\n- **Virtual local area network (VLAN)**: A group of hosts that communicate as if they were attached to the same broadcast domain, regardless of their physical location. VLAN membership can be configured through software instead of physically relocating devices or connections, which allows for easier centralized management.\\n- **VLAN hopping**: An exploit that allows an attacker on a VLAN to gain access to traffic on other VLANs that would normally not be accessible.\\n- **Private Branch Exchange (PBX)**: A telephone exchange that serves a particular business, makes connections among the internal telephones, and connects them to the public-switched telephone network (PSTN) via trunk lines.\\n- **Bastion host**: A highly exposed device that will most likely be targeted for attacks, and thus should be properly locked down.\\n- **Dual-homed firewall**: This device has two interfaces and sits between an untrusted network and trusted network to provide secure access.\\n- **Screened host**: A firewall that communicates directly with a perimeter router and the internal network. The router carries out filtering activities on the traffic before it reaches the firewall.\\n- **Screened subnet architecture**: When two filtering devices are used to create a DMZ. The external device screens the traffic entering the DMZ network, and the internal filtering device screens the traffic before it enters the internal network.\\n- **Proxy server**: A system that acts as an intermediary for requests from clients seeking resources from other sources. A client connects to the proxy server, requesting some service, and the proxy server evaluates the request according to its filtering rules and makes the connection on behalf of the client. Proxies can be open or carry out forwarding or reverse forwarding capabilities.\\n- **Honeypots**: Systems that entice with the goal of protecting critical production systems. If two or more honeypots are used together, this is considered a honeynet.\\n- **Network convergence**: The combining of server, storage, and network capabilities into a single framework, which decreases the costs and complexity of data centers. Converged infrastructures provide the ability to pool resources, automate resource provisioning, and increase and decrease processing capacity quickly to meet the needs of dynamic computing workloads.\\n- **Cloud computing**: The delivery of computer processing capabilities as a service rather than as a product, whereby shared resources, software, and information are provided to end users as a utility. Offerings are usually bundled as an infrastructure, platform, or software.\\n- **Metropolitan area network (MAN)**: A network that usually spans a city or a large campus, interconnects a number of LANs using a high capacity backbone technology, and provides up-link services to WANs or the Internet.\\n- **Synchronous Optical Networking (SONET) and Synchronous Digital Hierarchy (SDH)**: Standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber and allow for simultaneous transportation of many different circuits of differing origin within a single framing protocol.\\n- **Metro Ethernet**: A data link technology that is used as a metropolitan area network to connect customer networks to larger service networks or the Internet.\\n- **Wide area network (WAN)**: A telecommunication network that covers a broad area and allows a business to effectively carry out its daily function, regardless of location.\\n- **Multiplexing**: A method of combining multiple channels of data over a single transmission line.\\n- **T-carriers**: Dedicated lines that can carry voice and data information over trunk lines. \\n- **Time-division multiplexing (TDM)**: A type of multiplexing in which two or more bit streams or signals are transferred apparently simultaneously as sub-channels in one communication channel, but are physically taking turns on the single channel.\\n- **Wave-division multiplexing (WDM)**: Multiplying the available capacity of optical fibers through use of parallel channels, with each channel on a dedicated wavelength of light. The bandwidth of an optical fiber can be divided into as many as 160 channels.\\n- **Frequency-division multiplexing (FDM)**: Dividing available bandwidth into a series of non-overlapping frequency sub-bands that are then assigned to each communicating source and user pair. FDM is inherently an analog technology.\\n- **Statistical time-division multiplexing (STDM)**: Transmitting several types of data simultaneously across a single transmission line. STDM technologies analyze statistics related to the typical workload of each input device and make real-time decisions on how much time each device should be allocated for data transmission.\\n- **Channel Service Unit (CSU)**: A line bridging device for use with T-carriers, and that is required by PSTN providers at digital interfaces that terminate in a Data Service Unit (DSU) on the customer side. The DSU is a piece of telecommunications circuit terminating equipment that transforms digital data between telephone company lines and local equipment.\\n- **Public-switched telephone network (PSTN)**: The public circuit-switched telephone network, which is made up of telephone lines, fiber-optic cables, cellular networks, communications satellites, and undersea telephone cables and allows all phone-to-phone communication. It was a fixed-line analog telephone system, but is now almost entirely digital and includes mobile as well as fixed telephones.\\n- **Voice over IP (VoIP)**: The set of protocols, technologies, methodologies, and transmission techniques involved in the delivery of voice data and\\nmultimedia sessions over IP-based networks.\\n- **Session Initiation Protocol (SIP)**: The signaling protocol widely used for controlling communication, as in voice and video calls over IPbased\\nnetworks.\\n- **Vishing (voice and phishing)**: Social engineering activity over the telephone system, most often using features facilitated by VoIP, to gain unauthorized access to sensitive data.\\n- **H.323**: A standard that addresses call signaling and control, multimedia transport and control, and bandwidth control for point-to-point and multipoint conferences.\\n- **Real-time Transport Protocol (RTP)**: Used to transmit audio and video over IP-based networks. It is used in conjunction with the RTCP. RTP transmits the media data, and RTCP is used to monitor transmission statistics and QoS, and aids synchronization of multiple data streams.\\n- **War dialing**: When a specialized program is used to automatically scan a list of telephone numbers to search for computers for the purposes of exploitation and hacking.\\n- **Integrated Services Digital Network (ISDN)**: A circuit-switched telephone network system technology designed to allow digital transmission of voice and data over ordinary telephone copper wires.\\n- **Digital Subscriber Line (DSL)**: A set of technologies that provide Internet access by transmitting digital data over the wires of a local telephone network. DSL is used to digitize the “last mile” and provide fast Internet connectivity.\\n- **Cable modem**: A device that provides bidirectional data communication via radio frequency channels on cable TV infrastructures. Cable modems are primarily used to deliver broadband Internet access to homes.\\n- **Algorithm**: Set of mathematical and logic rules used in cryptographic functions.\\n- **Cipher**: Another name for algorithm.\\n- **Cryptography**: Science of secret writing that enables an entity to store and transmit data in a form that is available only to the intended individuals.\\n- **Cryptosystem**: Hardware or software implementation of cryptography that contains all the necessary software, protocols, algorithms, and keys.\\n- **Cryptanalysis**: Practice of uncovering flaws within cryptosystems.\\n- **Cryptology**: The study of both cryptography and cryptanalysis.\\n- **Encipher**: Act of transforming data into an unreadable format.\\n- **Decipher**: Act of transforming data into a readable format.\\n- **Key**: Sequence of bits that are used as instructions that govern the acts of cryptographic functions within an algorithm.\\n- **Key clustering**: Instance when two different keys generate the same ciphertext from the same plaintext.\\n- **Keyspace**: A range of possible values used to construct keys.\\n- **Plaintext**: Data in readable format, also referred to as cleartext.\\n- **Substitution Cipher**: Encryption method that uses an algorithm that changes out (substitutes) one value for another value. \\n- **Scytale Cipher**: Ancient encryption tool that used a type of paper and rod used by Greek military factions.\\n- **Kerckhoffs’ Principle**: Concept that an algorithm should be known and only the keys should be kept secret.\\n- **One-time pad**: Encryption method created by Gilbert Vernam that is considered impossible to crack if carried out properly.\\n- **Random Number generator**: Algorithm used to create values that are used in cryptographic functions to add randomness.\\n- **Running Key Cipher**: Substitution cipher that creates keystream values, commonly from agreed-upon text passages, to be used for encryption purposes.\\n- **Concealment Cipher**: Encryption method that hides a secret message within an open message.\\n- **Steganography**: Method of hiding data in another media type.\\n- **Digital Rights Management (DRM)**: Access control technologies commonly used to protect copyright material.\\n- **Transposition**: Encryption method that shifts (permutation) values.\\n- **Caesar Cipher**: Simple substitution algorithm created by Julius Caesar that shifts alphabetic values three positions during its encryption and decryption processes\\n- **Frequency analysis**: Cryptanalysis process used to identify weaknesses within cryptosystems by locating patterns in resulting ciphertext.\\n- **Key Derivation Functions (KDFs)**: Generation of secret keys (subkeys) from an initial value (master key).\\n- **Symmetric algorithm**: Encryption method where the sender and receiver use an instance of the same key for encryption and decryption purposes.\\n- **Out-of-band method**: Sending data through an alternate communication channel.\\n- **Asymmetric algorithm**: Encryption method that uses two different key types, public and private. Also called public key cryptography.\\n- **Public key**: Value used in public key cryptography that is used for encryption and signature validation that can be known by all parties.\\n- **Private key**: Value used in public key cryptography that is used for decryption and signature creation and known to only key owner.\\n- **Public key cryptography**: Asymmetric cryptography, which uses public and private key values for cryptographic functions.\\n- **Block cipher**: Symmetric algorithm type that encrypts chunks (blocks) of data at a time.\\n- **Diffusion**: Transposition processes used in encryption functions to increase randomness.\\n- **Confusion**: Substitution processes used in encryption functions to increase randomness.\\n- **Avalanche effect**: Algorithm design requirement so that slight changes to the input result in drastic changes to the output.\\n- **Stream cipher**: Algorithm type that generates a keystream (random values), which is XORd with plaintext for encryption purposes.\\n- **Keystream generator**: Component of a stream algorithm that creates random values for encryption purposes.\\n- **Initialization vectors (IVs)**: Values that are used with algorithms to increase randomness for cryptographic functions.\\n- **Hybrid cryptography**: Combined use of symmetric and asymmetric algorithms where the symmetric key encrypts data and an asymmetric key encrypts the symmetric key.\\n- **Session keys**: Symmetric keys that have a short lifespan, thus providing more protection than static keys with longer lifespans.\\n- **Rijndael**: Block symmetric cipher that was chosen to fulfil the Advanced Encryption Standard. It uses a 128-bit block size and various key lengths (128, 192, 256).\\n- **Triple DES (3-DES)**: Symmetric cipher that applies DES three times to each block of data during the encryption process.\\n- **International Data Encryption Algorithm (IDEA)**: Block symmetric cipher that uses a 128-bit key and 64-bit block size.\\n- **Blowfish**: Block symmetric cipher that uses 64-bit block sizes and variable-length keys.\\n- **RC4**: Stream symmetric cipher that was created by Ron Rivest of RSA. Used in SSL and WEP.\\n- **RC5**: Block symmetric cipher that uses variable block sizes (32, 64, 128) and variable-length key sizes (0–2040).\\n- **RC6**: Block symmetric cipher that uses a 128-bit block size and variable length key sizes (128, 192, 256). Built upon the RC5 algorithm.\\n- **Diffie-Hellman algorithm**: First asymmetric algorithm created and is used to exchange symmetric key values. Based upon logarithms in finite fields.\\n- **El Gamal algorithm**: Asymmetric algorithm based upon the Diffie-Hellman algorithm used for digital signatures, encryption, and key exchange.\\n- **Elliptic curve cryptosystem algorithm**: Asymmetric algorithm based upon the algebraic structure of elliptic curves over finite fields. Used for digital signatures, encryption, and key exchange.\\n- **Zero knowledge proof**: One entity can prove something to be true without providing a secret value.\\n- **One-way hash**: Cryptographic process that takes an arbitrary amount of data and generates a fixed-length value. Used for integrity protection.\\n- **Message authentication code (MAC)**: Keyed cryptographic hash function used for data integrity and data origin authentication.\\n- **Hashed message authentication code (HMAC)**: Cryptographic hash function that uses a symmetric key value and is used for data integrity and data origin authentication.\\n- **CBC-MAC**: Cipher block chaining message authentication code uses encryption for data integrity and data origin authentication.\\n- **CMAC**: Cipher message authentication code that is based upon and provides more security compared to CBC-MAC.\\n- **CMM**: Block cipher mode that combines the CTR encryption mode and CBC-MAC. One encryption key is used for both authentication and encryption purposes.\\n- **Collision**: When two different messages are computed by the same hashing algorithm and the same message digest value results.\\n- **Birthday attack**: Cryptographic attack that exploits the mathematics behind the birthday problem in the probability theory forces collisions within hashing functions.\\n- **Digital signature**: Ensuring the authenticity and integrity of a message through the use of hashing algorithms and asymmetric algorithms. The message digest is encrypted with the sender’s private key.\\n- **Certification Authority**: Component of a PKI that creates and maintains digital certificates throughout their life cycles.\\n- **Registration Authority (RA)**: Component of PKI that validates the identity of an entity requesting a digital certificate.\\n- **Certificate Revocation List (CRL)**: List that is maintained by the certificate authority of a PKI that contains information on all of the digital certificates that have been revoked.\\n- **Online Certificate Status Protocol (OCSP)**: Automated method of maintaining revoked certificates within a PKI.\\n- **Certificate**: Digital identity used within a PKI. Generated and maintained by a certificate authority and used for authentication.\\n- **Link encryption**: Technology that encrypts full packets (all headers and data payload) and is carried out without the sender’s interaction.\\n- **End-to-end encryption**: Encryption method used by the sender of data that encrypts individual messages and not full packets.\\n- **Multipurpose Internet Mail Extension (MIME)**: Standard that outlines the format of e-mail messages and allows binary attachments to be transmitted through e-mail.\\n- **Secure MIME (S/MIME)**: Secure/Multipurpose Internet Mail Extensions, which outlines how public key cryptography can be used to secure MIME data types.\\n- **Pretty Good Privacy (PGP) Cryptosystem**: used to integrate public key cryptography with e-mail functionality and data encryption, which was developed by Phil Zimmerman.\\n- **Quantum cryptography**: Use of quantum mechanical functions to provide strong cryptographic key exchange.\\n- **HTTPS**: A combination of HTTP and SSL\\\\TLS that is commonly used for secure Internet connections and e-commerce transactions.\\n- **Secure Electronic Transaction (SET)**: Secure e-commerce standard developed by Visa and MasterCard that has not been accepted within the marketplace.\\n- **Cookies**: Data files used by web browsers and servers to keep browser state information and browsing preferences.\\n- **Secure Shell (SSH)**: Network protocol that allows for a secure connection to a remote system. Developed to replace Telnet and other insecure remote shell methods.\\n- **IPSec**: Protocol suite used to protect IP traffic through encryption and authentication. De facto standard VPN protocol.\\n- **Authentication header (AH) protocol**: Protocol within the IPSec suite used for integrity and authentication.\\n- **Encapsulating Security Payload Protocol (ESP)**: Protocol within the IPSec suite used for integrity, authentication, and encryption.\\n- **Transport mode**: Mode that IPSec protocols can work in that provides protection for packet data payload.\\n- **Tunnel mode**: Mode that IPSec protocols can work in that provides protection for packet headers and data payload.\\n- **Internet Security Association and Key Management Protocol (ISAKMP)**: Used to establish security associates and an authentication framework in Internet connections. Commonly used by IKE for key exchange.\\n- **Passive attack**: Attack where the attacker does not interact with processing or communication activities, but only carries out observation and data collection, as in network sniffing.\\n- **Active attack**: Attack where the attacker does interact with processing or communication activities.\\n- **Ciphertext-only attack**: Cryptanalysis attack where the attacker is assumed to have access only to a set of ciphertexts.\\n- **Known-plaintext attack**: Cryptanalysis attack where the attacker is assumed to have access to sets of corresponding plaintext and ciphertext.\\n- **Chosen-plaintext attack**: Cryptanalysis attack where the attacker can choose arbitrary plaintexts to be encrypted and obtain the corresponding ciphertexts.\\n- **Chosen-ciphertext attack**: Cryptanalysis attack where the attacker chooses a ciphertext and obtains its decryption under an unknown key.\\n- **Differential cryptanalysis**: Cryptanalysis method that uses the study of how differences in an input can affect the resultant difference at the output.\\n- **Linear cryptanalysis**: Cryptanalysis method that uses the study of affine transformation approximation in encryption processes.\\n- **Side-channel attack**: Attack that uses information (timing, power consumption) that has been gathered to uncover sensitive data or processing functions.\\n- **Replay attack**: Valid data transmission is maliciously or fraudulently repeated to allow an entity gain unauthorized access.\\n- **Algebraic attack**: Cryptanalysis attack that exploits vulnerabilities within the intrinsic algebraic structure of mathematical functions.\\n- **Analytic attack**: Cryptanalysis attack that exploits vulnerabilities within the algorithm structure.\\n- **Statistical attack**: Cryptanalysis attack that uses identified statistical patterns.\\n- **Social engineering attack**: Manipulating individuals so that they will divulge confidential information, rather than by breaking in or using technical cracking techniques.\\n- **Meet-in-the-middle attack**: Cryptanalysis attack that tries to uncover a mathematical problem from two different ends.\\n- **Business continuity management (BCM)**: is the overarching approach to\\nmanaging all aspects of BCP and DRP.\\n- **Business Continuity Plan (BCP)**: Contains strategy documents that provide detailed procedures that ensure critical business functions are maintained and\\nthat help minimize losses of life, operations, and systems. A BCP provides procedures for emergency responses, extended backup operations, and post-disaster recovery.\\n- **Business Impact Analysis (BIA)**: One of the most important first steps in the planning development. Qualitative and quantitative data on the business impact of a disaster need to be gathered, analyzed, interpreted, and presented to management.\\n- **A reciprocal agreement**: One in which a company promises another company it can move in and share space if it experiences a disaster, and vice versa. Reciprocal agreements are very tricky to implement and are unenforceable.\\n- **A hot site**: Fully configured with hardware, software, and environmental needs. It can usually be up and running in a matter of hours. It is the most\\nexpensive option, but some companies cannot be out of business longer than a day without very detrimental results.\\n- **A warm site**: Does not have computers, but it does have some peripheral devices, such as disk drives, controllers, and tape drives. This option is less expensive than a hot site, but takes more effort and time to become operational.\\n- **A cold site**: Is just a building with power, raised floors, and utilities. No devices are available. This is the cheapest of the three options, but can take weeks to get up and operational.\\n- **Recovery Time Objective (RTO)**: The earliest time period and a service level within which a business process must be restored after a disaster to avoid\\nunacceptable consequences.\\n- **Recovery Point Objective (RPO)**: The acceptable amount of data loss measured in time.\\n- **Mean Time Between Failures (MTBF)**: The predicted amount of time between inherent failures of a system during operation.\\n- **Mean Time To Repair (MTTR)**: A measurement of the maintainability by representing the average time required to repair a failed component or device.\\n- **High availability**: Refers to a system, component, or environment that is continuously operational.\\n- **A checklist test**: Copies of the plan are handed out to each functional area for examination to ensure the plan properly deals with the area’s needs and\\nvulnerabilities.\\n- **A structured walk-through test**: Representatives from each functional area or department get together and walk through the plan from beginning to end.\\n- **A simulation test**: A practice execution of the plan takes place. A specific scenario is established, and the simulation continues up to the point of actual\\nrelocation to the alternate site.\\n- **A parallel test**: One in which some systems are actually run at the alternate site.\\n- **A full-interruption test**: One in which regular operations are stopped and processing is moved to the alternate site.\\n- **Remote journaling**: Involves transmitting the journal or transaction log offsite to a backup facility.\\n- **Dumpster diving**: Refers to going through someone’s trash to find confidential or useful information. It is legal, unless it involves trespassing, but in all cases it is considered unethical.\\n- **Wiretapping**: A passive attack that eavesdrops on communications. It is only legal with prior consent or a warrant.\\n- **Data diddling**: The act of willfully modifying information, programs, or documentation in an effort to commit fraud or disrupt production.\\n- **Patent**: Grants ownership and enables that owner to legally enforce his rights to exclude others from using the invention covered by the patent.\\n- **Copyright**: Protects the expression of ideas rather than the ideas themselves.\\n- **Trademarks**: Protect words, names, product shapes, symbols, colors, or a combination of these used to identify products or a company. These items are used to distinguish products from the competitors’ products.\\n- **Trade secrets**: Are deemed proprietary to a company and often include information that provides a competitive edge. The information is protected as long as the owner takes the necessary protective actions.\\n- **Personally Identifiable Information (PII)**: Data that can be used to uniquely identify, contact, or locate a single person or can be used with other sources to\\nuniquely identify a single individual.\\n- **System Development Life Cycle (SDLC)**: A methodical approach to standardize requirements discovery, design, development, testing, and implementation in every phase of a system. It is made up of the following phases: initiation, acquisition/development, implementation, operation/maintenance, and disposal.\\n- **Certification**: The technical testing of a system.\\n- **Accreditation**: The formal authorization given by management to allow a system to operate in a specific environment.\\n- **Statement of Work (SOW)**: Describes the product and customer requirements. A detailed-oriented SOW will help ensure that these requirements are properly understood and assumptions are not made.\\n- **Work breakdown structure (WBS)**: A project management tool used to define and group a project’s individual work elements in an organized manner.\\n- **Attack surface**: Components available to be used by an attacker against the product itself.\\n- **Threat modeling**: A systematic approach used to understand how different threats could be realized and how a successful compromise could take place.\\n- **Static analysis**: A debugging technique that is carried out by examining the code without executing the program, and therefore is carried out before the program is compiled.\\n- **Fuzzing**: A technique used to discover flaws and vulnerabilities in software.\\n- **Verification**: Determines if the product accurately represents and meets the specifications.\\n- **Validation**: Determines if the product provides the necessary solution for the intended real-world problem.\\n- **Capability Maturity Model Integration (CMMI) model**: A process improvement approach that provides organizations with the essential elements of effective processes, which will improve their performance.\\n- **Change control**: The process of controlling the changes that take place during the life cycle of a system and documenting the necessary change control activities.\\n- **Software Configuration Management (SCM)**: Identifies the attributes of software at various points in time, and performs a methodical control of changes for the purpose of maintaining software integrity and traceability throughout the software development life cycle.\\n- **Software escrow**: Storing of the source code of software with a third-party escrow agent. The software source code is released to the licensee if the licensor (software vendor) files for bankruptcy or fails to maintain and update the software product as promised in the software license agreement.\\n- **Machine language**: A set of instructions in binary format that the computer’s processor can understand and work with directly.\\n- **Assembly language**: A low-level programming language that is the mnemonic representation of machine-level instructions.\\n- **Assemblers**: Tools that convert assembly code into the necessary machine-compatible binary language for processing activities to take place.\\n- **High-level languages**: Otherwise known as third-generation programming languages, due to their refined programming structures, using abstract statements.\\n- **Very high-level languages**: Otherwise known as fourth-generation programming languages and are meant to take natural language-based statements one step ahead.\\n- **Natural languages**: Otherwise known as fifth-generation programming languages, which have the goal to create software that can solve problems by themselves. Used in systems that provide artificial intelligence.\\n- **Compilers**: Tools that convert high-level language statements into the necessary machine-level format (.exe, .dll, etc.) for specific processors to understand.\\n- **Interpreters**: Tools that convert code written in interpreted languages to the machine-level format for processing.\\n- **Garbage collector**: Identifies blocks of memory that were once allocated but are no longer in use and deallocates the blocks and marks them as free.\\n- **Abstraction**: The capability to suppress unnecessary details so the important, inherent properties can be examined and reviewed.\\n- **Polymorphism**: Two objects can receive the same input and have different outputs.\\n- **Data modeling**: Considers data independently of the way the data are processed and of the components that process the data. A process\\nused to define and analyze data requirements needed to support the business processes.\\n- **Cohesion**: A measurement that indicates how many different types of tasks a module needs to carry out.\\n- **Coupling**: A measurement that indicates how much interaction one module requires for carrying out its tasks.\\n- **Data structure**: A representation of the logical relationship between elements of data.\\n- **Mobile code**: Code that can be transmitted across a network, to be executed by a system or device on the other end.\\n- **Java applets**: Small components (applets) that provide various functionalities and are delivered to users in the form of Java bytecode. Java applets can run in a web browser using a Java Virtual Machine (JVM). Java is platform independent; thus, Java applets can be executed by browsers for many platforms.\\n- **Sandbox**: A virtual environment that allows for very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources.\\n- **ActiveX**: A Microsoft technology composed of a set of OOP technologies and tools based on COM and DCOM. It is a framework for defining reusable software components in a programming language–independent manner.\\n- **Authenticode**: A type of code signing, which is the process of digitally signing software components and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was digitally signed. Authenticode is Microsoft’s implementation of code signing.\\nKey Terms\\n- **Information gathering**: Usually the first step in an attacker’s methodology, in which the information gathered may allow an attacker to infer additional information that can be used to compromise systems.\\n- **Server side includes (SSI)**: An interpreted server-side scripting language used almost exclusively for web-based communication. It is commonly used to include the contents of one or more files into a web page on a web server. Allows web developers to reuse content by inserting the same content into multiple web documents.\\n- **Client-side validation**: Input validation is done at the client before it is even sent back to the server to process.\\n- **Cross-site scripting (XSS) attack**: An attack where a vulnerability is found on a web site that allows an attacker to inject malicious code into a web\\napplication.\\n- **Parameter validation**: The values that are being received by the application are validated to be within defined limits before the server application processes them within the system.\\n- **Web proxy**: A piece of software installed on a system that is designed to intercept all traffic between the local web browser and the web server. \\n- **Replay attack**: An attacker capturing the traffic from a legitimate session and replaying it with the goal of masquerading an authenticated user.\\nfollowing are some key database terms:\\n- **Record**: A collection of related data items.\\n- **File**: A collection of records of the same type.\\n- **Database**: A cross-referenced collection of data.\\n- **Database Management System (DBMS)**: Manages and controls the database.\\n- **Tuple**: A row in a two-dimensional database.\\n- **Attribute**: A column in a two-dimensional database.\\n- **Primary key**: Columns that make each row unique. (Every row of a table must include a primary key.)\\n- **View**: A virtual relation defined by the database administrator in order to keep subjects from viewing certain data.\\n- **Foreign key**: An attribute of one table that is related to the primary key of another table.\\n- **Cell**: An intersection of a row and a column.\\n- **Schema**: Defines the structure of the database.\\n- **Data dictionary**: Central repository of data elements and their relationships.\\n- **Relational database model**: Uses attributes (columns) and tuples (rows) to contain and organize information.\\n- **Hierarchical data model**: Combines records and fields that are related in a logical tree structure.\\n- **Object-oriented database**: Designed to handle a variety of data (images, audio, documents, video), which is more dynamic in nature than a relational database.\\n- **Object-relational database (ORD)**: Uses object-relational database management system (ORDBMS) and is a relational database with a software front end that is written in an object-oriented programming language.\\n- **Rollback**: An operation that ends a current transaction and cancels all the recent changes to the database until the previous checkpoint/ commit point.\\n- **Two-phase commit**: A mechanism that is another control used in databases to ensure the integrity of the data held within the database.\\n- **Cell suppression**: A technique used to hide specific cells that contain sensitive information.\\n- **Noise and perturbation**: A technique of inserting bogus information in the hopes of misdirecting an attacker or confusing the matter enough that the actual attack will not be fruitful.\\n- **Data warehousing**: Combines data from multiple databases or data sources into a large database for the purpose of providing more extensive information retrieval and data analysis.\\n- **Data mining**: Otherwise known as knowledge discovery in database (KDD), which is the process of massaging the data held in the data warehouse into more useful information.\\n- **Virus**: A small application, or string of code, that infects host applications. It is a programming code that can replicate itself and spread from one system to another.\\n- **Macro virus**: A virus written in a macro language and that is platform independent. Since many applications allow macro programs to be embedded in documents, the programs may be run automatically when the document is opened. This provides a distinct mechanism by which viruses can be spread.\\n- **Compression viruses**: Another type of virus that appends itself to executables on the system and compresses them by using the user’s permissions.\\n- **Stealth virus**: A virus that hides the modifications it has made. The virus tries to trick anti-virus software by intercepting its requests to the operating system and providing false and bogus information.\\n- **Polymorphic virus**: Produces varied but operational copies of itself. A polymorphic virus**: may have no parts that remain identical between infections, making it very difficult to detect directly using signatures.\\n- **Multipart virus**: Also called a multipartite virus, this has several components to it and can be distributed to different parts of the system. It infects and spreads in multiple ways, which makes it harder to eradicate when identified.\\n- **Self-garbling virus**: Attempts to hide from anti-virus software by modifying its own code so that it does not match predefined signatures.\\n- **Meme viruses**: These are not actual computer viruses, but types of e-mail messages that are continually forwarded around the Internet.\\n- **Bots**: Software applications that run automated tasks over the Internet, which perform tasks that are both simple and structurally repetitive. Malicious use of bots is the coordination and operation of an automated attack by a botnet (centrally controlled collection of bots).\\n- **Worms**: These are different from viruses in that they can reproduce on their own without a host application and are self-contained programs.\\n- **Logic bomb**: Executes a program, or string of code, when a certain event happens or a date and time arrives.\\n- **Rootkit**: Set of malicious tools that are loaded on a compromised system through stealthy techniques. The tools are used to carry out more attacks either on the infected systems or surrounding systems.\\n- **Trojan horse**: A program that is disguised as another program with the goal of carrying out malicious activities in the background without the user knowing.\\n- **Remote access Trojans (RATs)**: Malicious programs that run on systems and allow intruders to access and use a system remotely.\\n- **Immunizer**: Attaches code to the file or application, which would fool a virus into “thinking” it was already infected.\\n- **Behavior blocking**: Allowing the suspicious code to execute within the operating system and watches its interactions with the operating system, looking for suspicious activities.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>As I am currently studying to sit the CISSP exam in 2018 and because I've taken over 25,000 words in notes so far I thought I'd share what I have so that others might be able to study a bit easier.</p>\n<p>The relevant CISSP material is difficult to search for mainly I believe is due to the exam changes often or as people pass they tend to only pass on notes to friends, family, or colleagues.</p>\n<p>I've done my best to group relevant notes together in a coherent way to follow if you're just starting out, so enjoy this brain numbing content, and don't forget to share the content if you found it useful so others may also benefit by finding it too.</p>\n<h1 id=\"acronyms\">Acronyms</h1>\n<p><strong>GRC</strong> = Governance, Risk Management and Compliance<br>\n<strong>BIA</strong> = Business Impact Analysis<br>\n<strong>BCP</strong> = Business Continuity Plan<br>\n<strong>IDS</strong> = Intrusion detection system<br>\n<strong>IPS</strong> = Intrusion prevention system<br>\n<strong>SIEM</strong> = Security information and event management<br>\n<strong>DAC</strong> = Discretionary Access Control<br>\n<strong>DRP</strong> = Disaster Recovery Plan<br>\n<strong>RPO</strong> = Recovery Point Objective<br>\n<strong>RTO</strong> = Recovery Time Objective<br>\n<strong>MTD</strong> = Max Tolerable Downtime<br>\n<strong>MOE</strong> = Measures of Effectiveness<br>\n<strong>IPS</strong> = Voice Intrusion Prevention System</p>\n<h1 id=\"fundamentalprinciplesofsecurity\">Fundamental Principles of Security</h1>\n<ul>\n<li>Availability - Reliable and timely access to data and resources is provided to<br>\nauthorized individuals</li>\n<li>Integrity - Accuracy and reliability of the information and systems are provided<br>\nand any unauthorized modification is prevented</li>\n<li>Confidentiality - Necessary level of secrecy is enforced and unauthorized<br>\ndisclosure is prevented</li>\n</ul>\n<h1 id=\"securitydefinitions\">Security Definitions</h1>\n<ul>\n<li>Threat agent - Entity that can exploit a vulnerability, something (individual,<br>\nmachine, software, etc) that can exploit vulnerabilities</li>\n<li>Threat - The danger of a threat agent exploiting a vulnerability</li>\n<li>Vulnerability\t- Weakness or a lack of countermeasure, something a threat agent<br>\ncan act upon, a hole that can be exploited</li>\n<li>Risk - The probability of a threat agent exploiting a vulnerability and the<br>\nassociated impact, threat plus the impact (asset - something important<br>\nto business)</li>\n<li>Exposure - Presence of a vulnerability, which exposes the organization<br>\nto a threat, possibility of happening</li>\n<li>Control - Safeguard that is put in place to reduce a risk, also<br>\ncalled a countermeasure</li>\n</ul>\n<h1 id=\"securityframework\">Security Framework</h1>\n<ul>\n<li>act as reference points</li>\n<li>provide common language for communications</li>\n<li>allow us to share information and create relevancy</li>\n<li>ITIL</li>\n<li>COBIT</li>\n</ul>\n<h1 id=\"enterpriseframeworks\">Enterprise frameworks</h1>\n<ul>\n<li>TOGAF</li>\n<li>DoDAF</li>\n<li>MODAF</li>\n<li>SABSA</li>\n<li>COSO</li>\n</ul>\n<h2 id=\"isostandards\">ISO Standards</h2>\n<ul>\n<li>ISO27000 - built on BS7799</li>\n</ul>\n<h1 id=\"risk\">Risk</h1>\n<ul>\n<li>we have assets that threat agents may want to take advantage of<br>\nthrough vulnerabilities</li>\n<li>identify assets and their importance</li>\n<li>understand risk in the context of the business</li>\n</ul>\n<h1 id=\"riskmanagement\">Risk Management</h1>\n<ul>\n<li>have a risk management policy</li>\n<li>have a risk management team</li>\n<li>start by doing risk assessment</li>\n</ul>\n<h1 id=\"riskassessment\">Risk Assessment</h1>\n<ul>\n<li>identifying vulnerabilities our assets face</li>\n<li>create risk profile</li>\n<li>identify as much as possible</li>\n<li>cannot find them all</li>\n<li>must continually perform assessment</li>\n<li>Four main goals</li>\n<li>identify assets and their value to the organization</li>\n<li>identify vulnerabilities and threats</li>\n<li>quantify or measure the business impact</li>\n<li>balance economically the application of a countermeasure against the cost of the countermeasure, develop cost analysis</li>\n<li>has to be supported by the upper management</li>\n<li>risk analysis team</li>\n<li>made up of specialists</li>\n<li>risk management specialists</li>\n<li>change management specialists</li>\n<li>IT knowledge specialists</li>\n</ul>\n<h2 id=\"nist\">NIST</h2>\n<ul>\n<li>SP800-30 rev.1</li>\n<li><a href=\"http://csrc.nist.gov/publications/drafts/800-30-rev1/SP800-30-Rev1-ipd.pdf\">http://csrc.nist.gov/publications/drafts/800-30-rev1/SP800-30-Rev1-ipd.pdf</a></li>\n</ul>\n<h2 id=\"frap\">FRAP</h2>\n<ul>\n<li>Facilitated Risk Analysis Process</li>\n</ul>\n<h2 id=\"octave\">Octave</h2>\n<p>The Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE) approach defines a risk-based strategic assessment and planning technique for security. OCTAVE is a self-directed approach, meaning that people from an organization assume responsibility for setting the organization's security strategy.</p>\n<h2 id=\"iso27005\">ISO27005</h2>\n<p>The ISO27k standards are deliberately risk-aligned, meaning that organizations are encouraged to assess the security risks to their information (called “information security risks” in the standards, but in reality they are simply information risks) as a prelude to treating them in various ways. Dealing with the highest risks first makes sense from the practical implementation and management perspectives.</p>\n<h1 id=\"riskanalysisapproaches\">Risk Analysis Approaches</h1>\n<ul>\n<li>Quantitative risk analysis</li>\n<li>hard measures, numbers, dollar value, fact based</li>\n<li>Qualitative risk analysis</li>\n<li>soft measure, not easily defined, opinion based</li>\n</ul>\n<h1 id=\"quantitativeriskmeasurementnumberbased\">Quantitative Risk Measurement (Number Based)</h1>\n<ul>\n<li>SLE Single Loss Expectancy</li>\n<li>ARO Annualized Rate of Occurrence</li>\n<li>ALE Annualized Loss Expectancy (SLE x ARO)</li>\n</ul>\n<p>For example: purchasing a firewall. What is its purpose and capabilities on average, three times a year there is a breach, and data is compromised liability of restoring data in the end the cost of incident is $5000 this would be the single loss expectancy occurs three times, annual occurrence $15000 - 5000 * 3 = 15,000 this is ALE is the cost to mitigate greater or less than ALE.</p>\n<h1 id=\"quantitativeassessment\">Quantitative Assessment</h1>\n<ul>\n<li>systems</li>\n<li>training</li>\n<li>vulnerabilities</li>\n</ul>\n<h2 id=\"qualitativerisk\">Qualitative Risk</h2>\n<ul>\n<li>not so much hard numbers</li>\n<li>should be contextual to business policies and compliance requirements</li>\n</ul>\n<p>Qualitative risk analysis is a project management technique concerned with discovering the probability of a risk event occurring and the impact the risk will have if it does occur. All risks have both probability and impact.</p>\n<p><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/risk-matrix.jpg\" alt=\"risk-matrix\" loading=\"lazy\"></p>\n<h3 id=\"veryhigh\">Very High</h3>\n<p>Requires the prompt attention of management. The stakeholder (executive or operational owner) must undertake detailed research, identify risk reduction options and prepare a detailed risk management plan. Reporting these exposures and mitigations through relevant forums is essential. Monitoring the risk mitigation strategies is essential.</p>\n<h3 id=\"high\">High</h3>\n<p>High inherent risk requires the attention of the relevant manager so that appropriate controls can be set in place. The Risk &amp; Reputation Committee monitor the implementation of key enterprise risk controls.</p>\n<p>High residual risk is to be monitored to ensure the associated controls are working. Detailed research to identify additional risk reduction options and preparation of a detailed risk management plan is required.</p>\n<p>Reporting these exposures and mitigations through relevant forums is required. Monitoring the risk mitigation strategies is required</p>\n<h3 id=\"medium\">Medium</h3>\n<p>This is the threshold that delineates the higher-level risks from those of less concern. After considering the nature of the likelihood and consequence values in relation to whether the risk could increase in value, consider additional cost-effective mitigations to reduce the risk.</p>\n<p>Responsibility would fall on the relevant manager and specific monitoring of response procedures would occur.</p>\n<h3 id=\"low\">Low</h3>\n<p>Look to accept, monitor and report on the risk or manage it through the implementation/enhancement of procedures</p>\n<h3 id=\"verylow\">Very Low</h3>\n<p>These risks would be not considered any further unless they contribute to a scenario that could pose a serious event, or they escalate in either likelihood and/or consequence</p>\n<h2 id=\"likelihood\">Likelihood</h2>\n<p>Find relevant definitions for each category within your business. An example of rare could just mean it could occur but only in exceptional circumstances and decades apart where almost certain risks are known to occur frequently (monthly or bi-quarterly depending on business thresholds).</p>\n<h2 id=\"impactconsequence\">Impact Consequence</h2>\n<p>Find relevant definitions for each category within your business. An example of Insignificant is an impact can be absorbed within the day-to-day business running costs where-as extreme might be contractual non-compliance or breach of legislation with penalties or fines over a considerably higher amount then perhaps major would have had and may also come with extreme brand, operational, financial or reputational impacts.</p>\n<h1 id=\"implementation\">Implementation</h1>\n<h2 id=\"policies\">Policies</h2>\n<p>Overall general statement produced by senior management</p>\n<ul>\n<li>high level statements of intent</li>\n<li>what expectations of correct usage</li>\n</ul>\n<h2 id=\"standards\">Standards</h2>\n<p>Refer to mandatory activities or rules</p>\n<ul>\n<li>regulatory compliance</li>\n<li>mandated based on compliance regime</li>\n<li>in US, Sarbanes-Oxley, HIPPA</li>\n<li>Basel Accords</li>\n<li>Montreal Protocol</li>\n</ul>\n<h2 id=\"baselines\">Baselines</h2>\n<p>Template for comparison, measure deviation from normal</p>\n<ul>\n<li>standardized solution</li>\n<li>allows us to find and measure deviations</li>\n</ul>\n<h2 id=\"guidelines\">Guidelines</h2>\n<p>Recommendations</p>\n<ul>\n<li>not mandatory, optional</li>\n<li>best practices</li>\n</ul>\n<h2 id=\"procedures\">Procedures</h2>\n<p>Step by step activities that need to be performed in a certain order, detailed instructions.</p>\n<h1 id=\"businesscontinuityanddisasterrecovery\">Business Continuity and Disaster Recovery</h1>\n<p>The goal of disaster recovery is to minimize the effects of a disaster<br>\nThe goal of business continuity is to resume normal business operations as quickly as possible with the least amount of resources necessary to do so</p>\n<h2 id=\"disasterrecoveryplandrp\">Disaster Recovery Plan (DRP)</h2>\n<p>Carried out when everything is going to still be suffering from the effects of the disaster</p>\n<h2 id=\"businesscontinuityplanbcp\">Business Continuity Plan (BCP)</h2>\n<p>Used to return the business to normal operations</p>\n<h2 id=\"businesscontinuitymanagementbcm\">Business Continuity Management (BCM)</h2>\n<p>The process that is responsible for DRP and BCP.</p>\n<h3 id=\"overarchingprocess\">Over-arching process</h3>\n<p><a href=\"http://csrc.nist.gov/publications/PubsSPs.htm\">NIST SP800-34</a> Continuity Planning Guide for IT</p>\n<ol>\n<li>Develop the continuity planning policy statement</li>\n<li>Conduct the business impact analysis (BIA)</li>\n<li>Identify preventive controls</li>\n<li>Develop recovery strategies</li>\n<li>Develop the contingency plan</li>\n<li>Test the plan and conduct training and exercises</li>\n<li>Maintain the plan</li>\n</ol>\n<p><a href=\"http://www.itgovernance.co.uk/bs25999.aspx\">BS25999</a> British Standard for Business Continuity Management<br>\n<a href=\"http://www.iso.org/iso/catalogue_detail?csnumber=44374\">ISO 27031:2011</a> Business Continuity Planning<br>\n<a href=\"http://www.iso.org/iso/catalogue_detail?csnumber=50038\">ISO 22301:2012</a> Business Continuity Management Systems</p>\n<p>Department of Homeland Security BCP Kit <a href=\"http://www.ready.gov/business-continuity-planning-suite\">BCP Policy</a> Supplies the framework and governance for the BCP effort.</p>\n<p>Contains:</p>\n<ul>\n<li>Scope</li>\n<li>Mission Statement</li>\n<li>Principles</li>\n<li>Guidelines</li>\n<li>Standards</li>\n</ul>\n<p>Process to draft the policy</p>\n<ol>\n<li>Identify and document the components of the policy</li>\n<li>Identify and define existing policies that the BCP may affect</li>\n<li>Identify pertinent laws and standards</li>\n<li>Identify best practices</li>\n<li>Perform a GAP analysis</li>\n<li>Compose a draft of the new policy</li>\n<li>Internal review of the draft</li>\n<li>Incorporate feedback into draft</li>\n<li>Get approval of senior management</li>\n<li>Publish a final draft and distribute throughout organization</li>\n</ol>\n<h2 id=\"swotanalysis\">SWOT Analysis</h2>\n<ul>\n<li>Strength</li>\n<li>Weakness</li>\n<li>Opportunity</li>\n<li>Threat</li>\n</ul>\n<h2 id=\"businesscontinuityplanningrequirements\">Business Continuity Planning Requirements</h2>\n<ul>\n<li>Senior management support</li>\n<li>Management should be involved in setting the overall goals in continuity planning</li>\n</ul>\n<h2 id=\"businessimpactanalysisbia\">Business Impact Analysis (BIA)</h2>\n<ul>\n<li>Functional analysis</li>\n<li>Identifies which of the company's critical systems are needed for survival</li>\n<li>Estimates the down-time that can be tolerated by the system\n<ul>\n<li>Maximum Tolerable Down-time (MTD)</li>\n<li>Maximum Period Time of Disruption (MPTD)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"steps\">Steps:</h3>\n<ol>\n<li>Select individuals to interview for data gathering</li>\n<li>Create data gathering tools\n<ul>\n<li>Surveys</li>\n<li>Questionnaires</li>\n</ul>\n</li>\n<li>Identify company's critical business functions</li>\n<li>Identify the resources these functions depend on</li>\n<li>Calculate how long these functions can survive without these resources</li>\n<li>Identify vulnerabilities and threats to these functions</li>\n<li>Calculate the risk for each different business function</li>\n<li>Document findings and report them to senior management</li>\n</ol>\n<h3 id=\"goals\">Goals:</h3>\n<ul>\n<li>Determine criticality</li>\n<li>Estimate max downtime</li>\n<li>Evaluate internal and external resource requirements</li>\n</ul>\n<h3 id=\"process\">Process</h3>\n<ul>\n<li>Gather information</li>\n<li>Analyse information</li>\n<li>Perform threat analysis</li>\n<li>Document results and present recommendations</li>\n</ul>\n<h2 id=\"disasterrecoveryplanning\">Disaster Recovery Planning</h2>\n<p>Stage 1 - Business as usual<br>\nStage 2 - Disaster occurs<br>\nStage 3 - Recovery<br>\nStage 4 - Resume production</p>\n<p><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/disaster-recovery-planning.jpg\" alt=\"disaster-recovery-planning\" loading=\"lazy\"></p>\n<h3 id=\"recoverytimeobjectiverto\">Recovery Time Objective (RTO)</h3>\n<ul>\n<li>The earliest time period and a service level within which a business process must be restored after a disaster</li>\n<li>RTO value is smaller than the MTD  because the MTD represents the time after which an inability to recover will mean severe damage to the business's reputation and/or the bottom line.</li>\n<li>The RTO assumes there is a period of acceptable down-time</li>\n</ul>\n<h3 id=\"workrecoverytimewrt\">Work Recovery Time (WRT)</h3>\n<ul>\n<li>Remainder of the overall MTD value</li>\n</ul>\n<h3 id=\"recoverypointobjectiverpo\">Recovery Point Objective (RPO)</h3>\n<ul>\n<li>Acceptable amount of data loss measured in time</li>\n</ul>\n<blockquote>\n<p>MTD, RTO and RPO values are derived during the BIA</p>\n</blockquote>\n<h2 id=\"riskassessment\">Risk Assessment</h2>\n<p>Looks at the impact and likely-hood to various threats to the business<br>\nGoals:</p>\n<ul>\n<li>Identify and document single points of failure</li>\n<li>Make a prioritized list of threats</li>\n<li>Gather information to develop risk control strategies</li>\n<li>Document acceptance of identified risks</li>\n<li>Document acknowledgment of risks that may not be addressed</li>\n</ul>\n<h3 id=\"formula\">Formula:</h3>\n<p>Risk = Threat * Impact * Probability</p>\n<p>Main components:</p>\n<ol>\n<li>Review existing strategies for risk management</li>\n<li>Construct numerical scoring system for probability and impact</li>\n<li>Use numerical scoring to gauge effect of threats</li>\n<li>Estimate probability of threats</li>\n<li>Weigh each threat through the scoring system</li>\n<li>Calculate the risk by combining the scores of likelihood and impact of each threat</li>\n<li>Secure sponsor sign off on risk priorities</li>\n<li>Weigh appropriate measures</li>\n<li>Make sure planned measures do not heighten other risks</li>\n<li>Present assessment's findings to executive management</li>\n</ol>\n<h1 id=\"certificationvsaccreditation\">Certification vs Accreditation</h1>\n<p>Certification is the comprehensive technical evaluation of the security components and their compliance for the purpose of accreditation.<br>\nThe goal of a certification process is to ensure that a system, product, or network is right for the customer’s purposes.</p>\n<p>Accreditation is the formal acceptance of the adequacy of a system’s overall security and functionality by management.</p>\n<h1 id=\"accesscontrol\">Access Control</h1>\n<h2 id=\"accesscontrolsystems\">Access Control Systems</h2>\n<ul>\n<li>RADIUS (not vendor specific)</li>\n<li>AAA</li>\n<li>UDP</li>\n<li>Encrypts only the password from client to server</li>\n<li>works over PPP</li>\n<li>TACACS/xTACACS/TACACS+</li>\n<li>TCP</li>\n<li>Encrypts all traffic</li>\n<li>works over multiple protocols</li>\n<li>Diameter</li>\n<li>supports PAP, CHAP, EAP</li>\n<li>replay protection</li>\n<li>end to end protection</li>\n<li>enhanced accounting</li>\n</ul>\n<h3 id=\"contentdependentaccesscontrol\">Content dependent access control</h3>\n<ul>\n<li>focused on content of data</li>\n<li>data may be labeled, like HR or Finances</li>\n<li>dependent on the context of the usage of the data</li>\n</ul>\n<p><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/CISSP-access-controls.jpg\" alt=\"CISSP-access-controls\" loading=\"lazy\"></p>\n<blockquote>\n<p>Y = Control, X = Control Category, Color key = Control types</p>\n</blockquote>\n<h2 id=\"threatstosystems\">Threats to systems:</h2>\n<p>Maintenance hooks are a type of back door used by developers to get &quot;back into&quot; their code if necessary.</p>\n<h3 id=\"preventivemeasuresagainstbackdoors\">Preventive measures against back doors:</h3>\n<ul>\n<li>Use a host intrusion detection system to watch for any attackers using back doors into the system.</li>\n<li>Use file system encryption to protect sensitive information.</li>\n<li>Implement auditing to detect any type of back door use.</li>\n</ul>\n<p>A time-of-check/time-of-use (TOC/TOU) attack deals with the sequence of steps a system uses to complete a task. This type of attack takes advantage of the dependency on the timing of events that take place in a multitasking operating system. A TOC/TOU attack is when an attacker jumps in between two tasks and modifies something to control the result.<br>\nTo protect against TOC/TOU attacks, the operating system can apply software locks to the items it will use when it is carrying out its “checking” tasks.</p>\n<p>A race condition is when two different processes need to carry out their tasks on one resource. A race condition is an attack in which an attacker makes processes execute out of sequence to control the result.<br>\nTo protect against race condition attacks, do not split up critical tasks that can have their sequence altered.</p>\n<h3 id=\"tcpiposilayer\">TCP/IP OSI Layer</h3>\n<p><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/layers.JPG\" alt=\"layers\" loading=\"lazy\"></p>\n<h2 id=\"threatstoaccesscontrol\">Threats to Access Control</h2>\n<p>Defeating Access Controls</p>\n<ul>\n<li>Credentials can be mirrored</li>\n<li>Passwords can be brute forced</li>\n<li>Locks can be picked</li>\n<li>You must assess the weaknesses of your solution</li>\n</ul>\n<h2 id=\"keystrokemonitoring\">Keystroke monitoring</h2>\n<h3 id=\"objectreusedataremnants\">Object reuse (data remnants)</h3>\n<ul>\n<li>object disposal</li>\n</ul>\n<h3 id=\"tempestshielding\">Tempest Shielding</h3>\n<ul>\n<li>Faraday Cage</li>\n</ul>\n<h3 id=\"whitenoise\">White Noise</h3>\n<ul>\n<li>Voice assistants (Alexa, Siri, ect) can be activated by white noise</li>\n</ul>\n<h3 id=\"controlzone\">Control Zone</h3>\n<ul>\n<li>SCIF: A Sensitive Compartmented Information Facility which is a defense term for a secure room.</li>\n</ul>\n<h2 id=\"nids\">NIDS</h2>\n<p>Network Intrusion Detection Systems</p>\n<ul>\n<li>Real time traffic analysis</li>\n<li>Monitor though upload to logging server</li>\n<li>passive monitoring</li>\n<li>usually per subnet</li>\n</ul>\n<h2 id=\"hidships\">HIDS/HIPS</h2>\n<p>Host-based Intrusion Detection/Prevention System</p>\n<ul>\n<li>Software based agent installed on machine</li>\n<li>not real time</li>\n<li>aggregated on schedule</li>\n</ul>\n<h3 id=\"howdotheywork\">How do they work</h3>\n<p>HIDS: Passive, do not stop attack, maybe send alert<br>\nHIPS: Active, can stop attack as well as send alert, respond and stop attack</p>\n<ul>\n<li>Signature Based</li>\n<li>Pattern Matching</li>\n<li>Must be kept up to date</li>\n<li>antivirus</li>\n<li>Stateful Matching</li>\n<li>Looks at sequences across traffic</li>\n<li>Building pattern based on traffic</li>\n<li>Must be kept up to date</li>\n<li>Anomaly Based</li>\n<li>looks at stream of traffic</li>\n<li>Look at expected behavior (rule based)</li>\n<li>Detects Abnormalities</li>\n<li>Could be statistical</li>\n<li>Look at raw traffic</li>\n<li>Could look at protocol Anomalies</li>\n<li>Heuristics</li>\n<li>Create virtual sandbox in memory</li>\n<li>Looks for abnormalities</li>\n</ul>\n<h2 id=\"honeypothoneynets\">Honeypot (honeynets)</h2>\n<ul>\n<li>hacking magnet</li>\n<li>decoy</li>\n<li>honeynet.org</li>\n<li>enticement</li>\n</ul>\n<h3 id=\"enticement\">Enticement</h3>\n<ul>\n<li>legal</li>\n<li>let them make the decision to attack the system</li>\n</ul>\n<h3 id=\"entrapment\">Entrapment</h3>\n<ul>\n<li>illegal</li>\n<li>trick them into attacking the system</li>\n</ul>\n<h2 id=\"attacksagainsthoneypots\">Attacks against Honeypots</h2>\n<ul>\n<li>Network Sniffer\n<ul>\n<li>used to capture traffic</li>\n<li>Network monitor, etc.</li>\n</ul>\n</li>\n<li>Dictionary attack</li>\n<li>Brute force attack</li>\n<li>Phishing</li>\n<li>Pharming</li>\n<li>Whaling</li>\n<li>Spear Phishing</li>\n<li>Vishing</li>\n</ul>\n<h2 id=\"emanationssecurity\">Emanations Security</h2>\n<ul>\n<li>Data radiating out via electrical/wireless signals</li>\n<li>TEMPEST shielding can help contain these signals</li>\n<li>Faraday cage</li>\n</ul>\n<h2 id=\"intrusiondetection\">Intrusion Detection</h2>\n<p>IDS Types</p>\n<ul>\n<li>Signature Based\n<ul>\n<li>Stateful\n<ul>\n<li>Matches traffic patterns to activities</li>\n</ul>\n</li>\n<li>Signature\n<ul>\n<li>Matches individual packets to predefined definitions</li>\n</ul>\n</li>\n<li>Regular updates required</li>\n<li>Cannot detect previously unknown attacks</li>\n</ul>\n</li>\n<li>Anomaly Based\n<ul>\n<li>Learns the normal traffic on the network and then spots exceptions</li>\n<li>Requires a &quot;training&quot; period to reduce false positives</li>\n<li>Also known as heuristic scanning</li>\n<li>Statistical detection\n<ul>\n<li>Looks for traffic that is outside of the statistical norm</li>\n</ul>\n</li>\n<li>Protocol detection\n<ul>\n<li>Looks for protocols that are not typically in use on the network</li>\n</ul>\n</li>\n<li>Traffic detection\n<ul>\n<li>Looks for unusual activity inside of the traffic</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Rule Based\n<ul>\n<li>Detects attack traffic through pre-defined rules</li>\n<li>Can be coupled with expert systems to be dynamic</li>\n<li>Cannot detect previously unknown attacks</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"softwaredevelopmentsecurity\">Software Development Security</h1>\n<p>A system development life cycle (SDLC) is made up of the following basic components of each phase:</p>\n<h2 id=\"initiation\">Initiation</h2>\n<ul>\n<li>Need for a new system is defined; This phase addresses the questions, “What do we need and why do we need it?”</li>\n<li>A preliminary risk assessment should be carried out to develop an initial description of the confidentiality, integrity, and availability requirements of the system.</li>\n</ul>\n<h2 id=\"acquisitiondevelopment\">Acquisition/development</h2>\n<p>New system is either created or purchased</p>\n<p>Activities during this phase will include:</p>\n<ul>\n<li>Requirements analysis\n<ul>\n<li>In-depth study of what functions the company needs the desired system to carry out.</li>\n</ul>\n</li>\n<li>Formal risk assessment\n<ul>\n<li>Identifies vulnerabilities and threats in the proposed system and the potential risk levels as they pertain to confidentiality, integrity, and availability. This builds upon the initial risk assessment carried out in the previous phase. The results of this assessment help the team build the system’s security plan.</li>\n</ul>\n</li>\n<li>Security functional requirements analysis\n<ul>\n<li>Identifies the protection levels that must be provided by the system to meet all regulatory, legal, and policy compliance needs.</li>\n</ul>\n</li>\n<li>Security assurance requirements analysis\n<ul>\n<li>Identifies the assurance levels the system must provide. The activities that need to be carried out to ensure the desired level of confidence in the system are determined, which are usually specific types of tests and evaluations.</li>\n</ul>\n</li>\n<li>Third-party evaluations\n<ul>\n<li>Reviewing the level of service and quality a specific vendor will provide if the system is to be purchased.</li>\n</ul>\n</li>\n<li>Security plan\n<ul>\n<li>Documented security controls the system must contain to ensure compliance with the company’s security needs. This plan provides a complete description of the system and ties them to key company documents, as in configuration management, test and evaluation plans, system interconnection agreements, security accreditations, etc.</li>\n</ul>\n</li>\n<li>Security test and evaluation plan\n<ul>\n<li>Outlines how security controls should be evaluated before the system is approved and deployed.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"implementation\">Implementation</h3>\n<ul>\n<li>New system is installed into production environment.</li>\n<li>A system should have baselines set pertaining to the system’s hardware, software, and firmware configuration during the implementation phase.</li>\n</ul>\n<h3 id=\"operationmaintenance\">Operation/maintenance</h3>\n<ul>\n<li>System is used and cared for.</li>\n<li>Continuous monitoring needs to take place to ensure that baselines are always met.</li>\n<li>Defined configuration management and change control procedures help to ensure that a system’s baseline is always met.</li>\n<li>Vulnerability assessments and penetration testing should also take place in this phase.</li>\n</ul>\n<h3 id=\"disposal\">Disposal</h3>\n<ul>\n<li>System is removed from production environment.</li>\n<li>Disposal activities need to ensure that an orderly termination of the system takes place and that all necessary data are preserved.</li>\n</ul>\n<h2 id=\"softwaredevelopmentlifecyclesdlc\">Software Development Life Cycle (SDLC)</h2>\n<p>The life cycle of software development deals with putting repeatable and predictable processes in place that help ensure functionality, cost, quality, and delivery schedule requirements are met.</p>\n<h2 id=\"phases\">Phases:</h2>\n<h3 id=\"requirementsgathering\">Requirements gathering</h3>\n<ul>\n<li>Determine the why create this software, the what the software will do, and the for whom the software will be created.</li>\n<li>This is the phase when everyone involved attempts to understand why the project is needed and what the scope of the project entails.</li>\n<li>Security requirements</li>\n<li>Security risk assessment</li>\n<li>Privacy risk assessment</li>\n<li>Risk-level acceptance</li>\n</ul>\n<h3 id=\"design\">Design</h3>\n<ul>\n<li>Deals with how the software will accomplish the goals identified, which are encapsulated into a functional design.</li>\n<li>Attack surface analysis - identify and reduce the amount of code and functionality accessible to untrusted users.</li>\n<li>Threat modeling - systematic approach used to understand how different threats could be realized and how a successful compromise could take place.</li>\n</ul>\n<h3 id=\"development\">Development</h3>\n<ul>\n<li>Programming software code to meet specifications laid out in the design phase.</li>\n<li>The software design that was created in the previous phase is broken down into defined deliverables, and programmers develop code to meet the deliverable requirements.</li>\n</ul>\n<h3 id=\"testingvalidation\">Testing/Validation</h3>\n<ul>\n<li>Validating software to ensure that goals are met and the software works as planned.</li>\n<li>It is important to map security risks to test cases and code.</li>\n<li>Tests are conducted in an environment that should mirror the production environment.</li>\n<li>Security attacks and penetration tests usually take place during this phase to identify any missed vulnerabilities.</li>\n<li>The most common testing approaches:\n<ul>\n<li>Unit testing - Individual component is in a controlled environment where programmers validate data structure, logic, and boundary conditions.</li>\n<li>Integration testing - Verifying that components work together as outlined in design specifications.</li>\n<li>Acceptance testing - Ensuring that the code meets customer requirements.</li>\n<li>Regression testing - After a change to a system takes place, retesting to ensure functionality, performance, and protection.</li>\n</ul>\n</li>\n<li>Fuzzing is a technique used to discover flaws and vulnerabilities in software. Fuzzing is the act of sending random data to the target program in \torder to trigger failures.</li>\n<li>Dynamic analysis refers to the evaluation of a program in real time, i.e., when it is running. Dynamic analysis is carried out once a program has cleared the static analysis stage and basic programming flaws have been rectified offline.</li>\n<li>Static analysis a debugging technique that is carried out by examining the code without executing the program, and therefore is carried out before the program is compiled.</li>\n</ul>\n<h3 id=\"releasemaintenance\">Release/Maintenance</h3>\n<ul>\n<li>Deploying the software and then ensuring that it is properly configured, patched, and monitored.</li>\n</ul>\n<h2 id=\"statementofworksow\">Statement of Work (SOW)</h2>\n<p>Describes the product and customer requirements. A detailed-oriented SOW will help ensure that these requirements are properly understood and assumptions are not made.</p>\n<h3 id=\"workbreakdownstructurewbs\">Work breakdown structure (WBS)</h3>\n<p>A project management tool used to define and group a project’s individual work elements in an organized manner.</p>\n<h3 id=\"privacyimpactrating\">Privacy Impact Rating</h3>\n<p>Indicates the sensitivity level of the data that will be processed or made accessible.</p>\n<h3 id=\"computeraidedsoftwareengineeringcase\">Computer-aided software engineering (CASE)</h3>\n<p>Refers to software that allows for the automated development of software, which can come in the form of program editors, debuggers, code analyzers, version-control mechanisms, and more.</p>\n<h3 id=\"verification\">Verification</h3>\n<p>Determines if the product accurately represents and meets the specifications.</p>\n<h3 id=\"validation\">Validation</h3>\n<p>Determines if the product provides the necessary solution for the intended real-world problem.</p>\n<h2 id=\"softwaredevelopmentmodels\">Software Development Models</h2>\n<h3 id=\"buildandfixmodel\">Build and Fix Model</h3>\n<p>Development takes place immediately with little or no planning involved. Problems are dealt with as they occur, which is usually after the software product is released to the customer.</p>\n<h3 id=\"waterfallmodel\">Waterfall Model</h3>\n<p>A linear-sequential life-cycle approach. Each phase must be completed in its entirety before the next phase can begin. At the end of each phase, a review takes place to make sure the project is on the correct path and if the project should continue.<br>\nIn this model all requirements are gathered in the initial phase and there is no formal way to integrate changes as more information becomes available or requirements change.</p>\n<h3 id=\"vshapedmodelvmodel\">V-Shaped Model (V-Model)</h3>\n<p>Follows steps that are laid out in a V format.<br>\nThis model emphasizes the verification and validation of the product at each phase and provides a formal method of developing testing plans as each coding phase is executed.<br>\nEach phase must be completed before the next phase begins.</p>\n<h3 id=\"prototyping\">Prototyping</h3>\n<p>A sample of software code or a model (prototype) can be developed to explore a specific approach to a problem before investing expensive time and resources.<br>\nRapid prototyping is an approach that allows the development team to quickly create a prototype (sample) to test the validity of the current understanding of the project requirements.<br>\nEvolutionary prototypes are developed, with the goal of incremental improvement.</p>\n<h3 id=\"incrementalmodel\">Incremental Model</h3>\n<p>Each incremental phase results in a deliverable that is an operational product. This means that a working version of the software is produced after the first iteration and that version is improved upon in each of the subsequent iterations.</p>\n<h3 id=\"spiralmodel\">Spiral Model</h3>\n<p>Uses an iterative approach to software development and places emphasis on risk analysis. The model is made up of four main phases: planning, risk analysis, development and test, and evaluation.</p>\n<h3 id=\"rapidapplicationdevelopment\">Rapid Application Development</h3>\n<p>Relies more on the use of rapid prototyping instead of extensive upfront planning.<br>\nCombines the use of prototyping and iterative development procedures with the goal of accelerating the software development process.</p>\n<h3 id=\"agilemodel\">Agile Model</h3>\n<p>Focuses on incremental and iterative development methods that promote cross-functional teamwork and continuous feedback mechanisms.<br>\nThe Agile model does not use prototypes to represent the full product, but breaks the product down into individual features.</p>\n<h3 id=\"capabilitymaturitymodelintegrationcmmi\">Capability Maturity Model Integration (CMMI)</h3>\n<ul>\n<li>Initial Development process is ad hoc or even chaotic. The company does not use effective management procedures and plans. There is no assurance of consistency, and quality is unpredictable.</li>\n<li>Repeatable A formal management structure, change control, and quality assurance are in place. The company can properly repeat processes throughout each project. The company does not have formal process models defined.</li>\n<li>Defined Formal procedures are in place that outline and define processes carried out in each project. The organization has a way to allow for quantitative process improvement.</li>\n<li>Managed The company has formal processes in place to collect and analyze quantitative data, and metrics are defined and fed into the process improvement program.</li>\n<li>Optimizing The company has budgeted and integrated plans for continuous process improvement.</li>\n</ul>\n<h2 id=\"softwareescrow\">Software escrow</h2>\n<p>Storing of the source code of software with a third-party escrow agent. The software source code is released to the licensee if the licensor (software vendor) files for bankruptcy or fails to maintain and update the software product as promised in the software license agreement.</p>\n<h3 id=\"programminglanguagesandconcepts\">Programming Languages and Concepts:</h3>\n<ul>\n<li>(1st generation programming language) - Machine language is in a format that the computer’s processor can understand and work with directly.</li>\n<li>(2nd generation programming language) - Assembly language is considered a low-level programming language and is the symbolic representation of machine-level instructions. It is “one step above” machine language. It uses symbols (called mnemonics) to represent complicated binary codes. Programs written in assembly language are also hardware specific.</li>\n<li>(3rd generation programming language) - High-level languages use abstract statements. Abstraction naturalized multiple assembly language instructions into a single high-level statement, e.g., the IF – THEN – ELSE. High-level languages are processor independent. Code written in a high-level language can be converted to machine language for different processor architectures using compilers and interpreters.</li>\n<li>(4th generation programming language) - Very high-level languages focus on highly abstract algorithms that allow straightforward programming implementation in specific environments.</li>\n<li>(5th generation programming language) - Natural languages have the ultimate target of eliminating the need for programming expertise and instead use advanced knowledge-based processing and artificial intelligence.</li>\n</ul>\n<h3 id=\"assemblers\">Assemblers</h3>\n<p>Tools that convert assembly language source code into machine code.</p>\n<h3 id=\"compilers\">Compilers</h3>\n<p>Tools that convert high-level language statements into the necessary machine-level format (.exe, .dll, etc.) for specific processors to understand.</p>\n<h3 id=\"interpreters\">Interpreters</h3>\n<p>Tools that convert code written in interpreted languages to the machine-level format for processing.</p>\n<h3 id=\"garbagecollector\">Garbage collector</h3>\n<p>Identifies blocks of memory that were once allocated but are no longer in use and deallocates the blocks and marks them as free.</p>\n<h3 id=\"objectorientedprogrammingoop\">Object Oriented Programming (OOP)</h3>\n<p>Works with classes and objects.<br>\nA method is the functionality or procedure an object can carry out.<br>\nData hiding is provided by encapsulation, which protects an object’s private data from outside access. No object should be allowed to, or have the need to, access another object’s internal data or processes.</p>\n<h3 id=\"abstraction\">Abstraction</h3>\n<p>The capability to suppress unnecessary details so the important, inherent properties can be examined and reviewed.</p>\n<h3 id=\"polymorphism\">Polymorphism</h3>\n<p>Two objects can receive the same input and have different outputs.</p>\n<h3 id=\"datamodeling\">Data modeling</h3>\n<p>Considers data independently of the way the data are processed and of the components that process the data. A process used to define and analyze data requirements needed to support the business processes.</p>\n<h3 id=\"cohesion\">Cohesion</h3>\n<p>A measurement that indicates how many different types of tasks a module needs to carry out.</p>\n<h3 id=\"coupling\">Coupling</h3>\n<p>A measurement that indicates how much interaction one module requires for carrying out its tasks.</p>\n<h3 id=\"datastructure\">Data structure</h3>\n<p>A representation of the logical relationship between elements of data.</p>\n<h3 id=\"distributedcomputingenvironmentdce\">Distributed Computing Environment (DCE)</h3>\n<p>The first framework and development toolkit for developing client/server applications to allow for distributed computing.</p>\n<h3 id=\"commonobjectrequestbrokerarchitecturecorba\">Common Object Request Broker Architecture (CORBA)</h3>\n<p>Open objectoriented standard architecture developed by the Object Management Group (OMG). The standards enable software components written in different computer languages and running on different systems to communicate.</p>\n<h3 id=\"objectrequestbrokerorb\">Object request broker (ORB)</h3>\n<p>Manages all communications between components and enables them to interact in a heterogeneous and distributed environment. The ORB acts as a “broker” between a client request for a service from a distributed object and the completion of that request.</p>\n<h3 id=\"componentobjectmodelcom\">Component Object Model (COM)</h3>\n<p>A model developed by Microsoft that allows for interprocess communication between applications potentially written in different programming languages on the same computer system.</p>\n<h3 id=\"objectlinkingandembeddingole\">Object linking and embedding (OLE)</h3>\n<p>Provides a way for objects to be shared on a local computer and to use COM as their foundation. It is a technology developed by Microsoft that allows embedding and linking to documents and other objects.</p>\n<h3 id=\"javaplatformenterpriseeditionj2ee\">Java Platform, Enterprise Edition (J2EE)</h3>\n<p>Is based upon the Java programming language, which allows a modular approach to programming code with the goal of interoperability. J2EE defines a client/server model that is object oriented and platform independent.</p>\n<h3 id=\"serviceorientedarchitecturesoa\">Service-oriented architecture (SOA)</h3>\n<p>Provides standardized access to the most needed services to many different applications at one time. Service interactions are self-contained and loosely coupled, so that each interaction is independent of any other interaction.</p>\n<h3 id=\"simpleobjectaccessprotocolsoap\">Simple Object Access Protocol (SOAP)</h3>\n<p>An XML-based protocol that encodes messages in a web service environment.</p>\n<h3 id=\"mashup\">Mashup</h3>\n<p>The combination of functionality, data, and presentation capabilities of two or more sources to provide some type of new service or functionality.</p>\n<h3 id=\"softwareasaservicesaas\">Software as a Service (SAAS)</h3>\n<p>A software delivery model that allows applications and data to be centrally hosted and accessed by thin clients, commonly web browsers. A common delivery method of cloud computing.</p>\n<h3 id=\"cloudcomputing\">Cloud computing</h3>\n<p>A method of providing computing as a service rather than as a physical product. It is Internet-based computing, whereby shared resources and software are provided to computers and other devices on demand.</p>\n<h3 id=\"mobilecode\">Mobile code</h3>\n<p>Code that can be transmitted across a network, to be executed by a system or device on the other end.</p>\n<h3 id=\"javaapplets\">Java applets</h3>\n<p>Small components (applets) that provide various functionalities and are delivered to users in the form of Java bytecode. Java applets can run in a web browser using a Java Virtual Machine (JVM). Java is platform independent; thus, Java applets can be executed by browsers for many platforms.</p>\n<h3 id=\"sandbox\">Sandbox</h3>\n<p>A virtual environment that allows for very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources.</p>\n<h3 id=\"activex\">ActiveX</h3>\n<p>A Microsoft technology composed of a set of OOP technologies and tools based on COM and DCOM. It is a framework for defining reusable software components in a programming language–independent manner.</p>\n<h3 id=\"authenticode\">Authenticode</h3>\n<p>A type of code signing, which is the process of digitally signing software components and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was digitally signed. Authenticode is Microsoft’s implementation of code signing.</p>\n<h2 id=\"threatsforwebenvironments\">Threats for Web Environments</h2>\n<h3 id=\"informationgathering\">Information gathering</h3>\n<p>Usually the first step in an attacker’s methodology, in which the information gathered may allow an attacker to infer additional information that can be used to compromise systems.</p>\n<h3 id=\"serversideincludesssi\">Server side includes (SSI)</h3>\n<p>An interpreted server-side scripting language used almost exclusively for web-based communication. It is commonly used to include the contents of one or more files into a web page on a web server. Allows web developers to reuse content by inserting the same content into multiple web documents.</p>\n<h3 id=\"clientsidevalidation\">Client-side validation</h3>\n<ul>\n<li>Input validation is done at the client before it is even sent back to the server to process.</li>\n<li>Path or directory traversal - Attack is also known as the “dot dot slash” because it is perpetrated by inserting the characters “../” several times into a URL to back up or traverse into directories that were not supposed to be accessible from the Web.</li>\n<li>Unicode encoding - An attacker using Unicode could effectively make the same directory traversal request without using “/” but with any of the Unicode representations of that character (three exist: %c1%1c, %c0%9v, and %c0%af)</li>\n<li>URL Encoding - %20% = a space</li>\n</ul>\n<h3 id=\"crosssitescriptingxss\">Cross-site scripting (XSS)</h3>\n<ul>\n<li>An attack where a vulnerability is found on a web site that allows an attacker to inject malicious code into a web application.</li>\n<li>There are three different XSS vulnerabilities:\n<ul>\n<li>Nonpersistent XSS vulnerabilities, or reflected vulnerabilities, occur when an attacker tricks the victim into processing a URL programmed with a rogue script to steal the victim’s sensitive information (cookie, session ID, etc.). The principle behind this attack lies in exploiting the lack of proper input or output validation on dynamic web sites.</li>\n<li>Persistent XSS vulnerabilities, also known as stored or second order vulnerabilities, are generally targeted at web sites that allow users to input data which are stored in a database or any other such location, e.g., forums, message boards, guest books, etc. The attacker posts some text that contains some malicious JavaScript, and when other users later view the posts, their browsers render the page and execute the attackers JavaScript.</li>\n<li>DOM (Document Object Model)–based XSS vulnerabilities are also referred to as local cross-site scripting. DOM is the standard structure layout to represent HTML and XML documents in the browser. In such attacks the document components such as form fields and cookies can be referenced through JavaScript. The attacker uses the DOM environment to modify the original client-side JavaScript. This causes the victim’s browser to execute the resulting abusive JavaScript code.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"parametervalidation\">Parameter validation</h3>\n<p>The values that are being received by the application are validated to be within defined limits before the server application processes them within the system.</p>\n<h3 id=\"webproxy\">Web proxy</h3>\n<p>A piece of software installed on a system that is designed to intercept all traffic between the local web browser and the web server.</p>\n<h3 id=\"replayattack\">Replay attack</h3>\n<p>An attacker capturing the traffic from a legitimate session and replaying it with the goal of masquerading an authenticated user.</p>\n<h2 id=\"databasemanagementsoftware\">Database Management Software</h2>\n<p>A database is a collection of data stored in a meaningful way that enables multiple users and applications to access, view, and modify data as needed.</p>\n<p>Any type of database should have the following characteristics:</p>\n<ul>\n<li>It centralizes by not having data held on several different servers throughout the network.</li>\n<li>It allows for easier backup procedures.</li>\n<li>It provides transaction persistence.</li>\n<li>It allows for more consistency since all the data are held and maintained in one central location.</li>\n<li>It provides recovery and fault tolerance.</li>\n<li>It allows the sharing of data with multiple users.</li>\n<li>It provides security controls that implement integrity checking, access control, and the necessary level of confidentiality.</li>\n</ul>\n<p>Transaction persistence means the database procedures carrying out transactions are durable and reliable. The state of the database’s security should be the same after a transaction has occurred, and the integrity of the transaction needs to be ensured.</p>\n<h3 id=\"databasemodels\">Database Models</h3>\n<ul>\n<li><strong>Relational</strong>: uses attributes (columns) and tuples (rows) to contain and organize information. It presents information in the form of tables.</li>\n<li><strong>Hierarchical</strong>: combines records and fields that are related in a logical tree structure. The structure and relationship between the data elements are\tdifferent from those in a relational database. In the hierarchical database the parents can have one child, many children, or no children. The most commonly used implementation of the hierarchical model is in the Lightweight Directory Access Protocol (LDAP) model.</li>\n<li><strong>Network</strong>: allows each data element to have multiple parent and child records. This forms a redundant network-like structure instead of a strict tree structure.</li>\n<li><strong>Object-oriented</strong>: is designed to handle a variety of data types (images, audio, documents, video).</li>\n<li><strong>Object-relational</strong>: - a relational database with a software front end that is written in an object-oriented programming language.</li>\n<li><strong>Record</strong>: A collection of related data items.</li>\n<li><strong>File</strong>: A collection of records of the same type.</li>\n<li><strong>Database</strong>: A cross-referenced collection of data.</li>\n<li><strong>DBMS</strong>: Manages and controls the database.</li>\n<li><strong>Tuple</strong>: A row in a two-dimensional database.</li>\n<li><strong>Attribute</strong>: A column in a two-dimensional database.</li>\n<li><strong>Primary key</strong>: Columns that make each row unique. (Every row of a table must include a primary key.)</li>\n<li><strong>View</strong>: A virtual relation defined by the database administrator in order to keep subjects from viewing certain data.</li>\n<li><strong>Foreign key</strong>: An attribute of one table that is related to the primary key of another table.</li>\n<li><strong>Cell</strong>:  An intersection of a row and a column.</li>\n<li><strong>Schema</strong>: Defines the structure of the database.</li>\n<li><strong>Data dictionary</strong>: Central repository of data elements and their relationships.</li>\n<li><strong>Rollback</strong>: An operation that ends a current transaction and cancels all the recent changes to the database until the previous checkpoint/commit point.</li>\n<li><strong>Two-phase commit</strong>: A mechanism that is another control used in databases to ensure the integrity of the data held within the database.</li>\n<li><strong>Cell suppression</strong>: A technique used to hide specific cells that contain sensitive information.</li>\n<li><strong>Noise and perturbation</strong>: A technique of inserting bogus information in the hopes of misdirecting an attacker or confusing the matter enough that the actual attack will not be fruitful.</li>\n<li><strong>Data warehousing</strong>: Combines data from multiple databases or data sources into a large database for the purpose of providing more extensive information retrieval and data analysis.</li>\n<li><strong>Data mining</strong>: The process of massaging the data held in the data warehouse into more useful information.</li>\n</ul>\n<h3 id=\"databaseprogramminginterfaces\">Database Programming Interfaces</h3>\n<ul>\n<li>Open Database Connectivity (ODBC) An API that allows an application to communicate with a database, either locally or remotely</li>\n<li>Object Linking and Embedding Database (OLE DB) Separates data into components that run as middleware on a client or server. It provides a lowlevel interface to link information across different databases, and provides access to data no matter where they are located or how they are formatted.</li>\n<li>ActiveX Data Objects (ADO) An API that allows applications to access back-end database systems. It is a set of ODBC interfaces that exposes the functionality of data sources through accessible objects. ADO uses the OLE DB interface to connect with the database, and can be developed with many different scripting languages.</li>\n<li>Java Database Connectivity (JDBC) An API that allows a Java application to communicate with a database. The application can bridge through ODBC or directly to the database.</li>\n</ul>\n<h3 id=\"datadefinitionlanguageddl\">Data definition language (DDL)</h3>\n<p>Defines the structure and schema of the database.</p>\n<h3 id=\"datamanipulationlanguagedml\">Data manipulation language (DML)</h3>\n<p>Contains all the commands that enable a user to view, manipulate, and use the database (view, add, modify, sort, and delete commands).</p>\n<h3 id=\"querylanguageql\">Query language (QL)</h3>\n<p>Enables users to make requests of the database.</p>\n<h3 id=\"integrity\">Integrity:</h3>\n<p>Database software performs three main types of integrity services: semantic, referential, and entity.</p>\n<ul>\n<li>A semantic integrity mechanism makes sure structural and semantic rules are enforced. These rules pertain to data types, logical values, uniqueness constraints, and operations that could adversely affect the structure of the database.</li>\n<li>A database has referential integrity if all foreign keys reference existing primary keys. There should be a mechanism in place that ensures no foreign key contains a reference to a primary key of a nonexisting record, or a null value.</li>\n<li>Entity integrity guarantees that the tuples are uniquely identified by primary key values.</li>\n</ul>\n<h3 id=\"polyinstantiation\">Polyinstantiation</h3>\n<p>This enables a table that contains multiple tuples with the same primary keys, with each instance distinguished by a security level.</p>\n<h3 id=\"onlinetransactionprocessingoltp\">Online transaction processing (OLTP)</h3>\n<p>Used when databases are clustered to provide fault tolerance and higher performance. The main goal of OLTP is to ensure that transactions happen properly or they don’t happen at all.</p>\n<h3 id=\"theacidtest\">The ACID test:</h3>\n<ul>\n<li><strong>Atomicity</strong>: Divides transactions into units of work and ensures that all modifications take effect or none takes effect. Either the changes are committed or the database is rolled back.</li>\n<li><strong>Consistency</strong>: A transaction must follow the integrity policy developed for that particular database and ensure all data are consistent in the different databases.</li>\n<li><strong>Isolation</strong>: Transactions execute in isolation until completed, without interacting with other transactions. The results of the modification are not available until the transaction is completed.</li>\n<li><strong>Durability</strong>: Once the transaction is verified as accurate on all systems, it is committed and the databases cannot be rolled back.</li>\n</ul>\n<h2 id=\"expertsystems\">Expert systems</h2>\n<p>Use artificial intelligence (AI) to solve complex problems. They are systems that emulate the decision-making ability of a human expert.</p>\n<h2 id=\"inferenceengine\">Inference engine</h2>\n<p>A computer program that tries to derive answers from a knowledge base. It is the “brain” that expert systems use to reason about the data in the knowledge base for the ultimate purpose of formulating new conclusions.</p>\n<h2 id=\"rulebasedprogramming\">Rule-based programming</h2>\n<p>A common way of developing expert systems, with rules based on if-then logic units, and specifying a set of actions to be performed for a given situation.</p>\n<h2 id=\"artificialneuralnetworkann\">Artificial neural network (ANN)</h2>\n<p>A mathematical or computational model based on the neural structure of the brain.</p>\n<h2 id=\"malwaretypes\">Malware Types:</h2>\n<h3 id=\"virus\">Virus</h3>\n<p>A small application, or string of code, that infects host applications. It is a programming code that can replicate itself and spread from one system to another.</p>\n<h3 id=\"macrovirus\">Macro virus</h3>\n<p>A virus written in a macro language and that is platform independent. Since many applications allow macro programs to be embedded in documents, the programs may be run automatically when the document is opened. This provides a distinct mechanism by which viruses can be spread.</p>\n<h3 id=\"compressionviruses\">Compression viruses</h3>\n<p>Another type of virus that appends itself to executables on the system and compresses them by using the user’s permissions.</p>\n<h3 id=\"stealthvirus\">Stealth virus</h3>\n<p>A virus that hides the modifications it has made. The virus tries to trick antivirus software by intercepting its requests to the operating system and providing false and bogus information.</p>\n<h3 id=\"polymorphicvirus\">Polymorphic virus</h3>\n<p>Produces varied but operational copies of itself. A polymorphic virus may have no parts that remain identical between infections, making it very difficult to detect directly using signatures.</p>\n<h3 id=\"multipartvirus\">Multipart virus</h3>\n<p>Also called a multipartite virus, this has several components to it and can be distributed to different parts of the system. It infects and spreads in multiple ways, which makes it harder to eradicate when identified.</p>\n<h3 id=\"selfgarblingvirus\">Self-garbling virus</h3>\n<p>Attempts to hide from antivirus software by modifying its own code so that it does not match predefined signatures.</p>\n<h3 id=\"memeviruses\">Meme viruses</h3>\n<p>These are not actual computer viruses, but types of e-mail messages that are continually forwarded around the Internet.</p>\n<h3 id=\"bots\">Bots</h3>\n<p>Software applications that run automated tasks over the Internet, which perform tasks that are both simple and structurally repetitive. Malicious use of bots is the coordination and operation of an automated attack by a botnet (centrally controlled collection of bots).</p>\n<h3 id=\"worms\">Worms</h3>\n<p>These are different from viruses in that they can reproduce on their own without a host application and are self-contained programs.</p>\n<h3 id=\"logicbomb\">Logic bomb</h3>\n<p>Executes a program, or string of code, when a certain event happens or a date and time arrives.</p>\n<h3 id=\"rootkit\">Rootkit</h3>\n<p>Set of malicious tools that are loaded on a compromised system through stealthy techniques. The tools are used to carry out more attacks either on the infected systems or surrounding systems.</p>\n<h3 id=\"trojanhorse\">Trojan horse</h3>\n<p>A program that is disguised as another program with the goal of carrying out malicious activities in the background without the user knowing.</p>\n<h3 id=\"remoteaccesstrojansrats\">Remote access Trojans (RATs)</h3>\n<p>Malicious programs that run on systems and allow intruders to access and use a system remotely.</p>\n<h3 id=\"immunizer\">Immunizer</h3>\n<p>Attaches code to the file or application, which would fool a virus into “thinking” it was already infected.</p>\n<h3 id=\"behaviorblocking\">Behavior blocking</h3>\n<p>Allowing the suspicious code to execute within the operating system and watches its interactions with the operating system, looking for suspicious activities.</p>\n<h1 id=\"securityarchitectureanddesign\">Security Architecture and Design</h1>\n<p>Stakeholders for a system are the users, operators, maintainers, developers, and suppliers.</p>\n<p>Computer architecture encompasses all of the parts of a computer system that are necessary for it to function, including the operating system, memory chips, logic circuits, storage devices, input and output devices, security components, buses, and networking interfaces.</p>\n<h2 id=\"isoiec420102007\">ISO/IEC 42010:2007</h2>\n<p>International standard that provides guidelines on how to create and maintain system architectures.</p>\n<h2 id=\"centralprocessingunitcpu\">Central processing unit (CPU)</h2>\n<p>Carries out the execution of instructions within a computer.</p>\n<h2 id=\"arithmeticlogicunitalu\">Arithmetic logic unit (ALU)</h2>\n<p>Component of the CPU that carries out logic and mathematical functions as they are laid out in the programming code being processed by the CPU.</p>\n<h2 id=\"register\">Register</h2>\n<p>Small, temporary memory storage units integrated and used by the CPU during its processing functions.</p>\n<h2 id=\"controlunit\">Control unit</h2>\n<p>Part of the CPU that oversees the collection of instructions and data from memory and how they are passed to the processing components of the CPU.</p>\n<h2 id=\"generalregisters\">General registers</h2>\n<p>Temporary memory location the CPU uses during its processes of executing instructions. The ALU’s “scratch pad” it uses while carrying out logic and math functions.</p>\n<h2 id=\"specialregisters\">Special registers</h2>\n<p>Temporary memory location that holds critical processing parameters. They hold values as in the program counter, stack pointer, and program status word.</p>\n<h2 id=\"programcounter\">Program counter</h2>\n<p>Holds the memory address for the following instructions the CPU needs to act upon.</p>\n<h2 id=\"stack\">Stack</h2>\n<p>Memory segment used by processes to communicate instructions and data to each other.</p>\n<h2 id=\"programstatusword\">Program status word</h2>\n<p>Condition variable that indicates to the CPU what mode (kernel or user) instructions need to be carried out in.</p>\n<h2 id=\"usermodeproblemstate\">User mode (problem state)</h2>\n<p>Protection mode that a CPU works within when carrying out less trusted process instructions.</p>\n<h2 id=\"kernelmodesupervisorystateprivilegemode\">Kernel mode (supervisory state, privilege mode)</h2>\n<p>Mode that a CPU works within when carrying out more trusted process instructions. The process has access to more computer resources when working in kernel versus user mode.</p>\n<h2 id=\"addressbus\">Address bus</h2>\n<p>Physical connections between processing components and memory segments used to communicate the physical memory addresses being used during processing procedures.</p>\n<h2 id=\"databus\">Data bus</h2>\n<p>Physical connections between processing components and memory segments used to transmit data being used during processing procedures.</p>\n<h2 id=\"symmetricmodemultiprocessing\">Symmetric mode multiprocessing</h2>\n<p>When a computer has two or more CPUs and each CPU is being used in a load-balancing method.</p>\n<h2 id=\"asymmetricmodemultiprocessing\">Asymmetric mode multiprocessing</h2>\n<p>When a computer has two or more CPUs and one CPU is dedicated to a specific program while the other CPUs carry out general processing procedures.</p>\n<h2 id=\"process\">Process</h2>\n<p>Program loaded in memory within an operating system.</p>\n<h2 id=\"multiprogramming\">Multiprogramming</h2>\n<p>Interleaved execution of more than one program (process) or task by a single operating system.</p>\n<h2 id=\"multitasking\">Multitasking</h2>\n<p>Simultaneous execution of more than one program (process) or task by a single operating system.</p>\n<h2 id=\"cooperativemultitasking\">Cooperative multitasking</h2>\n<p>Multitasking scheduling scheme used by older operating systems to allow for computer resource time slicing. Processes had too much control over resources, which would allow for the programs or systems to “hang.”</p>\n<h2 id=\"preemptivemultitasking\">Preemptive multitasking</h2>\n<p>Multitasking scheduling scheme used by operating systems to allow for computer resource time slicing. Used in newer, more stable operating systems.</p>\n<h2 id=\"processstatesreadyrunningblocked\">Process states (ready, running, blocked)</h2>\n<p>Processes can be in various activity levels. Ready = waiting for input. Running = instructions being executed by CPU. Blocked = process is “suspended.”</p>\n<h2 id=\"interrupts\">Interrupts</h2>\n<p>Values assigned to computer components (hardware and software) to allow for efficient computer resource time slicing.</p>\n<h2 id=\"maskableinterrupt\">Maskable interrupt</h2>\n<p>Interrupt value assigned to a noncritical operating system activity.</p>\n<h2 id=\"nonmaskableinterrupt\">Nonmaskable interrupt</h2>\n<p>Interrupt value assigned to a critical operating system activity.</p>\n<h2 id=\"thread\">Thread</h2>\n<p>Instruction set generated by a process when it has a specific activity that needs to be carried out by an operating system. When the activity is finished, the thread is destroyed.</p>\n<h2 id=\"multithreading\">Multithreading</h2>\n<p>Applications that can carry out multiple activities simultaneously by generating different instruction sets (threads).</p>\n<h2 id=\"softwaredeadlock\">Software deadlock</h2>\n<p>Two processes cannot complete their activities because they are both waiting for system resources to be released.</p>\n<h2 id=\"processisolation\">Process isolation</h2>\n<p>Protection mechanism provided by operating systems that can be implemented as encapsulation, time multiplexing of shared resources, naming distinctions, and virtual memory mapping.</p>\n<p>When a process is encapsulated, no other process understands or interacts with its internal programming code.</p>\n<p>Encapsulation provides data hiding, which means that outside software components will not know how a process works and will not be able to manipulate the process’s internal code. This is an integrity mechanism and enforces modularity in programming code.</p>\n<h2 id=\"timemultiplexing\">Time multiplexing</h2>\n<p>A technology that allows processes to use the same resources.</p>\n<h2 id=\"virtualaddressmapping\">Virtual address mapping</h2>\n<p>Allows the different processes to have their own memory space. The memory manager ensures no processes improperly interact with another process’s memory. This provides integrity and confidentiality for the individual processes and their data and an overall stable processing environment for the operating system.</p>\n<p>The goals of memory management are to:</p>\n<ul>\n<li>Provide an abstraction level for programmers</li>\n<li>Maximize performance with the limited amount of memory available</li>\n<li>Protect the operating system and applications loaded into memory</li>\n</ul>\n<p>The memory manager has five basic responsibilities:</p>\n<ul>\n<li>\n<p>Relocation</p>\n<ul>\n<li>Swap contents from RAM to the hard drive as needed</li>\n<li>Provide pointers for applications if their instructions and memory segment have been moved to a different location in main memory</li>\n</ul>\n</li>\n<li>\n<p>Protection</p>\n<ul>\n<li>Limit processes to interact only with the memory segments assigned to them</li>\n<li>Provide access control to memory segments</li>\n</ul>\n</li>\n<li>\n<p>Sharing</p>\n<ul>\n<li>Use complex controls to ensure integrity and confidentiality when processes need to use the same shared memory segments</li>\n<li>Allow many users with different levels of access to interact with the same application running in one memory segment</li>\n</ul>\n</li>\n<li>\n<p>Logical organization</p>\n<ul>\n<li>Segment all memory types and provide an addressing scheme for each at an abstraction level</li>\n<li>Allow for the sharing of specific software modules, such as dynamic link library (DLL) procedures</li>\n</ul>\n</li>\n<li>\n<p>Physical organization</p>\n<ul>\n<li>Segment the physical memory space for application and operating system processes</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"dynamiclinklibrariesdlls\">Dynamic link libraries (DLLs)</h2>\n<p>A set of subroutines that are shared by different applications and operating system processes.</p>\n<h2 id=\"baseregisters\">Base registers</h2>\n<p>Beginning of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.</p>\n<h2 id=\"limitregisters\">Limit registers</h2>\n<p>Ending of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.</p>\n<h2 id=\"memoryprotectionissues\">Memory Protection Issues:</h2>\n<ul>\n<li>Every address reference is validated for protection.</li>\n<li>Two or more processes can share access to the same segment with potentially different access rights.</li>\n<li>Different instruction and data types can be assigned different levels of protection.</li>\n<li>Processes cannot generate an unpermitted address or gain access to an unpermitted segment</li>\n</ul>\n<h2 id=\"ram\">RAM</h2>\n<p>Memory sticks that are plugged into a computer’s motherboard and work as volatile memory space for an operating system.</p>\n<p>Additional types of RAM you should be familiar with:</p>\n<ul>\n<li>Synchronous DRAM (SDRAM) Synchronizes itself with the system’s CPU and synchronizes signal input and output on the RAM chip. It coordinates its activities with the CPU clock so the timing of the CPU and the timing of the memory activities are synchronized. This increases the speed of transmitting and executing data.</li>\n<li>Extended data out DRAM (EDO DRAM) This is faster than DRAM because DRAM can access only one block of data at a time, whereas EDO DRAM can capture the next block of data while the first block is being sent to the CPU for processing. It has a type of “look ahead” feature that speeds up memory access.</li>\n<li>Burst EDO DRAM (BEDO DRAM) Works like (and builds upon) EDO DRAM in that it can transmit data to the CPU as it carries out a read option, but it can send more data at once (burst). It reads and sends up to four memory addresses in a small number of clock cycles.</li>\n<li>Double data rate SDRAM (DDR SDRAM) Carries out read operations on the rising and falling cycles of a clock pulse. So instead of carrying out one operation per clock cycle, it carries out two and thus can deliver twice the throughput of SDRAM. Basically, it doubles the speed of memory activities, when compared to SDRAM, with a smaller number of clock cycles.</li>\n</ul>\n<h2 id=\"thrashing\">Thrashing</h2>\n<p>When a computer spends more time moving data from one small portion of memory to another than actually processing the data.</p>\n<h2 id=\"rom\">ROM</h2>\n<p>Nonvolatile memory that is used on motherboards for BIOS functionality and various device controllers to allow for operating system-to-device communication. Sometimes used for off-loading graphic rendering or cryptographic functionality.</p>\n<p>Types of ROM:</p>\n<ul>\n<li>Programmable read-only memory (PROM) is a form of ROM that can be modified after it has been manufactured. PROM can be programmed only one time because the voltage that is used to write bits into the memory cells actually burns out the fuses that connect the individual memory cells.</li>\n<li>Erasable programmable read-only memory (EPROM) can be erased, modified, and upgraded. EPROM holds data that can be electrically erased or written to.</li>\n<li>Flash memory is a special type of memory that is used in digital cameras, BIOS chips, memory cards, and video game consoles. It is a solid-state technology, meaning it does not have moving parts and is used more as a type of hard drive than memory.</li>\n</ul>\n<h2 id=\"hardwaresegmentation\">Hardware segmentation</h2>\n<p>Physically mapping software to individual memory segments.</p>\n<h2 id=\"cachememory\">Cache memory</h2>\n<p>Fast and expensive memory type that is used by a CPU to increase read and write operations.</p>\n<h2 id=\"absoluteaddresses\">Absolute addresses</h2>\n<p>Hardware addresses used by the CPU.</p>\n<h2 id=\"logicaladdresses\">Logical addresses</h2>\n<p>Indirect addressing used by processes within an operating system. The memory manager carries out logical-to-absolute address mapping.</p>\n<h2 id=\"stack\">Stack</h2>\n<p>Memory construct that is made up of individually addressable buffers. Process-to-process communication takes place through the use of stacks.</p>\n<h2 id=\"bufferoverflow\">Buffer overflow</h2>\n<p>Too much data is put into the buffers that make up a stack. Common attack vector used by hackers to run malicious code on a target system.</p>\n<h2 id=\"boundschecking\">Bounds checking</h2>\n<p>Ensuring the inputted data are of an acceptable length.</p>\n<h2 id=\"addressspacelayoutrandomizationaslr\">Address space layout randomization (ASLR)</h2>\n<p>Memory protection mechanism used by some operating systems. The addresses used by components of a process are randomized so that it is harder for an attacker to exploit specific memory vulnerabilities.</p>\n<h2 id=\"dataexecutionpreventiondep\">Data execution prevention (DEP)</h2>\n<p>Memory protection mechanism used by some operating systems. Memory segments may be marked as nonexecutable so that they cannot be misused by malicious software.</p>\n<h2 id=\"garbagecollector\">Garbage collector</h2>\n<p>Tool that marks unused memory segments as usable to ensure that an operating system does not run out of memory. Used to protect against memory leaks.</p>\n<h2 id=\"virtualmemory\">Virtual memory</h2>\n<p>Combination of main memory (RAM) and secondary memory within an operating system.</p>\n<h2 id=\"interrupt\">Interrupt</h2>\n<p>Software or hardware signal that indicates that system resources (i.e., CPU) are needed for instruction processing.</p>\n<h2 id=\"programmableio\">Programmable I/O</h2>\n<p>The CPU sends data to an I/O device and polls the device to see if it is ready to accept more data.</p>\n<h2 id=\"interruptdrivenio\">Interrupt-driven I/O</h2>\n<p>The CPU sends a character over to the printer and then goes and works on another process’s request.</p>\n<h2 id=\"iousingdirectmemoryaccessdma\">I/O using Direct memory access (DMA)</h2>\n<p>A way of transferring data between I/O devices and the system’s memory without using the CPU.</p>\n<h2 id=\"premappedio\">Premapped I/O</h2>\n<p>The CPU sends the physical memory address of the requesting process to the I/O device, and the I/O device is trusted enough to interact with the contents of memory directly, so the CPU does not control the interactions between the I/O device and memory.</p>\n<h2 id=\"fullymappedio\">Fully Mapped I/O</h2>\n<p>Under fully mapped I/O, the operating system does not trust the I/O device.</p>\n<h2 id=\"instructionset\">Instruction set</h2>\n<p>Set of operations and commands that can be implemented by a particular processor (CPU).</p>\n<h2 id=\"microarchitecture\">Microarchitecture</h2>\n<p>Specific design of a microprocessor, which includes physical components (registers, logic gates, ALU, cache, etc.) that support a specific instruction set.</p>\n<h2 id=\"ringbasedarchitectureprotectionrings\">Ring-based architecture (Protection Rings)</h2>\n<p>Mechanisms to protect data and functionality from faults (by improving fault tolerance) and malicious behaviour through  two or more hierarchical levels or layers of privilege within the architecture of a computer system</p>\n<h2 id=\"applicationprogramminginterface\">Application programming interface</h2>\n<p>Software interface that enables process-to-process interaction. Common way to provide access to standard routines to a set of software programs.</p>\n<h2 id=\"monolithicoperatingsystemarchitecture\">Monolithic operating system architecture</h2>\n<p>All of the code of the operating system working in kernel mode in an ad hoc and nonmodularized manner.</p>\n<h2 id=\"layeredoperatingsystemarchitecture\">Layered operating system architecture</h2>\n<p>Architecture that separates system functionality into hierarchical layers.</p>\n<h2 id=\"datahiding\">Data hiding</h2>\n<p>Use of segregation in design decisions to protect software components from negatively interacting with each other. Commonly enforced through strict interfaces.</p>\n<h2 id=\"microkernelarchitecture\">Microkernel architecture</h2>\n<p>Reduced amount of code running in kernel mode carrying out critical operating system functionality. Only the absolutely necessary code runs in kernel mode, and the remaining operating system code runs in user mode.</p>\n<h2 id=\"hybridmicrokernelarchitecture\">Hybrid microkernel architecture</h2>\n<p>Combination of monolithic and microkernel architectures. The microkernel carries out critical operating system functionality, and the remaining functionality is carried out in a client\\server model within kernel mode.</p>\n<h2 id=\"modetransition\">Mode transition</h2>\n<p>When the CPU has to change from processing code in user mode to kernel mode. This is a protection measure, but it causes a performance hit.</p>\n<h2 id=\"virtualization\">Virtualization</h2>\n<p>Creation of a simulated environment (hardware platform, operating system, storage, etc.) that allows for central control and scalability.</p>\n<h2 id=\"hypervisor\">Hypervisor</h2>\n<p>Central program used to manage virtual machines (guests) within a simulated environment (host).</p>\n<h2 id=\"securitypolicy\">Security policy</h2>\n<p>Strategic tool used to dictate how sensitive information and resources are to be managed and protected.</p>\n<h2 id=\"trustedcomputingbase\">Trusted computing base</h2>\n<p>A collection of all the hardware, software, and firmware components within a system that provide security and enforce the system’s security policy.</p>\n<h2 id=\"trustedpath\">Trusted path</h2>\n<p>Trustworthy software channel that is used for communication between two processes that cannot be circumvented.</p>\n<h2 id=\"securityperimeter\">Security perimeter</h2>\n<p>Mechanism used to delineate between the components within and outside of the trusted computing base.</p>\n<h2 id=\"referencemonitor\">Reference monitor</h2>\n<p>Concept that defines a set of design requirements of a reference validation mechanism (security kernel), which enforces an access control policy over subjects’ (processes, users) ability to perform operations (read, write, execute) on objects (files, resources) on a system.</p>\n<h2 id=\"securitykernel\">Security kernel</h2>\n<p>Hardware, software, and firmware components that fall within the TCB and implement and enforce the reference monitor concept.</p>\n<p>The security kernel has three main requirements:</p>\n<ul>\n<li>It must provide isolation for the processes carrying out the reference monitor concept, and the processes must be tamperproof.</li>\n<li>It must be invoked for every access attempt and must be impossible to circumvent. Thus, the security kernel must be implemented in a complete and foolproof way.</li>\n<li>It must be small enough to be tested and verified in a complete and comprehensive manner.</li>\n</ul>\n<h2 id=\"multilevelsecuritypolicies\">Multilevel security policies</h2>\n<p>Outlines how a system can simultaneously process information at different classifications for users with different clearance levels.</p>\n<h2 id=\"statemachinemodels\">State machine models</h2>\n<ul>\n<li>To verify the security of a system, the state is used, which means that all current permissions and all current instances of subjects accessing objects must be captured.</li>\n<li>A system that has employed a state machine model will be in a secure state in each and every instance of its existence.</li>\n</ul>\n<h2 id=\"securitymodels\">Security Models</h2>\n<h3 id=\"belllapadulamodel\">Bell-LaPadula model:</h3>\n<p>This is the first mathematical model of a multilevel security policy that defines the concept of a secure state and necessary modes of access. It ensures that information only flows in a manner that does not violate the system policy and is confidentiality focused.</p>\n<ul>\n<li>The simple security rule\n<ul>\n<li>A subject cannot read data at a higher security level (no read up).</li>\n</ul>\n</li>\n<li>The *-property rule\n<ul>\n<li>A subject cannot write to an object at a lower security level (no write down).</li>\n</ul>\n</li>\n<li>The strong star property rule\n<ul>\n<li>A subject can perform read and write functions only to the objects at its same security level.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"bibamodel\">Biba model</h3>\n<p>A formal state transition model that describes a set of access control rules designed to ensure data integrity.</p>\n<ul>\n<li>The simple integrity axiom A subject cannot read data at a lower integrity level (no read down).</li>\n<li>The *-integrity axiom A subject cannot modify an object in a higher integrity level (no write up).</li>\n</ul>\n<h3 id=\"clarkwilsonmodel\">Clark-Wilson model</h3>\n<p>This integrity model is implemented to protect the integrity of data and to ensure that properly formatted transactions take place. It addresses all three goals of integrity:</p>\n<ul>\n<li>Subjects can access objects only through authorized programs (access triple).</li>\n<li>Separation of duties is enforced.</li>\n<li>Auditing is required.</li>\n</ul>\n<h3 id=\"brewerandnashmodel\">Brewer and Nash model</h3>\n<p>This model allows for dynamically changing access controls that protect against conflicts of interest. Also known as the Chinese Wall model.</p>\n<h2 id=\"securitymodes\">Security Modes</h2>\n<p>Dedicated Security Mode All users must have:</p>\n<ul>\n<li>Proper clearance for all information on the system</li>\n<li>Formal access approval for all information on the system</li>\n<li>A signed NDA for all information on the system</li>\n<li>A valid need-to-know for all information on the system</li>\n<li>All users can access all data.</li>\n</ul>\n<p>System High-Security Mode All users must have:</p>\n<ul>\n<li>Proper clearance for all information on the system</li>\n<li>Formal access approval for all information on the system</li>\n<li>A signed NDA for all information on the system</li>\n<li>A valid need-to-know for some information on the system</li>\n<li>All users can access some data, based on their need-to-know.</li>\n</ul>\n<p>Compartmented Security Mode All users must have:</p>\n<ul>\n<li>Proper clearance for the highest level of data classification on the system</li>\n<li>Formal access approval for some information on the system</li>\n<li>A signed NDA for all information they will access on the system</li>\n<li>A valid need-to-know for some of the information on the system</li>\n<li>All users can access some data, based on their need-to-know and formal access approval.</li>\n</ul>\n<p>Multilevel Security Mode All users must have:</p>\n<ul>\n<li>Proper clearance for some of the information on the system</li>\n<li>Formal access approval for some of the information on the system</li>\n<li>A signed NDA for all information on the system</li>\n<li>A valid need-to-know for some of the information on the system</li>\n<li>All users can access some data, based on their need-to-know, clearance, and formal access approval.</li>\n</ul>\n<h2 id=\"thecommoncriteria\">The Common Criteria</h2>\n<p>Under the Common Criteria model, an evaluation is carried out on a product and it is assigned an Evaluation Assurance Level (EAL).</p>\n<ul>\n<li>EAL1 Functionally tested</li>\n<li>EAL2 Structurally tested</li>\n<li>EAL3 Methodically tested and checked</li>\n<li>EAL4 Methodically designed, tested, and reviewed</li>\n<li>EAL5 Semiformally designed and tested</li>\n<li>EAL6 Semiformally verified design and tested</li>\n<li>EAL7 Formally verified design and tested</li>\n</ul>\n<p>The Common Criteria uses protection profiles in its evaluation process.</p>\n<p>A protection profile contains the following five sections:</p>\n<ul>\n<li>Descriptive elements\n<ul>\n<li>Provides the name of the profile and a description of the security problem to be solved.</li>\n</ul>\n</li>\n<li>Rationale\n<ul>\n<li>Justifies the profile and gives a more detailed description of the real-world problem to be solved. The environment, usage assumptions, and threats are illustrated along with guidance on the security policies that can be supported by products and systems that conform to this profile.</li>\n</ul>\n</li>\n<li>Functional requirements\n<ul>\n<li>Establishes a protection boundary, meaning the threats or compromises within this boundary to be countered. The product or system must enforce the boundary established in this section.</li>\n</ul>\n</li>\n<li>Development assurance requirements\n<ul>\n<li>Identifies the specific requirements the product or system must meet during the development phases, from design to implementation.</li>\n</ul>\n</li>\n<li>Evaluation assurance requirements\n<ul>\n<li>Establishes the type and intensity of the evaluation.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"systemsevaluationmethods\">Systems Evaluation Methods</h2>\n<ul>\n<li>In the Trusted Computer System Evaluation Criteria (TCSEC), commonly known as the Orange Book,  the lower assurance level ratings look at a system’s protection mechanisms and testing results to produce an assurance rating, but the higher assurance level ratings look more at the system design, specifications, development procedures, supporting documentation, and testing results.</li>\n<li>An assurance evaluation examines the security-relevant parts of a system, meaning the TCB, access control mechanisms, reference monitor, kernel, and protection mechanisms. The relationship and interaction between these components are also evaluated.</li>\n<li>The Orange Book was used to evaluate whether a product contained the security properties the vendor claimed it did and whether the product was appropriate for a specific application or function.</li>\n<li>TCSEC addresses confidentiality, but not integrity. Functionality of the security mechanisms and the assurance of those mechanisms are not evaluated separately, but rather are combined and rated as a whole</li>\n<li>TCSEC provides a classification system that is divided into hierarchical divisions of assurance levels:\n<ul>\n<li>A. Verified protection</li>\n<li>B. Mandatory protection</li>\n<li>B1: Labeled Security\n<ul>\n<li>Each data object must contain a classification label and each subject must have a clearance label. When a subject attempts to access an object,<br>\nthe system must compare the subject’s and object’s security labels to ensure the requested actions are acceptable. Data leaving the system must also contain an accurate<br>\nsecurity label. The security policy is based on an informal statement, and the design specifications are reviewed and verified.<br>\nB2: Structured Protection</li>\n<li>The security policy is clearly defined and documented, and the system design and implementation are subjected to more thorough review and testing procedures. This class requires more stringent authentication mechanisms and well-defined interfaces among layers. Subjects and devices require labels, and the system must not allow covert channels.<br>\nB3: Security Domains</li>\n<li>In this class, more granularity is provided in each protection mechanism, and the programming code that is not necessary to support the security policy is excluded.</li>\n</ul>\n</li>\n<li>C. Discretionary protection</li>\n<li>C1: Discretionary Security Protection\n<ul>\n<li>Discretionary access control is based on individuals and/or groups. It requires a separation of users and information, and identification and authentication of individual entities.</li>\n</ul>\n</li>\n<li>C2: Controlled Access Protection\n<ul>\n<li>Users need to be identified individually to provide more precise access control and auditing functionality. Logical access control mechanisms are used to enforce authentication and the uniqueness of each individual’s identification.</li>\n</ul>\n</li>\n<li>D. Minimal security</li>\n<li>D1: The classes with higher numbers offer a greater degree of trust and assurance. The criteria breaks down into seven different areas:\n<ul>\n<li>Security policy The policy must be explicit and well defined and enforced by the mechanisms within the system.</li>\n<li>Identification Individual subjects must be uniquely identified.</li>\n<li>Labels Access control labels must be associated properly with objects.</li>\n<li>Documentation Documentation must be provided, including test, design, and specification documents, user guides, and manuals.</li>\n<li>Accountability Audit data must be captured and protected to enforce accountability.</li>\n<li>Life-cycle assurance Software, hardware, and firmware must be able to be tested individually to ensure that each enforces the security policy in an effective manner throughout their lifetimes.</li>\n<li>Continuous protection The security mechanisms and the system as a whole must perform predictably and acceptably in different situations continuously.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>The Red Book is referred to as the Trusted Network Interpretation (TNI). The following is a brief overview of the security items addressed in the Red Book:\n<ul>\n<li>Communication integrity</li>\n<li>Authentication Protects against masquerading and playback attacks. Mechanisms include digital signatures, encryption, timestamp, and passwords.</li>\n<li>Message integrity Protects the protocol header, routing information, and packet payload from being modified. Mechanisms include message authentication and encryption.</li>\n<li>Nonrepudiation Ensures that a sender cannot deny sending a message. Mechanisms include encryption, digital signatures, and notarization.</li>\n<li>Denial-of-service prevention</li>\n<li>Continuity of operations Ensures that the network is available even if attacked. Mechanisms include fault-tolerant and redundant systems and the capability to reconfigure network parameters in case of an emergency.</li>\n<li>Network management Monitors network performance and identifies attacks and failures. Mechanisms include components that enable network administrators to monitor and restrict resource access.</li>\n<li>Compromise protection</li>\n<li>Data confidentiality Protects data from being accessed in an unauthorized method during transmission. Mechanisms include access controls, encryption, and physical protection of cables.</li>\n<li>Traffic flow confidentiality Ensures that unauthorized entities are not aware of routing information or frequency of communication via traffic analysis. Mechanisms include padding messages, sending noise, or sending false messages.</li>\n<li>Selective routing Routes messages in a way to avoid specific threats. Mechanisms include network configuration and routing tables.</li>\n</ul>\n</li>\n<li>The Information Technology Security Evaluation Criteria (ITSEC) was the first attempt at establishing a single standard for evaluating security attributes of computer systems and products by many European countries.</li>\n<li>ITSEC evaluates two main attributes of a system’s protection mechanisms: functionality and assurance.</li>\n<li>A difference between ITSEC and TCSEC is that TCSEC bundles functionality and assurance into one rating, whereas ITSEC evaluates these two attributes separately.</li>\n</ul>\n<h1 id=\"penetrationtesting\">Penetration testing</h1>\n<h2 id=\"stepsforvulnerabilityassessment\">Steps for vulnerability assessment</h2>\n<ol>\n<li>Vulnerability scanning</li>\n<li>Analysis</li>\n<li>Communicate results</li>\n</ol>\n<h2 id=\"penetrationteststrategies\">Penetration test strategies</h2>\n<p>•\tExternal testing<br>\n•\tInternal testing<br>\n•\tBlind testing<br>\n•\tDouble-blind testing</p>\n<h2 id=\"categoriesofpenetrationtesting\">Categories of penetration testing</h2>\n<p>•\tZero knowledge<br>\n•\tPartial knowledge<br>\n•\tFull knowledge</p>\n<h2 id=\"penetrationtestmethodology\">Penetration test methodology</h2>\n<ol>\n<li>Reconnaissance</li>\n<li>Enumeration</li>\n<li>Vulnerability analysis</li>\n<li>Execution / Exploitation</li>\n<li>Document findings</li>\n</ol>\n<h1 id=\"accesscontrols\">Access Controls</h1>\n<h2 id=\"primaryconcerns\">Primary Concerns</h2>\n<ul>\n<li>Who owns the data?</li>\n<li>Who consumes/uses the data?</li>\n<li>Who shouldn't have access to the data?</li>\n</ul>\n<h2 id=\"securityconcepts\">Security Concepts</h2>\n<ul>\n<li>Confidentiality</li>\n<li>Integrity</li>\n<li>Availability</li>\n</ul>\n<h2 id=\"datacontrolterms\">Data control terms</h2>\n<ul>\n<li>Subject - A person / end user</li>\n<li>Object  - The data a subject is accessing</li>\n</ul>\n<h2 id=\"degreesofaccesscontrol\">Degrees of access control</h2>\n<ul>\n<li>Read-only   - Can see, but cannot edit the data.</li>\n<li>Contributor - Can read, and also modify/add data.</li>\n</ul>\n<h2 id=\"operationalterms\">Operational Terms</h2>\n<ol>\n<li>Identification - Determine the identity of a subject</li>\n<li>Authentication - Validate a subject's identity</li>\n<li>Authorization  - Validate their access against a directory LDAP - Lightweight Directory Access Protocol x.500 - LDAP standard</li>\n<li>Accountability - Ensuring that the access controls are applied Logging / Recording</li>\n</ol>\n<h3 id=\"racecondition\">Race Condition</h3>\n<p>When processes try to carry out whatever activity they are set to perform in an incorrect order.<br>\ne.g. Authorizing before authenticating</p>\n<h2 id=\"cookie\">Cookie</h2>\n<ul>\n<li>Stored in a text file</li>\n<li>Credential information stored for repeated use</li>\n<li>Allows web sites to track a user session across multiple pages.</li>\n</ul>\n<h2 id=\"singlesignonsso\">Single-Sign-On (SSO)</h2>\n<ul>\n<li>Login once access many</li>\n<li>Subjects login once and are then able to access objects without having to re-authenticate</li>\n</ul>\n<h2 id=\"userprovisioning\">User provisioning</h2>\n<ul>\n<li>Creation of user accounts / subjects</li>\n<li>Assignment of permissions and access at the time of creation</li>\n<li>Can be automated (Especially for large organizations)</li>\n</ul>\n<h2 id=\"federatedidentity\">Federated Identity</h2>\n<ul>\n<li>Trust between multiple systems</li>\n<li>Subjects can access objects in separate systems</li>\n</ul>\n<h2 id=\"markuplanguages\">Markup Languages</h2>\n<ul>\n<li>XML  - Extensible Markup Language</li>\n<li>SPML - Service Provisioning Markup Language</li>\n<li>SAML - Security Assertion Markup Language</li>\n<li>SOAP - Simple Object Access Protocol</li>\n</ul>\n<h2 id=\"biometrics\">Biometrics</h2>\n<p>Something we &quot;are&quot;</p>\n<ul>\n<li>Fingerprints</li>\n<li>Handprint</li>\n<li>Signature Dynamic</li>\n</ul>\n<h2 id=\"errors\">Errors</h2>\n<p>Type 1 - False Reject<br>\n- Subject is incorrectly denied</p>\n<p>Type 2 - False Accept<br>\n- Subject is incorrectly authenticated</p>\n<h3 id=\"crossovererrorratecer\">Crossover Error Rate (CER)</h3>\n<pre><code>- Where the rate of Type 1 and Type 2 intersect\n</code></pre>\n<h2 id=\"biometrictypes\">Biometric Types</h2>\n<h3 id=\"fingerprints\">Fingerprints</h3>\n<ul>\n<li>Most common</li>\n<li>Easily captured</li>\n<li>Stored digitally, can be large</li>\n<li>Easy to fool</li>\n</ul>\n<h3 id=\"palmscanning\">Palm scanning</h3>\n<h3 id=\"retinascanning\">Retina scanning</h3>\n<ul>\n<li>Pattern of blood vessels in the eye</li>\n</ul>\n<h3 id=\"irisscan\">Iris scan</h3>\n<ul>\n<li>Color pattern surrounding the pupil</li>\n<li>The most accurate</li>\n<li>The most secure</li>\n</ul>\n<h3 id=\"signaturedynamics\">Signature Dynamics</h3>\n<ul>\n<li>Force</li>\n<li>Inclination</li>\n<li>Letter characteristics</li>\n</ul>\n<h3 id=\"keystrokedynamics\">Keystroke Dynamics</h3>\n<ul>\n<li>Patterns used while typing</li>\n</ul>\n<h3 id=\"voiceprint\">Voice print</h3>\n<h3 id=\"facialscan\">Facial scan</h3>\n<h3 id=\"handtopography\">Hand topography</h3>\n<ul>\n<li>Peaks and valleys on the hand</li>\n</ul>\n<h3 id=\"handgeometry\">Hand geometry</h3>\n<ul>\n<li>Width of fingers</li>\n</ul>\n<h2 id=\"passwords\">Passwords</h2>\n<p>Something we &quot;know&quot;</p>\n<ul>\n<li>Various character lengths</li>\n<li>Alpha-numeric</li>\n<li>Special character</li>\n<li>String of characters identify subject</li>\n<li>The most common method of authentication</li>\n</ul>\n<h2 id=\"passwordweaknesses\">Password Weaknesses</h2>\n<h3 id=\"electronicmonitoring\">Electronic monitoring</h3>\n<ul>\n<li>Sniffers</li>\n<li>Password capture devices</li>\n<li>Cain and Abel</li>\n<li>Kismet</li>\n</ul>\n<h3 id=\"accessingthepasswordfile\">Accessing the password file</h3>\n<ul>\n<li>Targeting the authentication server</li>\n<li>Linux\n<ul>\n<li>/etc/shadow</li>\n<li>/etc/passwd</li>\n</ul>\n</li>\n<li>Windows\n<ul>\n<li>SAM Hive in the registry</li>\n<li>C:\\Windows\\Security</li>\n<li>C:\\Windows\\Security\\Database</li>\n</ul>\n</li>\n<li>Brute force attack\n<ul>\n<li>Attempting to &quot;guess&quot; the password</li>\n<li>l0pht Crack</li>\n</ul>\n</li>\n<li>Dictionary Attacks\n<ul>\n<li>Attempts passwords from a pre-compiled list of passwords</li>\n</ul>\n</li>\n<li>Social Engineering\n<ul>\n<li>Gather information by tricking employees</li>\n</ul>\n</li>\n<li>Rainbow tables\n<ul>\n<li>Collection of hash results</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"clippinglevel\">Clipping Level</h3>\n<ul>\n<li>Threshold</li>\n<li>Measurement of a certain amount of an activity</li>\n</ul>\n<h3 id=\"passwordchecker\">Password Checker</h3>\n<ul>\n<li>Checks the strength and compliance of existing passwords</li>\n</ul>\n<h3 id=\"passwordcracker\">Password Cracker</h3>\n<ul>\n<li>Attempts to break/decrypt a password</li>\n</ul>\n<h3 id=\"cognitivepassword\">Cognitive Password</h3>\n<ul>\n<li>Fact or Opinion Based</li>\n<li>Mother's maiden name</li>\n<li>First pet's name</li>\n</ul>\n<h2 id=\"onetimepassworddynamicpassword\">One-time Password (Dynamic Password)</h2>\n<p>Passwords that can only be used once</p>\n<h2 id=\"tokendevices\">Token Devices</h2>\n<ul>\n<li>Synchronous\n<ul>\n<li>Counter-synchronization</li>\n<li>Time-synchronization</li>\n</ul>\n</li>\n<li>Asynchronous\n<ul>\n<li>Challenge / Response</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"passphrase\">Passphrase</h2>\n<p>Sentence instead of a word</p>\n<blockquote>\n<p>&quot;I like the number 4!&quot;</p>\n</blockquote>\n<h2 id=\"memorycards\">Memory Cards</h2>\n<p>Hold information with no processing capabilities</p>\n<h2 id=\"smartcards\">Smartcards</h2>\n<ul>\n<li>Hold information and can process</li>\n<li>Contact</li>\n<li>Contactless</li>\n<li>Attacks\n<ul>\n<li>Fault generation</li>\n<li>Side-channel Attacks</li>\n<li>Micro-probing</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"kerberos\">Kerberos</h2>\n<p>See <a href=\"http://msdn.microsoft.com/en-us/library/ff649429.aspx\">MSDN Microsoft</a> and <a href=\"ftp://ftp.isi.edu/in-notes/rfc1510.txt\">Kerberos (RFC 1510)</a> and be aware it is susceptible to <a href=\"http://msdn.microsoft.com/en-us/library/ff647076.aspx\">time attacks</a>.</p>\n<h2 id=\"models\">Models</h2>\n<p>Discretionary Access Control (DAC): Permissions through inheritance</p>\n<ul>\n<li>User gains access through group membership Mandatory Access Control (MAC): Owner of data mandates who has access.</li>\n<li>Data must be classified (secret, top secret, SBU, etc).</li>\n<li>Classified by owner</li>\n<li>Sensitivity labels</li>\n<li>Role-based Access Control (RBAC)</li>\n<li>Backup administrator</li>\n<li>Task based</li>\n<li>Rule-based Access Control</li>\n<li>in gateway or border device like a router</li>\n</ul>\n<h2 id=\"constraineduserinterface\">Constrained user interface</h2>\n<ul>\n<li>Menu</li>\n<li>Shell</li>\n<li>Database views</li>\n</ul>\n<h1 id=\"securityriskmanagementglossary\">Security &amp; Risk Management Glossary</h1>\n<h2 id=\"keyterms\">Key Terms</h2>\n<ul>\n<li><strong>Availability</strong>: Reliable and timely access to data and resources is provided to authorized individuals.</li>\n<li><strong>Integrity</strong>: Accuracy and reliability of the information and systems are provided and any unauthorized modification is prevented.</li>\n<li><strong>Confidentiality</strong>: Necessary level of secrecy is enforced and unauthorized disclosure is prevented.</li>\n<li><strong>Shoulder surfing</strong>: Viewing information in an unauthorized manner by looking over the shoulder of someone else.</li>\n<li><strong>Social engineering</strong>: Gaining unauthorized access by tricking someone into divulging sensitive information.</li>\n<li><strong>Vulnerability</strong>: Weakness or a lack of a countermeasure.</li>\n<li><strong>Threat agent</strong>: Entity that can exploit a vulnerability.</li>\n<li><strong>Threat</strong>: The danger of a threat agent exploiting a vulnerability.</li>\n<li><strong>Risk</strong>: The probability of a threat agent exploiting a vulnerability and the associated impact.</li>\n<li><strong>Control</strong>: Safeguard that is put in place to reduce a risk, also called a countermeasure.</li>\n<li><strong>Exposure</strong>: Presence of a vulnerability, which exposes the organization to a threat.</li>\n<li><strong>Control types</strong>:<br>\nAdministrative, technical (logical), and physical</li>\n<li><strong>Control functions</strong>:\n<ul>\n<li><strong>Deterrent</strong>: Discourage a potential attacker</li>\n<li><strong>Preventive</strong>: Stop an incident from occurring</li>\n<li><strong>Corrective</strong>: Fix items after an incident has occurred</li>\n<li><strong>Recovery</strong>: Restore necessary components to return to normal operations</li>\n<li><strong>Detective</strong>: Identify an incident’s activities after it took place</li>\n<li><strong>Compensating</strong>: Alternative control that provides similar protection as the original control</li>\n</ul>\n</li>\n<li><strong>Defense-in-depth</strong>: Implementation of multiple controls so that successful penetration and compromise is more difficult to attain.</li>\n<li><strong>Security through obscurity</strong>: Relying upon the secrecy or complexity of an item as its security, instead of practicing solid security practices.</li>\n<li><strong>ISO/IEC 27000 series</strong>: Industry-recognized best practices for the development and management of an information security management system.</li>\n<li><strong>Zachman framework</strong>: Enterprise architecture framework used to define and understand a business environment developed by John Zachman.</li>\n<li><strong>TOGAF</strong>: Enterprise architecture framework used to define and understand a business environment developed by The Open Group.</li>\n<li><strong>SABSA</strong>: framework Risk-driven enterprise security architecture that maps to business initiatives, similar to the Zachman framework.</li>\n<li><strong>DoDAF</strong>: U.S. Department of Defense architecture framework that ensures interoperability of systems to meet military mission goals.</li>\n<li><strong>MODAF</strong>: Architecture framework used mainly in military support missions developed by the British Ministry of Defence.</li>\n<li><strong>CobiT</strong>: Set of control objectives used as a framework for IT governance developed by Information Systems Audit and Control Association (ISACA) and the IT Governance Institute (ITGI).</li>\n<li><strong>NIST SP 800-53</strong>: Set of controls that are used to secure U.S. federal systems developed by NIST.</li>\n<li><strong>COSO</strong>: Internal control model used for corporate governance to help prevent fraud developed by the Committee of Sponsoring Organizations (COSO) of the Treadway Commission.</li>\n<li><strong>ITIL</strong>: Best practices for information technology services management processes developed by the United Kingdom’s Office of Government Commerce.</li>\n<li><strong>Six Sigma</strong>: Business management strategy developed by Motorola with the goal of improving business processes.</li>\n<li><strong>Capability Maturity Model Integration (CMMI)</strong>: Process improvement model developed by Carnegie Mellon.</li>\n<li><strong>NIST SP 800-30</strong>: Risk Management Guide for Information Technology Systems A U.S. federal standard that is focused on IT risks.</li>\n<li><strong>Facilitated Risk Analysis Process (FRAP)</strong>: A focused, qualitative approach that carries out pre-screening to save time and money.</li>\n<li><strong>Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE)</strong>: Team-oriented approach that assesses organizational and IT risks through facilitated workshops.</li>\n<li><strong>AS/NZS 4360</strong>: Australia and New Zealand business risk management assessment approach.</li>\n<li><strong>ISO/IEC 27005</strong>: International standard for the implementation of a risk management program that integrates into an information security management system (ISMS).</li>\n<li><strong>Failure Modes and Effect Analysis (FMEA)</strong>: Approach that dissects a component into its basic functions to identify flaws and those flaw's effects.</li>\n<li><strong>Fault tree analysis</strong>: Approach to map specific flaws to root causes in complex systems.</li>\n<li><strong>CRAMM</strong>: Central Computing and Telecommunications Agency Risk Analysis and Management Method.</li>\n<li><strong>Quantitative risk analysis</strong>: Assigning monetary and numeric values to all the data elements of a risk assessment.</li>\n<li><strong>Qualitative risk analysis</strong>: Opinion-based method of analyzing risk with the use of scenarios and ratings.</li>\n<li><strong>Single loss expectancy (SLE)</strong>: One instance of an expected loss if a specific vulnerability is exploited and how it affects a single asset. Asset Value × Exposure Factor = SLE.</li>\n<li><strong>Annualized loss expectancy (ALE)</strong>: Annual expected loss if a specific vulnerability is exploited and how it affects a single asset. SLE × ARO = ALE.</li>\n<li><strong>Uncertainty analysis</strong>: Assigning confidence level values to data elements.</li>\n<li><strong>Delphi method</strong>: Data collection method that happens in an anonymous fashion.</li>\n<li><strong>Cost/benefit analysis</strong>: Calculating the value of a control. (ALE before implementing a control) – (ALE after implementing a control) – (annual cost of control) = value of control.</li>\n<li><strong>Functionality versus effectiveness of control</strong>: Functionality is what a control does, and its effectiveness is how well the control does it.</li>\n<li><strong>Total risk</strong>: Full risk amount before a control is put into place. Threats × vulnerabilities × assets = total risk.</li>\n<li><strong>Residual risk</strong>: Risk that remains after implementing a control. Threats × vulnerabilities × assets × (control gap) = residual risk.</li>\n<li><strong>Accepted ways for handling risk</strong>: Accept, transfer, mitigate, avoid.</li>\n<li><strong>Policy</strong>: High-level document that outlines senior management’s security directives.</li>\n<li><strong>Standard</strong>: Compulsory rules that support the security policies.</li>\n<li><strong>Guideline</strong>: Suggestions and best practices.</li>\n<li><strong>Procedures</strong>: Step-by-step implementation instructions.</li>\n<li><strong>Data owner</strong>: Individual responsible for the protection and classification of a specific data set.</li>\n<li><strong>Data custodian</strong>: Individual responsible for implementing and maintaining security controls to meet security requirements outlined by data owner.</li>\n<li><strong>Separation of duties</strong>: Preventive administrative control used to ensure one person cannot carry out a critical task alone.</li>\n<li><strong>Collusion</strong>: Two or more people working together to carry out fraudulent activities.</li>\n<li><strong>Rotation of duties</strong>: Detective administrative control used to uncover potential fraudulent activities.</li>\n<li><strong>Mandatory vacation</strong>: Detective administrative control used to uncover potential fraudulent activities by requiring a person to be away from the organization for a period of time.</li>\n<li><strong>Access controls</strong>: are security features that control how users and systems communicate and interact with other systems and resources.</li>\n<li><strong>Access</strong>: The flow of information between a subject and an object.</li>\n<li><strong>Subject</strong>: An active entity that requests access to an object or the data within an object.</li>\n<li><strong>Object</strong>: Can be a computer, database, file, computer program, directory, or field contained in a table within a database.</li>\n<li><strong>Race condition</strong>: When processes carry out their tasks on a shared resource in an incorrect order.</li>\n<li><strong>User provisioning</strong>: The creation, maintenance, and deactivation of user objects and attributes as they exist in one or more systems, directories, or applications, in response to business processes.</li>\n<li><strong>Federated identity</strong>: A portable identity, and its associated entitlements, that can be used across business boundaries.</li>\n<li><strong>Security Assertion Markup Language (SAML)</strong>: An XML standard that allows the exchange of authentication and authorization data to be shared between security domains.</li>\n<li><strong>Service Provisioning Markup Language (SPML)</strong>: Allows for the automation of user management (account creation, amendments, revocation) and access entitlement configuration related to electronically published services across multiple provisioning systems.</li>\n<li><strong>Simple Object Access Protocol (SOAP)</strong>: SOAP is a specification that outlines how information pertaining to web services is exchanged in a structured manner.</li>\n<li><strong>Type I error</strong>: When a biometric system rejects an authorized individual (false rejection rate).</li>\n<li><strong>Type II error</strong>: When the system accepts impostors who should be rejected(false acceptance rate).</li>\n<li><strong>Clipping Level</strong>: A threshold.</li>\n<li><strong>Cognitive passwords</strong>: Fact or opinion based information used to verify an individual’s identity.</li>\n<li><strong>Asynchronous token–generating method</strong>: Employs a challenge/response scheme to authenticate the user.</li>\n<li><strong>Synchronous token device</strong>: Synchronizes with the authentication service by using time or a counter as the core piece of the authentication process. If the<br>\nsynchronization is time-based, the token device and the authentication service must hold the same time within their internal clocks.</li>\n<li><strong>Memory card</strong>: Holds information but cannot process information.</li>\n<li><strong>Smart card</strong>: Holds information and has the necessary hardware and software to actually process that information.</li>\n<li><strong>Side-channel attacks</strong>: Non-intrusive and are used to uncover sensitive information about how a component works, without trying to compromise any type of flaw or<br>\nweakness.</li>\n<li><strong>Security domain</strong>: Resources within this logical structure (domain) are working under the same security policy and managed by the same group.</li>\n<li><strong>Access Control Model</strong>: An access control model is a framework that dictates how subjects access objects.</li>\n<li><strong>Access Control Matrix</strong>: A table of subjects and objects indicating what actions individual subjects can take upon individual objects.</li>\n<li><strong>Capability Table</strong>: A capability table specifies the access rights a certain subject possesses pertaining to specific objects. A capability table is different from an ACL because the subject is bound to the capability table, whereas the object is bound to the ACL.</li>\n<li><strong>Content-based access</strong>: Bases access decisions on the sensitivity of the data, not solely on subject identity.</li>\n<li><strong>Context-based access</strong>: Bases access decisions on the state of the situation, not solely on identity or content sensitivity.</li>\n<li><strong>Restricted interface</strong>: Limits the user’s environment within the system, thus limiting access to objects.</li>\n<li><strong>Rule-based access</strong>: Restricts subject's access attempts by predefined rules.</li>\n<li><strong>Remote Authentication Dial-In User Service (RADIUS)</strong>: A network protocol that provides client/server authentication and authorization, and audits remote users.</li>\n<li><strong>Central processing unit (CPU)</strong>: A silicon component made up of integrated chips with millions of transistors that carry out the execution of instructions within a computer.</li>\n<li><strong>Arithmetic logic unit (ALU)</strong>: Component of the CPU that carries out logic and mathematical functions as they are laid out in the programming code being processed by the CPU.</li>\n<li><strong>Register</strong>: Small, temporary memory storage units integrated and used by the CPU during its processing functions.</li>\n<li><strong>Control unit</strong>: Part of the CPU that oversees the collection of instructions and data from memory and how they are passed to the processing components of the CPU.</li>\n<li><strong>General registers</strong>: Temporary memory location the CPU uses during its processes of executing instructions. The ALU’s “scratch pad” it uses while carrying out logic and math functions.</li>\n<li><strong>Special registers</strong>: Temporary memory location that holds critical processing parameters. They hold values as in the program counter, stack pointer, and program status word.</li>\n<li><strong>Program counter</strong>: Holds the memory address for the following instructions the CPU needs to act upon.</li>\n<li><strong>Stack Memory</strong>: Segment used by processes to communicate instructions and data to each other.</li>\n<li><strong>Program status word (PSW)</strong>: Condition variable that indicates to the CPU what mode (kernel or user) instructions need to be carried out in.</li>\n<li><strong>User mode (problem state)</strong>: Protection mode that a CPU works within when carrying out less trusted process instructions.</li>\n<li><strong>Kernel mode (supervisory state, privilege mode)</strong>: Mode that a CPU works within when carrying out more trusted process instructions. The process has access to more computer resources when working in kernel versus user mode.</li>\n<li><strong>Address bus</strong>: Physical connections between processing components and memory segments used to communicate the physical memory addresses being used during processing procedures.</li>\n<li><strong>Data bus</strong>: Physical connections between processing components and memory segments used to transmit data being used during processing<br>\nprocedures.</li>\n<li><strong>Symmetric mode multiprocessing</strong>: When a computer has two or more CPUs and each CPU is being used in a load-balancing method.</li>\n<li><strong>Asymmetric mode multiprocessing</strong>: When a computer has two or more CPUs and one CPU is dedicated to a specific program while the other CPUs carry out general processing procedures.</li>\n<li><strong>Process</strong>: Program loaded in memory within an operating system.</li>\n<li><strong>Multiprogramming</strong>: Interleaved execution of more than one program (process) or task by a single operating system.</li>\n<li><strong>Multitasking</strong>: Simultaneous execution of more than one program (process) or task by a single operating system.</li>\n<li><strong>Cooperative multitasking</strong>: Multitasking scheduling scheme used by older operating systems to allow for computer resource time slicing.</li>\n<li><strong>Preemptive multitasking</strong>: Multitasking scheduling scheme used by operating systems to allow for computer resource time slicing. Used in newer, more stable operating systems.</li>\n<li><strong>Process states (ready, running, blocked)</strong>: Processes can be in various activity levels. Ready = waiting for input. Running = instructions being executed by CPU. Blocked = process is “suspended.”</li>\n<li><strong>Interrupts</strong>: Values assigned to computer components (hardware and software) to allow for efficient computer resource time slicing.</li>\n<li><strong>Maskable interrupt</strong>: Interrupt value assigned to a non-critical operating system activity.</li>\n<li><strong>Non-maskable interrupt</strong>: Interrupt value assigned to a critical operating system activity.</li>\n<li><strong>Thread</strong>: Instruction set generated by a process when it has a specific activity that needs to be carried out by an operating system. When the activity is finished, the thread is destroyed.</li>\n<li><strong>Multi-threading</strong>: Applications that can carry out multiple activities simultaneously by generating different instruction sets (threads).</li>\n<li><strong>Software deadlock</strong>: Two processes cannot complete their activities because they are both waiting for system resources to be released.</li>\n<li><strong>Process isolation</strong>: Protection mechanism provided by operating systems that can be implemented as encapsulation, time multiplexing of shared resources, naming distinctions, and virtual memory mapping.</li>\n<li><strong>Dynamic link libraries (DLLs)</strong>: A set of subroutines that are shared by different applications and operating system processes.</li>\n<li><strong>Base registers</strong>: Beginning of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.</li>\n<li><strong>Limit registers</strong>: Ending of address space assigned to a process. Used to ensure a process does not make a request outside its assigned memory boundaries.</li>\n<li><strong>RAM</strong>: Memory sticks that are plugged into a computer’s motherboard and work as volatile memory space for an operating system.</li>\n<li><strong>ROM</strong>: Non-volatile memory that is used on motherboards for BIOS functionality and various device controllers to allow for operating system-to-device communication. Sometimes used for off-loading graphic rendering or cryptographic functionality.</li>\n<li><strong>Hardware segmentation</strong>: Physically mapping software to individual memory segments.</li>\n<li><strong>Cache memory</strong>: Fast memory type that is used by a CPU to increase read and write operations.</li>\n<li><strong>Absolute addresses</strong>: Hardware addresses used by the CPU.</li>\n<li><strong>Logical addresses</strong>: Indirect addressing used by processes within an operating system. The memory manager carries out logical-to-absolute address mapping.</li>\n<li><strong>Stack Memory</strong>: Construct that is made up of individually addressable buffers. Process-to-process communication takes place through the use of stacks.</li>\n<li><strong>Buffer overflow</strong>: Too much data is put into the buffers that make up a stack. Common attack vector used by hackers to run malicious code on a target system.</li>\n<li><strong>Address space layout randomization (ASLR)</strong>: Memory protection mechanism used by some operating systems. The addresses used by components of a process are randomized so that it is harder for an attacker to exploit specific memory vulnerabilities.</li>\n<li><strong>Data execution prevention (DEP)</strong>: Memory protection mechanism used by some operating systems. Memory segments may be marked as non-executable so that they cannot be misused by malicious software.</li>\n<li><strong>Garbage collector</strong>: Tool that marks unused memory segments as usable to ensure that an operating system does not run out of memory.</li>\n<li><strong>Virtual memory</strong>: Combination of main memory (RAM) and secondary memory within an operating system.</li>\n<li><strong>Time multiplexing</strong>: Technology that allows processes to use the same resources.</li>\n<li><strong>Interrupt</strong>: Software or hardware signal that indicates that system resources (i.e., CPU) are needed for instruction processing.</li>\n<li><strong>Instruction set</strong>: Set of operations and commands that can be implemented by a particular processor (CPU).</li>\n<li><strong>Microarchitecture</strong>: Specific design of a microprocessor, which includes physical components (registers, logic gates, ALU, cache, etc.) that support a specific instruction set.</li>\n<li><strong>Application programming interface (API)</strong>: Software interface that enables process-to-process interaction. Common way to provide access to standard routines to a set of software programs.</li>\n<li><strong>Monolithic operating system architecture</strong>: All of the code of the operating system working in kernel mode in an ad-hoc and non-modularized manner.</li>\n<li><strong>Layered operating system architecture</strong>: Architecture that separates system functionality into hierarchical layers.</li>\n<li><strong>Data hiding</strong>: Use of segregation in design decisions to protect software components from negatively interacting with each other. Commonly enforced through strict interfaces.</li>\n<li><strong>Microkernel architecture</strong>: Reduced amount of code running in kernel mode carrying out critical operating system functionality. Only the absolutely necessary code runs in kernel mode, and the remaining operating system code runs in user mode.</li>\n<li><strong>Hybrid microkernel architecture</strong>: Combination of monolithic and microkernel architectures. The microkernel carries out critical operating system  functionality, and the remaining functionality is carried out in a client\\server model within kernel mode.</li>\n<li><strong>Mode transition</strong>: When the CPU has to change from processing code in user mode to kernel mode.</li>\n<li><strong>Virtualization</strong>: Creation of a simulated environment (hardware platform, operating system, storage, etc.) that allows for central control and scalability.</li>\n<li><strong>Hypervisor</strong>: Central program used to manage virtual machines (guests) within a simulated environment (host).</li>\n<li><strong>Security policy</strong>: Strategic tool used to dictate how sensitive information and resources are to be managed and protected.</li>\n<li><strong>Trusted computing base</strong>: A collection of all the hardware, software, and firmware components within a system that provide security and enforce the system’s security policy.</li>\n<li><strong>Trusted path</strong>: Trustworthy software channel that is used for communication between two processes that cannot be circumvented.</li>\n<li><strong>Security perimeter</strong>: Mechanism used to delineate between the components within and outside of the trusted computing base.</li>\n<li><strong>Reference monitor</strong>: Concept that defines a set of design requirements of a reference validation mechanism (security kernel), which enforces an access control policy over subject's (processes, users) ability to perform operations (read, write, execute) on objects (files, resources) on a system.</li>\n<li><strong>Security kernel</strong>: Hardware, software, and firmware components that fall within the TCB and implement and enforce the reference monitor concept.</li>\n<li><strong>Multilevel security policies</strong>: Outlines how a system can simultaneously process information at different classifications for users with different<br>\nclearance levels.</li>\n<li><strong>Protection profile</strong>: Description of a needed security solution.</li>\n<li><strong>Target of evaluation (TOE)</strong>: Product proposed to provide a needed security solution.</li>\n<li><strong>Security target</strong>: Vendor’s written explanation of the security functionality and assurance mechanisms that meet the needed security solution.</li>\n<li><strong>Security functional requirements</strong>: Individual security functions which must be provided by a product.</li>\n<li><strong>Security assurance requirements</strong>: Measures taken during development and evaluation of the product to assure compliance with the claimed security functionality.</li>\n<li><strong>Packages—EALs</strong>: Functional and assurance requirements are bundled into packages for reuse. This component describes what must be met to achieve specific EAL ratings.</li>\n<li><strong>Assurance evaluation criteria</strong>: Check-list and process of examining the security-relevant parts of a system (TCB, reference monitor, security kernel) and assigning the system an assurance rating.</li>\n<li><strong>Trusted Computer System Evaluation Criteria (TCSEC) (aka Orange Book)</strong>: U.S. DoD standard used to assess the effectiveness of the security controls built into a system. Replaced by the Common Criteria.</li>\n<li><strong>Information Technology Security Evaluation Criteria (ITSEC)</strong>: European standard used to assess the effectiveness of the security controls built into a system.</li>\n<li><strong>Common Criteria</strong>: International standard used to assess the effectiveness of the security controls built into a system from functional and assurance perspectives.</li>\n<li><strong>Certification</strong>: Technical evaluation of the security components and their compliance to a predefined security policy for the purpose of accreditation.</li>\n<li><strong>Accreditation</strong>: Formal acceptance of the adequacy of a system’s overall security by management.</li>\n<li><strong>Open system</strong>: Designs are built upon accepted standards to allow for interoperability.</li>\n<li><strong>Closed system</strong>: Designs are built upon proprietary procedures, which inhibit interoperability capabilities.</li>\n<li><strong>Maintenance hooks</strong>: Code within software that provides a back door entry capability.</li>\n<li><strong>Time-of-check/time-of-use (TOC/TOU) attack</strong>: Attacker manipulates the “condition check” step and the “use” step within software to allow for unauthorized activity.</li>\n<li><strong>Race condition</strong>: Two or more processes attempt to carry out their activity on one resource at the same time. Unexpected behaviour can result if the sequence of execution does not take place in the proper order.</li>\n<li><strong>Open Systems Interconnection (OSI) model</strong>: International standardization of system-based network communication through a modular seven-layer architecture.</li>\n<li><strong>TCP/IP model</strong>: Standardization of device-based network communication through a modular four-layer architecture. Specific to the IP suite, created in 1970 by an agency of the U.S. Department of Defense (DoD).</li>\n<li><strong>Transmission Control Protocol (TCP)</strong>: Core protocol of the TCP/IP suite, which provides connection-oriented, end-to-end, reliable network connectivity.</li>\n<li><strong>Internet Protocol (IP)</strong>: Core protocol of the TCP/IP suite. Provides packet construction, addressing, and routing functionality.</li>\n<li><strong>User Datagram Protocol (UDP)</strong>: Connectionless, unreliable transport layer protocol, which is considered a “best effort” protocol.</li>\n<li><strong>Ports</strong>: Software construct that allows for application- <strong>or service-specific communication between systems on a network. Ports are broken down into categories</strong>: well known (0–1023), registered (1024–49151), and dynamic (49152–65535).</li>\n<li><strong>SYN flood</strong>: DoS attack where an attacker sends a succession of SYN packets with the goal of overwhelming the victim system so that it is unresponsive to legitimate traffic.</li>\n<li><strong>Session hijacking</strong>: Attack method that allows an attacker to overtake and control a communication session between two systems.</li>\n<li><strong>IPv6</strong>: IP version 6 is the successor to IP version 4 and provides 128-bit addressing, integrated IPSec security protocol, simplified header formats, and some automated configuration.</li>\n<li><strong>Subnet</strong>: Logical subdivision of a network that improves network administration and helps reduce network traffic congestion. Process of segmenting a network into smaller networks through the use of an addressing scheme made up of network and host portions.</li>\n<li><strong>Classless Interdomain Routing (CIDR)</strong>: Variable-length subnet masking, which allows a network to be divided into different-sized subnets. The goal is to increase the efficiency of the use of IP addresses since classful addressing schemes commonly end up in unused addresses.</li>\n<li><strong>6to4</strong>: Transition mechanism for migrating from IPv4 to IPv6. It allows systems to use IPv6 to communicate if their traffic has to transverse an IPv4 network.</li>\n<li><strong>Teredo</strong>: Transition mechanism for migrating from IPv4 to IPv6. It allows systems to use IPv6 to communicate if their traffic has to transverse an IPv4 network, but also performs its function behind NAT devices.</li>\n<li><strong>Intra-Site Automatic Tunnel Addressing Protocol (ISATAP)</strong>: An IPv6 transition mechanism meant to transmit IPv6 packets between dual-stack nodes on top of an IPv4 network.</li>\n<li><strong>IEEE 802.1AE (MACSec)</strong>: Standard that specifies a set of protocols to meet the security requirements for protecting data traversing Ethernet LANs.</li>\n<li><strong>IEEE 802.1AR</strong>: Standard that specifies unique per-device identifiers (DevID) and the management and cryptographic binding of a device (router, switch, access point) to its identifiers.</li>\n<li><strong>Digital signals</strong>: Binary digits are represented and transmitted as discrete electrical pulses.</li>\n<li><strong>Analog signals</strong>: Continuously varying electromagnetic wave that represents and transmits data.</li>\n<li><strong>Asynchronous communication</strong>: Transmission sequencing technology that uses start and stop bits or similar encoding mechanism. Used in environments that transmit a variable amount of data in a periodic fashion.</li>\n<li><strong>Synchronous communication</strong>: Transmission sequencing technology that uses a clocking pulse or timing scheme for data transfer synchronization.</li>\n<li><strong>Baseband transmission</strong>: Uses the full bandwidth for only one communication channel and has a low data transfer rate compared to broadband.</li>\n<li><strong>Broadband transmission</strong>: Divides the bandwidth of a communication channel into many channels, enabling different types of data to be transmitted at one time.</li>\n<li><strong>Unshielded twisted pair (UTP)</strong>: Cabling in which copper wires are twisted together for the purposes of canceling out EMI from external sources. UTP cables are found in many Ethernet networks and telephone systems.</li>\n<li><strong>Shielded twisted pair (STP)</strong>: Twisted-pair cables are often shielded in an attempt to prevent RFI and EMI. This shielding can be applied to individual pairs or to the collection of pairs.</li>\n<li><strong>Attenuation</strong>: Gradual loss in intensity of any kind of flux through a medium. As an electrical signal travels down a cable, the signal can degrade and distort or corrupt the data it is carrying.</li>\n<li><strong>Crosstalk</strong>: A signal on one channel of a transmission creates an undesired effect in another channel by interacting with it. The signal from one cable “spills over” into another cable.</li>\n<li><strong>Plenum cables</strong>: Cable is jacketed with a fire-retardant plastic cover that does not release toxic chemicals when burned.</li>\n<li><strong>Ring topology</strong>: Each system connects to two other systems, forming a single, unidirectional network pathway for signals, thus forming a ring.</li>\n<li><strong>Bus topology</strong>: Systems are connected to a single transmission channel (i.e., network cable), forming a linear construct.</li>\n<li><strong>Star topology</strong>: Network consists of one central device, which acts as a conduit to transmit messages. The central device, to which all other nodes are connected, provides a common connection point for all nodes.</li>\n<li><strong>Mesh topology</strong>: Network where each system must not only capture and disseminate its own data, but also serve as a relay for other systems; that is, it must collaborate to propagate the data in the network.</li>\n<li><strong>Ethernet</strong>: Common LAN media access technology standardized by IEEE 802.3. Uses 48-bit MAC addressing, works in contention-based networks, and has extended outside of just LAN environments.</li>\n<li><strong>Token ring</strong>: LAN medium access technology that controls network communication traffic through the use of token frames. This technology has been mostly replaced by Ethernet.</li>\n<li><strong>Fiber Distributed Data Interface (FDDI)</strong>: Ring-based token network protocol that was derived from the IEEE 802.4 token bus timed token protocol.<br>\nIt can work in LAN or MAN environments and provides fault tolerance through dual-ring architecture.</li>\n<li><strong>Carrier sense multiple access with collision detection (CSMA/CD)</strong>: A media access control method that uses a carrier sensing scheme. When a transmitting system detects another signal while transmitting a frame, it stops transmitting that frame, transmits a jam signal, and then waits for a random time interval before trying to resend the frame. This reduces collisions on a network.</li>\n<li><strong>Carrier sense multiple access with collision avoidance (CSMA/CA)</strong>: A media access control method that uses a carrier sensing scheme. A system wishing to transmit data has to first listen to the channel for a predetermined amount of time to determine whether or not another system is transmitting on the channel. If the channel is sensed as “idle,” then the system is permitted to begin the transmission process. If the channel is sensed as “busy,” the system defers its transmission for a random period of time.</li>\n<li><strong>Internet Group Management Protocol (IGMP)</strong>: Used by systems and adjacent routers on IP networks to establish and maintain multicast group memberships.</li>\n<li><strong>Media access control (MAC)</strong>: Data communication protocol sub-layer of the data link layer specified in the OSI model. It provides hardware addressing and channel access control mechanisms that make it possible for several nodes to communicate within a multiple-access network that incorporates a shared medium.</li>\n<li><strong>Address Resolution Protocol (ARP)</strong>: A networking protocol used for resolution of network layer IP addresses into link layer MAC addresses.</li>\n<li><strong>Dynamic Host Configuration Protocol (DHCP)</strong>: A network configuration service for hosts on IP networks. It provides IP addressing, DNS server, subnet mask, and other important network configuration data to each host through automation.</li>\n<li><strong>Internet Control Message Protocol (ICMP)</strong>: A core protocol of the IP suite used to send status and error messages.</li>\n<li><strong>Ping of Death</strong>: A DoS attack type on a computer that involves sending malformed or oversized ICMP packets to a target.</li>\n<li><strong>Smurf attack</strong>: A DDoS attack type on a computer that floods the target system with spoofed broadcast ICMP packets.</li>\n<li><strong>Fraggle attack</strong>: A DDoS attack type on a computer that floods the target system with a large amount of UDP echo traffic to IP broadcast addresses.</li>\n<li><strong>Simple Network Management Protocol (SNMP)</strong>: A protocol within the IP suite that is used for network device management activities through the use of a structure that uses managers, agents, and Management Information Bases.</li>\n<li><strong>Domain Name System (DNS)</strong>: A hierarchical distributed naming system for computers, services, or any resource connected to an IP based network. It associates various pieces of information with domain names assigned to each of the participating entities.</li>\n<li><strong>DNS zone transfer</strong>: The process of replicating the databases containing the DNS data across a set of DNS servers.</li>\n<li><strong>DNSSEC</strong>: A set of extensions to DNS that provide to DNS clients (resolvers) origin authentication of DNS data to reduce the threat of DNS poisoning, spoofing, and similar attack types.</li>\n<li><strong>Simple Mail Transfer Protocol (SMTP)</strong>: An Internet standard protocol for electronic mail (e-mail) transmission across IP-based networks.</li>\n<li><strong>Post Office Protocol (POP)</strong>: An Internet standard protocol used by e-mail clients to retrieve e-mail from a remote server and supports simple download-and-delete requirements for access to remote mailboxes.</li>\n<li><strong>Internet Message Access Protocol (IMAP)</strong>: An Internet standard protocol used by e-mail clients to retrieve e-mail from a remote server. E-mail clients using IMAP generally leave messages on the server until the user explicitly deletes them.</li>\n<li><strong>Open mail relay</strong>: An SMTP server configured in such a way that it allows anyone on the Internet to send e-mail through it, not just mail destined to or originating from known users.</li>\n<li><strong>E-mail spoofing</strong>: Activity in which the sender address and other arts of the e-mail header are altered to appear as though the e-mail originated from a different source. Since SMTP does not provide any<br>\nauthentication, it is easy to impersonate and forge e-mails.</li>\n<li><strong>Sender Policy Framework (SPF)</strong>: An e-mail validation system designed to prevent e-mail spam by detecting e-mail spoofing, a common vulnerability, by verifying sender IP addresses.</li>\n<li><strong>Phishing</strong>: A way of attempting to obtain data such as usernames, passwords, credit card information, and other sensitive data by masquerading as an authenticated entity in an electronic communication. Spear phishing targets individuals, and whaling targets people with high authorization (C-Level Executives).</li>\n<li><strong>Network address translation (NAT)</strong>: The process of modifying IP address information in packet headers while in transit across a traffic routing device, with the goal of reducing the demand for public IP addresses.</li>\n<li><strong>Distance-vector routing protocol</strong>: A routing protocol that calculates paths based on the distance (or number of hops) and a vector (a direction).</li>\n<li><strong>Link-state routing protocol</strong>: A routing protocol used in packet-switching networks where each router constructs a map of the connectivity within the network and calculates the best logical paths, which form its routing table.</li>\n<li><strong>Border Gateway Protocol (BGP)</strong>: The protocol that carries out core routing decisions on the Internet. It maintains a table of IP networks, or “prefixes,” which designate network reachability among autonomous systems.</li>\n<li><strong>Wormhole attack</strong>: This takes place when an attacker captures packets at one location in the network and tunnels them to another location in the network for a second attacker to use against a target system.</li>\n<li><strong>Spanning Tree Protocol (STP)</strong>: A network protocol that ensures a loop-free topology for any bridged Ethernet LAN and allows redundant links to be available in case connection links go down.</li>\n<li><strong>Source routing</strong>: Allows a sender of a packet to specify the route the packet takes through the network versus routers determining the path.</li>\n<li><strong>Multi-protocol Label Switching (MPLS)</strong>: A networking technology that directs data from one network node to the next based on short path labels rather than long network addresses, avoiding complex lookups in a routing table.</li>\n<li><strong>Virtual local area network (VLAN)</strong>: A group of hosts that communicate as if they were attached to the same broadcast domain, regardless of their physical location. VLAN membership can be configured through software instead of physically relocating devices or connections, which allows for easier centralized management.</li>\n<li><strong>VLAN hopping</strong>: An exploit that allows an attacker on a VLAN to gain access to traffic on other VLANs that would normally not be accessible.</li>\n<li><strong>Private Branch Exchange (PBX)</strong>: A telephone exchange that serves a particular business, makes connections among the internal telephones, and connects them to the public-switched telephone network (PSTN) via trunk lines.</li>\n<li><strong>Bastion host</strong>: A highly exposed device that will most likely be targeted for attacks, and thus should be properly locked down.</li>\n<li><strong>Dual-homed firewall</strong>: This device has two interfaces and sits between an untrusted network and trusted network to provide secure access.</li>\n<li><strong>Screened host</strong>: A firewall that communicates directly with a perimeter router and the internal network. The router carries out filtering activities on the traffic before it reaches the firewall.</li>\n<li><strong>Screened subnet architecture</strong>: When two filtering devices are used to create a DMZ. The external device screens the traffic entering the DMZ network, and the internal filtering device screens the traffic before it enters the internal network.</li>\n<li><strong>Proxy server</strong>: A system that acts as an intermediary for requests from clients seeking resources from other sources. A client connects to the proxy server, requesting some service, and the proxy server evaluates the request according to its filtering rules and makes the connection on behalf of the client. Proxies can be open or carry out forwarding or reverse forwarding capabilities.</li>\n<li><strong>Honeypots</strong>: Systems that entice with the goal of protecting critical production systems. If two or more honeypots are used together, this is considered a honeynet.</li>\n<li><strong>Network convergence</strong>: The combining of server, storage, and network capabilities into a single framework, which decreases the costs and complexity of data centers. Converged infrastructures provide the ability to pool resources, automate resource provisioning, and increase and decrease processing capacity quickly to meet the needs of dynamic computing workloads.</li>\n<li><strong>Cloud computing</strong>: The delivery of computer processing capabilities as a service rather than as a product, whereby shared resources, software, and information are provided to end users as a utility. Offerings are usually bundled as an infrastructure, platform, or software.</li>\n<li><strong>Metropolitan area network (MAN)</strong>: A network that usually spans a city or a large campus, interconnects a number of LANs using a high capacity backbone technology, and provides up-link services to WANs or the Internet.</li>\n<li><strong>Synchronous Optical Networking (SONET) and Synchronous Digital Hierarchy (SDH)</strong>: Standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber and allow for simultaneous transportation of many different circuits of differing origin within a single framing protocol.</li>\n<li><strong>Metro Ethernet</strong>: A data link technology that is used as a metropolitan area network to connect customer networks to larger service networks or the Internet.</li>\n<li><strong>Wide area network (WAN)</strong>: A telecommunication network that covers a broad area and allows a business to effectively carry out its daily function, regardless of location.</li>\n<li><strong>Multiplexing</strong>: A method of combining multiple channels of data over a single transmission line.</li>\n<li><strong>T-carriers</strong>: Dedicated lines that can carry voice and data information over trunk lines.</li>\n<li><strong>Time-division multiplexing (TDM)</strong>: A type of multiplexing in which two or more bit streams or signals are transferred apparently simultaneously as sub-channels in one communication channel, but are physically taking turns on the single channel.</li>\n<li><strong>Wave-division multiplexing (WDM)</strong>: Multiplying the available capacity of optical fibers through use of parallel channels, with each channel on a dedicated wavelength of light. The bandwidth of an optical fiber can be divided into as many as 160 channels.</li>\n<li><strong>Frequency-division multiplexing (FDM)</strong>: Dividing available bandwidth into a series of non-overlapping frequency sub-bands that are then assigned to each communicating source and user pair. FDM is inherently an analog technology.</li>\n<li><strong>Statistical time-division multiplexing (STDM)</strong>: Transmitting several types of data simultaneously across a single transmission line. STDM technologies analyze statistics related to the typical workload of each input device and make real-time decisions on how much time each device should be allocated for data transmission.</li>\n<li><strong>Channel Service Unit (CSU)</strong>: A line bridging device for use with T-carriers, and that is required by PSTN providers at digital interfaces that terminate in a Data Service Unit (DSU) on the customer side. The DSU is a piece of telecommunications circuit terminating equipment that transforms digital data between telephone company lines and local equipment.</li>\n<li><strong>Public-switched telephone network (PSTN)</strong>: The public circuit-switched telephone network, which is made up of telephone lines, fiber-optic cables, cellular networks, communications satellites, and undersea telephone cables and allows all phone-to-phone communication. It was a fixed-line analog telephone system, but is now almost entirely digital and includes mobile as well as fixed telephones.</li>\n<li><strong>Voice over IP (VoIP)</strong>: The set of protocols, technologies, methodologies, and transmission techniques involved in the delivery of voice data and<br>\nmultimedia sessions over IP-based networks.</li>\n<li><strong>Session Initiation Protocol (SIP)</strong>: The signaling protocol widely used for controlling communication, as in voice and video calls over IPbased<br>\nnetworks.</li>\n<li><strong>Vishing (voice and phishing)</strong>: Social engineering activity over the telephone system, most often using features facilitated by VoIP, to gain unauthorized access to sensitive data.</li>\n<li><strong>H.323</strong>: A standard that addresses call signaling and control, multimedia transport and control, and bandwidth control for point-to-point and multipoint conferences.</li>\n<li><strong>Real-time Transport Protocol (RTP)</strong>: Used to transmit audio and video over IP-based networks. It is used in conjunction with the RTCP. RTP transmits the media data, and RTCP is used to monitor transmission statistics and QoS, and aids synchronization of multiple data streams.</li>\n<li><strong>War dialing</strong>: When a specialized program is used to automatically scan a list of telephone numbers to search for computers for the purposes of exploitation and hacking.</li>\n<li><strong>Integrated Services Digital Network (ISDN)</strong>: A circuit-switched telephone network system technology designed to allow digital transmission of voice and data over ordinary telephone copper wires.</li>\n<li><strong>Digital Subscriber Line (DSL)</strong>: A set of technologies that provide Internet access by transmitting digital data over the wires of a local telephone network. DSL is used to digitize the “last mile” and provide fast Internet connectivity.</li>\n<li><strong>Cable modem</strong>: A device that provides bidirectional data communication via radio frequency channels on cable TV infrastructures. Cable modems are primarily used to deliver broadband Internet access to homes.</li>\n<li><strong>Algorithm</strong>: Set of mathematical and logic rules used in cryptographic functions.</li>\n<li><strong>Cipher</strong>: Another name for algorithm.</li>\n<li><strong>Cryptography</strong>: Science of secret writing that enables an entity to store and transmit data in a form that is available only to the intended individuals.</li>\n<li><strong>Cryptosystem</strong>: Hardware or software implementation of cryptography that contains all the necessary software, protocols, algorithms, and keys.</li>\n<li><strong>Cryptanalysis</strong>: Practice of uncovering flaws within cryptosystems.</li>\n<li><strong>Cryptology</strong>: The study of both cryptography and cryptanalysis.</li>\n<li><strong>Encipher</strong>: Act of transforming data into an unreadable format.</li>\n<li><strong>Decipher</strong>: Act of transforming data into a readable format.</li>\n<li><strong>Key</strong>: Sequence of bits that are used as instructions that govern the acts of cryptographic functions within an algorithm.</li>\n<li><strong>Key clustering</strong>: Instance when two different keys generate the same ciphertext from the same plaintext.</li>\n<li><strong>Keyspace</strong>: A range of possible values used to construct keys.</li>\n<li><strong>Plaintext</strong>: Data in readable format, also referred to as cleartext.</li>\n<li><strong>Substitution Cipher</strong>: Encryption method that uses an algorithm that changes out (substitutes) one value for another value.</li>\n<li><strong>Scytale Cipher</strong>: Ancient encryption tool that used a type of paper and rod used by Greek military factions.</li>\n<li><strong>Kerckhoffs’ Principle</strong>: Concept that an algorithm should be known and only the keys should be kept secret.</li>\n<li><strong>One-time pad</strong>: Encryption method created by Gilbert Vernam that is considered impossible to crack if carried out properly.</li>\n<li><strong>Random Number generator</strong>: Algorithm used to create values that are used in cryptographic functions to add randomness.</li>\n<li><strong>Running Key Cipher</strong>: Substitution cipher that creates keystream values, commonly from agreed-upon text passages, to be used for encryption purposes.</li>\n<li><strong>Concealment Cipher</strong>: Encryption method that hides a secret message within an open message.</li>\n<li><strong>Steganography</strong>: Method of hiding data in another media type.</li>\n<li><strong>Digital Rights Management (DRM)</strong>: Access control technologies commonly used to protect copyright material.</li>\n<li><strong>Transposition</strong>: Encryption method that shifts (permutation) values.</li>\n<li><strong>Caesar Cipher</strong>: Simple substitution algorithm created by Julius Caesar that shifts alphabetic values three positions during its encryption and decryption processes</li>\n<li><strong>Frequency analysis</strong>: Cryptanalysis process used to identify weaknesses within cryptosystems by locating patterns in resulting ciphertext.</li>\n<li><strong>Key Derivation Functions (KDFs)</strong>: Generation of secret keys (subkeys) from an initial value (master key).</li>\n<li><strong>Symmetric algorithm</strong>: Encryption method where the sender and receiver use an instance of the same key for encryption and decryption purposes.</li>\n<li><strong>Out-of-band method</strong>: Sending data through an alternate communication channel.</li>\n<li><strong>Asymmetric algorithm</strong>: Encryption method that uses two different key types, public and private. Also called public key cryptography.</li>\n<li><strong>Public key</strong>: Value used in public key cryptography that is used for encryption and signature validation that can be known by all parties.</li>\n<li><strong>Private key</strong>: Value used in public key cryptography that is used for decryption and signature creation and known to only key owner.</li>\n<li><strong>Public key cryptography</strong>: Asymmetric cryptography, which uses public and private key values for cryptographic functions.</li>\n<li><strong>Block cipher</strong>: Symmetric algorithm type that encrypts chunks (blocks) of data at a time.</li>\n<li><strong>Diffusion</strong>: Transposition processes used in encryption functions to increase randomness.</li>\n<li><strong>Confusion</strong>: Substitution processes used in encryption functions to increase randomness.</li>\n<li><strong>Avalanche effect</strong>: Algorithm design requirement so that slight changes to the input result in drastic changes to the output.</li>\n<li><strong>Stream cipher</strong>: Algorithm type that generates a keystream (random values), which is XORd with plaintext for encryption purposes.</li>\n<li><strong>Keystream generator</strong>: Component of a stream algorithm that creates random values for encryption purposes.</li>\n<li><strong>Initialization vectors (IVs)</strong>: Values that are used with algorithms to increase randomness for cryptographic functions.</li>\n<li><strong>Hybrid cryptography</strong>: Combined use of symmetric and asymmetric algorithms where the symmetric key encrypts data and an asymmetric key encrypts the symmetric key.</li>\n<li><strong>Session keys</strong>: Symmetric keys that have a short lifespan, thus providing more protection than static keys with longer lifespans.</li>\n<li><strong>Rijndael</strong>: Block symmetric cipher that was chosen to fulfil the Advanced Encryption Standard. It uses a 128-bit block size and various key lengths (128, 192, 256).</li>\n<li><strong>Triple DES (3-DES)</strong>: Symmetric cipher that applies DES three times to each block of data during the encryption process.</li>\n<li><strong>International Data Encryption Algorithm (IDEA)</strong>: Block symmetric cipher that uses a 128-bit key and 64-bit block size.</li>\n<li><strong>Blowfish</strong>: Block symmetric cipher that uses 64-bit block sizes and variable-length keys.</li>\n<li><strong>RC4</strong>: Stream symmetric cipher that was created by Ron Rivest of RSA. Used in SSL and WEP.</li>\n<li><strong>RC5</strong>: Block symmetric cipher that uses variable block sizes (32, 64, 128) and variable-length key sizes (0–2040).</li>\n<li><strong>RC6</strong>: Block symmetric cipher that uses a 128-bit block size and variable length key sizes (128, 192, 256). Built upon the RC5 algorithm.</li>\n<li><strong>Diffie-Hellman algorithm</strong>: First asymmetric algorithm created and is used to exchange symmetric key values. Based upon logarithms in finite fields.</li>\n<li><strong>El Gamal algorithm</strong>: Asymmetric algorithm based upon the Diffie-Hellman algorithm used for digital signatures, encryption, and key exchange.</li>\n<li><strong>Elliptic curve cryptosystem algorithm</strong>: Asymmetric algorithm based upon the algebraic structure of elliptic curves over finite fields. Used for digital signatures, encryption, and key exchange.</li>\n<li><strong>Zero knowledge proof</strong>: One entity can prove something to be true without providing a secret value.</li>\n<li><strong>One-way hash</strong>: Cryptographic process that takes an arbitrary amount of data and generates a fixed-length value. Used for integrity protection.</li>\n<li><strong>Message authentication code (MAC)</strong>: Keyed cryptographic hash function used for data integrity and data origin authentication.</li>\n<li><strong>Hashed message authentication code (HMAC)</strong>: Cryptographic hash function that uses a symmetric key value and is used for data integrity and data origin authentication.</li>\n<li><strong>CBC-MAC</strong>: Cipher block chaining message authentication code uses encryption for data integrity and data origin authentication.</li>\n<li><strong>CMAC</strong>: Cipher message authentication code that is based upon and provides more security compared to CBC-MAC.</li>\n<li><strong>CMM</strong>: Block cipher mode that combines the CTR encryption mode and CBC-MAC. One encryption key is used for both authentication and encryption purposes.</li>\n<li><strong>Collision</strong>: When two different messages are computed by the same hashing algorithm and the same message digest value results.</li>\n<li><strong>Birthday attack</strong>: Cryptographic attack that exploits the mathematics behind the birthday problem in the probability theory forces collisions within hashing functions.</li>\n<li><strong>Digital signature</strong>: Ensuring the authenticity and integrity of a message through the use of hashing algorithms and asymmetric algorithms. The message digest is encrypted with the sender’s private key.</li>\n<li><strong>Certification Authority</strong>: Component of a PKI that creates and maintains digital certificates throughout their life cycles.</li>\n<li><strong>Registration Authority (RA)</strong>: Component of PKI that validates the identity of an entity requesting a digital certificate.</li>\n<li><strong>Certificate Revocation List (CRL)</strong>: List that is maintained by the certificate authority of a PKI that contains information on all of the digital certificates that have been revoked.</li>\n<li><strong>Online Certificate Status Protocol (OCSP)</strong>: Automated method of maintaining revoked certificates within a PKI.</li>\n<li><strong>Certificate</strong>: Digital identity used within a PKI. Generated and maintained by a certificate authority and used for authentication.</li>\n<li><strong>Link encryption</strong>: Technology that encrypts full packets (all headers and data payload) and is carried out without the sender’s interaction.</li>\n<li><strong>End-to-end encryption</strong>: Encryption method used by the sender of data that encrypts individual messages and not full packets.</li>\n<li><strong>Multipurpose Internet Mail Extension (MIME)</strong>: Standard that outlines the format of e-mail messages and allows binary attachments to be transmitted through e-mail.</li>\n<li><strong>Secure MIME (S/MIME)</strong>: Secure/Multipurpose Internet Mail Extensions, which outlines how public key cryptography can be used to secure MIME data types.</li>\n<li><strong>Pretty Good Privacy (PGP) Cryptosystem</strong>: used to integrate public key cryptography with e-mail functionality and data encryption, which was developed by Phil Zimmerman.</li>\n<li><strong>Quantum cryptography</strong>: Use of quantum mechanical functions to provide strong cryptographic key exchange.</li>\n<li><strong>HTTPS</strong>: A combination of HTTP and SSL\\TLS that is commonly used for secure Internet connections and e-commerce transactions.</li>\n<li><strong>Secure Electronic Transaction (SET)</strong>: Secure e-commerce standard developed by Visa and MasterCard that has not been accepted within the marketplace.</li>\n<li><strong>Cookies</strong>: Data files used by web browsers and servers to keep browser state information and browsing preferences.</li>\n<li><strong>Secure Shell (SSH)</strong>: Network protocol that allows for a secure connection to a remote system. Developed to replace Telnet and other insecure remote shell methods.</li>\n<li><strong>IPSec</strong>: Protocol suite used to protect IP traffic through encryption and authentication. De facto standard VPN protocol.</li>\n<li><strong>Authentication header (AH) protocol</strong>: Protocol within the IPSec suite used for integrity and authentication.</li>\n<li><strong>Encapsulating Security Payload Protocol (ESP)</strong>: Protocol within the IPSec suite used for integrity, authentication, and encryption.</li>\n<li><strong>Transport mode</strong>: Mode that IPSec protocols can work in that provides protection for packet data payload.</li>\n<li><strong>Tunnel mode</strong>: Mode that IPSec protocols can work in that provides protection for packet headers and data payload.</li>\n<li><strong>Internet Security Association and Key Management Protocol (ISAKMP)</strong>: Used to establish security associates and an authentication framework in Internet connections. Commonly used by IKE for key exchange.</li>\n<li><strong>Passive attack</strong>: Attack where the attacker does not interact with processing or communication activities, but only carries out observation and data collection, as in network sniffing.</li>\n<li><strong>Active attack</strong>: Attack where the attacker does interact with processing or communication activities.</li>\n<li><strong>Ciphertext-only attack</strong>: Cryptanalysis attack where the attacker is assumed to have access only to a set of ciphertexts.</li>\n<li><strong>Known-plaintext attack</strong>: Cryptanalysis attack where the attacker is assumed to have access to sets of corresponding plaintext and ciphertext.</li>\n<li><strong>Chosen-plaintext attack</strong>: Cryptanalysis attack where the attacker can choose arbitrary plaintexts to be encrypted and obtain the corresponding ciphertexts.</li>\n<li><strong>Chosen-ciphertext attack</strong>: Cryptanalysis attack where the attacker chooses a ciphertext and obtains its decryption under an unknown key.</li>\n<li><strong>Differential cryptanalysis</strong>: Cryptanalysis method that uses the study of how differences in an input can affect the resultant difference at the output.</li>\n<li><strong>Linear cryptanalysis</strong>: Cryptanalysis method that uses the study of affine transformation approximation in encryption processes.</li>\n<li><strong>Side-channel attack</strong>: Attack that uses information (timing, power consumption) that has been gathered to uncover sensitive data or processing functions.</li>\n<li><strong>Replay attack</strong>: Valid data transmission is maliciously or fraudulently repeated to allow an entity gain unauthorized access.</li>\n<li><strong>Algebraic attack</strong>: Cryptanalysis attack that exploits vulnerabilities within the intrinsic algebraic structure of mathematical functions.</li>\n<li><strong>Analytic attack</strong>: Cryptanalysis attack that exploits vulnerabilities within the algorithm structure.</li>\n<li><strong>Statistical attack</strong>: Cryptanalysis attack that uses identified statistical patterns.</li>\n<li><strong>Social engineering attack</strong>: Manipulating individuals so that they will divulge confidential information, rather than by breaking in or using technical cracking techniques.</li>\n<li><strong>Meet-in-the-middle attack</strong>: Cryptanalysis attack that tries to uncover a mathematical problem from two different ends.</li>\n<li><strong>Business continuity management (BCM)</strong>: is the overarching approach to<br>\nmanaging all aspects of BCP and DRP.</li>\n<li><strong>Business Continuity Plan (BCP)</strong>: Contains strategy documents that provide detailed procedures that ensure critical business functions are maintained and<br>\nthat help minimize losses of life, operations, and systems. A BCP provides procedures for emergency responses, extended backup operations, and post-disaster recovery.</li>\n<li><strong>Business Impact Analysis (BIA)</strong>: One of the most important first steps in the planning development. Qualitative and quantitative data on the business impact of a disaster need to be gathered, analyzed, interpreted, and presented to management.</li>\n<li><strong>A reciprocal agreement</strong>: One in which a company promises another company it can move in and share space if it experiences a disaster, and vice versa. Reciprocal agreements are very tricky to implement and are unenforceable.</li>\n<li><strong>A hot site</strong>: Fully configured with hardware, software, and environmental needs. It can usually be up and running in a matter of hours. It is the most<br>\nexpensive option, but some companies cannot be out of business longer than a day without very detrimental results.</li>\n<li><strong>A warm site</strong>: Does not have computers, but it does have some peripheral devices, such as disk drives, controllers, and tape drives. This option is less expensive than a hot site, but takes more effort and time to become operational.</li>\n<li><strong>A cold site</strong>: Is just a building with power, raised floors, and utilities. No devices are available. This is the cheapest of the three options, but can take weeks to get up and operational.</li>\n<li><strong>Recovery Time Objective (RTO)</strong>: The earliest time period and a service level within which a business process must be restored after a disaster to avoid<br>\nunacceptable consequences.</li>\n<li><strong>Recovery Point Objective (RPO)</strong>: The acceptable amount of data loss measured in time.</li>\n<li><strong>Mean Time Between Failures (MTBF)</strong>: The predicted amount of time between inherent failures of a system during operation.</li>\n<li><strong>Mean Time To Repair (MTTR)</strong>: A measurement of the maintainability by representing the average time required to repair a failed component or device.</li>\n<li><strong>High availability</strong>: Refers to a system, component, or environment that is continuously operational.</li>\n<li><strong>A checklist test</strong>: Copies of the plan are handed out to each functional area for examination to ensure the plan properly deals with the area’s needs and<br>\nvulnerabilities.</li>\n<li><strong>A structured walk-through test</strong>: Representatives from each functional area or department get together and walk through the plan from beginning to end.</li>\n<li><strong>A simulation test</strong>: A practice execution of the plan takes place. A specific scenario is established, and the simulation continues up to the point of actual<br>\nrelocation to the alternate site.</li>\n<li><strong>A parallel test</strong>: One in which some systems are actually run at the alternate site.</li>\n<li><strong>A full-interruption test</strong>: One in which regular operations are stopped and processing is moved to the alternate site.</li>\n<li><strong>Remote journaling</strong>: Involves transmitting the journal or transaction log offsite to a backup facility.</li>\n<li><strong>Dumpster diving</strong>: Refers to going through someone’s trash to find confidential or useful information. It is legal, unless it involves trespassing, but in all cases it is considered unethical.</li>\n<li><strong>Wiretapping</strong>: A passive attack that eavesdrops on communications. It is only legal with prior consent or a warrant.</li>\n<li><strong>Data diddling</strong>: The act of willfully modifying information, programs, or documentation in an effort to commit fraud or disrupt production.</li>\n<li><strong>Patent</strong>: Grants ownership and enables that owner to legally enforce his rights to exclude others from using the invention covered by the patent.</li>\n<li><strong>Copyright</strong>: Protects the expression of ideas rather than the ideas themselves.</li>\n<li><strong>Trademarks</strong>: Protect words, names, product shapes, symbols, colors, or a combination of these used to identify products or a company. These items are used to distinguish products from the competitors’ products.</li>\n<li><strong>Trade secrets</strong>: Are deemed proprietary to a company and often include information that provides a competitive edge. The information is protected as long as the owner takes the necessary protective actions.</li>\n<li><strong>Personally Identifiable Information (PII)</strong>: Data that can be used to uniquely identify, contact, or locate a single person or can be used with other sources to<br>\nuniquely identify a single individual.</li>\n<li><strong>System Development Life Cycle (SDLC)</strong>: A methodical approach to standardize requirements discovery, design, development, testing, and implementation in every phase of a system. It is made up of the following phases: initiation, acquisition/development, implementation, operation/maintenance, and disposal.</li>\n<li><strong>Certification</strong>: The technical testing of a system.</li>\n<li><strong>Accreditation</strong>: The formal authorization given by management to allow a system to operate in a specific environment.</li>\n<li><strong>Statement of Work (SOW)</strong>: Describes the product and customer requirements. A detailed-oriented SOW will help ensure that these requirements are properly understood and assumptions are not made.</li>\n<li><strong>Work breakdown structure (WBS)</strong>: A project management tool used to define and group a project’s individual work elements in an organized manner.</li>\n<li><strong>Attack surface</strong>: Components available to be used by an attacker against the product itself.</li>\n<li><strong>Threat modeling</strong>: A systematic approach used to understand how different threats could be realized and how a successful compromise could take place.</li>\n<li><strong>Static analysis</strong>: A debugging technique that is carried out by examining the code without executing the program, and therefore is carried out before the program is compiled.</li>\n<li><strong>Fuzzing</strong>: A technique used to discover flaws and vulnerabilities in software.</li>\n<li><strong>Verification</strong>: Determines if the product accurately represents and meets the specifications.</li>\n<li><strong>Validation</strong>: Determines if the product provides the necessary solution for the intended real-world problem.</li>\n<li><strong>Capability Maturity Model Integration (CMMI) model</strong>: A process improvement approach that provides organizations with the essential elements of effective processes, which will improve their performance.</li>\n<li><strong>Change control</strong>: The process of controlling the changes that take place during the life cycle of a system and documenting the necessary change control activities.</li>\n<li><strong>Software Configuration Management (SCM)</strong>: Identifies the attributes of software at various points in time, and performs a methodical control of changes for the purpose of maintaining software integrity and traceability throughout the software development life cycle.</li>\n<li><strong>Software escrow</strong>: Storing of the source code of software with a third-party escrow agent. The software source code is released to the licensee if the licensor (software vendor) files for bankruptcy or fails to maintain and update the software product as promised in the software license agreement.</li>\n<li><strong>Machine language</strong>: A set of instructions in binary format that the computer’s processor can understand and work with directly.</li>\n<li><strong>Assembly language</strong>: A low-level programming language that is the mnemonic representation of machine-level instructions.</li>\n<li><strong>Assemblers</strong>: Tools that convert assembly code into the necessary machine-compatible binary language for processing activities to take place.</li>\n<li><strong>High-level languages</strong>: Otherwise known as third-generation programming languages, due to their refined programming structures, using abstract statements.</li>\n<li><strong>Very high-level languages</strong>: Otherwise known as fourth-generation programming languages and are meant to take natural language-based statements one step ahead.</li>\n<li><strong>Natural languages</strong>: Otherwise known as fifth-generation programming languages, which have the goal to create software that can solve problems by themselves. Used in systems that provide artificial intelligence.</li>\n<li><strong>Compilers</strong>: Tools that convert high-level language statements into the necessary machine-level format (.exe, .dll, etc.) for specific processors to understand.</li>\n<li><strong>Interpreters</strong>: Tools that convert code written in interpreted languages to the machine-level format for processing.</li>\n<li><strong>Garbage collector</strong>: Identifies blocks of memory that were once allocated but are no longer in use and deallocates the blocks and marks them as free.</li>\n<li><strong>Abstraction</strong>: The capability to suppress unnecessary details so the important, inherent properties can be examined and reviewed.</li>\n<li><strong>Polymorphism</strong>: Two objects can receive the same input and have different outputs.</li>\n<li><strong>Data modeling</strong>: Considers data independently of the way the data are processed and of the components that process the data. A process<br>\nused to define and analyze data requirements needed to support the business processes.</li>\n<li><strong>Cohesion</strong>: A measurement that indicates how many different types of tasks a module needs to carry out.</li>\n<li><strong>Coupling</strong>: A measurement that indicates how much interaction one module requires for carrying out its tasks.</li>\n<li><strong>Data structure</strong>: A representation of the logical relationship between elements of data.</li>\n<li><strong>Mobile code</strong>: Code that can be transmitted across a network, to be executed by a system or device on the other end.</li>\n<li><strong>Java applets</strong>: Small components (applets) that provide various functionalities and are delivered to users in the form of Java bytecode. Java applets can run in a web browser using a Java Virtual Machine (JVM). Java is platform independent; thus, Java applets can be executed by browsers for many platforms.</li>\n<li><strong>Sandbox</strong>: A virtual environment that allows for very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources.</li>\n<li><strong>ActiveX</strong>: A Microsoft technology composed of a set of OOP technologies and tools based on COM and DCOM. It is a framework for defining reusable software components in a programming language–independent manner.</li>\n<li><strong>Authenticode</strong>: A type of code signing, which is the process of digitally signing software components and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was digitally signed. Authenticode is Microsoft’s implementation of code signing.<br>\nKey Terms</li>\n<li><strong>Information gathering</strong>: Usually the first step in an attacker’s methodology, in which the information gathered may allow an attacker to infer additional information that can be used to compromise systems.</li>\n<li><strong>Server side includes (SSI)</strong>: An interpreted server-side scripting language used almost exclusively for web-based communication. It is commonly used to include the contents of one or more files into a web page on a web server. Allows web developers to reuse content by inserting the same content into multiple web documents.</li>\n<li><strong>Client-side validation</strong>: Input validation is done at the client before it is even sent back to the server to process.</li>\n<li><strong>Cross-site scripting (XSS) attack</strong>: An attack where a vulnerability is found on a web site that allows an attacker to inject malicious code into a web<br>\napplication.</li>\n<li><strong>Parameter validation</strong>: The values that are being received by the application are validated to be within defined limits before the server application processes them within the system.</li>\n<li><strong>Web proxy</strong>: A piece of software installed on a system that is designed to intercept all traffic between the local web browser and the web server.</li>\n<li><strong>Replay attack</strong>: An attacker capturing the traffic from a legitimate session and replaying it with the goal of masquerading an authenticated user.<br>\nfollowing are some key database terms:</li>\n<li><strong>Record</strong>: A collection of related data items.</li>\n<li><strong>File</strong>: A collection of records of the same type.</li>\n<li><strong>Database</strong>: A cross-referenced collection of data.</li>\n<li><strong>Database Management System (DBMS)</strong>: Manages and controls the database.</li>\n<li><strong>Tuple</strong>: A row in a two-dimensional database.</li>\n<li><strong>Attribute</strong>: A column in a two-dimensional database.</li>\n<li><strong>Primary key</strong>: Columns that make each row unique. (Every row of a table must include a primary key.)</li>\n<li><strong>View</strong>: A virtual relation defined by the database administrator in order to keep subjects from viewing certain data.</li>\n<li><strong>Foreign key</strong>: An attribute of one table that is related to the primary key of another table.</li>\n<li><strong>Cell</strong>: An intersection of a row and a column.</li>\n<li><strong>Schema</strong>: Defines the structure of the database.</li>\n<li><strong>Data dictionary</strong>: Central repository of data elements and their relationships.</li>\n<li><strong>Relational database model</strong>: Uses attributes (columns) and tuples (rows) to contain and organize information.</li>\n<li><strong>Hierarchical data model</strong>: Combines records and fields that are related in a logical tree structure.</li>\n<li><strong>Object-oriented database</strong>: Designed to handle a variety of data (images, audio, documents, video), which is more dynamic in nature than a relational database.</li>\n<li><strong>Object-relational database (ORD)</strong>: Uses object-relational database management system (ORDBMS) and is a relational database with a software front end that is written in an object-oriented programming language.</li>\n<li><strong>Rollback</strong>: An operation that ends a current transaction and cancels all the recent changes to the database until the previous checkpoint/ commit point.</li>\n<li><strong>Two-phase commit</strong>: A mechanism that is another control used in databases to ensure the integrity of the data held within the database.</li>\n<li><strong>Cell suppression</strong>: A technique used to hide specific cells that contain sensitive information.</li>\n<li><strong>Noise and perturbation</strong>: A technique of inserting bogus information in the hopes of misdirecting an attacker or confusing the matter enough that the actual attack will not be fruitful.</li>\n<li><strong>Data warehousing</strong>: Combines data from multiple databases or data sources into a large database for the purpose of providing more extensive information retrieval and data analysis.</li>\n<li><strong>Data mining</strong>: Otherwise known as knowledge discovery in database (KDD), which is the process of massaging the data held in the data warehouse into more useful information.</li>\n<li><strong>Virus</strong>: A small application, or string of code, that infects host applications. It is a programming code that can replicate itself and spread from one system to another.</li>\n<li><strong>Macro virus</strong>: A virus written in a macro language and that is platform independent. Since many applications allow macro programs to be embedded in documents, the programs may be run automatically when the document is opened. This provides a distinct mechanism by which viruses can be spread.</li>\n<li><strong>Compression viruses</strong>: Another type of virus that appends itself to executables on the system and compresses them by using the user’s permissions.</li>\n<li><strong>Stealth virus</strong>: A virus that hides the modifications it has made. The virus tries to trick anti-virus software by intercepting its requests to the operating system and providing false and bogus information.</li>\n<li><strong>Polymorphic virus</strong>: Produces varied but operational copies of itself. A polymorphic virus**: may have no parts that remain identical between infections, making it very difficult to detect directly using signatures.</li>\n<li><strong>Multipart virus</strong>: Also called a multipartite virus, this has several components to it and can be distributed to different parts of the system. It infects and spreads in multiple ways, which makes it harder to eradicate when identified.</li>\n<li><strong>Self-garbling virus</strong>: Attempts to hide from anti-virus software by modifying its own code so that it does not match predefined signatures.</li>\n<li><strong>Meme viruses</strong>: These are not actual computer viruses, but types of e-mail messages that are continually forwarded around the Internet.</li>\n<li><strong>Bots</strong>: Software applications that run automated tasks over the Internet, which perform tasks that are both simple and structurally repetitive. Malicious use of bots is the coordination and operation of an automated attack by a botnet (centrally controlled collection of bots).</li>\n<li><strong>Worms</strong>: These are different from viruses in that they can reproduce on their own without a host application and are self-contained programs.</li>\n<li><strong>Logic bomb</strong>: Executes a program, or string of code, when a certain event happens or a date and time arrives.</li>\n<li><strong>Rootkit</strong>: Set of malicious tools that are loaded on a compromised system through stealthy techniques. The tools are used to carry out more attacks either on the infected systems or surrounding systems.</li>\n<li><strong>Trojan horse</strong>: A program that is disguised as another program with the goal of carrying out malicious activities in the background without the user knowing.</li>\n<li><strong>Remote access Trojans (RATs)</strong>: Malicious programs that run on systems and allow intruders to access and use a system remotely.</li>\n<li><strong>Immunizer</strong>: Attaches code to the file or application, which would fool a virus into “thinking” it was already infected.</li>\n<li><strong>Behavior blocking</strong>: Allowing the suspicious code to execute within the operating system and watches its interactions with the operating system, looking for suspicious activities.</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"5a24ae71fa17d805d622d7e9","plaintext":"As I am currently studying to sit the CISSP exam in 2018 and because I've taken\nover 25,000 words in notes so far I thought I'd share what I have so that others\nmight be able to study a bit easier.\n\nThe relevant CISSP material is difficult to search for mainly I believe is due\nto the exam changes often or as people pass they tend to only pass on notes to\nfriends, family, or colleagues.\n\nI've done my best to group relevant notes together in a coherent way to follow\nif you're just starting out, so enjoy this brain numbing content, and don't\nforget to share the content if you found it useful so others may also benefit by\nfinding it too.\n\nAcronyms\nGRC = Governance, Risk Management and Compliance\nBIA = Business Impact Analysis\nBCP = Business Continuity Plan\nIDS = Intrusion detection system\nIPS = Intrusion prevention system\nSIEM = Security information and event management\nDAC = Discretionary Access Control\nDRP = Disaster Recovery Plan\nRPO = Recovery Point Objective\nRTO = Recovery Time Objective\nMTD = Max Tolerable Downtime\nMOE = Measures of Effectiveness\nIPS = Voice Intrusion Prevention System\n\nFundamental Principles of Security\n * Availability - Reliable and timely access to data and resources is provided\n   to\n   authorized individuals\n * Integrity - Accuracy and reliability of the information and systems are\n   provided\n   and any unauthorized modification is prevented\n * Confidentiality - Necessary level of secrecy is enforced and unauthorized\n   disclosure is prevented\n\nSecurity Definitions\n * Threat agent - Entity that can exploit a vulnerability, something\n   (individual,\n   machine, software, etc) that can exploit vulnerabilities\n * Threat - The danger of a threat agent exploiting a vulnerability\n * Vulnerability\t- Weakness or a lack of countermeasure, something a threat\n   agent\n   can act upon, a hole that can be exploited\n * Risk - The probability of a threat agent exploiting a vulnerability and the\n   associated impact, threat plus the impact (asset - something important\n   to business)\n * Exposure - Presence of a vulnerability, which exposes the organization\n   to a threat, possibility of happening\n * Control - Safeguard that is put in place to reduce a risk, also\n   called a countermeasure\n\nSecurity Framework\n * act as reference points\n * provide common language for communications\n * allow us to share information and create relevancy\n * ITIL\n * COBIT\n\nEnterprise frameworks\n * TOGAF\n * DoDAF\n * MODAF\n * SABSA\n * COSO\n\nISO Standards\n * ISO27000 - built on BS7799\n\nRisk\n * we have assets that threat agents may want to take advantage of\n   through vulnerabilities\n * identify assets and their importance\n * understand risk in the context of the business\n\nRisk Management\n * have a risk management policy\n * have a risk management team\n * start by doing risk assessment\n\nRisk Assessment\n * identifying vulnerabilities our assets face\n * create risk profile\n * identify as much as possible\n * cannot find them all\n * must continually perform assessment\n * Four main goals\n * identify assets and their value to the organization\n * identify vulnerabilities and threats\n * quantify or measure the business impact\n * balance economically the application of a countermeasure against the cost of\n   the countermeasure, develop cost analysis\n * has to be supported by the upper management\n * risk analysis team\n * made up of specialists\n * risk management specialists\n * change management specialists\n * IT knowledge specialists\n\nNIST\n * SP800-30 rev.1\n * http://csrc.nist.gov/publications/drafts/800-30-rev1/SP800-30-Rev1-ipd.pdf\n\nFRAP\n * Facilitated Risk Analysis Process\n\nOctave\nThe Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE)\napproach defines a risk-based strategic assessment and planning technique for\nsecurity. OCTAVE is a self-directed approach, meaning that people from an\norganization assume responsibility for setting the organization's security\nstrategy.\n\nISO27005\nThe ISO27k standards are deliberately risk-aligned, meaning that organizations\nare encouraged to assess the security risks to their information (called\n“information security risks” in the standards, but in reality they are simply\ninformation risks) as a prelude to treating them in various ways. Dealing with\nthe highest risks first makes sense from the practical implementation and\nmanagement perspectives.\n\nRisk Analysis Approaches\n * Quantitative risk analysis\n * hard measures, numbers, dollar value, fact based\n * Qualitative risk analysis\n * soft measure, not easily defined, opinion based\n\nQuantitative Risk Measurement (Number Based)\n * SLE Single Loss Expectancy\n * ARO Annualized Rate of Occurrence\n * ALE Annualized Loss Expectancy (SLE x ARO)\n\nFor example: purchasing a firewall. What is its purpose and capabilities on\naverage, three times a year there is a breach, and data is compromised liability\nof restoring data in the end the cost of incident is $5000 this would be the\nsingle loss expectancy occurs three times, annual occurrence $15000 - 5000 * 3 =\n15,000 this is ALE is the cost to mitigate greater or less than ALE.\n\nQuantitative Assessment\n * systems\n * training\n * vulnerabilities\n\nQualitative Risk\n * not so much hard numbers\n * should be contextual to business policies and compliance requirements\n\nQualitative risk analysis is a project management technique concerned with\ndiscovering the probability of a risk event occurring and the impact the risk\nwill have if it does occur. All risks have both probability and impact.\n\n\n\nVery High\nRequires the prompt attention of management. The stakeholder (executive or\noperational owner) must undertake detailed research, identify risk reduction\noptions and prepare a detailed risk management plan. Reporting these exposures\nand mitigations through relevant forums is essential. Monitoring the risk\nmitigation strategies is essential.\n\nHigh\nHigh inherent risk requires the attention of the relevant manager so that\nappropriate controls can be set in place. The Risk & Reputation Committee\nmonitor the implementation of key enterprise risk controls.\n\nHigh residual risk is to be monitored to ensure the associated controls are\nworking. Detailed research to identify additional risk reduction options and\npreparation of a detailed risk management plan is required.\n\nReporting these exposures and mitigations through relevant forums is required.\nMonitoring the risk mitigation strategies is required\n\nMedium\nThis is the threshold that delineates the higher-level risks from those of less\nconcern. After considering the nature of the likelihood and consequence values\nin relation to whether the risk could increase in value, consider additional\ncost-effective mitigations to reduce the risk.\n\nResponsibility would fall on the relevant manager and specific monitoring of\nresponse procedures would occur.\n\nLow\nLook to accept, monitor and report on the risk or manage it through the\nimplementation/enhancement of procedures\n\nVery Low\nThese risks would be not considered any further unless they contribute to a\nscenario that could pose a serious event, or they escalate in either likelihood\nand/or consequence\n\nLikelihood\nFind relevant definitions for each category within your business. An example of\nrare could just mean it could occur but only in exceptional circumstances and\ndecades apart where almost certain risks are known to occur frequently (monthly\nor bi-quarterly depending on business thresholds).\n\nImpact Consequence\nFind relevant definitions for each category within your business. An example of\nInsignificant is an impact can be absorbed within the day-to-day business\nrunning costs where-as extreme might be contractual non-compliance or breach of\nlegislation with penalties or fines over a considerably higher amount then\nperhaps major would have had and may also come with extreme brand, operational,\nfinancial or reputational impacts.\n\nImplementation\nPolicies\nOverall general statement produced by senior management\n\n * high level statements of intent\n * what expectations of correct usage\n\nStandards\nRefer to mandatory activities or rules\n\n * regulatory compliance\n * mandated based on compliance regime\n * in US, Sarbanes-Oxley, HIPPA\n * Basel Accords\n * Montreal Protocol\n\nBaselines\nTemplate for comparison, measure deviation from normal\n\n * standardized solution\n * allows us to find and measure deviations\n\nGuidelines\nRecommendations\n\n * not mandatory, optional\n * best practices\n\nProcedures\nStep by step activities that need to be performed in a certain order, detailed\ninstructions.\n\nBusiness Continuity and Disaster Recovery\nThe goal of disaster recovery is to minimize the effects of a disaster\nThe goal of business continuity is to resume normal business operations as\nquickly as possible with the least amount of resources necessary to do so\n\nDisaster Recovery Plan (DRP)\nCarried out when everything is going to still be suffering from the effects of\nthe disaster\n\nBusiness Continuity Plan (BCP)\nUsed to return the business to normal operations\n\nBusiness Continuity Management (BCM)\nThe process that is responsible for DRP and BCP.\n\nOver-arching process\nNIST SP800-34 [http://csrc.nist.gov/publications/PubsSPs.htm] Continuity\nPlanning Guide for IT\n\n 1. Develop the continuity planning policy statement\n 2. Conduct the business impact analysis (BIA)\n 3. Identify preventive controls\n 4. Develop recovery strategies\n 5. Develop the contingency plan\n 6. Test the plan and conduct training and exercises\n 7. Maintain the plan\n\nBS25999 [http://www.itgovernance.co.uk/bs25999.aspx] British Standard for\nBusiness Continuity Management\nISO 27031:2011 [http://www.iso.org/iso/catalogue_detail?csnumber=44374] Business\nContinuity Planning\nISO 22301:2012 [http://www.iso.org/iso/catalogue_detail?csnumber=50038] Business\nContinuity Management Systems\n\nDepartment of Homeland Security BCP Kit BCP Policy\n[http://www.ready.gov/business-continuity-planning-suite] Supplies the framework\nand governance for the BCP effort.\n\nContains:\n\n * Scope\n * Mission Statement\n * Principles\n * Guidelines\n * Standards\n\nProcess to draft the policy\n\n 1.  Identify and document the components of the policy\n 2.  Identify and define existing policies that the BCP may affect\n 3.  Identify pertinent laws and standards\n 4.  Identify best practices\n 5.  Perform a GAP analysis\n 6.  Compose a draft of the new policy\n 7.  Internal review of the draft\n 8.  Incorporate feedback into draft\n 9.  Get approval of senior management\n 10. Publish a final draft and distribute throughout organization\n\nSWOT Analysis\n * Strength\n * Weakness\n * Opportunity\n * Threat\n\nBusiness Continuity Planning Requirements\n * Senior management support\n * Management should be involved in setting the overall goals in continuity\n   planning\n\nBusiness Impact Analysis (BIA)\n * Functional analysis\n * Identifies which of the company's critical systems are needed for survival\n * Estimates the down-time that can be tolerated by the system * Maximum\n      Tolerable Down-time (MTD)\n    * Maximum Period\n      Time of Disruption (MPTD)\n   \n   \n\nSteps:\n 1. Select individuals to interview for data gathering\n 2. Create data gathering tools * Surveys\n     * Questionnaires\n    \n    \n 3. Identify company's critical business functions\n 4. Identify the resources these functions depend on\n 5. Calculate how long these functions can survive without these resources\n 6. Identify vulnerabilities and threats to these functions\n 7. Calculate the risk for each different business function\n 8. Document findings and report them to senior management\n\nGoals:\n * Determine criticality\n * Estimate max downtime\n * Evaluate internal and external resource requirements\n\nProcess\n * Gather information\n * Analyse information\n * Perform threat analysis\n * Document results and present recommendations\n\nDisaster Recovery Planning\nStage 1 - Business as usual\nStage 2 - Disaster occurs\nStage 3 - Recovery\nStage 4 - Resume production\n\n\n\nRecovery Time Objective (RTO)\n * The earliest time period and a service level within which a business process\n   must be restored after a disaster\n * RTO value is smaller than the MTD because the MTD represents the time after\n   which an inability to recover will mean severe damage to the business's\n   reputation and/or the bottom line.\n * The RTO assumes there is a period of acceptable down-time\n\nWork Recovery Time (WRT)\n * Remainder of the overall MTD value\n\nRecovery Point Objective (RPO)\n * Acceptable amount of data loss measured in time\n\n> MTD, RTO and RPO values are derived during the BIA\n\n\nRisk Assessment\nLooks at the impact and likely-hood to various threats to the business\nGoals:\n\n * Identify and document single points of failure\n * Make a prioritized list of threats\n * Gather information to develop risk control strategies\n * Document acceptance of identified risks\n * Document acknowledgment of risks that may not be addressed\n\nFormula:\nRisk = Threat * Impact * Probability\n\nMain components:\n\n 1.  Review existing strategies for risk management\n 2.  Construct numerical scoring system for probability and impact\n 3.  Use numerical scoring to gauge effect of threats\n 4.  Estimate probability of threats\n 5.  Weigh each threat through the scoring system\n 6.  Calculate the risk by combining the scores of likelihood and impact of each\n     threat\n 7.  Secure sponsor sign off on risk priorities\n 8.  Weigh appropriate measures\n 9.  Make sure planned measures do not heighten other risks\n 10. Present assessment's findings to executive management\n\nCertification vs Accreditation\nCertification is the comprehensive technical evaluation of the security\ncomponents and their compliance for the purpose of accreditation.\nThe goal of a certification process is to ensure that a system, product, or\nnetwork is right for the customer’s purposes.\n\nAccreditation is the formal acceptance of the adequacy of a system’s overall\nsecurity and functionality by management.\n\nAccess Control\nAccess Control Systems\n * RADIUS (not vendor specific)\n * AAA\n * UDP\n * Encrypts only the password from client to server\n * works over PPP\n * TACACS/xTACACS/TACACS+\n * TCP\n * Encrypts all traffic\n * works over multiple protocols\n * Diameter\n * supports PAP, CHAP, EAP\n * replay protection\n * end to end protection\n * enhanced accounting\n\nContent dependent access control\n * focused on content of data\n * data may be labeled, like HR or Finances\n * dependent on the context of the usage of the data\n\n\n\n> Y = Control, X = Control Category, Color key = Control types\n\n\nThreats to systems:\nMaintenance hooks are a type of back door used by developers to get \"back into\"\ntheir code if necessary.\n\nPreventive measures against back doors:\n * Use a host intrusion detection system to watch for any attackers using back\n   doors into the system.\n * Use file system encryption to protect sensitive information.\n * Implement auditing to detect any type of back door use.\n\nA time-of-check/time-of-use (TOC/TOU) attack deals with the sequence of steps a\nsystem uses to complete a task. This type of attack takes advantage of the\ndependency on the timing of events that take place in a multitasking operating\nsystem. A TOC/TOU attack is when an attacker jumps in between two tasks and\nmodifies something to control the result.\nTo protect against TOC/TOU attacks, the operating system can apply software\nlocks to the items it will use when it is carrying out its “checking” tasks.\n\nA race condition is when two different processes need to carry out their tasks\non one resource. A race condition is an attack in which an attacker makes\nprocesses execute out of sequence to control the result.\nTo protect against race condition attacks, do not split up critical tasks that\ncan have their sequence altered.\n\nTCP/IP OSI Layer\n\n\nThreats to Access Control\nDefeating Access Controls\n\n * Credentials can be mirrored\n * Passwords can be brute forced\n * Locks can be picked\n * You must assess the weaknesses of your solution\n\nKeystroke monitoring\nObject reuse (data remnants)\n * object disposal\n\nTempest Shielding\n * Faraday Cage\n\nWhite Noise\n * Voice assistants (Alexa, Siri, ect) can be activated by white noise\n\nControl Zone\n * SCIF: A Sensitive Compartmented Information Facility which is a defense term\n   for a secure room.\n\nNIDS\nNetwork Intrusion Detection Systems\n\n * Real time traffic analysis\n * Monitor though upload to logging server\n * passive monitoring\n * usually per subnet\n\nHIDS/HIPS\nHost-based Intrusion Detection/Prevention System\n\n * Software based agent installed on machine\n * not real time\n * aggregated on schedule\n\nHow do they work\nHIDS: Passive, do not stop attack, maybe send alert\nHIPS: Active, can stop attack as well as send alert, respond and stop attack\n\n * Signature Based\n * Pattern Matching\n * Must be kept up to date\n * antivirus\n * Stateful Matching\n * Looks at sequences across traffic\n * Building pattern based on traffic\n * Must be kept up to date\n * Anomaly Based\n * looks at stream of traffic\n * Look at expected behavior (rule based)\n * Detects Abnormalities\n * Could be statistical\n * Look at raw traffic\n * Could look at protocol Anomalies\n * Heuristics\n * Create virtual sandbox in memory\n * Looks for abnormalities\n\nHoneypot (honeynets)\n * hacking magnet\n * decoy\n * honeynet.org\n * enticement\n\nEnticement\n * legal\n * let them make the decision to attack the system\n\nEntrapment\n * illegal\n * trick them into attacking the system\n\nAttacks against Honeypots\n * Network Sniffer * used to capture traffic\n    * Network monitor, etc.\n   \n   \n * Dictionary attack\n * Brute force attack\n * Phishing\n * Pharming\n * Whaling\n * Spear Phishing\n * Vishing\n\nEmanations Security\n * Data radiating out via electrical/wireless signals\n * TEMPEST shielding can help contain these signals\n * Faraday cage\n\nIntrusion Detection\nIDS Types\n\n * Signature Based * Stateful * Matches traffic patterns to activities\n      \n      \n    * Signature * Matches individual packets to predefined definitions\n      \n      \n    * Regular updates required\n    * Cannot detect previously unknown attacks\n   \n   \n * Anomaly Based * Learns the normal traffic on the network and then spots\n      exceptions\n    * Requires a \"training\" period to reduce false positives\n    * Also known as heuristic scanning\n    * Statistical detection * Looks for traffic that is outside of the\n         statistical norm\n      \n      \n    * Protocol detection * Looks for protocols that are not typically in use on\n         the network\n      \n      \n    * Traffic detection * Looks for unusual activity inside of the traffic\n      \n      \n   \n   \n * Rule Based * Detects attack traffic through pre-defined rules\n    * Can be coupled with expert systems to be dynamic\n    * Cannot detect previously unknown attacks\n   \n   \n\nSoftware Development Security\nA system development life cycle (SDLC) is made up of the following basic\ncomponents of each phase:\n\nInitiation\n * Need for a new system is defined; This phase addresses the questions, “What\n   do we need and why do we need it?”\n * A preliminary risk assessment should be carried out to develop an initial\n   description of the confidentiality, integrity, and availability requirements\n   of the system.\n\nAcquisition/development\nNew system is either created or purchased\n\nActivities during this phase will include:\n\n * Requirements analysis * In-depth study of what functions the company needs\n      the desired system to carry out.\n   \n   \n * Formal risk assessment * Identifies vulnerabilities and threats in the\n      proposed system and the potential risk levels as they pertain to\n      confidentiality, integrity, and availability. This builds upon the initial\n      risk assessment carried out in the previous phase. The results of this\n      assessment help the team build the system’s security plan.\n   \n   \n * Security functional requirements analysis * Identifies the protection levels\n      that must be provided by the system to meet all regulatory, legal, and\n      policy compliance needs.\n   \n   \n * Security assurance requirements analysis * Identifies the assurance levels\n      the system must provide. The activities that need to be carried out to\n      ensure the desired level of confidence in the system are determined, which\n      are usually specific types of tests and evaluations.\n   \n   \n * Third-party evaluations * Reviewing the level of service and quality a\n      specific vendor will provide if the system is to be purchased.\n   \n   \n * Security plan * Documented security controls the system must contain to\n      ensure compliance with the company’s security needs. This plan provides a\n      complete description of the system and ties them to key company documents,\n      as in configuration management, test and evaluation plans, system\n      interconnection agreements, security accreditations, etc.\n   \n   \n * Security test and evaluation plan * Outlines how security controls should be\n      evaluated before the system is approved and deployed.\n   \n   \n\nImplementation\n * New system is installed into production environment.\n * A system should have baselines set pertaining to the system’s hardware,\n   software, and firmware configuration during the implementation phase.\n\nOperation/maintenance\n * System is used and cared for.\n * Continuous monitoring needs to take place to ensure that baselines are always\n   met.\n * Defined configuration management and change control procedures help to ensure\n   that a system’s baseline is always met.\n * Vulnerability assessments and penetration testing should also take place in\n   this phase.\n\nDisposal\n * System is removed from production environment.\n * Disposal activities need to ensure that an orderly termination of the system\n   takes place and that all necessary data are preserved.\n\nSoftware Development Life Cycle (SDLC)\nThe life cycle of software development deals with putting repeatable and\npredictable processes in place that help ensure functionality, cost, quality,\nand delivery schedule requirements are met.\n\nPhases:\nRequirements gathering\n * Determine the why create this software, the what the software will do, and\n   the for whom the software will be created.\n * This is the phase when everyone involved attempts to understand why the\n   project is needed and what the scope of the project entails.\n * Security requirements\n * Security risk assessment\n * Privacy risk assessment\n * Risk-level acceptance\n\nDesign\n * Deals with how the software will accomplish the goals identified, which are\n   encapsulated into a functional design.\n * Attack surface analysis - identify and reduce the amount of code and\n   functionality accessible to untrusted users.\n * Threat modeling - systematic approach used to understand how different\n   threats could be realized and how a successful compromise could take place.\n\nDevelopment\n * Programming software code to meet specifications laid out in the design\n   phase.\n * The software design that was created in the previous phase is broken down\n   into defined deliverables, and programmers develop code to meet the\n   deliverable requirements.\n\nTesting/Validation\n * Validating software to ensure that goals are met and the software works as\n   planned.\n * It is important to map security risks to test cases and code.\n * Tests are conducted in an environment that should mirror the production\n   environment.\n * Security attacks and penetration tests usually take place during this phase\n   to identify any missed vulnerabilities.\n * The most common testing approaches: * Unit testing - Individual component is\n      in a controlled environment where programmers validate data structure,\n      logic, and boundary conditions.\n    * Integration testing - Verifying that\n      components work together as outlined in design specifications.\n    * Acceptance testing - Ensuring that the\n      code meets customer requirements.\n    * Regression testing - After a change to\n      a system takes place, retesting to ensure functionality, performance, and\n      protection.\n   \n   \n * Fuzzing is a technique used to discover flaws and vulnerabilities in\n   software. Fuzzing is the act of sending random data to the target program in\n   \torder to trigger failures.\n * Dynamic analysis refers to the evaluation of a program in real time, i.e.,\n   when it is running. Dynamic analysis is carried out once a program has\n   cleared the static analysis stage and basic programming flaws have been\n   rectified offline.\n * Static analysis a debugging technique that is carried out by examining the\n   code without executing the program, and therefore is carried out before the\n   program is compiled.\n\nRelease/Maintenance\n * Deploying the software and then ensuring that it is properly configured,\n   patched, and monitored.\n\nStatement of Work (SOW)\nDescribes the product and customer requirements. A detailed-oriented SOW will\nhelp ensure that these requirements are properly understood and assumptions are\nnot made.\n\nWork breakdown structure (WBS)\nA project management tool used to define and group a project’s individual work\nelements in an organized manner.\n\nPrivacy Impact Rating\nIndicates the sensitivity level of the data that will be processed or made\naccessible.\n\nComputer-aided software engineering (CASE)\nRefers to software that allows for the automated development of software, which\ncan come in the form of program editors, debuggers, code analyzers,\nversion-control mechanisms, and more.\n\nVerification\nDetermines if the product accurately represents and meets the specifications.\n\nValidation\nDetermines if the product provides the necessary solution for the intended\nreal-world problem.\n\nSoftware Development Models\nBuild and Fix Model\nDevelopment takes place immediately with little or no planning involved.\nProblems are dealt with as they occur, which is usually after the software\nproduct is released to the customer.\n\nWaterfall Model\nA linear-sequential life-cycle approach. Each phase must be completed in its\nentirety before the next phase can begin. At the end of each phase, a review\ntakes place to make sure the project is on the correct path and if the project\nshould continue.\nIn this model all requirements are gathered in the initial phase and there is no\nformal way to integrate changes as more information becomes available or\nrequirements change.\n\nV-Shaped Model (V-Model)\nFollows steps that are laid out in a V format.\nThis model emphasizes the verification and validation of the product at each\nphase and provides a formal method of developing testing plans as each coding\nphase is executed.\nEach phase must be completed before the next phase begins.\n\nPrototyping\nA sample of software code or a model (prototype) can be developed to explore a\nspecific approach to a problem before investing expensive time and resources.\nRapid prototyping is an approach that allows the development team to quickly\ncreate a prototype (sample) to test the validity of the current understanding of\nthe project requirements.\nEvolutionary prototypes are developed, with the goal of incremental improvement.\n\nIncremental Model\nEach incremental phase results in a deliverable that is an operational product.\nThis means that a working version of the software is produced after the first\niteration and that version is improved upon in each of the subsequent\niterations.\n\nSpiral Model\nUses an iterative approach to software development and places emphasis on risk\nanalysis. The model is made up of four main phases: planning, risk analysis,\ndevelopment and test, and evaluation.\n\nRapid Application Development\nRelies more on the use of rapid prototyping instead of extensive upfront\nplanning.\nCombines the use of prototyping and iterative development procedures with the\ngoal of accelerating the software development process.\n\nAgile Model\nFocuses on incremental and iterative development methods that promote\ncross-functional teamwork and continuous feedback mechanisms.\nThe Agile model does not use prototypes to represent the full product, but\nbreaks the product down into individual features.\n\nCapability Maturity Model Integration (CMMI)\n * Initial Development process is ad hoc or even chaotic. The company does not\n   use effective management procedures and plans. There is no assurance of\n   consistency, and quality is unpredictable.\n * Repeatable A formal management structure, change control, and quality\n   assurance are in place. The company can properly repeat processes throughout\n   each project. The company does not have formal process models defined.\n * Defined Formal procedures are in place that outline and define processes\n   carried out in each project. The organization has a way to allow for\n   quantitative process improvement.\n * Managed The company has formal processes in place to collect and analyze\n   quantitative data, and metrics are defined and fed into the process\n   improvement program.\n * Optimizing The company has budgeted and integrated plans for continuous\n   process improvement.\n\nSoftware escrow\nStoring of the source code of software with a third-party escrow agent. The\nsoftware source code is released to the licensee if the licensor (software\nvendor) files for bankruptcy or fails to maintain and update the software\nproduct as promised in the software license agreement.\n\nProgramming Languages and Concepts:\n * (1st generation programming language) - Machine language is in a format that\n   the computer’s processor can understand and work with directly.\n * (2nd generation programming language) - Assembly language is considered a\n   low-level programming language and is the symbolic representation of\n   machine-level instructions. It is “one step above” machine language. It uses\n   symbols (called mnemonics) to represent complicated binary codes. Programs\n   written in assembly language are also hardware specific.\n * (3rd generation programming language) - High-level languages use abstract\n   statements. Abstraction naturalized multiple assembly language instructions\n   into a single high-level statement, e.g., the IF – THEN – ELSE. High-level\n   languages are processor independent. Code written in a high-level language\n   can be converted to machine language for different processor architectures\n   using compilers and interpreters.\n * (4th generation programming language) - Very high-level languages focus on\n   highly abstract algorithms that allow straightforward programming\n   implementation in specific environments.\n * (5th generation programming language) - Natural languages have the ultimate\n   target of eliminating the need for programming expertise and instead use\n   advanced knowledge-based processing and artificial intelligence.\n\nAssemblers\nTools that convert assembly language source code into machine code.\n\nCompilers\nTools that convert high-level language statements into the necessary\nmachine-level format (.exe, .dll, etc.) for specific processors to understand.\n\nInterpreters\nTools that convert code written in interpreted languages to the machine-level\nformat for processing.\n\nGarbage collector\nIdentifies blocks of memory that were once allocated but are no longer in use\nand deallocates the blocks and marks them as free.\n\nObject Oriented Programming (OOP)\nWorks with classes and objects.\nA method is the functionality or procedure an object can carry out.\nData hiding is provided by encapsulation, which protects an object’s private\ndata from outside access. No object should be allowed to, or have the need to,\naccess another object’s internal data or processes.\n\nAbstraction\nThe capability to suppress unnecessary details so the important, inherent\nproperties can be examined and reviewed.\n\nPolymorphism\nTwo objects can receive the same input and have different outputs.\n\nData modeling\nConsiders data independently of the way the data are processed and of the\ncomponents that process the data. A process used to define and analyze data\nrequirements needed to support the business processes.\n\nCohesion\nA measurement that indicates how many different types of tasks a module needs to\ncarry out.\n\nCoupling\nA measurement that indicates how much interaction one module requires for\ncarrying out its tasks.\n\nData structure\nA representation of the logical relationship between elements of data.\n\nDistributed Computing Environment (DCE)\nThe first framework and development toolkit for developing client/server\napplications to allow for distributed computing.\n\nCommon Object Request Broker Architecture (CORBA)\nOpen objectoriented standard architecture developed by the Object Management\nGroup (OMG). The standards enable software components written in different\ncomputer languages and running on different systems to communicate.\n\nObject request broker (ORB)\nManages all communications between components and enables them to interact in a\nheterogeneous and distributed environment. The ORB acts as a “broker” between a\nclient request for a service from a distributed object and the completion of\nthat request.\n\nComponent Object Model (COM)\nA model developed by Microsoft that allows for interprocess communication\nbetween applications potentially written in different programming languages on\nthe same computer system.\n\nObject linking and embedding (OLE)\nProvides a way for objects to be shared on a local computer and to use COM as\ntheir foundation. It is a technology developed by Microsoft that allows\nembedding and linking to documents and other objects.\n\nJava Platform, Enterprise Edition (J2EE)\nIs based upon the Java programming language, which allows a modular approach to\nprogramming code with the goal of interoperability. J2EE defines a client/server\nmodel that is object oriented and platform independent.\n\nService-oriented architecture (SOA)\nProvides standardized access to the most needed services to many different\napplications at one time. Service interactions are self-contained and loosely\ncoupled, so that each interaction is independent of any other interaction.\n\nSimple Object Access Protocol (SOAP)\nAn XML-based protocol that encodes messages in a web service environment.\n\nMashup\nThe combination of functionality, data, and presentation capabilities of two or\nmore sources to provide some type of new service or functionality.\n\nSoftware as a Service (SAAS)\nA software delivery model that allows applications and data to be centrally\nhosted and accessed by thin clients, commonly web browsers. A common delivery\nmethod of cloud computing.\n\nCloud computing\nA method of providing computing as a service rather than as a physical product.\nIt is Internet-based computing, whereby shared resources and software are\nprovided to computers and other devices on demand.\n\nMobile code\nCode that can be transmitted across a network, to be executed by a system or\ndevice on the other end.\n\nJava applets\nSmall components (applets) that provide various functionalities and are\ndelivered to users in the form of Java bytecode. Java applets can run in a web\nbrowser using a Java Virtual Machine (JVM). Java is platform independent; thus,\nJava applets can be executed by browsers for many platforms.\n\nSandbox\nA virtual environment that allows for very fine-grained control over the actions\nthat code within the machine is permitted to take. This is designed to allow\nsafe execution of untrusted code from remote sources.\n\nActiveX\nA Microsoft technology composed of a set of OOP technologies and tools based on\nCOM and DCOM. It is a framework for defining reusable software components in a\nprogramming language–independent manner.\n\nAuthenticode\nA type of code signing, which is the process of digitally signing software\ncomponents and scripts to confirm the software author and guarantee that the\ncode has not been altered or corrupted since it was digitally signed.\nAuthenticode is Microsoft’s implementation of code signing.\n\nThreats for Web Environments\nInformation gathering\nUsually the first step in an attacker’s methodology, in which the information\ngathered may allow an attacker to infer additional information that can be used\nto compromise systems.\n\nServer side includes (SSI)\nAn interpreted server-side scripting language used almost exclusively for\nweb-based communication. It is commonly used to include the contents of one or\nmore files into a web page on a web server. Allows web developers to reuse\ncontent by inserting the same content into multiple web documents.\n\nClient-side validation\n * Input validation is done at the client before it is even sent back to the\n   server to process.\n * Path or directory traversal - Attack is also known as the “dot dot slash”\n   because it is perpetrated by inserting the characters “../” several times\n   into a URL to back up or traverse into directories that were not supposed to\n   be accessible from the Web.\n * Unicode encoding - An attacker using Unicode could effectively make the same\n   directory traversal request without using “/” but with any of the Unicode\n   representations of that character (three exist: %c1%1c, %c0%9v, and %c0%af)\n * URL Encoding - %20% = a space\n\nCross-site scripting (XSS)\n * An attack where a vulnerability is found on a web site that allows an\n   attacker to inject malicious code into a web application.\n * There are three different XSS vulnerabilities: * Nonpersistent XSS\n      vulnerabilities, or reflected vulnerabilities, occur when an attacker\n      tricks the victim into processing a URL programmed with a rogue script to\n      steal the victim’s sensitive information (cookie, session ID, etc.). The\n      principle behind this attack lies in exploiting the lack of proper input\n      or output validation on dynamic web sites.\n    * Persistent XSS\n      vulnerabilities, also known as stored or second order vulnerabilities, are\n      generally targeted at web sites that allow users to input data which are\n      stored in a database or any other such location, e.g., forums, message\n      boards, guest books, etc. The attacker posts some text that contains some\n      malicious JavaScript, and when other users later view the posts, their\n      browsers render the page and execute the attackers JavaScript.\n    * DOM (Document Object\n      Model)–based XSS vulnerabilities are also referred to as local cross-site\n      scripting. DOM is the standard structure layout to represent HTML and XML\n      documents in the browser. In such attacks the document components such as\n      form fields and cookies can be referenced through JavaScript. The attacker\n      uses the DOM environment to modify the original client-side JavaScript.\n      This causes the victim’s browser to execute the resulting abusive\n      JavaScript code.\n   \n   \n\nParameter validation\nThe values that are being received by the application are validated to be within\ndefined limits before the server application processes them within the system.\n\nWeb proxy\nA piece of software installed on a system that is designed to intercept all\ntraffic between the local web browser and the web server.\n\nReplay attack\nAn attacker capturing the traffic from a legitimate session and replaying it\nwith the goal of masquerading an authenticated user.\n\nDatabase Management Software\nA database is a collection of data stored in a meaningful way that enables\nmultiple users and applications to access, view, and modify data as needed.\n\nAny type of database should have the following characteristics:\n\n * It centralizes by not having data held on several different servers\n   throughout the network.\n * It allows for easier backup procedures.\n * It provides transaction persistence.\n * It allows for more consistency since all the data are held and maintained in\n   one central location.\n * It provides recovery and fault tolerance.\n * It allows the sharing of data with multiple users.\n * It provides security controls that implement integrity checking, access\n   control, and the necessary level of confidentiality.\n\nTransaction persistence means the database procedures carrying out transactions\nare durable and reliable. The state of the database’s security should be the\nsame after a transaction has occurred, and the integrity of the transaction\nneeds to be ensured.\n\nDatabase Models\n * Relational: uses attributes (columns) and tuples (rows) to contain and\n   organize information. It presents information in the form of tables.\n * Hierarchical: combines records and fields that are related in a logical tree\n   structure. The structure and relationship between the data elements\n   are\tdifferent from those in a relational database. In the hierarchical\n   database the parents can have one child, many children, or no children. The\n   most commonly used implementation of the hierarchical model is in the\n   Lightweight Directory Access Protocol (LDAP) model.\n * Network: allows each data element to have multiple parent and child records.\n   This forms a redundant network-like structure instead of a strict tree\n   structure.\n * Object-oriented: is designed to handle a variety of data types (images,\n   audio, documents, video).\n * Object-relational: - a relational database with a software front end that is\n   written in an object-oriented programming language.\n * Record: A collection of related data items.\n * File: A collection of records of the same type.\n * Database: A cross-referenced collection of data.\n * DBMS: Manages and controls the database.\n * Tuple: A row in a two-dimensional database.\n * Attribute: A column in a two-dimensional database.\n * Primary key: Columns that make each row unique. (Every row of a table must\n   include a primary key.)\n * View: A virtual relation defined by the database administrator in order to\n   keep subjects from viewing certain data.\n * Foreign key: An attribute of one table that is related to the primary key of\n   another table.\n * Cell: An intersection of a row and a column.\n * Schema: Defines the structure of the database.\n * Data dictionary: Central repository of data elements and their relationships.\n * Rollback: An operation that ends a current transaction and cancels all the\n   recent changes to the database until the previous checkpoint/commit point.\n * Two-phase commit: A mechanism that is another control used in databases to\n   ensure the integrity of the data held within the database.\n * Cell suppression: A technique used to hide specific cells that contain\n   sensitive information.\n * Noise and perturbation: A technique of inserting bogus information in the\n   hopes of misdirecting an attacker or confusing the matter enough that the\n   actual attack will not be fruitful.\n * Data warehousing: Combines data from multiple databases or data sources into\n   a large database for the purpose of providing more extensive information\n   retrieval and data analysis.\n * Data mining: The process of massaging the data held in the data warehouse\n   into more useful information.\n\nDatabase Programming Interfaces\n * Open Database Connectivity (ODBC) An API that allows an application to\n   communicate with a database, either locally or remotely\n * Object Linking and Embedding Database (OLE DB) Separates data into components\n   that run as middleware on a client or server. It provides a lowlevel\n   interface to link information across different databases, and provides access\n   to data no matter where they are located or how they are formatted.\n * ActiveX Data Objects (ADO) An API that allows applications to access back-end\n   database systems. It is a set of ODBC interfaces that exposes the\n   functionality of data sources through accessible objects. ADO uses the OLE DB\n   interface to connect with the database, and can be developed with many\n   different scripting languages.\n * Java Database Connectivity (JDBC) An API that allows a Java application to\n   communicate with a database. The application can bridge through ODBC or\n   directly to the database.\n\nData definition language (DDL)\nDefines the structure and schema of the database.\n\nData manipulation language (DML)\nContains all the commands that enable a user to view, manipulate, and use the\ndatabase (view, add, modify, sort, and delete commands).\n\nQuery language (QL)\nEnables users to make requests of the database.\n\nIntegrity:\nDatabase software performs three main types of integrity services: semantic,\nreferential, and entity.\n\n * A semantic integrity mechanism makes sure structural and semantic rules are\n   enforced. These rules pertain to data types, logical values, uniqueness\n   constraints, and operations that could adversely affect the structure of the\n   database.\n * A database has referential integrity if all foreign keys reference existing\n   primary keys. There should be a mechanism in place that ensures no foreign\n   key contains a reference to a primary key of a nonexisting record, or a null\n   value.\n * Entity integrity guarantees that the tuples are uniquely identified by\n   primary key values.\n\nPolyinstantiation\nThis enables a table that contains multiple tuples with the same primary keys,\nwith each instance distinguished by a security level.\n\nOnline transaction processing (OLTP)\nUsed when databases are clustered to provide fault tolerance and higher\nperformance. The main goal of OLTP is to ensure that transactions happen\nproperly or they don’t happen at all.\n\nThe ACID test:\n * Atomicity: Divides transactions into units of work and ensures that all\n   modifications take effect or none takes effect. Either the changes are\n   committed or the database is rolled back.\n * Consistency: A transaction must follow the integrity policy developed for\n   that particular database and ensure all data are consistent in the different\n   databases.\n * Isolation: Transactions execute in isolation until completed, without\n   interacting with other transactions. The results of the modification are not\n   available until the transaction is completed.\n * Durability: Once the transaction is verified as accurate on all systems, it\n   is committed and the databases cannot be rolled back.\n\nExpert systems\nUse artificial intelligence (AI) to solve complex problems. They are systems\nthat emulate the decision-making ability of a human expert.\n\nInference engine\nA computer program that tries to derive answers from a knowledge base. It is the\n“brain” that expert systems use to reason about the data in the knowledge base\nfor the ultimate purpose of formulating new conclusions.\n\nRule-based programming\nA common way of developing expert systems, with rules based on if-then logic\nunits, and specifying a set of actions to be performed for a given situation.\n\nArtificial neural network (ANN)\nA mathematical or computational model based on the neural structure of the\nbrain.\n\nMalware Types:\nVirus\nA small application, or string of code, that infects host applications. It is a\nprogramming code that can replicate itself and spread from one system to\nanother.\n\nMacro virus\nA virus written in a macro language and that is platform independent. Since many\napplications allow macro programs to be embedded in documents, the programs may\nbe run automatically when the document is opened. This provides a distinct\nmechanism by which viruses can be spread.\n\nCompression viruses\nAnother type of virus that appends itself to executables on the system and\ncompresses them by using the user’s permissions.\n\nStealth virus\nA virus that hides the modifications it has made. The virus tries to trick\nantivirus software by intercepting its requests to the operating system and\nproviding false and bogus information.\n\nPolymorphic virus\nProduces varied but operational copies of itself. A polymorphic virus may have\nno parts that remain identical between infections, making it very difficult to\ndetect directly using signatures.\n\nMultipart virus\nAlso called a multipartite virus, this has several components to it and can be\ndistributed to different parts of the system. It infects and spreads in multiple\nways, which makes it harder to eradicate when identified.\n\nSelf-garbling virus\nAttempts to hide from antivirus software by modifying its own code so that it\ndoes not match predefined signatures.\n\nMeme viruses\nThese are not actual computer viruses, but types of e-mail messages that are\ncontinually forwarded around the Internet.\n\nBots\nSoftware applications that run automated tasks over the Internet, which perform\ntasks that are both simple and structurally repetitive. Malicious use of bots is\nthe coordination and operation of an automated attack by a botnet (centrally\ncontrolled collection of bots).\n\nWorms\nThese are different from viruses in that they can reproduce on their own without\na host application and are self-contained programs.\n\nLogic bomb\nExecutes a program, or string of code, when a certain event happens or a date\nand time arrives.\n\nRootkit\nSet of malicious tools that are loaded on a compromised system through stealthy\ntechniques. The tools are used to carry out more attacks either on the infected\nsystems or surrounding systems.\n\nTrojan horse\nA program that is disguised as another program with the goal of carrying out\nmalicious activities in the background without the user knowing.\n\nRemote access Trojans (RATs)\nMalicious programs that run on systems and allow intruders to access and use a\nsystem remotely.\n\nImmunizer\nAttaches code to the file or application, which would fool a virus into\n“thinking” it was already infected.\n\nBehavior blocking\nAllowing the suspicious code to execute within the operating system and watches\nits interactions with the operating system, looking for suspicious activities.\n\nSecurity Architecture and Design\nStakeholders for a system are the users, operators, maintainers, developers, and\nsuppliers.\n\nComputer architecture encompasses all of the parts of a computer system that are\nnecessary for it to function, including the operating system, memory chips,\nlogic circuits, storage devices, input and output devices, security components,\nbuses, and networking interfaces.\n\nISO/IEC 42010:2007\nInternational standard that provides guidelines on how to create and maintain\nsystem architectures.\n\nCentral processing unit (CPU)\nCarries out the execution of instructions within a computer.\n\nArithmetic logic unit (ALU)\nComponent of the CPU that carries out logic and mathematical functions as they\nare laid out in the programming code being processed by the CPU.\n\nRegister\nSmall, temporary memory storage units integrated and used by the CPU during its\nprocessing functions.\n\nControl unit\nPart of the CPU that oversees the collection of instructions and data from\nmemory and how they are passed to the processing components of the CPU.\n\nGeneral registers\nTemporary memory location the CPU uses during its processes of executing\ninstructions. The ALU’s “scratch pad” it uses while carrying out logic and math\nfunctions.\n\nSpecial registers\nTemporary memory location that holds critical processing parameters. They hold\nvalues as in the program counter, stack pointer, and program status word.\n\nProgram counter\nHolds the memory address for the following instructions the CPU needs to act\nupon.\n\nStack\nMemory segment used by processes to communicate instructions and data to each\nother.\n\nProgram status word\nCondition variable that indicates to the CPU what mode (kernel or user)\ninstructions need to be carried out in.\n\nUser mode (problem state)\nProtection mode that a CPU works within when carrying out less trusted process\ninstructions.\n\nKernel mode (supervisory state, privilege mode)\nMode that a CPU works within when carrying out more trusted process\ninstructions. The process has access to more computer resources when working in\nkernel versus user mode.\n\nAddress bus\nPhysical connections between processing components and memory segments used to\ncommunicate the physical memory addresses being used during processing\nprocedures.\n\nData bus\nPhysical connections between processing components and memory segments used to\ntransmit data being used during processing procedures.\n\nSymmetric mode multiprocessing\nWhen a computer has two or more CPUs and each CPU is being used in a\nload-balancing method.\n\nAsymmetric mode multiprocessing\nWhen a computer has two or more CPUs and one CPU is dedicated to a specific\nprogram while the other CPUs carry out general processing procedures.\n\nProcess\nProgram loaded in memory within an operating system.\n\nMultiprogramming\nInterleaved execution of more than one program (process) or task by a single\noperating system.\n\nMultitasking\nSimultaneous execution of more than one program (process) or task by a single\noperating system.\n\nCooperative multitasking\nMultitasking scheduling scheme used by older operating systems to allow for\ncomputer resource time slicing. Processes had too much control over resources,\nwhich would allow for the programs or systems to “hang.”\n\nPreemptive multitasking\nMultitasking scheduling scheme used by operating systems to allow for computer\nresource time slicing. Used in newer, more stable operating systems.\n\nProcess states (ready, running, blocked)\nProcesses can be in various activity levels. Ready = waiting for input. Running\n= instructions being executed by CPU. Blocked = process is “suspended.”\n\nInterrupts\nValues assigned to computer components (hardware and software) to allow for\nefficient computer resource time slicing.\n\nMaskable interrupt\nInterrupt value assigned to a noncritical operating system activity.\n\nNonmaskable interrupt\nInterrupt value assigned to a critical operating system activity.\n\nThread\nInstruction set generated by a process when it has a specific activity that\nneeds to be carried out by an operating system. When the activity is finished,\nthe thread is destroyed.\n\nMultithreading\nApplications that can carry out multiple activities simultaneously by generating\ndifferent instruction sets (threads).\n\nSoftware deadlock\nTwo processes cannot complete their activities because they are both waiting for\nsystem resources to be released.\n\nProcess isolation\nProtection mechanism provided by operating systems that can be implemented as\nencapsulation, time multiplexing of shared resources, naming distinctions, and\nvirtual memory mapping.\n\nWhen a process is encapsulated, no other process understands or interacts with\nits internal programming code.\n\nEncapsulation provides data hiding, which means that outside software components\nwill not know how a process works and will not be able to manipulate the\nprocess’s internal code. This is an integrity mechanism and enforces modularity\nin programming code.\n\nTime multiplexing\nA technology that allows processes to use the same resources.\n\nVirtual address mapping\nAllows the different processes to have their own memory space. The memory\nmanager ensures no processes improperly interact with another process’s memory.\nThis provides integrity and confidentiality for the individual processes and\ntheir data and an overall stable processing environment for the operating\nsystem.\n\nThe goals of memory management are to:\n\n * Provide an abstraction level for programmers\n * Maximize performance with the limited amount of memory available\n * Protect the operating system and applications loaded into memory\n\nThe memory manager has five basic responsibilities:\n\n * Relocation\n   \n    * Swap contents from RAM to the hard drive as needed\n    * Provide pointers for applications if their instructions and memory segment\n      have been moved to a different location in main memory\n   \n   \n * Protection\n   \n    * Limit processes to interact only with the memory segments assigned to them\n    * Provide access control to memory segments\n   \n   \n * Sharing\n   \n    * Use complex controls to ensure integrity and confidentiality when\n      processes need to use the same shared memory segments\n    * Allow many users with different levels of access to interact with the same\n      application running in one memory segment\n   \n   \n * Logical organization\n   \n    * Segment all memory types and provide an addressing scheme for each at an\n      abstraction level\n    * Allow for the sharing of specific software modules, such as dynamic link\n      library (DLL) procedures\n   \n   \n * Physical organization\n   \n    * Segment the physical memory space for application and operating system\n      processes\n   \n   \n\nDynamic link libraries (DLLs)\nA set of subroutines that are shared by different applications and operating\nsystem processes.\n\nBase registers\nBeginning of address space assigned to a process. Used to ensure a process does\nnot make a request outside its assigned memory boundaries.\n\nLimit registers\nEnding of address space assigned to a process. Used to ensure a process does not\nmake a request outside its assigned memory boundaries.\n\nMemory Protection Issues:\n * Every address reference is validated for protection.\n * Two or more processes can share access to the same segment with potentially\n   different access rights.\n * Different instruction and data types can be assigned different levels of\n   protection.\n * Processes cannot generate an unpermitted address or gain access to an\n   unpermitted segment\n\nRAM\nMemory sticks that are plugged into a computer’s motherboard and work as\nvolatile memory space for an operating system.\n\nAdditional types of RAM you should be familiar with:\n\n * Synchronous DRAM (SDRAM) Synchronizes itself with the system’s CPU and\n   synchronizes signal input and output on the RAM chip. It coordinates its\n   activities with the CPU clock so the timing of the CPU and the timing of the\n   memory activities are synchronized. This increases the speed of transmitting\n   and executing data.\n * Extended data out DRAM (EDO DRAM) This is faster than DRAM because DRAM can\n   access only one block of data at a time, whereas EDO DRAM can capture the\n   next block of data while the first block is being sent to the CPU for\n   processing. It has a type of “look ahead” feature that speeds up memory\n   access.\n * Burst EDO DRAM (BEDO DRAM) Works like (and builds upon) EDO DRAM in that it\n   can transmit data to the CPU as it carries out a read option, but it can send\n   more data at once (burst). It reads and sends up to four memory addresses in\n   a small number of clock cycles.\n * Double data rate SDRAM (DDR SDRAM) Carries out read operations on the rising\n   and falling cycles of a clock pulse. So instead of carrying out one operation\n   per clock cycle, it carries out two and thus can deliver twice the throughput\n   of SDRAM. Basically, it doubles the speed of memory activities, when compared\n   to SDRAM, with a smaller number of clock cycles.\n\nThrashing\nWhen a computer spends more time moving data from one small portion of memory to\nanother than actually processing the data.\n\nROM\nNonvolatile memory that is used on motherboards for BIOS functionality and\nvarious device controllers to allow for operating system-to-device\ncommunication. Sometimes used for off-loading graphic rendering or cryptographic\nfunctionality.\n\nTypes of ROM:\n\n * Programmable read-only memory (PROM) is a form of ROM that can be modified\n   after it has been manufactured. PROM can be programmed only one time because\n   the voltage that is used to write bits into the memory cells actually burns\n   out the fuses that connect the individual memory cells.\n * Erasable programmable read-only memory (EPROM) can be erased, modified, and\n   upgraded. EPROM holds data that can be electrically erased or written to.\n * Flash memory is a special type of memory that is used in digital cameras,\n   BIOS chips, memory cards, and video game consoles. It is a solid-state\n   technology, meaning it does not have moving parts and is used more as a type\n   of hard drive than memory.\n\nHardware segmentation\nPhysically mapping software to individual memory segments.\n\nCache memory\nFast and expensive memory type that is used by a CPU to increase read and write\noperations.\n\nAbsolute addresses\nHardware addresses used by the CPU.\n\nLogical addresses\nIndirect addressing used by processes within an operating system. The memory\nmanager carries out logical-to-absolute address mapping.\n\nStack\nMemory construct that is made up of individually addressable buffers.\nProcess-to-process communication takes place through the use of stacks.\n\nBuffer overflow\nToo much data is put into the buffers that make up a stack. Common attack vector\nused by hackers to run malicious code on a target system.\n\nBounds checking\nEnsuring the inputted data are of an acceptable length.\n\nAddress space layout randomization (ASLR)\nMemory protection mechanism used by some operating systems. The addresses used\nby components of a process are randomized so that it is harder for an attacker\nto exploit specific memory vulnerabilities.\n\nData execution prevention (DEP)\nMemory protection mechanism used by some operating systems. Memory segments may\nbe marked as nonexecutable so that they cannot be misused by malicious software.\n\nGarbage collector\nTool that marks unused memory segments as usable to ensure that an operating\nsystem does not run out of memory. Used to protect against memory leaks.\n\nVirtual memory\nCombination of main memory (RAM) and secondary memory within an operating\nsystem.\n\nInterrupt\nSoftware or hardware signal that indicates that system resources (i.e., CPU) are\nneeded for instruction processing.\n\nProgrammable I/O\nThe CPU sends data to an I/O device and polls the device to see if it is ready\nto accept more data.\n\nInterrupt-driven I/O\nThe CPU sends a character over to the printer and then goes and works on another\nprocess’s request.\n\nI/O using Direct memory access (DMA)\nA way of transferring data between I/O devices and the system’s memory without\nusing the CPU.\n\nPremapped I/O\nThe CPU sends the physical memory address of the requesting process to the I/O\ndevice, and the I/O device is trusted enough to interact with the contents of\nmemory directly, so the CPU does not control the interactions between the I/O\ndevice and memory.\n\nFully Mapped I/O\nUnder fully mapped I/O, the operating system does not trust the I/O device.\n\nInstruction set\nSet of operations and commands that can be implemented by a particular processor\n(CPU).\n\nMicroarchitecture\nSpecific design of a microprocessor, which includes physical components\n(registers, logic gates, ALU, cache, etc.) that support a specific instruction\nset.\n\nRing-based architecture (Protection Rings)\nMechanisms to protect data and functionality from faults (by improving fault\ntolerance) and malicious behaviour through two or more hierarchical levels or\nlayers of privilege within the architecture of a computer system\n\nApplication programming interface\nSoftware interface that enables process-to-process interaction. Common way to\nprovide access to standard routines to a set of software programs.\n\nMonolithic operating system architecture\nAll of the code of the operating system working in kernel mode in an ad hoc and\nnonmodularized manner.\n\nLayered operating system architecture\nArchitecture that separates system functionality into hierarchical layers.\n\nData hiding\nUse of segregation in design decisions to protect software components from\nnegatively interacting with each other. Commonly enforced through strict\ninterfaces.\n\nMicrokernel architecture\nReduced amount of code running in kernel mode carrying out critical operating\nsystem functionality. Only the absolutely necessary code runs in kernel mode,\nand the remaining operating system code runs in user mode.\n\nHybrid microkernel architecture\nCombination of monolithic and microkernel architectures. The microkernel carries\nout critical operating system functionality, and the remaining functionality is\ncarried out in a client\\server model within kernel mode.\n\nMode transition\nWhen the CPU has to change from processing code in user mode to kernel mode.\nThis is a protection measure, but it causes a performance hit.\n\nVirtualization\nCreation of a simulated environment (hardware platform, operating system,\nstorage, etc.) that allows for central control and scalability.\n\nHypervisor\nCentral program used to manage virtual machines (guests) within a simulated\nenvironment (host).\n\nSecurity policy\nStrategic tool used to dictate how sensitive information and resources are to be\nmanaged and protected.\n\nTrusted computing base\nA collection of all the hardware, software, and firmware components within a\nsystem that provide security and enforce the system’s security policy.\n\nTrusted path\nTrustworthy software channel that is used for communication between two\nprocesses that cannot be circumvented.\n\nSecurity perimeter\nMechanism used to delineate between the components within and outside of the\ntrusted computing base.\n\nReference monitor\nConcept that defines a set of design requirements of a reference validation\nmechanism (security kernel), which enforces an access control policy over\nsubjects’ (processes, users) ability to perform operations (read, write,\nexecute) on objects (files, resources) on a system.\n\nSecurity kernel\nHardware, software, and firmware components that fall within the TCB and\nimplement and enforce the reference monitor concept.\n\nThe security kernel has three main requirements:\n\n * It must provide isolation for the processes carrying out the reference\n   monitor concept, and the processes must be tamperproof.\n * It must be invoked for every access attempt and must be impossible to\n   circumvent. Thus, the security kernel must be implemented in a complete and\n   foolproof way.\n * It must be small enough to be tested and verified in a complete and\n   comprehensive manner.\n\nMultilevel security policies\nOutlines how a system can simultaneously process information at different\nclassifications for users with different clearance levels.\n\nState machine models\n * To verify the security of a system, the state is used, which means that all\n   current permissions and all current instances of subjects accessing objects\n   must be captured.\n * A system that has employed a state machine model will be in a secure state in\n   each and every instance of its existence.\n\nSecurity Models\nBell-LaPadula model:\nThis is the first mathematical model of a multilevel security policy that\ndefines the concept of a secure state and necessary modes of access. It ensures\nthat information only flows in a manner that does not violate the system policy\nand is confidentiality focused.\n\n * The simple security rule * A subject cannot read data at a higher security\n      level (no read up).\n   \n   \n * The *-property rule * A subject cannot write to an object at a lower security\n      level (no write down).\n   \n   \n * The strong star property rule * A subject can perform read and write\n      functions only to the objects at its same security level.\n   \n   \n\nBiba model\nA formal state transition model that describes a set of access control rules\ndesigned to ensure data integrity.\n\n * The simple integrity axiom A subject cannot read data at a lower integrity\n   level (no read down).\n * The *-integrity axiom A subject cannot modify an object in a higher integrity\n   level (no write up).\n\nClark-Wilson model\nThis integrity model is implemented to protect the integrity of data and to\nensure that properly formatted transactions take place. It addresses all three\ngoals of integrity:\n\n * Subjects can access objects only through authorized programs (access triple).\n * Separation of duties is enforced.\n * Auditing is required.\n\nBrewer and Nash model\nThis model allows for dynamically changing access controls that protect against\nconflicts of interest. Also known as the Chinese Wall model.\n\nSecurity Modes\nDedicated Security Mode All users must have:\n\n * Proper clearance for all information on the system\n * Formal access approval for all information on the system\n * A signed NDA for all information on the system\n * A valid need-to-know for all information on the system\n * All users can access all data.\n\nSystem High-Security Mode All users must have:\n\n * Proper clearance for all information on the system\n * Formal access approval for all information on the system\n * A signed NDA for all information on the system\n * A valid need-to-know for some information on the system\n * All users can access some data, based on their need-to-know.\n\nCompartmented Security Mode All users must have:\n\n * Proper clearance for the highest level of data classification on the system\n * Formal access approval for some information on the system\n * A signed NDA for all information they will access on the system\n * A valid need-to-know for some of the information on the system\n * All users can access some data, based on their need-to-know and formal access\n   approval.\n\nMultilevel Security Mode All users must have:\n\n * Proper clearance for some of the information on the system\n * Formal access approval for some of the information on the system\n * A signed NDA for all information on the system\n * A valid need-to-know for some of the information on the system\n * All users can access some data, based on their need-to-know, clearance, and\n   formal access approval.\n\nThe Common Criteria\nUnder the Common Criteria model, an evaluation is carried out on a product and\nit is assigned an Evaluation Assurance Level (EAL).\n\n * EAL1 Functionally tested\n * EAL2 Structurally tested\n * EAL3 Methodically tested and checked\n * EAL4 Methodically designed, tested, and reviewed\n * EAL5 Semiformally designed and tested\n * EAL6 Semiformally verified design and tested\n * EAL7 Formally verified design and tested\n\nThe Common Criteria uses protection profiles in its evaluation process.\n\nA protection profile contains the following five sections:\n\n * Descriptive elements * Provides the name of the profile and a description of\n      the security problem to be solved.\n   \n   \n * Rationale * Justifies the profile and gives a more detailed description of\n      the real-world problem to be solved. The environment, usage assumptions,\n      and threats are illustrated along with guidance on the security policies\n      that can be supported by products and systems that conform to this\n      profile.\n   \n   \n * Functional requirements * Establishes a protection boundary, meaning the\n      threats or compromises within this boundary to be countered. The product\n      or system must enforce the boundary established in this section.\n   \n   \n * Development assurance requirements * Identifies the specific requirements the\n      product or system must meet during the development phases, from design to\n      implementation.\n   \n   \n * Evaluation assurance requirements * Establishes the type and intensity of the\n      evaluation.\n   \n   \n\nSystems Evaluation Methods\n * In the Trusted Computer System Evaluation Criteria (TCSEC), commonly known as\n   the Orange Book, the lower assurance level ratings look at a system’s\n   protection mechanisms and testing results to produce an assurance rating, but\n   the higher assurance level ratings look more at the system design,\n   specifications, development procedures, supporting documentation, and testing\n   results.\n * An assurance evaluation examines the security-relevant parts of a system,\n   meaning the TCB, access control mechanisms, reference monitor, kernel, and\n   protection mechanisms. The relationship and interaction between these\n   components are also evaluated.\n * The Orange Book was used to evaluate whether a product contained the security\n   properties the vendor claimed it did and whether the product was appropriate\n   for a specific application or function.\n * TCSEC addresses confidentiality, but not integrity. Functionality of the\n   security mechanisms and the assurance of those mechanisms are not evaluated\n   separately, but rather are combined and rated as a whole\n * TCSEC provides a classification system that is divided into hierarchical\n   divisions of assurance levels: * A. Verified protection\n    * B. Mandatory protection\n    * B1: Labeled Security * Each data object must contain a classification\n         label and each subject must have a clearance label. When a subject\n         attempts to access an object,\n         the system must compare the subject’s and object’s security labels to\n         ensure the requested actions are acceptable. Data leaving the system\n         must also contain an accurate\n         security label. The security policy is based on an informal statement,\n         and the design specifications are reviewed and verified.\n         B2: Structured Protection\n       * The security policy is clearly defined and\n         documented, and the system design and implementation are subjected to\n         more thorough review and testing procedures. This class requires more\n         stringent authentication mechanisms and well-defined interfaces among\n         layers. Subjects and devices require labels, and the system must not\n         allow covert channels.\n         B3: Security Domains\n       * In this class, more granularity is provided in each\n         protection mechanism, and the programming code that is not necessary to\n         support the security policy is excluded.\n      \n      \n    * C. Discretionary protection\n    * C1: Discretionary Security Protection * Discretionary access control is\n         based on individuals and/or groups. It requires a separation of users\n         and information, and identification and authentication of individual\n         entities.\n      \n      \n    * C2: Controlled Access Protection * Users need to be identified\n         individually to provide more precise access control and auditing\n         functionality. Logical access control mechanisms are used to enforce\n         authentication and the uniqueness of each individual’s identification.\n      \n      \n    * D. Minimal security\n    * D1: The classes with higher numbers offer a\n      greater degree of trust and assurance. The criteria breaks down into seven\n      different areas: * Security policy The policy must be explicit and well\n         defined and enforced by the mechanisms within the system.\n       * Identification Individual subjects must be uniquely\n         identified.\n       * Labels Access control labels must be associated\n         properly with objects.\n       * Documentation Documentation must be provided, including\n         test, design, and specification documents, user guides, and manuals.\n       * Accountability Audit data must be captured and\n         protected to enforce accountability.\n       * Life-cycle assurance Software, hardware, and firmware\n         must be able to be tested individually to ensure that each enforces the\n         security policy in an effective manner throughout their lifetimes.\n       * Continuous protection The security mechanisms and the\n         system as a whole must perform predictably and acceptably in different\n         situations continuously.\n      \n      \n   \n   \n * The Red Book is referred to as the Trusted Network Interpretation (TNI). The\n   following is a brief overview of the security items addressed in the Red\n   Book: * Communication integrity\n    * Authentication Protects against masquerading and playback attacks.\n      Mechanisms include digital signatures, encryption, timestamp, and\n      passwords.\n    * Message integrity Protects the protocol header, routing information,\n      and packet payload from being modified. Mechanisms include message\n      authentication and encryption.\n    * Nonrepudiation Ensures that a sender cannot deny sending a message.\n      Mechanisms include encryption, digital signatures, and notarization.\n    * Denial-of-service prevention\n    * Continuity of operations Ensures that the network is available even\n      if attacked. Mechanisms include fault-tolerant and redundant systems and\n      the capability to reconfigure network parameters in case of an emergency.\n    * Network management Monitors network performance and identifies\n      attacks and failures. Mechanisms include components that enable network\n      administrators to monitor and restrict resource access.\n    * Compromise protection\n    * Data confidentiality Protects data from being accessed in an\n      unauthorized method during transmission. Mechanisms include access\n      controls, encryption, and physical protection of cables.\n    * Traffic flow confidentiality Ensures that unauthorized entities are\n      not aware of routing information or frequency of communication via traffic\n      analysis. Mechanisms include padding messages, sending noise, or sending\n      false messages.\n    * Selective routing Routes messages in a way to avoid specific threats.\n      Mechanisms include network configuration and routing tables.\n   \n   \n * The Information Technology Security Evaluation Criteria (ITSEC) was the first\n   attempt at establishing a single standard for evaluating security attributes\n   of computer systems and products by many European countries.\n * ITSEC evaluates two main attributes of a system’s protection mechanisms:\n   functionality and assurance.\n * A difference between ITSEC and TCSEC is that TCSEC bundles functionality and\n   assurance into one rating, whereas ITSEC evaluates these two attributes\n   separately.\n\nPenetration testing\nSteps for vulnerability assessment\n 1. Vulnerability scanning\n 2. Analysis\n 3. Communicate results\n\nPenetration test strategies\n•\tExternal testing\n•\tInternal testing\n•\tBlind testing\n•\tDouble-blind testing\n\nCategories of penetration testing\n•\tZero knowledge\n•\tPartial knowledge\n•\tFull knowledge\n\nPenetration test methodology\n 1. Reconnaissance\n 2. Enumeration\n 3. Vulnerability analysis\n 4. Execution / Exploitation\n 5. Document findings\n\nAccess Controls\nPrimary Concerns\n * Who owns the data?\n * Who consumes/uses the data?\n * Who shouldn't have access to the data?\n\nSecurity Concepts\n * Confidentiality\n * Integrity\n * Availability\n\nData control terms\n * Subject - A person / end user\n * Object - The data a subject is accessing\n\nDegrees of access control\n * Read-only - Can see, but cannot edit the data.\n * Contributor - Can read, and also modify/add data.\n\nOperational Terms\n 1. Identification - Determine the identity of a subject\n 2. Authentication - Validate a subject's identity\n 3. Authorization - Validate their access against a directory LDAP - Lightweight\n    Directory Access Protocol x.500 - LDAP standard\n 4. Accountability - Ensuring that the access controls are applied Logging /\n    Recording\n\nRace Condition\nWhen processes try to carry out whatever activity they are set to perform in an\nincorrect order.\ne.g. Authorizing before authenticating\n\nCookie\n * Stored in a text file\n * Credential information stored for repeated use\n * Allows web sites to track a user session across multiple pages.\n\nSingle-Sign-On (SSO)\n * Login once access many\n * Subjects login once and are then able to access objects without having to\n   re-authenticate\n\nUser provisioning\n * Creation of user accounts / subjects\n * Assignment of permissions and access at the time of creation\n * Can be automated (Especially for large organizations)\n\nFederated Identity\n * Trust between multiple systems\n * Subjects can access objects in separate systems\n\nMarkup Languages\n * XML - Extensible Markup Language\n * SPML - Service Provisioning Markup Language\n * SAML - Security Assertion Markup Language\n * SOAP - Simple Object Access Protocol\n\nBiometrics\nSomething we \"are\"\n\n * Fingerprints\n * Handprint\n * Signature Dynamic\n\nErrors\nType 1 - False Reject\n- Subject is incorrectly denied\n\nType 2 - False Accept\n- Subject is incorrectly authenticated\n\nCrossover Error Rate (CER)\n- Where the rate of Type 1 and Type 2 intersect\n\n\nBiometric Types\nFingerprints\n * Most common\n * Easily captured\n * Stored digitally, can be large\n * Easy to fool\n\nPalm scanning\nRetina scanning\n * Pattern of blood vessels in the eye\n\nIris scan\n * Color pattern surrounding the pupil\n * The most accurate\n * The most secure\n\nSignature Dynamics\n * Force\n * Inclination\n * Letter characteristics\n\nKeystroke Dynamics\n * Patterns used while typing\n\nVoice print\nFacial scan\nHand topography\n * Peaks and valleys on the hand\n\nHand geometry\n * Width of fingers\n\nPasswords\nSomething we \"know\"\n\n * Various character lengths\n * Alpha-numeric\n * Special character\n * String of characters identify subject\n * The most common method of authentication\n\nPassword Weaknesses\nElectronic monitoring\n * Sniffers\n * Password capture devices\n * Cain and Abel\n * Kismet\n\nAccessing the password file\n * Targeting the authentication server\n * Linux * /etc/shadow\n    * /etc/passwd\n   \n   \n * Windows * SAM Hive in the registry\n    * C:\\Windows\\Security\n    * C:\\Windows\\Security\\Database\n   \n   \n * Brute force attack * Attempting to \"guess\" the password\n    * l0pht Crack\n   \n   \n * Dictionary Attacks * Attempts passwords from a pre-compiled list of passwords\n   \n   \n * Social Engineering * Gather information by tricking employees\n   \n   \n * Rainbow tables * Collection of hash results\n   \n   \n\nClipping Level\n * Threshold\n * Measurement of a certain amount of an activity\n\nPassword Checker\n * Checks the strength and compliance of existing passwords\n\nPassword Cracker\n * Attempts to break/decrypt a password\n\nCognitive Password\n * Fact or Opinion Based\n * Mother's maiden name\n * First pet's name\n\nOne-time Password (Dynamic Password)\nPasswords that can only be used once\n\nToken Devices\n * Synchronous * Counter-synchronization\n    * Time-synchronization\n   \n   \n * Asynchronous * Challenge / Response\n   \n   \n\nPassphrase\nSentence instead of a word\n\n> \"I like the number 4!\"\n\n\nMemory Cards\nHold information with no processing capabilities\n\nSmartcards\n * Hold information and can process\n * Contact\n * Contactless\n * Attacks * Fault generation\n    * Side-channel Attacks\n    * Micro-probing\n   \n   \n\nKerberos\nSee MSDN Microsoft [http://msdn.microsoft.com/en-us/library/ff649429.aspx] and \nKerberos (RFC 1510) [ftp://ftp.isi.edu/in-notes/rfc1510.txt] and be aware it is\nsusceptible to time attacks\n[http://msdn.microsoft.com/en-us/library/ff647076.aspx].\n\nModels\nDiscretionary Access Control (DAC): Permissions through inheritance\n\n * User gains access through group membership Mandatory Access Control (MAC):\n   Owner of data mandates who has access.\n * Data must be classified (secret, top secret, SBU, etc).\n * Classified by owner\n * Sensitivity labels\n * Role-based Access Control (RBAC)\n * Backup administrator\n * Task based\n * Rule-based Access Control\n * in gateway or border device like a router\n\nConstrained user interface\n * Menu\n * Shell\n * Database views\n\nSecurity & Risk Management Glossary\nKey Terms\n * Availability: Reliable and timely access to data and resources is provided to\n   authorized individuals.\n * Integrity: Accuracy and reliability of the information and systems are\n   provided and any unauthorized modification is prevented.\n * Confidentiality: Necessary level of secrecy is enforced and unauthorized\n   disclosure is prevented.\n * Shoulder surfing: Viewing information in an unauthorized manner by looking\n   over the shoulder of someone else.\n * Social engineering: Gaining unauthorized access by tricking someone into\n   divulging sensitive information.\n * Vulnerability: Weakness or a lack of a countermeasure.\n * Threat agent: Entity that can exploit a vulnerability.\n * Threat: The danger of a threat agent exploiting a vulnerability.\n * Risk: The probability of a threat agent exploiting a vulnerability and the\n   associated impact.\n * Control: Safeguard that is put in place to reduce a risk, also called a\n   countermeasure.\n * Exposure: Presence of a vulnerability, which exposes the organization to a\n   threat.\n * Control types:\n   Administrative, technical (logical), and physical\n * Control functions: * Deterrent: Discourage a potential attacker\n    * Preventive: Stop an incident from occurring\n    * Corrective: Fix items after an incident has occurred\n    * Recovery: Restore necessary components to return to normal operations\n    * Detective: Identify an incident’s activities after it took place\n    * Compensating: Alternative control that provides similar protection as the\n      original control\n   \n   \n * Defense-in-depth: Implementation of multiple controls so that successful\n   penetration and compromise is more difficult to attain.\n * Security through obscurity: Relying upon the secrecy or complexity of an item\n   as its security, instead of practicing solid security practices.\n * ISO/IEC 27000 series: Industry-recognized best practices for the development\n   and management of an information security management system.\n * Zachman framework: Enterprise architecture framework used to define and\n   understand a business environment developed by John Zachman.\n * TOGAF: Enterprise architecture framework used to define and understand a\n   business environment developed by The Open Group.\n * SABSA: framework Risk-driven enterprise security architecture that maps to\n   business initiatives, similar to the Zachman framework.\n * DoDAF: U.S. Department of Defense architecture framework that ensures\n   interoperability of systems to meet military mission goals.\n * MODAF: Architecture framework used mainly in military support missions\n   developed by the British Ministry of Defence.\n * CobiT: Set of control objectives used as a framework for IT governance\n   developed by Information Systems Audit and Control Association (ISACA) and\n   the IT Governance Institute (ITGI).\n * NIST SP 800-53: Set of controls that are used to secure U.S. federal systems\n   developed by NIST.\n * COSO: Internal control model used for corporate governance to help prevent\n   fraud developed by the Committee of Sponsoring Organizations (COSO) of the\n   Treadway Commission.\n * ITIL: Best practices for information technology services management processes\n   developed by the United Kingdom’s Office of Government Commerce.\n * Six Sigma: Business management strategy developed by Motorola with the goal\n   of improving business processes.\n * Capability Maturity Model Integration (CMMI): Process improvement model\n   developed by Carnegie Mellon.\n * NIST SP 800-30: Risk Management Guide for Information Technology Systems A\n   U.S. federal standard that is focused on IT risks.\n * Facilitated Risk Analysis Process (FRAP): A focused, qualitative approach\n   that carries out pre-screening to save time and money.\n * Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE):\n   Team-oriented approach that assesses organizational and IT risks through\n   facilitated workshops.\n * AS/NZS 4360: Australia and New Zealand business risk management assessment\n   approach.\n * ISO/IEC 27005: International standard for the implementation of a risk\n   management program that integrates into an information security management\n   system (ISMS).\n * Failure Modes and Effect Analysis (FMEA): Approach that dissects a component\n   into its basic functions to identify flaws and those flaw's effects.\n * Fault tree analysis: Approach to map specific flaws to root causes in complex\n   systems.\n * CRAMM: Central Computing and Telecommunications Agency Risk Analysis and\n   Management Method.\n * Quantitative risk analysis: Assigning monetary and numeric values to all the\n   data elements of a risk assessment.\n * Qualitative risk analysis: Opinion-based method of analyzing risk with the\n   use of scenarios and ratings.\n * Single loss expectancy (SLE): One instance of an expected loss if a specific\n   vulnerability is exploited and how it affects a single asset. Asset Value ×\n   Exposure Factor = SLE.\n * Annualized loss expectancy (ALE): Annual expected loss if a specific\n   vulnerability is exploited and how it affects a single asset. SLE × ARO =\n   ALE.\n * Uncertainty analysis: Assigning confidence level values to data elements.\n * Delphi method: Data collection method that happens in an anonymous fashion.\n * Cost/benefit analysis: Calculating the value of a control. (ALE before\n   implementing a control) – (ALE after implementing a control) – (annual cost\n   of control) = value of control.\n * Functionality versus effectiveness of control: Functionality is what a\n   control does, and its effectiveness is how well the control does it.\n * Total risk: Full risk amount before a control is put into place. Threats ×\n   vulnerabilities × assets = total risk.\n * Residual risk: Risk that remains after implementing a control. Threats ×\n   vulnerabilities × assets × (control gap) = residual risk.\n * Accepted ways for handling risk: Accept, transfer, mitigate, avoid.\n * Policy: High-level document that outlines senior management’s security\n   directives.\n * Standard: Compulsory rules that support the security policies.\n * Guideline: Suggestions and best practices.\n * Procedures: Step-by-step implementation instructions.\n * Data owner: Individual responsible for the protection and classification of a\n   specific data set.\n * Data custodian: Individual responsible for implementing and maintaining\n   security controls to meet security requirements outlined by data owner.\n * Separation of duties: Preventive administrative control used to ensure one\n   person cannot carry out a critical task alone.\n * Collusion: Two or more people working together to carry out fraudulent\n   activities.\n * Rotation of duties: Detective administrative control used to uncover\n   potential fraudulent activities.\n * Mandatory vacation: Detective administrative control used to uncover\n   potential fraudulent activities by requiring a person to be away from the\n   organization for a period of time.\n * Access controls: are security features that control how users and systems\n   communicate and interact with other systems and resources.\n * Access: The flow of information between a subject and an object.\n * Subject: An active entity that requests access to an object or the data\n   within an object.\n * Object: Can be a computer, database, file, computer program, directory, or\n   field contained in a table within a database.\n * Race condition: When processes carry out their tasks on a shared resource in\n   an incorrect order.\n * User provisioning: The creation, maintenance, and deactivation of user\n   objects and attributes as they exist in one or more systems, directories, or\n   applications, in response to business processes.\n * Federated identity: A portable identity, and its associated entitlements,\n   that can be used across business boundaries.\n * Security Assertion Markup Language (SAML): An XML standard that allows the\n   exchange of authentication and authorization data to be shared between\n   security domains.\n * Service Provisioning Markup Language (SPML): Allows for the automation of\n   user management (account creation, amendments, revocation) and access\n   entitlement configuration related to electronically published services across\n   multiple provisioning systems.\n * Simple Object Access Protocol (SOAP): SOAP is a specification that outlines\n   how information pertaining to web services is exchanged in a structured\n   manner.\n * Type I error: When a biometric system rejects an authorized individual (false\n   rejection rate).\n * Type II error: When the system accepts impostors who should be rejected(false\n   acceptance rate).\n * Clipping Level: A threshold.\n * Cognitive passwords: Fact or opinion based information used to verify an\n   individual’s identity.\n * Asynchronous token–generating method: Employs a challenge/response scheme to\n   authenticate the user.\n * Synchronous token device: Synchronizes with the authentication service by\n   using time or a counter as the core piece of the authentication process. If\n   the\n   synchronization is time-based, the token device and the authentication\n   service must hold the same time within their internal clocks.\n * Memory card: Holds information but cannot process information.\n * Smart card: Holds information and has the necessary hardware and software to\n   actually process that information.\n * Side-channel attacks: Non-intrusive and are used to uncover sensitive\n   information about how a component works, without trying to compromise any\n   type of flaw or\n   weakness.\n * Security domain: Resources within this logical structure (domain) are working\n   under the same security policy and managed by the same group.\n * Access Control Model: An access control model is a framework that dictates\n   how subjects access objects.\n * Access Control Matrix: A table of subjects and objects indicating what\n   actions individual subjects can take upon individual objects.\n * Capability Table: A capability table specifies the access rights a certain\n   subject possesses pertaining to specific objects. A capability table is\n   different from an ACL because the subject is bound to the capability table,\n   whereas the object is bound to the ACL.\n * Content-based access: Bases access decisions on the sensitivity of the data,\n   not solely on subject identity.\n * Context-based access: Bases access decisions on the state of the situation,\n   not solely on identity or content sensitivity.\n * Restricted interface: Limits the user’s environment within the system, thus\n   limiting access to objects.\n * Rule-based access: Restricts subject's access attempts by predefined rules.\n * Remote Authentication Dial-In User Service (RADIUS): A network protocol that\n   provides client/server authentication and authorization, and audits remote\n   users.\n * Central processing unit (CPU): A silicon component made up of integrated\n   chips with millions of transistors that carry out the execution of\n   instructions within a computer.\n * Arithmetic logic unit (ALU): Component of the CPU that carries out logic and\n   mathematical functions as they are laid out in the programming code being\n   processed by the CPU.\n * Register: Small, temporary memory storage units integrated and used by the\n   CPU during its processing functions.\n * Control unit: Part of the CPU that oversees the collection of instructions\n   and data from memory and how they are passed to the processing components of\n   the CPU.\n * General registers: Temporary memory location the CPU uses during its\n   processes of executing instructions. The ALU’s “scratch pad” it uses while\n   carrying out logic and math functions.\n * Special registers: Temporary memory location that holds critical processing\n   parameters. They hold values as in the program counter, stack pointer, and\n   program status word.\n * Program counter: Holds the memory address for the following instructions the\n   CPU needs to act upon.\n * Stack Memory: Segment used by processes to communicate instructions and data\n   to each other.\n * Program status word (PSW): Condition variable that indicates to the CPU what\n   mode (kernel or user) instructions need to be carried out in.\n * User mode (problem state): Protection mode that a CPU works within when\n   carrying out less trusted process instructions.\n * Kernel mode (supervisory state, privilege mode): Mode that a CPU works within\n   when carrying out more trusted process instructions. The process has access\n   to more computer resources when working in kernel versus user mode.\n * Address bus: Physical connections between processing components and memory\n   segments used to communicate the physical memory addresses being used during\n   processing procedures.\n * Data bus: Physical connections between processing components and memory\n   segments used to transmit data being used during processing\n   procedures.\n * Symmetric mode multiprocessing: When a computer has two or more CPUs and each\n   CPU is being used in a load-balancing method.\n * Asymmetric mode multiprocessing: When a computer has two or more CPUs and one\n   CPU is dedicated to a specific program while the other CPUs carry out general\n   processing procedures.\n * Process: Program loaded in memory within an operating system.\n * Multiprogramming: Interleaved execution of more than one program (process) or\n   task by a single operating system.\n * Multitasking: Simultaneous execution of more than one program (process) or\n   task by a single operating system.\n * Cooperative multitasking: Multitasking scheduling scheme used by older\n   operating systems to allow for computer resource time slicing.\n * Preemptive multitasking: Multitasking scheduling scheme used by operating\n   systems to allow for computer resource time slicing. Used in newer, more\n   stable operating systems.\n * Process states (ready, running, blocked): Processes can be in various\n   activity levels. Ready = waiting for input. Running = instructions being\n   executed by CPU. Blocked = process is “suspended.”\n * Interrupts: Values assigned to computer components (hardware and software) to\n   allow for efficient computer resource time slicing.\n * Maskable interrupt: Interrupt value assigned to a non-critical operating\n   system activity.\n * Non-maskable interrupt: Interrupt value assigned to a critical operating\n   system activity.\n * Thread: Instruction set generated by a process when it has a specific\n   activity that needs to be carried out by an operating system. When the\n   activity is finished, the thread is destroyed.\n * Multi-threading: Applications that can carry out multiple activities\n   simultaneously by generating different instruction sets (threads).\n * Software deadlock: Two processes cannot complete their activities because\n   they are both waiting for system resources to be released.\n * Process isolation: Protection mechanism provided by operating systems that\n   can be implemented as encapsulation, time multiplexing of shared resources,\n   naming distinctions, and virtual memory mapping.\n * Dynamic link libraries (DLLs): A set of subroutines that are shared by\n   different applications and operating system processes.\n * Base registers: Beginning of address space assigned to a process. Used to\n   ensure a process does not make a request outside its assigned memory\n   boundaries.\n * Limit registers: Ending of address space assigned to a process. Used to\n   ensure a process does not make a request outside its assigned memory\n   boundaries.\n * RAM: Memory sticks that are plugged into a computer’s motherboard and work as\n   volatile memory space for an operating system.\n * ROM: Non-volatile memory that is used on motherboards for BIOS functionality\n   and various device controllers to allow for operating system-to-device\n   communication. Sometimes used for off-loading graphic rendering or\n   cryptographic functionality.\n * Hardware segmentation: Physically mapping software to individual memory\n   segments.\n * Cache memory: Fast memory type that is used by a CPU to increase read and\n   write operations.\n * Absolute addresses: Hardware addresses used by the CPU.\n * Logical addresses: Indirect addressing used by processes within an operating\n   system. The memory manager carries out logical-to-absolute address mapping.\n * Stack Memory: Construct that is made up of individually addressable buffers.\n   Process-to-process communication takes place through the use of stacks.\n * Buffer overflow: Too much data is put into the buffers that make up a stack.\n   Common attack vector used by hackers to run malicious code on a target\n   system.\n * Address space layout randomization (ASLR): Memory protection mechanism used\n   by some operating systems. The addresses used by components of a process are\n   randomized so that it is harder for an attacker to exploit specific memory\n   vulnerabilities.\n * Data execution prevention (DEP): Memory protection mechanism used by some\n   operating systems. Memory segments may be marked as non-executable so that\n   they cannot be misused by malicious software.\n * Garbage collector: Tool that marks unused memory segments as usable to ensure\n   that an operating system does not run out of memory.\n * Virtual memory: Combination of main memory (RAM) and secondary memory within\n   an operating system.\n * Time multiplexing: Technology that allows processes to use the same\n   resources.\n * Interrupt: Software or hardware signal that indicates that system resources\n   (i.e., CPU) are needed for instruction processing.\n * Instruction set: Set of operations and commands that can be implemented by a\n   particular processor (CPU).\n * Microarchitecture: Specific design of a microprocessor, which includes\n   physical components (registers, logic gates, ALU, cache, etc.) that support a\n   specific instruction set.\n * Application programming interface (API): Software interface that enables\n   process-to-process interaction. Common way to provide access to standard\n   routines to a set of software programs.\n * Monolithic operating system architecture: All of the code of the operating\n   system working in kernel mode in an ad-hoc and non-modularized manner.\n * Layered operating system architecture: Architecture that separates system\n   functionality into hierarchical layers.\n * Data hiding: Use of segregation in design decisions to protect software\n   components from negatively interacting with each other. Commonly enforced\n   through strict interfaces.\n * Microkernel architecture: Reduced amount of code running in kernel mode\n   carrying out critical operating system functionality. Only the absolutely\n   necessary code runs in kernel mode, and the remaining operating system code\n   runs in user mode.\n * Hybrid microkernel architecture: Combination of monolithic and microkernel\n   architectures. The microkernel carries out critical operating system\n   functionality, and the remaining functionality is carried out in a\n   client\\server model within kernel mode.\n * Mode transition: When the CPU has to change from processing code in user mode\n   to kernel mode.\n * Virtualization: Creation of a simulated environment (hardware platform,\n   operating system, storage, etc.) that allows for central control and\n   scalability.\n * Hypervisor: Central program used to manage virtual machines (guests) within a\n   simulated environment (host).\n * Security policy: Strategic tool used to dictate how sensitive information and\n   resources are to be managed and protected.\n * Trusted computing base: A collection of all the hardware, software, and\n   firmware components within a system that provide security and enforce the\n   system’s security policy.\n * Trusted path: Trustworthy software channel that is used for communication\n   between two processes that cannot be circumvented.\n * Security perimeter: Mechanism used to delineate between the components within\n   and outside of the trusted computing base.\n * Reference monitor: Concept that defines a set of design requirements of a\n   reference validation mechanism (security kernel), which enforces an access\n   control policy over subject's (processes, users) ability to perform\n   operations (read, write, execute) on objects (files, resources) on a system.\n * Security kernel: Hardware, software, and firmware components that fall within\n   the TCB and implement and enforce the reference monitor concept.\n * Multilevel security policies: Outlines how a system can simultaneously\n   process information at different classifications for users with different\n   clearance levels.\n * Protection profile: Description of a needed security solution.\n * Target of evaluation (TOE): Product proposed to provide a needed security\n   solution.\n * Security target: Vendor’s written explanation of the security functionality\n   and assurance mechanisms that meet the needed security solution.\n * Security functional requirements: Individual security functions which must be\n   provided by a product.\n * Security assurance requirements: Measures taken during development and\n   evaluation of the product to assure compliance with the claimed security\n   functionality.\n * Packages—EALs: Functional and assurance requirements are bundled into\n   packages for reuse. This component describes what must be met to achieve\n   specific EAL ratings.\n * Assurance evaluation criteria: Check-list and process of examining the\n   security-relevant parts of a system (TCB, reference monitor, security kernel)\n   and assigning the system an assurance rating.\n * Trusted Computer System Evaluation Criteria (TCSEC) (aka Orange Book): U.S.\n   DoD standard used to assess the effectiveness of the security controls built\n   into a system. Replaced by the Common Criteria.\n * Information Technology Security Evaluation Criteria (ITSEC): European\n   standard used to assess the effectiveness of the security controls built into\n   a system.\n * Common Criteria: International standard used to assess the effectiveness of\n   the security controls built into a system from functional and assurance\n   perspectives.\n * Certification: Technical evaluation of the security components and their\n   compliance to a predefined security policy for the purpose of accreditation.\n * Accreditation: Formal acceptance of the adequacy of a system’s overall\n   security by management.\n * Open system: Designs are built upon accepted standards to allow for\n   interoperability.\n * Closed system: Designs are built upon proprietary procedures, which inhibit\n   interoperability capabilities.\n * Maintenance hooks: Code within software that provides a back door entry\n   capability.\n * Time-of-check/time-of-use (TOC/TOU) attack: Attacker manipulates the\n   “condition check” step and the “use” step within software to allow for\n   unauthorized activity.\n * Race condition: Two or more processes attempt to carry out their activity on\n   one resource at the same time. Unexpected behaviour can result if the\n   sequence of execution does not take place in the proper order.\n * Open Systems Interconnection (OSI) model: International standardization of\n   system-based network communication through a modular seven-layer\n   architecture.\n * TCP/IP model: Standardization of device-based network communication through a\n   modular four-layer architecture. Specific to the IP suite, created in 1970 by\n   an agency of the U.S. Department of Defense (DoD).\n * Transmission Control Protocol (TCP): Core protocol of the TCP/IP suite, which\n   provides connection-oriented, end-to-end, reliable network connectivity.\n * Internet Protocol (IP): Core protocol of the TCP/IP suite. Provides packet\n   construction, addressing, and routing functionality.\n * User Datagram Protocol (UDP): Connectionless, unreliable transport layer\n   protocol, which is considered a “best effort” protocol.\n * Ports: Software construct that allows for application- or service-specific\n   communication between systems on a network. Ports are broken down into\n   categories: well known (0–1023), registered (1024–49151), and dynamic\n   (49152–65535).\n * SYN flood: DoS attack where an attacker sends a succession of SYN packets\n   with the goal of overwhelming the victim system so that it is unresponsive to\n   legitimate traffic.\n * Session hijacking: Attack method that allows an attacker to overtake and\n   control a communication session between two systems.\n * IPv6: IP version 6 is the successor to IP version 4 and provides 128-bit\n   addressing, integrated IPSec security protocol, simplified header formats,\n   and some automated configuration.\n * Subnet: Logical subdivision of a network that improves network administration\n   and helps reduce network traffic congestion. Process of segmenting a network\n   into smaller networks through the use of an addressing scheme made up of\n   network and host portions.\n * Classless Interdomain Routing (CIDR): Variable-length subnet masking, which\n   allows a network to be divided into different-sized subnets. The goal is to\n   increase the efficiency of the use of IP addresses since classful addressing\n   schemes commonly end up in unused addresses.\n * 6to4: Transition mechanism for migrating from IPv4 to IPv6. It allows systems\n   to use IPv6 to communicate if their traffic has to transverse an IPv4\n   network.\n * Teredo: Transition mechanism for migrating from IPv4 to IPv6. It allows\n   systems to use IPv6 to communicate if their traffic has to transverse an IPv4\n   network, but also performs its function behind NAT devices.\n * Intra-Site Automatic Tunnel Addressing Protocol (ISATAP): An IPv6 transition\n   mechanism meant to transmit IPv6 packets between dual-stack nodes on top of\n   an IPv4 network.\n * IEEE 802.1AE (MACSec): Standard that specifies a set of protocols to meet the\n   security requirements for protecting data traversing Ethernet LANs.\n * IEEE 802.1AR: Standard that specifies unique per-device identifiers (DevID)\n   and the management and cryptographic binding of a device (router, switch,\n   access point) to its identifiers.\n * Digital signals: Binary digits are represented and transmitted as discrete\n   electrical pulses.\n * Analog signals: Continuously varying electromagnetic wave that represents and\n   transmits data.\n * Asynchronous communication: Transmission sequencing technology that uses\n   start and stop bits or similar encoding mechanism. Used in environments that\n   transmit a variable amount of data in a periodic fashion.\n * Synchronous communication: Transmission sequencing technology that uses a\n   clocking pulse or timing scheme for data transfer synchronization.\n * Baseband transmission: Uses the full bandwidth for only one communication\n   channel and has a low data transfer rate compared to broadband.\n * Broadband transmission: Divides the bandwidth of a communication channel into\n   many channels, enabling different types of data to be transmitted at one\n   time.\n * Unshielded twisted pair (UTP): Cabling in which copper wires are twisted\n   together for the purposes of canceling out EMI from external sources. UTP\n   cables are found in many Ethernet networks and telephone systems.\n * Shielded twisted pair (STP): Twisted-pair cables are often shielded in an\n   attempt to prevent RFI and EMI. This shielding can be applied to individual\n   pairs or to the collection of pairs.\n * Attenuation: Gradual loss in intensity of any kind of flux through a medium.\n   As an electrical signal travels down a cable, the signal can degrade and\n   distort or corrupt the data it is carrying.\n * Crosstalk: A signal on one channel of a transmission creates an undesired\n   effect in another channel by interacting with it. The signal from one cable\n   “spills over” into another cable.\n * Plenum cables: Cable is jacketed with a fire-retardant plastic cover that\n   does not release toxic chemicals when burned.\n * Ring topology: Each system connects to two other systems, forming a single,\n   unidirectional network pathway for signals, thus forming a ring.\n * Bus topology: Systems are connected to a single transmission channel (i.e.,\n   network cable), forming a linear construct.\n * Star topology: Network consists of one central device, which acts as a\n   conduit to transmit messages. The central device, to which all other nodes\n   are connected, provides a common connection point for all nodes.\n * Mesh topology: Network where each system must not only capture and\n   disseminate its own data, but also serve as a relay for other systems; that\n   is, it must collaborate to propagate the data in the network.\n * Ethernet: Common LAN media access technology standardized by IEEE 802.3. Uses\n   48-bit MAC addressing, works in contention-based networks, and has extended\n   outside of just LAN environments.\n * Token ring: LAN medium access technology that controls network communication\n   traffic through the use of token frames. This technology has been mostly\n   replaced by Ethernet.\n * Fiber Distributed Data Interface (FDDI): Ring-based token network protocol\n   that was derived from the IEEE 802.4 token bus timed token protocol.\n   It can work in LAN or MAN environments and provides fault tolerance through\n   dual-ring architecture.\n * Carrier sense multiple access with collision detection (CSMA/CD): A media\n   access control method that uses a carrier sensing scheme. When a transmitting\n   system detects another signal while transmitting a frame, it stops\n   transmitting that frame, transmits a jam signal, and then waits for a random\n   time interval before trying to resend the frame. This reduces collisions on a\n   network.\n * Carrier sense multiple access with collision avoidance (CSMA/CA): A media\n   access control method that uses a carrier sensing scheme. A system wishing to\n   transmit data has to first listen to the channel for a predetermined amount\n   of time to determine whether or not another system is transmitting on the\n   channel. If the channel is sensed as “idle,” then the system is permitted to\n   begin the transmission process. If the channel is sensed as “busy,” the\n   system defers its transmission for a random period of time.\n * Internet Group Management Protocol (IGMP): Used by systems and adjacent\n   routers on IP networks to establish and maintain multicast group memberships.\n * Media access control (MAC): Data communication protocol sub-layer of the data\n   link layer specified in the OSI model. It provides hardware addressing and\n   channel access control mechanisms that make it possible for several nodes to\n   communicate within a multiple-access network that incorporates a shared\n   medium.\n * Address Resolution Protocol (ARP): A networking protocol used for resolution\n   of network layer IP addresses into link layer MAC addresses.\n * Dynamic Host Configuration Protocol (DHCP): A network configuration service\n   for hosts on IP networks. It provides IP addressing, DNS server, subnet mask,\n   and other important network configuration data to each host through\n   automation.\n * Internet Control Message Protocol (ICMP): A core protocol of the IP suite\n   used to send status and error messages.\n * Ping of Death: A DoS attack type on a computer that involves sending\n   malformed or oversized ICMP packets to a target.\n * Smurf attack: A DDoS attack type on a computer that floods the target system\n   with spoofed broadcast ICMP packets.\n * Fraggle attack: A DDoS attack type on a computer that floods the target\n   system with a large amount of UDP echo traffic to IP broadcast addresses.\n * Simple Network Management Protocol (SNMP): A protocol within the IP suite\n   that is used for network device management activities through the use of a\n   structure that uses managers, agents, and Management Information Bases.\n * Domain Name System (DNS): A hierarchical distributed naming system for\n   computers, services, or any resource connected to an IP based network. It\n   associates various pieces of information with domain names assigned to each\n   of the participating entities.\n * DNS zone transfer: The process of replicating the databases containing the\n   DNS data across a set of DNS servers.\n * DNSSEC: A set of extensions to DNS that provide to DNS clients (resolvers)\n   origin authentication of DNS data to reduce the threat of DNS poisoning,\n   spoofing, and similar attack types.\n * Simple Mail Transfer Protocol (SMTP): An Internet standard protocol for\n   electronic mail (e-mail) transmission across IP-based networks.\n * Post Office Protocol (POP): An Internet standard protocol used by e-mail\n   clients to retrieve e-mail from a remote server and supports simple\n   download-and-delete requirements for access to remote mailboxes.\n * Internet Message Access Protocol (IMAP): An Internet standard protocol used\n   by e-mail clients to retrieve e-mail from a remote server. E-mail clients\n   using IMAP generally leave messages on the server until the user explicitly\n   deletes them.\n * Open mail relay: An SMTP server configured in such a way that it allows\n   anyone on the Internet to send e-mail through it, not just mail destined to\n   or originating from known users.\n * E-mail spoofing: Activity in which the sender address and other arts of the\n   e-mail header are altered to appear as though the e-mail originated from a\n   different source. Since SMTP does not provide any\n   authentication, it is easy to impersonate and forge e-mails.\n * Sender Policy Framework (SPF): An e-mail validation system designed to\n   prevent e-mail spam by detecting e-mail spoofing, a common vulnerability, by\n   verifying sender IP addresses.\n * Phishing: A way of attempting to obtain data such as usernames, passwords,\n   credit card information, and other sensitive data by masquerading as an\n   authenticated entity in an electronic communication. Spear phishing targets\n   individuals, and whaling targets people with high authorization (C-Level\n   Executives).\n * Network address translation (NAT): The process of modifying IP address\n   information in packet headers while in transit across a traffic routing\n   device, with the goal of reducing the demand for public IP addresses.\n * Distance-vector routing protocol: A routing protocol that calculates paths\n   based on the distance (or number of hops) and a vector (a direction).\n * Link-state routing protocol: A routing protocol used in packet-switching\n   networks where each router constructs a map of the connectivity within the\n   network and calculates the best logical paths, which form its routing table.\n * Border Gateway Protocol (BGP): The protocol that carries out core routing\n   decisions on the Internet. It maintains a table of IP networks, or\n   “prefixes,” which designate network reachability among autonomous systems.\n * Wormhole attack: This takes place when an attacker captures packets at one\n   location in the network and tunnels them to another location in the network\n   for a second attacker to use against a target system.\n * Spanning Tree Protocol (STP): A network protocol that ensures a loop-free\n   topology for any bridged Ethernet LAN and allows redundant links to be\n   available in case connection links go down.\n * Source routing: Allows a sender of a packet to specify the route the packet\n   takes through the network versus routers determining the path.\n * Multi-protocol Label Switching (MPLS): A networking technology that directs\n   data from one network node to the next based on short path labels rather than\n   long network addresses, avoiding complex lookups in a routing table.\n * Virtual local area network (VLAN): A group of hosts that communicate as if\n   they were attached to the same broadcast domain, regardless of their physical\n   location. VLAN membership can be configured through software instead of\n   physically relocating devices or connections, which allows for easier\n   centralized management.\n * VLAN hopping: An exploit that allows an attacker on a VLAN to gain access to\n   traffic on other VLANs that would normally not be accessible.\n * Private Branch Exchange (PBX): A telephone exchange that serves a particular\n   business, makes connections among the internal telephones, and connects them\n   to the public-switched telephone network (PSTN) via trunk lines.\n * Bastion host: A highly exposed device that will most likely be targeted for\n   attacks, and thus should be properly locked down.\n * Dual-homed firewall: This device has two interfaces and sits between an\n   untrusted network and trusted network to provide secure access.\n * Screened host: A firewall that communicates directly with a perimeter router\n   and the internal network. The router carries out filtering activities on the\n   traffic before it reaches the firewall.\n * Screened subnet architecture: When two filtering devices are used to create a\n   DMZ. The external device screens the traffic entering the DMZ network, and\n   the internal filtering device screens the traffic before it enters the\n   internal network.\n * Proxy server: A system that acts as an intermediary for requests from clients\n   seeking resources from other sources. A client connects to the proxy server,\n   requesting some service, and the proxy server evaluates the request according\n   to its filtering rules and makes the connection on behalf of the client.\n   Proxies can be open or carry out forwarding or reverse forwarding\n   capabilities.\n * Honeypots: Systems that entice with the goal of protecting critical\n   production systems. If two or more honeypots are used together, this is\n   considered a honeynet.\n * Network convergence: The combining of server, storage, and network\n   capabilities into a single framework, which decreases the costs and\n   complexity of data centers. Converged infrastructures provide the ability to\n   pool resources, automate resource provisioning, and increase and decrease\n   processing capacity quickly to meet the needs of dynamic computing workloads.\n * Cloud computing: The delivery of computer processing capabilities as a\n   service rather than as a product, whereby shared resources, software, and\n   information are provided to end users as a utility. Offerings are usually\n   bundled as an infrastructure, platform, or software.\n * Metropolitan area network (MAN): A network that usually spans a city or a\n   large campus, interconnects a number of LANs using a high capacity backbone\n   technology, and provides up-link services to WANs or the Internet.\n * Synchronous Optical Networking (SONET) and Synchronous Digital Hierarchy\n   (SDH): Standardized multiplexing protocols that transfer multiple digital bit\n   streams over optical fiber and allow for simultaneous transportation of many\n   different circuits of differing origin within a single framing protocol.\n * Metro Ethernet: A data link technology that is used as a metropolitan area\n   network to connect customer networks to larger service networks or the\n   Internet.\n * Wide area network (WAN): A telecommunication network that covers a broad area\n   and allows a business to effectively carry out its daily function, regardless\n   of location.\n * Multiplexing: A method of combining multiple channels of data over a single\n   transmission line.\n * T-carriers: Dedicated lines that can carry voice and data information over\n   trunk lines.\n * Time-division multiplexing (TDM): A type of multiplexing in which two or more\n   bit streams or signals are transferred apparently simultaneously as\n   sub-channels in one communication channel, but are physically taking turns on\n   the single channel.\n * Wave-division multiplexing (WDM): Multiplying the available capacity of\n   optical fibers through use of parallel channels, with each channel on a\n   dedicated wavelength of light. The bandwidth of an optical fiber can be\n   divided into as many as 160 channels.\n * Frequency-division multiplexing (FDM): Dividing available bandwidth into a\n   series of non-overlapping frequency sub-bands that are then assigned to each\n   communicating source and user pair. FDM is inherently an analog technology.\n * Statistical time-division multiplexing (STDM): Transmitting several types of\n   data simultaneously across a single transmission line. STDM technologies\n   analyze statistics related to the typical workload of each input device and\n   make real-time decisions on how much time each device should be allocated for\n   data transmission.\n * Channel Service Unit (CSU): A line bridging device for use with T-carriers,\n   and that is required by PSTN providers at digital interfaces that terminate\n   in a Data Service Unit (DSU) on the customer side. The DSU is a piece of\n   telecommunications circuit terminating equipment that transforms digital data\n   between telephone company lines and local equipment.\n * Public-switched telephone network (PSTN): The public circuit-switched\n   telephone network, which is made up of telephone lines, fiber-optic cables,\n   cellular networks, communications satellites, and undersea telephone cables\n   and allows all phone-to-phone communication. It was a fixed-line analog\n   telephone system, but is now almost entirely digital and includes mobile as\n   well as fixed telephones.\n * Voice over IP (VoIP): The set of protocols, technologies, methodologies, and\n   transmission techniques involved in the delivery of voice data and\n   multimedia sessions over IP-based networks.\n * Session Initiation Protocol (SIP): The signaling protocol widely used for\n   controlling communication, as in voice and video calls over IPbased\n   networks.\n * Vishing (voice and phishing): Social engineering activity over the telephone\n   system, most often using features facilitated by VoIP, to gain unauthorized\n   access to sensitive data.\n * H.323: A standard that addresses call signaling and control, multimedia\n   transport and control, and bandwidth control for point-to-point and\n   multipoint conferences.\n * Real-time Transport Protocol (RTP): Used to transmit audio and video over\n   IP-based networks. It is used in conjunction with the RTCP. RTP transmits the\n   media data, and RTCP is used to monitor transmission statistics and QoS, and\n   aids synchronization of multiple data streams.\n * War dialing: When a specialized program is used to automatically scan a list\n   of telephone numbers to search for computers for the purposes of exploitation\n   and hacking.\n * Integrated Services Digital Network (ISDN): A circuit-switched telephone\n   network system technology designed to allow digital transmission of voice and\n   data over ordinary telephone copper wires.\n * Digital Subscriber Line (DSL): A set of technologies that provide Internet\n   access by transmitting digital data over the wires of a local telephone\n   network. DSL is used to digitize the “last mile” and provide fast Internet\n   connectivity.\n * Cable modem: A device that provides bidirectional data communication via\n   radio frequency channels on cable TV infrastructures. Cable modems are\n   primarily used to deliver broadband Internet access to homes.\n * Algorithm: Set of mathematical and logic rules used in cryptographic\n   functions.\n * Cipher: Another name for algorithm.\n * Cryptography: Science of secret writing that enables an entity to store and\n   transmit data in a form that is available only to the intended individuals.\n * Cryptosystem: Hardware or software implementation of cryptography that\n   contains all the necessary software, protocols, algorithms, and keys.\n * Cryptanalysis: Practice of uncovering flaws within cryptosystems.\n * Cryptology: The study of both cryptography and cryptanalysis.\n * Encipher: Act of transforming data into an unreadable format.\n * Decipher: Act of transforming data into a readable format.\n * Key: Sequence of bits that are used as instructions that govern the acts of\n   cryptographic functions within an algorithm.\n * Key clustering: Instance when two different keys generate the same ciphertext\n   from the same plaintext.\n * Keyspace: A range of possible values used to construct keys.\n * Plaintext: Data in readable format, also referred to as cleartext.\n * Substitution Cipher: Encryption method that uses an algorithm that changes\n   out (substitutes) one value for another value.\n * Scytale Cipher: Ancient encryption tool that used a type of paper and rod\n   used by Greek military factions.\n * Kerckhoffs’ Principle: Concept that an algorithm should be known and only the\n   keys should be kept secret.\n * One-time pad: Encryption method created by Gilbert Vernam that is considered\n   impossible to crack if carried out properly.\n * Random Number generator: Algorithm used to create values that are used in\n   cryptographic functions to add randomness.\n * Running Key Cipher: Substitution cipher that creates keystream values,\n   commonly from agreed-upon text passages, to be used for encryption purposes.\n * Concealment Cipher: Encryption method that hides a secret message within an\n   open message.\n * Steganography: Method of hiding data in another media type.\n * Digital Rights Management (DRM): Access control technologies commonly used to\n   protect copyright material.\n * Transposition: Encryption method that shifts (permutation) values.\n * Caesar Cipher: Simple substitution algorithm created by Julius Caesar that\n   shifts alphabetic values three positions during its encryption and decryption\n   processes\n * Frequency analysis: Cryptanalysis process used to identify weaknesses within\n   cryptosystems by locating patterns in resulting ciphertext.\n * Key Derivation Functions (KDFs): Generation of secret keys (subkeys) from an\n   initial value (master key).\n * Symmetric algorithm: Encryption method where the sender and receiver use an\n   instance of the same key for encryption and decryption purposes.\n * Out-of-band method: Sending data through an alternate communication channel.\n * Asymmetric algorithm: Encryption method that uses two different key types,\n   public and private. Also called public key cryptography.\n * Public key: Value used in public key cryptography that is used for encryption\n   and signature validation that can be known by all parties.\n * Private key: Value used in public key cryptography that is used for\n   decryption and signature creation and known to only key owner.\n * Public key cryptography: Asymmetric cryptography, which uses public and\n   private key values for cryptographic functions.\n * Block cipher: Symmetric algorithm type that encrypts chunks (blocks) of data\n   at a time.\n * Diffusion: Transposition processes used in encryption functions to increase\n   randomness.\n * Confusion: Substitution processes used in encryption functions to increase\n   randomness.\n * Avalanche effect: Algorithm design requirement so that slight changes to the\n   input result in drastic changes to the output.\n * Stream cipher: Algorithm type that generates a keystream (random values),\n   which is XORd with plaintext for encryption purposes.\n * Keystream generator: Component of a stream algorithm that creates random\n   values for encryption purposes.\n * Initialization vectors (IVs): Values that are used with algorithms to\n   increase randomness for cryptographic functions.\n * Hybrid cryptography: Combined use of symmetric and asymmetric algorithms\n   where the symmetric key encrypts data and an asymmetric key encrypts the\n   symmetric key.\n * Session keys: Symmetric keys that have a short lifespan, thus providing more\n   protection than static keys with longer lifespans.\n * Rijndael: Block symmetric cipher that was chosen to fulfil the Advanced\n   Encryption Standard. It uses a 128-bit block size and various key lengths\n   (128, 192, 256).\n * Triple DES (3-DES): Symmetric cipher that applies DES three times to each\n   block of data during the encryption process.\n * International Data Encryption Algorithm (IDEA): Block symmetric cipher that\n   uses a 128-bit key and 64-bit block size.\n * Blowfish: Block symmetric cipher that uses 64-bit block sizes and\n   variable-length keys.\n * RC4: Stream symmetric cipher that was created by Ron Rivest of RSA. Used in\n   SSL and WEP.\n * RC5: Block symmetric cipher that uses variable block sizes (32, 64, 128) and\n   variable-length key sizes (0–2040).\n * RC6: Block symmetric cipher that uses a 128-bit block size and variable\n   length key sizes (128, 192, 256). Built upon the RC5 algorithm.\n * Diffie-Hellman algorithm: First asymmetric algorithm created and is used to\n   exchange symmetric key values. Based upon logarithms in finite fields.\n * El Gamal algorithm: Asymmetric algorithm based upon the Diffie-Hellman\n   algorithm used for digital signatures, encryption, and key exchange.\n * Elliptic curve cryptosystem algorithm: Asymmetric algorithm based upon the\n   algebraic structure of elliptic curves over finite fields. Used for digital\n   signatures, encryption, and key exchange.\n * Zero knowledge proof: One entity can prove something to be true without\n   providing a secret value.\n * One-way hash: Cryptographic process that takes an arbitrary amount of data\n   and generates a fixed-length value. Used for integrity protection.\n * Message authentication code (MAC): Keyed cryptographic hash function used for\n   data integrity and data origin authentication.\n * Hashed message authentication code (HMAC): Cryptographic hash function that\n   uses a symmetric key value and is used for data integrity and data origin\n   authentication.\n * CBC-MAC: Cipher block chaining message authentication code uses encryption\n   for data integrity and data origin authentication.\n * CMAC: Cipher message authentication code that is based upon and provides more\n   security compared to CBC-MAC.\n * CMM: Block cipher mode that combines the CTR encryption mode and CBC-MAC. One\n   encryption key is used for both authentication and encryption purposes.\n * Collision: When two different messages are computed by the same hashing\n   algorithm and the same message digest value results.\n * Birthday attack: Cryptographic attack that exploits the mathematics behind\n   the birthday problem in the probability theory forces collisions within\n   hashing functions.\n * Digital signature: Ensuring the authenticity and integrity of a message\n   through the use of hashing algorithms and asymmetric algorithms. The message\n   digest is encrypted with the sender’s private key.\n * Certification Authority: Component of a PKI that creates and maintains\n   digital certificates throughout their life cycles.\n * Registration Authority (RA): Component of PKI that validates the identity of\n   an entity requesting a digital certificate.\n * Certificate Revocation List (CRL): List that is maintained by the certificate\n   authority of a PKI that contains information on all of the digital\n   certificates that have been revoked.\n * Online Certificate Status Protocol (OCSP): Automated method of maintaining\n   revoked certificates within a PKI.\n * Certificate: Digital identity used within a PKI. Generated and maintained by\n   a certificate authority and used for authentication.\n * Link encryption: Technology that encrypts full packets (all headers and data\n   payload) and is carried out without the sender’s interaction.\n * End-to-end encryption: Encryption method used by the sender of data that\n   encrypts individual messages and not full packets.\n * Multipurpose Internet Mail Extension (MIME): Standard that outlines the\n   format of e-mail messages and allows binary attachments to be transmitted\n   through e-mail.\n * Secure MIME (S/MIME): Secure/Multipurpose Internet Mail Extensions, which\n   outlines how public key cryptography can be used to secure MIME data types.\n * Pretty Good Privacy (PGP) Cryptosystem: used to integrate public key\n   cryptography with e-mail functionality and data encryption, which was\n   developed by Phil Zimmerman.\n * Quantum cryptography: Use of quantum mechanical functions to provide strong\n   cryptographic key exchange.\n * HTTPS: A combination of HTTP and SSL\\TLS that is commonly used for secure\n   Internet connections and e-commerce transactions.\n * Secure Electronic Transaction (SET): Secure e-commerce standard developed by\n   Visa and MasterCard that has not been accepted within the marketplace.\n * Cookies: Data files used by web browsers and servers to keep browser state\n   information and browsing preferences.\n * Secure Shell (SSH): Network protocol that allows for a secure connection to a\n   remote system. Developed to replace Telnet and other insecure remote shell\n   methods.\n * IPSec: Protocol suite used to protect IP traffic through encryption and\n   authentication. De facto standard VPN protocol.\n * Authentication header (AH) protocol: Protocol within the IPSec suite used for\n   integrity and authentication.\n * Encapsulating Security Payload Protocol (ESP): Protocol within the IPSec\n   suite used for integrity, authentication, and encryption.\n * Transport mode: Mode that IPSec protocols can work in that provides\n   protection for packet data payload.\n * Tunnel mode: Mode that IPSec protocols can work in that provides protection\n   for packet headers and data payload.\n * Internet Security Association and Key Management Protocol (ISAKMP): Used to\n   establish security associates and an authentication framework in Internet\n   connections. Commonly used by IKE for key exchange.\n * Passive attack: Attack where the attacker does not interact with processing\n   or communication activities, but only carries out observation and data\n   collection, as in network sniffing.\n * Active attack: Attack where the attacker does interact with processing or\n   communication activities.\n * Ciphertext-only attack: Cryptanalysis attack where the attacker is assumed to\n   have access only to a set of ciphertexts.\n * Known-plaintext attack: Cryptanalysis attack where the attacker is assumed to\n   have access to sets of corresponding plaintext and ciphertext.\n * Chosen-plaintext attack: Cryptanalysis attack where the attacker can choose\n   arbitrary plaintexts to be encrypted and obtain the corresponding\n   ciphertexts.\n * Chosen-ciphertext attack: Cryptanalysis attack where the attacker chooses a\n   ciphertext and obtains its decryption under an unknown key.\n * Differential cryptanalysis: Cryptanalysis method that uses the study of how\n   differences in an input can affect the resultant difference at the output.\n * Linear cryptanalysis: Cryptanalysis method that uses the study of affine\n   transformation approximation in encryption processes.\n * Side-channel attack: Attack that uses information (timing, power consumption)\n   that has been gathered to uncover sensitive data or processing functions.\n * Replay attack: Valid data transmission is maliciously or fraudulently\n   repeated to allow an entity gain unauthorized access.\n * Algebraic attack: Cryptanalysis attack that exploits vulnerabilities within\n   the intrinsic algebraic structure of mathematical functions.\n * Analytic attack: Cryptanalysis attack that exploits vulnerabilities within\n   the algorithm structure.\n * Statistical attack: Cryptanalysis attack that uses identified statistical\n   patterns.\n * Social engineering attack: Manipulating individuals so that they will divulge\n   confidential information, rather than by breaking in or using technical\n   cracking techniques.\n * Meet-in-the-middle attack: Cryptanalysis attack that tries to uncover a\n   mathematical problem from two different ends.\n * Business continuity management (BCM): is the overarching approach to\n   managing all aspects of BCP and DRP.\n * Business Continuity Plan (BCP): Contains strategy documents that provide\n   detailed procedures that ensure critical business functions are maintained\n   and\n   that help minimize losses of life, operations, and systems. A BCP provides\n   procedures for emergency responses, extended backup operations, and\n   post-disaster recovery.\n * Business Impact Analysis (BIA): One of the most important first steps in the\n   planning development. Qualitative and quantitative data on the business\n   impact of a disaster need to be gathered, analyzed, interpreted, and\n   presented to management.\n * A reciprocal agreement: One in which a company promises another company it\n   can move in and share space if it experiences a disaster, and vice versa.\n   Reciprocal agreements are very tricky to implement and are unenforceable.\n * A hot site: Fully configured with hardware, software, and environmental\n   needs. It can usually be up and running in a matter of hours. It is the most\n   expensive option, but some companies cannot be out of business longer than a\n   day without very detrimental results.\n * A warm site: Does not have computers, but it does have some peripheral\n   devices, such as disk drives, controllers, and tape drives. This option is\n   less expensive than a hot site, but takes more effort and time to become\n   operational.\n * A cold site: Is just a building with power, raised floors, and utilities. No\n   devices are available. This is the cheapest of the three options, but can\n   take weeks to get up and operational.\n * Recovery Time Objective (RTO): The earliest time period and a service level\n   within which a business process must be restored after a disaster to avoid\n   unacceptable consequences.\n * Recovery Point Objective (RPO): The acceptable amount of data loss measured\n   in time.\n * Mean Time Between Failures (MTBF): The predicted amount of time between\n   inherent failures of a system during operation.\n * Mean Time To Repair (MTTR): A measurement of the maintainability by\n   representing the average time required to repair a failed component or\n   device.\n * High availability: Refers to a system, component, or environment that is\n   continuously operational.\n * A checklist test: Copies of the plan are handed out to each functional area\n   for examination to ensure the plan properly deals with the area’s needs and\n   vulnerabilities.\n * A structured walk-through test: Representatives from each functional area or\n   department get together and walk through the plan from beginning to end.\n * A simulation test: A practice execution of the plan takes place. A specific\n   scenario is established, and the simulation continues up to the point of\n   actual\n   relocation to the alternate site.\n * A parallel test: One in which some systems are actually run at the alternate\n   site.\n * A full-interruption test: One in which regular operations are stopped and\n   processing is moved to the alternate site.\n * Remote journaling: Involves transmitting the journal or transaction log\n   offsite to a backup facility.\n * Dumpster diving: Refers to going through someone’s trash to find confidential\n   or useful information. It is legal, unless it involves trespassing, but in\n   all cases it is considered unethical.\n * Wiretapping: A passive attack that eavesdrops on communications. It is only\n   legal with prior consent or a warrant.\n * Data diddling: The act of willfully modifying information, programs, or\n   documentation in an effort to commit fraud or disrupt production.\n * Patent: Grants ownership and enables that owner to legally enforce his rights\n   to exclude others from using the invention covered by the patent.\n * Copyright: Protects the expression of ideas rather than the ideas themselves.\n * Trademarks: Protect words, names, product shapes, symbols, colors, or a\n   combination of these used to identify products or a company. These items are\n   used to distinguish products from the competitors’ products.\n * Trade secrets: Are deemed proprietary to a company and often include\n   information that provides a competitive edge. The information is protected as\n   long as the owner takes the necessary protective actions.\n * Personally Identifiable Information (PII): Data that can be used to uniquely\n   identify, contact, or locate a single person or can be used with other\n   sources to\n   uniquely identify a single individual.\n * System Development Life Cycle (SDLC): A methodical approach to standardize\n   requirements discovery, design, development, testing, and implementation in\n   every phase of a system. It is made up of the following phases: initiation,\n   acquisition/development, implementation, operation/maintenance, and disposal.\n * Certification: The technical testing of a system.\n * Accreditation: The formal authorization given by management to allow a system\n   to operate in a specific environment.\n * Statement of Work (SOW): Describes the product and customer requirements. A\n   detailed-oriented SOW will help ensure that these requirements are properly\n   understood and assumptions are not made.\n * Work breakdown structure (WBS): A project management tool used to define and\n   group a project’s individual work elements in an organized manner.\n * Attack surface: Components available to be used by an attacker against the\n   product itself.\n * Threat modeling: A systematic approach used to understand how different\n   threats could be realized and how a successful compromise could take place.\n * Static analysis: A debugging technique that is carried out by examining the\n   code without executing the program, and therefore is carried out before the\n   program is compiled.\n * Fuzzing: A technique used to discover flaws and vulnerabilities in software.\n * Verification: Determines if the product accurately represents and meets the\n   specifications.\n * Validation: Determines if the product provides the necessary solution for the\n   intended real-world problem.\n * Capability Maturity Model Integration (CMMI) model: A process improvement\n   approach that provides organizations with the essential elements of effective\n   processes, which will improve their performance.\n * Change control: The process of controlling the changes that take place during\n   the life cycle of a system and documenting the necessary change control\n   activities.\n * Software Configuration Management (SCM): Identifies the attributes of\n   software at various points in time, and performs a methodical control of\n   changes for the purpose of maintaining software integrity and traceability\n   throughout the software development life cycle.\n * Software escrow: Storing of the source code of software with a third-party\n   escrow agent. The software source code is released to the licensee if the\n   licensor (software vendor) files for bankruptcy or fails to maintain and\n   update the software product as promised in the software license agreement.\n * Machine language: A set of instructions in binary format that the computer’s\n   processor can understand and work with directly.\n * Assembly language: A low-level programming language that is the mnemonic\n   representation of machine-level instructions.\n * Assemblers: Tools that convert assembly code into the necessary\n   machine-compatible binary language for processing activities to take place.\n * High-level languages: Otherwise known as third-generation programming\n   languages, due to their refined programming structures, using abstract\n   statements.\n * Very high-level languages: Otherwise known as fourth-generation programming\n   languages and are meant to take natural language-based statements one step\n   ahead.\n * Natural languages: Otherwise known as fifth-generation programming languages,\n   which have the goal to create software that can solve problems by themselves.\n   Used in systems that provide artificial intelligence.\n * Compilers: Tools that convert high-level language statements into the\n   necessary machine-level format (.exe, .dll, etc.) for specific processors to\n   understand.\n * Interpreters: Tools that convert code written in interpreted languages to the\n   machine-level format for processing.\n * Garbage collector: Identifies blocks of memory that were once allocated but\n   are no longer in use and deallocates the blocks and marks them as free.\n * Abstraction: The capability to suppress unnecessary details so the important,\n   inherent properties can be examined and reviewed.\n * Polymorphism: Two objects can receive the same input and have different\n   outputs.\n * Data modeling: Considers data independently of the way the data are processed\n   and of the components that process the data. A process\n   used to define and analyze data requirements needed to support the business\n   processes.\n * Cohesion: A measurement that indicates how many different types of tasks a\n   module needs to carry out.\n * Coupling: A measurement that indicates how much interaction one module\n   requires for carrying out its tasks.\n * Data structure: A representation of the logical relationship between elements\n   of data.\n * Mobile code: Code that can be transmitted across a network, to be executed by\n   a system or device on the other end.\n * Java applets: Small components (applets) that provide various functionalities\n   and are delivered to users in the form of Java bytecode. Java applets can run\n   in a web browser using a Java Virtual Machine (JVM). Java is platform\n   independent; thus, Java applets can be executed by browsers for many\n   platforms.\n * Sandbox: A virtual environment that allows for very fine-grained control over\n   the actions that code within the machine is permitted to take. This is\n   designed to allow safe execution of untrusted code from remote sources.\n * ActiveX: A Microsoft technology composed of a set of OOP technologies and\n   tools based on COM and DCOM. It is a framework for defining reusable software\n   components in a programming language–independent manner.\n * Authenticode: A type of code signing, which is the process of digitally\n   signing software components and scripts to confirm the software author and\n   guarantee that the code has not been altered or corrupted since it was\n   digitally signed. Authenticode is Microsoft’s implementation of code signing.\n   Key Terms\n * Information gathering: Usually the first step in an attacker’s methodology,\n   in which the information gathered may allow an attacker to infer additional\n   information that can be used to compromise systems.\n * Server side includes (SSI): An interpreted server-side scripting language\n   used almost exclusively for web-based communication. It is commonly used to\n   include the contents of one or more files into a web page on a web server.\n   Allows web developers to reuse content by inserting the same content into\n   multiple web documents.\n * Client-side validation: Input validation is done at the client before it is\n   even sent back to the server to process.\n * Cross-site scripting (XSS) attack: An attack where a vulnerability is found\n   on a web site that allows an attacker to inject malicious code into a web\n   application.\n * Parameter validation: The values that are being received by the application\n   are validated to be within defined limits before the server application\n   processes them within the system.\n * Web proxy: A piece of software installed on a system that is designed to\n   intercept all traffic between the local web browser and the web server.\n * Replay attack: An attacker capturing the traffic from a legitimate session\n   and replaying it with the goal of masquerading an authenticated user.\n   following are some key database terms:\n * Record: A collection of related data items.\n * File: A collection of records of the same type.\n * Database: A cross-referenced collection of data.\n * Database Management System (DBMS): Manages and controls the database.\n * Tuple: A row in a two-dimensional database.\n * Attribute: A column in a two-dimensional database.\n * Primary key: Columns that make each row unique. (Every row of a table must\n   include a primary key.)\n * View: A virtual relation defined by the database administrator in order to\n   keep subjects from viewing certain data.\n * Foreign key: An attribute of one table that is related to the primary key of\n   another table.\n * Cell: An intersection of a row and a column.\n * Schema: Defines the structure of the database.\n * Data dictionary: Central repository of data elements and their relationships.\n * Relational database model: Uses attributes (columns) and tuples (rows) to\n   contain and organize information.\n * Hierarchical data model: Combines records and fields that are related in a\n   logical tree structure.\n * Object-oriented database: Designed to handle a variety of data (images,\n   audio, documents, video), which is more dynamic in nature than a relational\n   database.\n * Object-relational database (ORD): Uses object-relational database management\n   system (ORDBMS) and is a relational database with a software front end that\n   is written in an object-oriented programming language.\n * Rollback: An operation that ends a current transaction and cancels all the\n   recent changes to the database until the previous checkpoint/ commit point.\n * Two-phase commit: A mechanism that is another control used in databases to\n   ensure the integrity of the data held within the database.\n * Cell suppression: A technique used to hide specific cells that contain\n   sensitive information.\n * Noise and perturbation: A technique of inserting bogus information in the\n   hopes of misdirecting an attacker or confusing the matter enough that the\n   actual attack will not be fruitful.\n * Data warehousing: Combines data from multiple databases or data sources into\n   a large database for the purpose of providing more extensive information\n   retrieval and data analysis.\n * Data mining: Otherwise known as knowledge discovery in database (KDD), which\n   is the process of massaging the data held in the data warehouse into more\n   useful information.\n * Virus: A small application, or string of code, that infects host\n   applications. It is a programming code that can replicate itself and spread\n   from one system to another.\n * Macro virus: A virus written in a macro language and that is platform\n   independent. Since many applications allow macro programs to be embedded in\n   documents, the programs may be run automatically when the document is opened.\n   This provides a distinct mechanism by which viruses can be spread.\n * Compression viruses: Another type of virus that appends itself to executables\n   on the system and compresses them by using the user’s permissions.\n * Stealth virus: A virus that hides the modifications it has made. The virus\n   tries to trick anti-virus software by intercepting its requests to the\n   operating system and providing false and bogus information.\n * Polymorphic virus: Produces varied but operational copies of itself. A\n   polymorphic virus**: may have no parts that remain identical between\n   infections, making it very difficult to detect directly using signatures.\n * Multipart virus: Also called a multipartite virus, this has several\n   components to it and can be distributed to different parts of the system. It\n   infects and spreads in multiple ways, which makes it harder to eradicate when\n   identified.\n * Self-garbling virus: Attempts to hide from anti-virus software by modifying\n   its own code so that it does not match predefined signatures.\n * Meme viruses: These are not actual computer viruses, but types of e-mail\n   messages that are continually forwarded around the Internet.\n * Bots: Software applications that run automated tasks over the Internet, which\n   perform tasks that are both simple and structurally repetitive. Malicious use\n   of bots is the coordination and operation of an automated attack by a botnet\n   (centrally controlled collection of bots).\n * Worms: These are different from viruses in that they can reproduce on their\n   own without a host application and are self-contained programs.\n * Logic bomb: Executes a program, or string of code, when a certain event\n   happens or a date and time arrives.\n * Rootkit: Set of malicious tools that are loaded on a compromised system\n   through stealthy techniques. The tools are used to carry out more attacks\n   either on the infected systems or surrounding systems.\n * Trojan horse: A program that is disguised as another program with the goal of\n   carrying out malicious activities in the background without the user knowing.\n * Remote access Trojans (RATs): Malicious programs that run on systems and\n   allow intruders to access and use a system remotely.\n * Immunizer: Attaches code to the file or application, which would fool a virus\n   into “thinking” it was already infected.\n * Behavior blocking: Allowing the suspicious code to execute within the\n   operating system and watches its interactions with the operating system,\n   looking for suspicious activities.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/1324596_6c02_5.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-12-04 02:09:53","created_by":"1","updated_at":"2021-03-31 14:11:27","updated_by":"1","published_at":"2017-12-04 05:06:56","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb2","uuid":"b640be30-e378-4fad-bafe-bd73b685c54a","title":"Cryptocurrency cannot be trusted (yet)","slug":"cryptocurrency-cannot-be-trusted-yet","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"So I posted a response to an article on Medium titled [Making Money Trustworthy - Bitcoin Explained](https://medium.com/@tessr/making-money-trustworthy-6c552a1cfc25) which wasn't a particularly outrageous article and had a lot of good content in it, but it was lacking an understanding of what **trust** even is and it didn't particularly try too hard to dig deep into why Homo Sapiens in the 21st century even inherently trust without questioning the value of currency.\\n\\nSo I responded with a titled post [The real reason bank currency is trustworthy and crypto currency cannot be (yet)](https://medium.com/@stoff/the-real-reason-bank-currency-is-trustworthy-and-crypto-currency-cannot-yet-861596a6f95f) which might not be direct enough...\\n\\nThe OP posted a 2-part series on the topic of Bitcoin and the first part was titled [Making Money - Bitcoin Explained](https://medium.com/@tessr/making-money-530d2bb2b8f7)\\n\\nHere I plan to reorganise that mess of thoughts I had left on Medium, into something more informative and provide some backup posts others have posted so I don't come across as a mad man (there are others like me that distrusts Bitcoin I swear!).\\n\\n# Bitcoin blockchain is an immutable, public, distributed ledger (usefulness?)\\n\\nAgree, but this is mostly technical jargon and the only useful part to people with short attention spans is where it says _Public_.\\n\\nHere is what the blockchain looks like;\\n\\n[![Transaction-Visualization-48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/Transaction-Visualization-48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194.jpg)](https://blockchain.info/tx/48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194)\\n\\nAnd if you click to follow the link in that visualisation you'll see data of when the transaction took place, of what fee's were taken, the BTC that was exchanged, and a lot of other _pseudonymous_ info (technical info) that is quite useless to the average person.\\n\\nSo being _Public_ really only helps people who understand the blockchain or offer a service to those that don't (where that service is currently missing in the ecosystem, things like legal or personal financial/tax services).\\n\\n# Authority is decentralised (a lie?)\\n\\nThis is a term thrown around as a matter-of-fact and nobody ever explains it well. The author I responded to wrote this phrase in part 1 and did not bother to explain it or even compare this statement to how it is not centralised! But somehow feels it appropriate to write in the conclusion \\n\\n> We’ve learned that Bitcoin is a decentralised currency\\n\\nDid you? I doubt you _learned_ it, rather you read it somewhere and matter-of-fact stated it in your post without demonstrating understanding of what decentralised means in more then 2 words.\\n\\nThere was a great post that appeared in Hacker News recently titled [Stop. Calling. Bitcoin. Decentralized.](https://medium.com/@homakov/stop-calling-bitcoin-decentralized-cb703d69dc27) which not only demonstrates understanding of what decentralised means it also describes with proof of how Bitcoin is not decentralised even if it was intended to be. I could not have described better how _Proof of Work_ is essentially broken.\\n\\n# Consensus Mechanism (hackable?)\\n\\nA lot of this discussion about decentralisation is actually due to the flaw in _Proof of Work_ can hack the Consensus Mechanisms in blockchain.\\n\\nYou can read an awesome post titled [Review of blockchain consensus mechanisms](https://blog.wavesplatform.com/review-of-blockchain-consensus-mechanisms-f575afae38f2) which is explained in simple terms of a deep dive into the topic.\\n\\nBasically, If _Proof of Work_ was suddenly not considered a Consensus Mechanism and we stick to one of the 2 versions of _Proof of Stake_ (LPoS and DPoS) then perhaps decentralisation won't be a lie any more. Time will tell.\\n\\n# Value of currency\\n\\nMany will ask\\n\\n> How does currency have value?\\n\\nThe simple answer is; when people use it to trade for items like food, cars, houses.\\n\\n_**Last month (Nov 2017) Australia saw a person offering to sell their Home in exchange for bitcoin, the most common known crypto currency.**_\\n\\nWhat an economist will tell you about the value of currency is that people deem it valuable when they can use it to trade for an item they deem as valuable as the currency used to complete the trade but just as importantly there is a consensus with the recipient of the currency, that they also deem that the same currency can later be used to exchange for an equal or more valuable item. This trade should occur with consensus, as in neither party needs to justify value or convince the other because that will define bartering which is not how currency works.\\n\\nSince value is consensus there is inherent trust that must exist.\\n\\n# How is currency trustworthy?\\n\\nMost people will say things like\\n\\n> I don’t know, because the banks say it’s safe\\n\\nOr\\n\\n> The government … words … so I trust it\\n\\nIt’s no shock that most people have such answers because who has ever questioned currency in our lifetime? or our grand parents lifetime?\\n\\nIf you’ve worked in enterprise, information, or cybersecurity and have an idea about how trust works it’s pretty much immediately obvious that trust in such fields applies to economics and even human relationships for that matter.\\n\\n_**Trust is when you don’t fear getting screwed**_\\n\\nThis is because there are systematic controls in place around how currency exists in our life, at all levels, from the *governance and process* over banks that protect the currency owner when the banks have possession of your currency, to *governance and process* over the human and machine tellers that will take deposit or dispense of your currency, and the information technology has *governance and processes* in place too.\\n\\n**Governance and Process** protect normal people, so you don’t even need to think about anything that you fear in regards to your currency being lost, stolen, lose value, all the bad things.\\n\\n> This is why you can't answer why you trust bank currency, because the system **WORKS** to maintain that trust and it does so without you ever knowing or not ever knowing you don't know for that matter!\\n\\nNow these are more controls then just *governance and processes* which are for protection of threats that are known. Bad people always find ways to do bad things and that’s where *law* and *due process* mostly help to compensate you when shit happens. You’ll probably get compensated if this *due process* works out in your favor.\\n\\n*Insurance* steps in here too, to remedy some other bad situations.\\n\\nSo why precisely do we trust currency?\\n\\n* Governance\\n* Process\\n* Law\\n* Due process\\n* Insurance\\n\\nBecause of all these protections, that have existed all our lives, our currency has continued to be traded and no one has screwed us out of our money. They may try but we except this unlikely risk, this low risk, and we are satisfied knowing we have a chance of getting back what we lost in value if these protections don’t fail us, we inherently trust currency.\\n\\n# Cryptocurrency doesn't deserve trust\\nBeside the fact cryptocurrency is the choice of money launderers and criminals generally, and if we are ignoring all cryptocurrencies portray the concise definition of a pyramid scheme; something designed to screw the people who invest so the ones at the top benefit.\\n\\nBeing ignorant to these, can you honestly say crypto currency has earned your trust? Do you simply trust it because you give away your trust so freely? Maybe until you get screwed then you’ll withdraw your trust?\\n\\nThat’s fair I guess, but only makes sense when you have no fear and being fearless is like being ignorant.\\n\\nOr do you trust cryptocurrency because one of your friends, peers, or family told you they trust cryptocurrency so it must be trustworthy right? (Heard about blind leading blind, con artists, or deception?).\\n\\nOk maybe cryptocurrency is trustworthy because what isn’t trust by definition if the majority agree? So I’ll try to educate to remove any ignorance and reveal the fear.\\n\\nHow about we compare cryptocurrency to something we do trust, actual bank currency, does it resemble any of the trustworthy traits listed above?\\n\\n_**In 2017 we saw the second largest cryptocurrency ETH release a software bug that wiped out millions of transactions by the time it was resolved. That’s like the bank claiming they lost all credits and debits from 5 days ago including your pay and the chairman of the board’s yacht purchase and they claim they have no way to recover any transaction so they’ll just go on pretending nothing happened. You have a full pay check missing and the chairman has a yacht proof of purchase.**_\\n\\nThis puts the context of the earlier topic of the blockchain consensus mechanism into context (read above if you skipped it) in a way that makes sense, you can literally buy majority shares in anything and with cryptocurrency it is demonstratable. Literally half of who knows who (the 51%) are needed to screw you out of your currency, and they can double their money at the least, creative thinking could allow them to be a evil as imagination allows.\\n\\nThis consensus mechanism has happened already, there’s a lot of precedent here, and a decentralised blockchain was meant to protect us from this.\\n\\n# It’s a Bubble\\nI’m going to paraphrase an ETH founder here, Charles Hoskinson once said that\\n\\n> Cryptocurrency is a ticking time-bomb\\n\\nHe goes on to explain that once regulations are set the whole cryptocurrency ecosystem will implode and the pieces left behind will learn how to start this again from scratch properly.\\n\\n## Let that sink in. \\n## Reread it.\\n\\nThe founder of ETH believes we will get governance soon and knowing what he knows about how crypto currency works, it won’t survive!\\n\\nWhat value is there in something like that?\\n\\nSo we have no governance, no laws, no protections, but we have proof of deception.\\n\\n> Be afraid, be very afraid. \\n\\nDon’t trust crypto currency (yet)\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>So I posted a response to an article on Medium titled <a href=\"https://medium.com/@tessr/making-money-trustworthy-6c552a1cfc25\">Making Money Trustworthy - Bitcoin Explained</a> which wasn't a particularly outrageous article and had a lot of good content in it, but it was lacking an understanding of what <strong>trust</strong> even is and it didn't particularly try too hard to dig deep into why Homo Sapiens in the 21st century even inherently trust without questioning the value of currency.</p>\n<p>So I responded with a titled post <a href=\"https://medium.com/@stoff/the-real-reason-bank-currency-is-trustworthy-and-crypto-currency-cannot-yet-861596a6f95f\">The real reason bank currency is trustworthy and crypto currency cannot be (yet)</a> which might not be direct enough...</p>\n<p>The OP posted a 2-part series on the topic of Bitcoin and the first part was titled <a href=\"https://medium.com/@tessr/making-money-530d2bb2b8f7\">Making Money - Bitcoin Explained</a></p>\n<p>Here I plan to reorganise that mess of thoughts I had left on Medium, into something more informative and provide some backup posts others have posted so I don't come across as a mad man (there are others like me that distrusts Bitcoin I swear!).</p>\n<h1 id=\"bitcoinblockchainisanimmutablepublicdistributedledgerusefulness\">Bitcoin blockchain is an immutable, public, distributed ledger (usefulness?)</h1>\n<p>Agree, but this is mostly technical jargon and the only useful part to people with short attention spans is where it says <em>Public</em>.</p>\n<p>Here is what the blockchain looks like;</p>\n<p><a href=\"https://blockchain.info/tx/48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194\"><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/Transaction-Visualization-48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194.jpg\" alt=\"Transaction-Visualization-48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194\" loading=\"lazy\"></a></p>\n<p>And if you click to follow the link in that visualisation you'll see data of when the transaction took place, of what fee's were taken, the BTC that was exchanged, and a lot of other <em>pseudonymous</em> info (technical info) that is quite useless to the average person.</p>\n<p>So being <em>Public</em> really only helps people who understand the blockchain or offer a service to those that don't (where that service is currently missing in the ecosystem, things like legal or personal financial/tax services).</p>\n<h1 id=\"authorityisdecentralisedalie\">Authority is decentralised (a lie?)</h1>\n<p>This is a term thrown around as a matter-of-fact and nobody ever explains it well. The author I responded to wrote this phrase in part 1 and did not bother to explain it or even compare this statement to how it is not centralised! But somehow feels it appropriate to write in the conclusion</p>\n<blockquote>\n<p>We’ve learned that Bitcoin is a decentralised currency</p>\n</blockquote>\n<p>Did you? I doubt you <em>learned</em> it, rather you read it somewhere and matter-of-fact stated it in your post without demonstrating understanding of what decentralised means in more then 2 words.</p>\n<p>There was a great post that appeared in Hacker News recently titled <a href=\"https://medium.com/@homakov/stop-calling-bitcoin-decentralized-cb703d69dc27\">Stop. Calling. Bitcoin. Decentralized.</a> which not only demonstrates understanding of what decentralised means it also describes with proof of how Bitcoin is not decentralised even if it was intended to be. I could not have described better how <em>Proof of Work</em> is essentially broken.</p>\n<h1 id=\"consensusmechanismhackable\">Consensus Mechanism (hackable?)</h1>\n<p>A lot of this discussion about decentralisation is actually due to the flaw in <em>Proof of Work</em> can hack the Consensus Mechanisms in blockchain.</p>\n<p>You can read an awesome post titled <a href=\"https://blog.wavesplatform.com/review-of-blockchain-consensus-mechanisms-f575afae38f2\">Review of blockchain consensus mechanisms</a> which is explained in simple terms of a deep dive into the topic.</p>\n<p>Basically, If <em>Proof of Work</em> was suddenly not considered a Consensus Mechanism and we stick to one of the 2 versions of <em>Proof of Stake</em> (LPoS and DPoS) then perhaps decentralisation won't be a lie any more. Time will tell.</p>\n<h1 id=\"valueofcurrency\">Value of currency</h1>\n<p>Many will ask</p>\n<blockquote>\n<p>How does currency have value?</p>\n</blockquote>\n<p>The simple answer is; when people use it to trade for items like food, cars, houses.</p>\n<p><em><strong>Last month (Nov 2017) Australia saw a person offering to sell their Home in exchange for bitcoin, the most common known crypto currency.</strong></em></p>\n<p>What an economist will tell you about the value of currency is that people deem it valuable when they can use it to trade for an item they deem as valuable as the currency used to complete the trade but just as importantly there is a consensus with the recipient of the currency, that they also deem that the same currency can later be used to exchange for an equal or more valuable item. This trade should occur with consensus, as in neither party needs to justify value or convince the other because that will define bartering which is not how currency works.</p>\n<p>Since value is consensus there is inherent trust that must exist.</p>\n<h1 id=\"howiscurrencytrustworthy\">How is currency trustworthy?</h1>\n<p>Most people will say things like</p>\n<blockquote>\n<p>I don’t know, because the banks say it’s safe</p>\n</blockquote>\n<p>Or</p>\n<blockquote>\n<p>The government … words … so I trust it</p>\n</blockquote>\n<p>It’s no shock that most people have such answers because who has ever questioned currency in our lifetime? or our grand parents lifetime?</p>\n<p>If you’ve worked in enterprise, information, or cybersecurity and have an idea about how trust works it’s pretty much immediately obvious that trust in such fields applies to economics and even human relationships for that matter.</p>\n<p><em><strong>Trust is when you don’t fear getting screwed</strong></em></p>\n<p>This is because there are systematic controls in place around how currency exists in our life, at all levels, from the <em>governance and process</em> over banks that protect the currency owner when the banks have possession of your currency, to <em>governance and process</em> over the human and machine tellers that will take deposit or dispense of your currency, and the information technology has <em>governance and processes</em> in place too.</p>\n<p><strong>Governance and Process</strong> protect normal people, so you don’t even need to think about anything that you fear in regards to your currency being lost, stolen, lose value, all the bad things.</p>\n<blockquote>\n<p>This is why you can't answer why you trust bank currency, because the system <strong>WORKS</strong> to maintain that trust and it does so without you ever knowing or not ever knowing you don't know for that matter!</p>\n</blockquote>\n<p>Now these are more controls then just <em>governance and processes</em> which are for protection of threats that are known. Bad people always find ways to do bad things and that’s where <em>law</em> and <em>due process</em> mostly help to compensate you when shit happens. You’ll probably get compensated if this <em>due process</em> works out in your favor.</p>\n<p><em>Insurance</em> steps in here too, to remedy some other bad situations.</p>\n<p>So why precisely do we trust currency?</p>\n<ul>\n<li>Governance</li>\n<li>Process</li>\n<li>Law</li>\n<li>Due process</li>\n<li>Insurance</li>\n</ul>\n<p>Because of all these protections, that have existed all our lives, our currency has continued to be traded and no one has screwed us out of our money. They may try but we except this unlikely risk, this low risk, and we are satisfied knowing we have a chance of getting back what we lost in value if these protections don’t fail us, we inherently trust currency.</p>\n<h1 id=\"cryptocurrencydoesntdeservetrust\">Cryptocurrency doesn't deserve trust</h1>\n<p>Beside the fact cryptocurrency is the choice of money launderers and criminals generally, and if we are ignoring all cryptocurrencies portray the concise definition of a pyramid scheme; something designed to screw the people who invest so the ones at the top benefit.</p>\n<p>Being ignorant to these, can you honestly say crypto currency has earned your trust? Do you simply trust it because you give away your trust so freely? Maybe until you get screwed then you’ll withdraw your trust?</p>\n<p>That’s fair I guess, but only makes sense when you have no fear and being fearless is like being ignorant.</p>\n<p>Or do you trust cryptocurrency because one of your friends, peers, or family told you they trust cryptocurrency so it must be trustworthy right? (Heard about blind leading blind, con artists, or deception?).</p>\n<p>Ok maybe cryptocurrency is trustworthy because what isn’t trust by definition if the majority agree? So I’ll try to educate to remove any ignorance and reveal the fear.</p>\n<p>How about we compare cryptocurrency to something we do trust, actual bank currency, does it resemble any of the trustworthy traits listed above?</p>\n<p><em><strong>In 2017 we saw the second largest cryptocurrency ETH release a software bug that wiped out millions of transactions by the time it was resolved. That’s like the bank claiming they lost all credits and debits from 5 days ago including your pay and the chairman of the board’s yacht purchase and they claim they have no way to recover any transaction so they’ll just go on pretending nothing happened. You have a full pay check missing and the chairman has a yacht proof of purchase.</strong></em></p>\n<p>This puts the context of the earlier topic of the blockchain consensus mechanism into context (read above if you skipped it) in a way that makes sense, you can literally buy majority shares in anything and with cryptocurrency it is demonstratable. Literally half of who knows who (the 51%) are needed to screw you out of your currency, and they can double their money at the least, creative thinking could allow them to be a evil as imagination allows.</p>\n<p>This consensus mechanism has happened already, there’s a lot of precedent here, and a decentralised blockchain was meant to protect us from this.</p>\n<h1 id=\"itsabubble\">It’s a Bubble</h1>\n<p>I’m going to paraphrase an ETH founder here, Charles Hoskinson once said that</p>\n<blockquote>\n<p>Cryptocurrency is a ticking time-bomb</p>\n</blockquote>\n<p>He goes on to explain that once regulations are set the whole cryptocurrency ecosystem will implode and the pieces left behind will learn how to start this again from scratch properly.</p>\n<h2 id=\"letthatsinkin\">Let that sink in.</h2>\n<h2 id=\"rereadit\">Reread it.</h2>\n<p>The founder of ETH believes we will get governance soon and knowing what he knows about how crypto currency works, it won’t survive!</p>\n<p>What value is there in something like that?</p>\n<p>So we have no governance, no laws, no protections, but we have proof of deception.</p>\n<blockquote>\n<p>Be afraid, be very afraid.</p>\n</blockquote>\n<p>Don’t trust crypto currency (yet)</p>\n<!--kg-card-end: markdown-->","comment_id":"5a25f75cd918b0058e125f3c","plaintext":"So I posted a response to an article on Medium titled Making Money Trustworthy\n-\nBitcoin Explained\n[https://medium.com/@tessr/making-money-trustworthy-6c552a1cfc25] which wasn't a\nparticularly outrageous article and had a lot of good content in it, but it was\nlacking an understanding of what trust even is and it didn't particularly try\ntoo hard to dig deep into why Homo Sapiens in the 21st century even inherently\ntrust without questioning the value of currency.\n\nSo I responded with a titled post The real reason bank currency is trustworthy\nand crypto currency cannot be (yet)\n[https://medium.com/@stoff/the-real-reason-bank-currency-is-trustworthy-and-crypto-currency-cannot-yet-861596a6f95f] \nwhich might not be direct enough...\n\nThe OP posted a 2-part series on the topic of Bitcoin and the first part was\ntitled Making Money - Bitcoin Explained\n[https://medium.com/@tessr/making-money-530d2bb2b8f7]\n\nHere I plan to reorganise that mess of thoughts I had left on Medium, into\nsomething more informative and provide some backup posts others have posted so I\ndon't come across as a mad man (there are others like me that distrusts Bitcoin\nI swear!).\n\nBitcoin blockchain is an immutable, public, distributed ledger (usefulness?)\nAgree, but this is mostly technical jargon and the only useful part to people\nwith short attention spans is where it says Public.\n\nHere is what the blockchain looks like;\n\n \n[https://blockchain.info/tx/48290844f040c1343a38633428a1d8cbcb3751939616520cfb031f5ea5180194]\n\nAnd if you click to follow the link in that visualisation you'll see data of\nwhen the transaction took place, of what fee's were taken, the BTC that was\nexchanged, and a lot of other pseudonymous info (technical info) that is quite\nuseless to the average person.\n\nSo being Public really only helps people who understand the blockchain or offer\na service to those that don't (where that service is currently missing in the\necosystem, things like legal or personal financial/tax services).\n\nAuthority is decentralised (a lie?)\nThis is a term thrown around as a matter-of-fact and nobody ever explains it\nwell. The author I responded to wrote this phrase in part 1 and did not bother\nto explain it or even compare this statement to how it is not centralised! But\nsomehow feels it appropriate to write in the conclusion\n\n> We’ve learned that Bitcoin is a decentralised currency\n\n\nDid you? I doubt you learned it, rather you read it somewhere and matter-of-fact\nstated it in your post without demonstrating understanding of what decentralised\nmeans in more then 2 words.\n\nThere was a great post that appeared in Hacker News recently titled Stop.\nCalling. Bitcoin. Decentralized.\n[https://medium.com/@homakov/stop-calling-bitcoin-decentralized-cb703d69dc27] \nwhich not only demonstrates understanding of what decentralised means it also\ndescribes with proof of how Bitcoin is not decentralised even if it was intended\nto be. I could not have described better how Proof of Work is essentially\nbroken.\n\nConsensus Mechanism (hackable?)\nA lot of this discussion about decentralisation is actually due to the flaw in \nProof of Work can hack the Consensus Mechanisms in blockchain.\n\nYou can read an awesome post titled Review of blockchain consensus mechanisms\n[https://blog.wavesplatform.com/review-of-blockchain-consensus-mechanisms-f575afae38f2] \nwhich is explained in simple terms of a deep dive into the topic.\n\nBasically, If Proof of Work was suddenly not considered a Consensus Mechanism\nand we stick to one of the 2 versions of Proof of Stake (LPoS and DPoS) then\nperhaps decentralisation won't be a lie any more. Time will tell.\n\nValue of currency\nMany will ask\n\n> How does currency have value?\n\n\nThe simple answer is; when people use it to trade for items like food, cars,\nhouses.\n\nLast month (Nov 2017) Australia saw a person offering to sell their Home in\nexchange for bitcoin, the most common known crypto currency.\n\nWhat an economist will tell you about the value of currency is that people deem\nit valuable when they can use it to trade for an item they deem as valuable as\nthe currency used to complete the trade but just as importantly there is a\nconsensus with the recipient of the currency, that they also deem that the same\ncurrency can later be used to exchange for an equal or more valuable item. This\ntrade should occur with consensus, as in neither party needs to justify value or\nconvince the other because that will define bartering which is not how currency\nworks.\n\nSince value is consensus there is inherent trust that must exist.\n\nHow is currency trustworthy?\nMost people will say things like\n\n> I don’t know, because the banks say it’s safe\n\n\nOr\n\n> The government … words … so I trust it\n\n\nIt’s no shock that most people have such answers because who has ever questioned\ncurrency in our lifetime? or our grand parents lifetime?\n\nIf you’ve worked in enterprise, information, or cybersecurity and have an idea\nabout how trust works it’s pretty much immediately obvious that trust in such\nfields applies to economics and even human relationships for that matter.\n\nTrust is when you don’t fear getting screwed\n\nThis is because there are systematic controls in place around how currency\nexists in our life, at all levels, from the governance and process over banks\nthat protect the currency owner when the banks have possession of your currency,\nto governance and process over the human and machine tellers that will take\ndeposit or dispense of your currency, and the information technology has \ngovernance and processes in place too.\n\nGovernance and Process protect normal people, so you don’t even need to think\nabout anything that you fear in regards to your currency being lost, stolen,\nlose value, all the bad things.\n\n> This is why you can't answer why you trust bank currency, because the system \nWORKS to maintain that trust and it does so without you ever knowing or not ever\nknowing you don't know for that matter!\n\n\nNow these are more controls then just governance and processes which are for\nprotection of threats that are known. Bad people always find ways to do bad\nthings and that’s where law and due process mostly help to compensate you when\nshit happens. You’ll probably get compensated if this due process works out in\nyour favor.\n\nInsurance steps in here too, to remedy some other bad situations.\n\nSo why precisely do we trust currency?\n\n * Governance\n * Process\n * Law\n * Due process\n * Insurance\n\nBecause of all these protections, that have existed all our lives, our currency\nhas continued to be traded and no one has screwed us out of our money. They may\ntry but we except this unlikely risk, this low risk, and we are satisfied\nknowing we have a chance of getting back what we lost in value if these\nprotections don’t fail us, we inherently trust currency.\n\nCryptocurrency doesn't deserve trust\nBeside the fact cryptocurrency is the choice of money launderers and criminals\ngenerally, and if we are ignoring all cryptocurrencies portray the concise\ndefinition of a pyramid scheme; something designed to screw the people who\ninvest so the ones at the top benefit.\n\nBeing ignorant to these, can you honestly say crypto currency has earned your\ntrust? Do you simply trust it because you give away your trust so freely? Maybe\nuntil you get screwed then you’ll withdraw your trust?\n\nThat’s fair I guess, but only makes sense when you have no fear and being\nfearless is like being ignorant.\n\nOr do you trust cryptocurrency because one of your friends, peers, or family\ntold you they trust cryptocurrency so it must be trustworthy right? (Heard about\nblind leading blind, con artists, or deception?).\n\nOk maybe cryptocurrency is trustworthy because what isn’t trust by definition if\nthe majority agree? So I’ll try to educate to remove any ignorance and reveal\nthe fear.\n\nHow about we compare cryptocurrency to something we do trust, actual bank\ncurrency, does it resemble any of the trustworthy traits listed above?\n\nIn 2017 we saw the second largest cryptocurrency ETH release a software bug that\nwiped out millions of transactions by the time it was resolved. That’s like the\nbank claiming they lost all credits and debits from 5 days ago including your\npay and the chairman of the board’s yacht purchase and they claim they have no\nway to recover any transaction so they’ll just go on pretending nothing\nhappened. You have a full pay check missing and the chairman has a yacht proof\nof purchase.\n\nThis puts the context of the earlier topic of the blockchain consensus mechanism\ninto context (read above if you skipped it) in a way that makes sense, you can\nliterally buy majority shares in anything and with cryptocurrency it is\ndemonstratable. Literally half of who knows who (the 51%) are needed to screw\nyou out of your currency, and they can double their money at the least, creative\nthinking could allow them to be a evil as imagination allows.\n\nThis consensus mechanism has happened already, there’s a lot of precedent here,\nand a decentralised blockchain was meant to protect us from this.\n\nIt’s a Bubble\nI’m going to paraphrase an ETH founder here, Charles Hoskinson once said that\n\n> Cryptocurrency is a ticking time-bomb\n\n\nHe goes on to explain that once regulations are set the whole cryptocurrency\necosystem will implode and the pieces left behind will learn how to start this\nagain from scratch properly.\n\nLet that sink in.\nReread it.\nThe founder of ETH believes we will get governance soon and knowing what he\nknows about how crypto currency works, it won’t survive!\n\nWhat value is there in something like that?\n\nSo we have no governance, no laws, no protections, but we have proof of\ndeception.\n\n> Be afraid, be very afraid.\n\n\nDon’t trust crypto currency (yet)","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/cryptocoins.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2017-12-05 01:33:16","created_by":"1","updated_at":"2021-03-31 14:11:05","updated_by":"1","published_at":"2017-12-04 08:17:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb3","uuid":"ddcd86e0-88d0-4577-926e-ce3d81bb4a76","title":"Privacy Controls for your Risk Assessment","slug":"privacy-in-your-risk-assessment-process","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"With GDPR (General Data Protection Regulation) making waves, countries around the world have started changing their Privacy laws.\\n\\nBeing Australian, most businesses I work with mainly operate within Australia and New Zealand before GDPR they were either not subject to or not overly concerned with International laws.\\n\\n> [Start here](https://gdpr-info.eu) to learn how GDPR reaches far beyond Europe\\n\\nAustralia amended the Privacy Act which mainly included compulsory breach notification requirements, this was applied in 2017 and the grace period for enforcement ends 22nd February 2018.\\nPrior to this (2014) 13 Privacy Principles were established in Australia and similarly New Zealand law lays out 12 Information Privacy Principles.\\n\\n# When to assess Privacy\\nIt is different for each country so always consult a professional (shameless pitch for my services).\\n\\nThe primary indicators for knowing if you should do an assessment would be any one of;\\n- Data is collected directly from the individual\\n- The data can be used as is or in combination with other data to identify the individual\\n- The individual is a natural living person\\n- Data is not just text it can be audio, image, and video too\\n\\nThese are guidelines, not inclusive, and only 1 indicator would mean you should complete your assessment.\\n\\nIt is also important to realize that in most cases Privacy law may be superseded by other laws, for instance in health care across the globe you will encounter laws that are more stringent and apply to the same data so make sure you treat Privacy as a starting point.\\n\\n# Breach Notification\\n\\nNew Zealand are currently in the process of passing new law that proposes compulsory breach notification but at the time of writing there are no current requirements to do so, but it is highly encouraged. However if you collect data of citizens of other countries and operate in New Zealand there may still be a requirement for you to notify someone.\\n\\nIn Australia where breach notifications are now compulsory, and due to GDPR, my advice is to create a process around breach notifications and practice it regularly so you are prepared to carry out your obligations when required.\\n\\nEstablish a process for breach notification drills, and practice it. by doing this you are going to ensure that you're up to date and fully grasp all obligations you must fulfill prior to a breach event occurring.\\n\\n# Recommended Controls\\n\\nThe main concerns you will have is how to protect the data and show an auditor evidence of your diligence.\\n\\n## Encrypt encrypt encrypt\\n\\nConsider both at rest and in-transit encryption. There is no better advice.\\n\\n> If you take only one thing away let it be this, because it may negate the need to perform a compulsory breach notification altogether\\n\\nFor specifics here, it is important to understand that the `use` and `disclosure` of data have unique definitions. For instance a system that securely transfers data to another company (entity, agency, etc) such as marketing might be considered a `use` whereas an employee that emails that same data to the same recipient might be considered a `disclosure`.\\n\\nThis is due to the nature of email being inherently insecure and unable to be secured. DKIM only goes so far as to try to assure the identity of senders, it does not provide security over the transmission or the data and emails are open to interceptions and MIIM (man in the middle) attacks.\\n\\nTo prevent accidental disclosure during operations (people are flawed) encourage the use of tools that encrypt data, and for emails always send as encrypted attachments which might mean asking the recipient for a public key so they can decrypt the message at their end.\\n\\nIf your company is in the business of handling private or restricted data and rely on humans to do any task that involves access to that data it is going to be key to your success to educate everyone at all levels about encryption. Find the right tools and make it habit that your employees ask for public keys to securely transmit encrypted data, and are empowered to confidentially issue public keys themselves when requesting data from outside or trusted third parties.\\n\\nJust make sure keys are confidentially exchanged using a different channel to where the data exchange will be done.\\n\\n## Write up your analysis\\n\\nAuditors use evidence based process to be deterministic in their approach, so it is important to have a paper trail to prove you were diligent from planning, to implementation, and finally during BAU (business as usual).\\nYou may wish to consider documenting a simple privacy impact assessment if not only to show you fulfilled the planning diligence but it is a great internal reference for you when you start a new project, it can be a repeatable process and promote agility.\\n\\n> There is no \\\"right\\\" way of writing a privacy impact assessment.\\n\\nMake your privacy impact assessment as detailed as it needs to be to help you make the right decisions. Frequently refer back to it during breach drills and such to make sure you're up to date.\\n\\nIt is possible to arrange an [independent AoC](https://www.oaic.gov.au/privacy-law/assessments/) (Attestation of Compliance) so that you are sure where you stand. The measurable benefit to attaining an AoC is with it you can promptly supply customers assurance that you've been independently assessed and meet the requirements which is a huge time saver when normally you would be conducting unique risk assessments with each customer.\\n\\n## Information Accuracy\\n\\nIn terms of Australia and GDPR there are some serious concerns around accuracy due to how broad Privacy extends (banking, credit, claims, etc), so I encourage seeking professional advice once again.\\n\\nIt may be surprising to some that New Zealand has consequences that may be faced when dealing with accuracy of Privacy data. IPP 8 talks to the need of accuracy to be checked before use. \\n> The more likely it is that the information is inaccurate, out of date, or incomplete, the more reasonable it is for validation at time of use.\\n\\n- Things such as birth place or dates do not become out of date or incomplete\\n- Citizenship, occupation, addresses, and marital status or surname may be inaccurate and can easily become out of date\\n- More complex data like a criminal history or medical report can be inaccurate, out of date, and incomplete\\n\\nDepending on the seriousness of the consequences it may be best to check with the original source before use.\\n\\nFor the technical people data sanity checking can be hard, but it is my experience that if you build verification into the collection phase it is a matter of automating the same technique at a later time to reduce the effort of manual checking in the future.\\n\\n## Data retention\\n\\nSimply due to the New Zealand IPP 9 requirement that personal information must not be kept longer than necessary, you might want to determine a retention period for all of the Privacy information you store.\\n\\nIn almost all cases retention will be dictated by the terms for which the data was collected, so as part of the data collection process it is important that you obtain permission from individuals of how their data may be used.\\n\\nFor GDPR and New Zealand it is not reasonable to simply gather data with an all encompassing privacy statement that allows you to retain data indefinitely or without being specific in how it will be used.\\n\\nFor example, if you harvest email addresses at a trade show or event for the purposes of marketing into the future you will have a tough time being compliant, however if the individual was providing their data to you as an expression of interest to be converted to a customer you have the ability to stipulate wording in the agreement for this use case and an expected period for retention. This provides the individual expectation that data will be deleted by the agreed date if they chose not to convert to being a customer and your business the right to contact them reasonably.\\n\\nThe take away here for the technically minded is that you should ensure data has been classified and that whatever classification model you chose you ensure that you place a definitive retention date for all Privacy data.\\n\\n# Resources\\nInternational Privacy Law Library\\nhttp://www.worldlii.org/int/special/privacy/\\n\\nThe Privacy Commissioner's Office of New Zealand\\nhttps://www.privacy.org.nz/privacy-for-agencies/getting-started/\\n\\nGeneral Data Protection Regulation (GDPR)\\nhttps://gdpr-info.eu\\n\\nNotifiable Data Breaches scheme of Australian  Privacy Act 1988\\nhttps://www.oaic.gov.au/privacy-law/privacy-act/notifiable-data-breaches-scheme\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>With GDPR (General Data Protection Regulation) making waves, countries around the world have started changing their Privacy laws.</p>\n<p>Being Australian, most businesses I work with mainly operate within Australia and New Zealand before GDPR they were either not subject to or not overly concerned with International laws.</p>\n<blockquote>\n<p><a href=\"https://gdpr-info.eu\">Start here</a> to learn how GDPR reaches far beyond Europe</p>\n</blockquote>\n<p>Australia amended the Privacy Act which mainly included compulsory breach notification requirements, this was applied in 2017 and the grace period for enforcement ends 22nd February 2018.<br>\nPrior to this (2014) 13 Privacy Principles were established in Australia and similarly New Zealand law lays out 12 Information Privacy Principles.</p>\n<h1 id=\"whentoassessprivacy\">When to assess Privacy</h1>\n<p>It is different for each country so always consult a professional (shameless pitch for my services).</p>\n<p>The primary indicators for knowing if you should do an assessment would be any one of;</p>\n<ul>\n<li>Data is collected directly from the individual</li>\n<li>The data can be used as is or in combination with other data to identify the individual</li>\n<li>The individual is a natural living person</li>\n<li>Data is not just text it can be audio, image, and video too</li>\n</ul>\n<p>These are guidelines, not inclusive, and only 1 indicator would mean you should complete your assessment.</p>\n<p>It is also important to realize that in most cases Privacy law may be superseded by other laws, for instance in health care across the globe you will encounter laws that are more stringent and apply to the same data so make sure you treat Privacy as a starting point.</p>\n<h1 id=\"breachnotification\">Breach Notification</h1>\n<p>New Zealand are currently in the process of passing new law that proposes compulsory breach notification but at the time of writing there are no current requirements to do so, but it is highly encouraged. However if you collect data of citizens of other countries and operate in New Zealand there may still be a requirement for you to notify someone.</p>\n<p>In Australia where breach notifications are now compulsory, and due to GDPR, my advice is to create a process around breach notifications and practice it regularly so you are prepared to carry out your obligations when required.</p>\n<p>Establish a process for breach notification drills, and practice it. by doing this you are going to ensure that you're up to date and fully grasp all obligations you must fulfill prior to a breach event occurring.</p>\n<h1 id=\"recommendedcontrols\">Recommended Controls</h1>\n<p>The main concerns you will have is how to protect the data and show an auditor evidence of your diligence.</p>\n<h2 id=\"encryptencryptencrypt\">Encrypt encrypt encrypt</h2>\n<p>Consider both at rest and in-transit encryption. There is no better advice.</p>\n<blockquote>\n<p>If you take only one thing away let it be this, because it may negate the need to perform a compulsory breach notification altogether</p>\n</blockquote>\n<p>For specifics here, it is important to understand that the <code>use</code> and <code>disclosure</code> of data have unique definitions. For instance a system that securely transfers data to another company (entity, agency, etc) such as marketing might be considered a <code>use</code> whereas an employee that emails that same data to the same recipient might be considered a <code>disclosure</code>.</p>\n<p>This is due to the nature of email being inherently insecure and unable to be secured. DKIM only goes so far as to try to assure the identity of senders, it does not provide security over the transmission or the data and emails are open to interceptions and MIIM (man in the middle) attacks.</p>\n<p>To prevent accidental disclosure during operations (people are flawed) encourage the use of tools that encrypt data, and for emails always send as encrypted attachments which might mean asking the recipient for a public key so they can decrypt the message at their end.</p>\n<p>If your company is in the business of handling private or restricted data and rely on humans to do any task that involves access to that data it is going to be key to your success to educate everyone at all levels about encryption. Find the right tools and make it habit that your employees ask for public keys to securely transmit encrypted data, and are empowered to confidentially issue public keys themselves when requesting data from outside or trusted third parties.</p>\n<p>Just make sure keys are confidentially exchanged using a different channel to where the data exchange will be done.</p>\n<h2 id=\"writeupyouranalysis\">Write up your analysis</h2>\n<p>Auditors use evidence based process to be deterministic in their approach, so it is important to have a paper trail to prove you were diligent from planning, to implementation, and finally during BAU (business as usual).<br>\nYou may wish to consider documenting a simple privacy impact assessment if not only to show you fulfilled the planning diligence but it is a great internal reference for you when you start a new project, it can be a repeatable process and promote agility.</p>\n<blockquote>\n<p>There is no &quot;right&quot; way of writing a privacy impact assessment.</p>\n</blockquote>\n<p>Make your privacy impact assessment as detailed as it needs to be to help you make the right decisions. Frequently refer back to it during breach drills and such to make sure you're up to date.</p>\n<p>It is possible to arrange an <a href=\"https://www.oaic.gov.au/privacy-law/assessments/\">independent AoC</a> (Attestation of Compliance) so that you are sure where you stand. The measurable benefit to attaining an AoC is with it you can promptly supply customers assurance that you've been independently assessed and meet the requirements which is a huge time saver when normally you would be conducting unique risk assessments with each customer.</p>\n<h2 id=\"informationaccuracy\">Information Accuracy</h2>\n<p>In terms of Australia and GDPR there are some serious concerns around accuracy due to how broad Privacy extends (banking, credit, claims, etc), so I encourage seeking professional advice once again.</p>\n<p>It may be surprising to some that New Zealand has consequences that may be faced when dealing with accuracy of Privacy data. IPP 8 talks to the need of accuracy to be checked before use.</p>\n<blockquote>\n<p>The more likely it is that the information is inaccurate, out of date, or incomplete, the more reasonable it is for validation at time of use.</p>\n</blockquote>\n<ul>\n<li>Things such as birth place or dates do not become out of date or incomplete</li>\n<li>Citizenship, occupation, addresses, and marital status or surname may be inaccurate and can easily become out of date</li>\n<li>More complex data like a criminal history or medical report can be inaccurate, out of date, and incomplete</li>\n</ul>\n<p>Depending on the seriousness of the consequences it may be best to check with the original source before use.</p>\n<p>For the technical people data sanity checking can be hard, but it is my experience that if you build verification into the collection phase it is a matter of automating the same technique at a later time to reduce the effort of manual checking in the future.</p>\n<h2 id=\"dataretention\">Data retention</h2>\n<p>Simply due to the New Zealand IPP 9 requirement that personal information must not be kept longer than necessary, you might want to determine a retention period for all of the Privacy information you store.</p>\n<p>In almost all cases retention will be dictated by the terms for which the data was collected, so as part of the data collection process it is important that you obtain permission from individuals of how their data may be used.</p>\n<p>For GDPR and New Zealand it is not reasonable to simply gather data with an all encompassing privacy statement that allows you to retain data indefinitely or without being specific in how it will be used.</p>\n<p>For example, if you harvest email addresses at a trade show or event for the purposes of marketing into the future you will have a tough time being compliant, however if the individual was providing their data to you as an expression of interest to be converted to a customer you have the ability to stipulate wording in the agreement for this use case and an expected period for retention. This provides the individual expectation that data will be deleted by the agreed date if they chose not to convert to being a customer and your business the right to contact them reasonably.</p>\n<p>The take away here for the technically minded is that you should ensure data has been classified and that whatever classification model you chose you ensure that you place a definitive retention date for all Privacy data.</p>\n<h1 id=\"resources\">Resources</h1>\n<p>International Privacy Law Library<br>\n<a href=\"http://www.worldlii.org/int/special/privacy/\">http://www.worldlii.org/int/special/privacy/</a></p>\n<p>The Privacy Commissioner's Office of New Zealand<br>\n<a href=\"https://www.privacy.org.nz/privacy-for-agencies/getting-started/\">https://www.privacy.org.nz/privacy-for-agencies/getting-started/</a></p>\n<p>General Data Protection Regulation (GDPR)<br>\n<a href=\"https://gdpr-info.eu\">https://gdpr-info.eu</a></p>\n<p>Notifiable Data Breaches scheme of Australian  Privacy Act 1988<br>\n<a href=\"https://www.oaic.gov.au/privacy-law/privacy-act/notifiable-data-breaches-scheme\">https://www.oaic.gov.au/privacy-law/privacy-act/notifiable-data-breaches-scheme</a></p>\n<!--kg-card-end: markdown-->","comment_id":"5a4f827ad918b0058e125f4c","plaintext":"With GDPR (General Data Protection Regulation) making waves, countries around\nthe world have started changing their Privacy laws.\n\nBeing Australian, most businesses I work with mainly operate within Australia\nand New Zealand before GDPR they were either not subject to or not overly\nconcerned with International laws.\n\n> Start here [https://gdpr-info.eu] to learn how GDPR reaches far beyond Europe\n\n\nAustralia amended the Privacy Act which mainly included compulsory breach\nnotification requirements, this was applied in 2017 and the grace period for\nenforcement ends 22nd February 2018.\nPrior to this (2014) 13 Privacy Principles were established in Australia and\nsimilarly New Zealand law lays out 12 Information Privacy Principles.\n\nWhen to assess Privacy\nIt is different for each country so always consult a professional (shameless\npitch for my services).\n\nThe primary indicators for knowing if you should do an assessment would be any\none of;\n\n * Data is collected directly from the individual\n * The data can be used as is or in combination with other data to identify the\n   individual\n * The individual is a natural living person\n * Data is not just text it can be audio, image, and video too\n\nThese are guidelines, not inclusive, and only 1 indicator would mean you should\ncomplete your assessment.\n\nIt is also important to realize that in most cases Privacy law may be superseded\nby other laws, for instance in health care across the globe you will encounter\nlaws that are more stringent and apply to the same data so make sure you treat\nPrivacy as a starting point.\n\nBreach Notification\nNew Zealand are currently in the process of passing new law that proposes\ncompulsory breach notification but at the time of writing there are no current\nrequirements to do so, but it is highly encouraged. However if you collect data\nof citizens of other countries and operate in New Zealand there may still be a\nrequirement for you to notify someone.\n\nIn Australia where breach notifications are now compulsory, and due to GDPR, my\nadvice is to create a process around breach notifications and practice it\nregularly so you are prepared to carry out your obligations when required.\n\nEstablish a process for breach notification drills, and practice it. by doing\nthis you are going to ensure that you're up to date and fully grasp all\nobligations you must fulfill prior to a breach event occurring.\n\nRecommended Controls\nThe main concerns you will have is how to protect the data and show an auditor\nevidence of your diligence.\n\nEncrypt encrypt encrypt\nConsider both at rest and in-transit encryption. There is no better advice.\n\n> If you take only one thing away let it be this, because it may negate the need\nto perform a compulsory breach notification altogether\n\n\nFor specifics here, it is important to understand that the use and disclosure of\ndata have unique definitions. For instance a system that securely transfers data\nto another company (entity, agency, etc) such as marketing might be considered a \nuse whereas an employee that emails that same data to the same recipient might\nbe considered a disclosure.\n\nThis is due to the nature of email being inherently insecure and unable to be\nsecured. DKIM only goes so far as to try to assure the identity of senders, it\ndoes not provide security over the transmission or the data and emails are open\nto interceptions and MIIM (man in the middle) attacks.\n\nTo prevent accidental disclosure during operations (people are flawed) encourage\nthe use of tools that encrypt data, and for emails always send as encrypted\nattachments which might mean asking the recipient for a public key so they can\ndecrypt the message at their end.\n\nIf your company is in the business of handling private or restricted data and\nrely on humans to do any task that involves access to that data it is going to\nbe key to your success to educate everyone at all levels about encryption. Find\nthe right tools and make it habit that your employees ask for public keys to\nsecurely transmit encrypted data, and are empowered to confidentially issue\npublic keys themselves when requesting data from outside or trusted third\nparties.\n\nJust make sure keys are confidentially exchanged using a different channel to\nwhere the data exchange will be done.\n\nWrite up your analysis\nAuditors use evidence based process to be deterministic in their approach, so it\nis important to have a paper trail to prove you were diligent from planning, to\nimplementation, and finally during BAU (business as usual).\nYou may wish to consider documenting a simple privacy impact assessment if not\nonly to show you fulfilled the planning diligence but it is a great internal\nreference for you when you start a new project, it can be a repeatable process\nand promote agility.\n\n> There is no \"right\" way of writing a privacy impact assessment.\n\n\nMake your privacy impact assessment as detailed as it needs to be to help you\nmake the right decisions. Frequently refer back to it during breach drills and\nsuch to make sure you're up to date.\n\nIt is possible to arrange an independent AoC\n[https://www.oaic.gov.au/privacy-law/assessments/] (Attestation of Compliance)\nso that you are sure where you stand. The measurable benefit to attaining an AoC\nis with it you can promptly supply customers assurance that you've been\nindependently assessed and meet the requirements which is a huge time saver when\nnormally you would be conducting unique risk assessments with each customer.\n\nInformation Accuracy\nIn terms of Australia and GDPR there are some serious concerns around accuracy\ndue to how broad Privacy extends (banking, credit, claims, etc), so I encourage\nseeking professional advice once again.\n\nIt may be surprising to some that New Zealand has consequences that may be faced\nwhen dealing with accuracy of Privacy data. IPP 8 talks to the need of accuracy\nto be checked before use.\n\n> The more likely it is that the information is inaccurate, out of date, or\nincomplete, the more reasonable it is for validation at time of use.\n\n\n * Things such as birth place or dates do not become out of date or incomplete\n * Citizenship, occupation, addresses, and marital status or surname may be\n   inaccurate and can easily become out of date\n * More complex data like a criminal history or medical report can be\n   inaccurate, out of date, and incomplete\n\nDepending on the seriousness of the consequences it may be best to check with\nthe original source before use.\n\nFor the technical people data sanity checking can be hard, but it is my\nexperience that if you build verification into the collection phase it is a\nmatter of automating the same technique at a later time to reduce the effort of\nmanual checking in the future.\n\nData retention\nSimply due to the New Zealand IPP 9 requirement that personal information must\nnot be kept longer than necessary, you might want to determine a retention\nperiod for all of the Privacy information you store.\n\nIn almost all cases retention will be dictated by the terms for which the data\nwas collected, so as part of the data collection process it is important that\nyou obtain permission from individuals of how their data may be used.\n\nFor GDPR and New Zealand it is not reasonable to simply gather data with an all\nencompassing privacy statement that allows you to retain data indefinitely or\nwithout being specific in how it will be used.\n\nFor example, if you harvest email addresses at a trade show or event for the\npurposes of marketing into the future you will have a tough time being\ncompliant, however if the individual was providing their data to you as an\nexpression of interest to be converted to a customer you have the ability to\nstipulate wording in the agreement for this use case and an expected period for\nretention. This provides the individual expectation that data will be deleted by\nthe agreed date if they chose not to convert to being a customer and your\nbusiness the right to contact them reasonably.\n\nThe take away here for the technically minded is that you should ensure data has\nbeen classified and that whatever classification model you chose you ensure that\nyou place a definitive retention date for all Privacy data.\n\nResources\nInternational Privacy Law Library\nhttp://www.worldlii.org/int/special/privacy/\n\nThe Privacy Commissioner's Office of New Zealand\nhttps://www.privacy.org.nz/privacy-for-agencies/getting-started/\n\nGeneral Data Protection Regulation (GDPR)\nhttps://gdpr-info.eu\n\nNotifiable Data Breaches scheme of Australian Privacy Act 1988\nhttps://www.oaic.gov.au/privacy-law/privacy-act/notifiable-data-breaches-scheme","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/01/privacy-and-security.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-01-05 13:49:46","created_by":"1","updated_at":"2021-03-31 14:11:16","updated_by":"1","published_at":"2018-01-21 09:39:00","published_by":"1","custom_excerpt":"Information to help you deal with Privacy risk, When to assess Privacy, Breach Notification, Recommended Controls, and Resources.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb4","uuid":"7c3b42c2-4cc3-40a7-94c1-fea13bf498e7","title":"GDPR compliance beyond Europe","slug":"gdpr-compliance-beyond-europe","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"GDPR comes into effect in May 2018 and one of the buzz phrases you might have heard is \\n> the right to be forgotten\\n\\nBut there is much more you need to know and the effects are far reaching beyond Europe.\\n\\n# What Assessments are needed?\\nData protection impact assessments (DPIAs) or Privacy Impact Assessment (PIAs) are required by GDPR.\\n\\nA D/PIA must be carried out prior to the implementation of the technology, project, activity or process and ideally as early as practical in the design process.\\n\\nThe D/PIA will also need to be updated and/or steps repeated as the process develops, particularly if issues are identified which may affect the severity or likelihood of risk to the data protection rights of affected individuals.\\n\\n# GDPR Assessment Requirements \\nGDPR article 35(7) lists the minimum requirements a DPIA must provide and contain:\\n\\n* a systematic description of the envisaged processing operations and the purposes of the processing, including, where applicable, the legitimate interest pursued by the controller\\n* an assessment of the necessity and proportionality of the processing operations in relation to the purposes\\n* an assessment of the risks to the rights and freedoms of data subjects\\n* the measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data and to demonstrate compliance with this Regulation taking into account the rights and legitimate interests of data subjects and other persons concerned.\\n\\nA DPIA report must also contain\\n\\n* purposes of processing\\n* the stakeholders\\n* categories and types of private data processed in the system\\n* characterization of types of data flows (e.g. is the data transferred?)\\n\\nThese descriptions should be clear. Clarity is valued in DPIA. Do not take any opportunity to document ambiguity for business benefit, it will only work against your efforts to be deemed compliant.\\n\\n# Australian business considerations\\n\\nYou should ensure that the entity fits into these GDPR scopes before starting a DPIA.\\n\\nAccording to OAIC: [consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation](https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation)\\n\\nAustralian businesses that may be covered include:\\n\\n* an Australian business with an office in the EU\\n* recital 23 - an Australian business whose website enables EU customers to order goods or services in a European language (other than English) or enables payment in euros\\n* recital 23 - an Australian business whose website mentions customers or users in the EU\\n* recital 24 - an Australian business that tracks individuals in the EU on the Internet and uses data processing techniques to profile individuals to analyse and predict personal preferences, behaviours, and attitudes - (section 6)\\n\\nIf the entity fits into these GDPR scopes you would need to consider the whole OAIC guidance.\\n\\nTo check whether you need to also comply with the Privacy Act, you can complete the privacy checklist for small business found on the OAIC website [here](http://www.oaic.gov.au/privacy/privacy-resources/privacy-business-resources/privacy-business-resource-10).\\n\\n# You may not need to do anything\\n\\nSo you've identified that your business must comply but what exactly does that mean?\\n\\nIt may mean you need to complete a few additional tasks, or none at all. It all depends on your current privacy posture.\\n\\n## Assessments\\n\\nIf you currently complete a PIA or DPIA on a project by project basis to meet the privacy obligations of your country (Such is the case in Australia), you do not need to change a thing about this practice.\\nHowever to meet the obligations of GDPR and you do not currently complete either a PIA or DPIA you must be prepared to do so for all previous and future projects.\\n\\n## Accountability and governance\\n\\nBoth accountability and governance are key concepts in GDPR (Article 5, 24, and 25 speak to them) and are required if you wish to be compliant. Having an appointed CISO and thus a Security Steering Committee Policy is a great start as it provides a level of accountability and governance however a well considered ISMS or alternatively seek standards accreditation such as ISO/IEC 27014:2013 are assurances well considered also.\\n\\n## Consent\\n\\nIn all cases where privacy data is collected you should now be seeking consent from the individual not only that you may use their data but exactly how you intend to use the data now and into the future. If you intend to share their data with any other entities this must also be consented too. And if any of these change you must obtain consent anew. \\n\\nThe GDPR includes a new definition of consent, which states that it must be freely given, specific, informed, and an unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing (Article 4(11)).\\n\\nConsent is not freely given if the individual has no genuine or free choice or is unable to refuse or withdraw consent at any time (Article 7 and recital 42). Businesses also need to make the withdrawal of consent as easy as giving consent, and, before individuals give consent, must inform individuals about this right to withdraw consent (Article 7(3)).\\n\\nSpecific requirements apply in relation to children’s consent if an individual below 16 years.\\n\\n## Breach Notifications\\n\\nAt the time of writing; in four days, Australia enters into a new era where mandatory data breach notifications are enforcible.\\nHaving been lawful legislation for 12 months, Australian businesses have enjoyed a grace period for which they hopefully spent becoming compliant. Which is actually a simple process to start; by creating a data breach response plan and ongoing practice/drills to remain relevant.\\n\\nSo it is no extra effort for Australian businesses to comply with GDPR as the breach notification obligations are aligned. Outside Australia however you may still need to establish a data breach response plan if you wish to be complaint with GDPR.\\n\\n## The right to erasure\\n\\nKnown as the `right to be forgotten`, where the individual withdraws their consent you must delete their data. Additionally where the information is no longer necessary for the purpose for which it was collected and there is no other legal ground for processing their data, you must also delete the individuals data.\\n\\nThere is no equivalent \\\"right to erasure\\\" under the Privacy Act in Australia, however APP 11.2 requires an APP entity that holds personal information to take reasonable steps to destroy the information or to ensure it is de-identified if the information is no longer needed for any purpose permitted under the Privacy Act; which is aligned to the second right of GDPR. If you have already demonstrated your compliance to APP 11.2 and have also provided the means for individuals to withdraw consent, then you are already aligned to this obligation of GDPR.\\n\\n# Sanctions\\n\\nThe GDPR gives supervisory authorities the power to impose administrative fines for contraventions, with fines of up to €20 million or 4% of annual worldwide turnover, whichever is greater for certain types of contraventions (Article 83(5)).\\n\\nBasically, you don't want to be unprepared. Many businesses such as online advertisers, recruitment agencies, and sales consultants should take a moment and consider their practices - any individual may be protected by European laws and therefore you are responsible for your businesses compliance to GDPR.\\n\\n# Where can I get help or more information?\\n\\n- Find a GRC Information Security Consultant (GRC = Governance, Regulation, and Compliance) like myself to guide you through the process and provide assistance to complete each step with you and your business objectives.\\n- [OAIC Consultation draft for Australian businesses to comply with GDPR](https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation)\\n- [UK ICO website GDPR guidance](https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/)\\n- [European Commission](http://ec.europa.eu/justice/data-protection/reform/index_en.htm)\\n\\nFollow me on Twitter [@chrisdlangton](https://twitter.com/chrisdlangton)\\n\\nOr connect with me [on LinkedIn](https://www.linkedin.com/in/chrisdlangton/)\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>GDPR comes into effect in May 2018 and one of the buzz phrases you might have heard is</p>\n<blockquote>\n<p>the right to be forgotten</p>\n</blockquote>\n<p>But there is much more you need to know and the effects are far reaching beyond Europe.</p>\n<h1 id=\"whatassessmentsareneeded\">What Assessments are needed?</h1>\n<p>Data protection impact assessments (DPIAs) or Privacy Impact Assessment (PIAs) are required by GDPR.</p>\n<p>A D/PIA must be carried out prior to the implementation of the technology, project, activity or process and ideally as early as practical in the design process.</p>\n<p>The D/PIA will also need to be updated and/or steps repeated as the process develops, particularly if issues are identified which may affect the severity or likelihood of risk to the data protection rights of affected individuals.</p>\n<h1 id=\"gdprassessmentrequirements\">GDPR Assessment Requirements</h1>\n<p>GDPR article 35(7) lists the minimum requirements a DPIA must provide and contain:</p>\n<ul>\n<li>a systematic description of the envisaged processing operations and the purposes of the processing, including, where applicable, the legitimate interest pursued by the controller</li>\n<li>an assessment of the necessity and proportionality of the processing operations in relation to the purposes</li>\n<li>an assessment of the risks to the rights and freedoms of data subjects</li>\n<li>the measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data and to demonstrate compliance with this Regulation taking into account the rights and legitimate interests of data subjects and other persons concerned.</li>\n</ul>\n<p>A DPIA report must also contain</p>\n<ul>\n<li>purposes of processing</li>\n<li>the stakeholders</li>\n<li>categories and types of private data processed in the system</li>\n<li>characterization of types of data flows (e.g. is the data transferred?)</li>\n</ul>\n<p>These descriptions should be clear. Clarity is valued in DPIA. Do not take any opportunity to document ambiguity for business benefit, it will only work against your efforts to be deemed compliant.</p>\n<h1 id=\"australianbusinessconsiderations\">Australian business considerations</h1>\n<p>You should ensure that the entity fits into these GDPR scopes before starting a DPIA.</p>\n<p>According to OAIC: <a href=\"https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation\">consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation</a></p>\n<p>Australian businesses that may be covered include:</p>\n<ul>\n<li>an Australian business with an office in the EU</li>\n<li>recital 23 - an Australian business whose website enables EU customers to order goods or services in a European language (other than English) or enables payment in euros</li>\n<li>recital 23 - an Australian business whose website mentions customers or users in the EU</li>\n<li>recital 24 - an Australian business that tracks individuals in the EU on the Internet and uses data processing techniques to profile individuals to analyse and predict personal preferences, behaviours, and attitudes - (section 6)</li>\n</ul>\n<p>If the entity fits into these GDPR scopes you would need to consider the whole OAIC guidance.</p>\n<p>To check whether you need to also comply with the Privacy Act, you can complete the privacy checklist for small business found on the OAIC website <a href=\"http://www.oaic.gov.au/privacy/privacy-resources/privacy-business-resources/privacy-business-resource-10\">here</a>.</p>\n<h1 id=\"youmaynotneedtodoanything\">You may not need to do anything</h1>\n<p>So you've identified that your business must comply but what exactly does that mean?</p>\n<p>It may mean you need to complete a few additional tasks, or none at all. It all depends on your current privacy posture.</p>\n<h2 id=\"assessments\">Assessments</h2>\n<p>If you currently complete a PIA or DPIA on a project by project basis to meet the privacy obligations of your country (Such is the case in Australia), you do not need to change a thing about this practice.<br>\nHowever to meet the obligations of GDPR and you do not currently complete either a PIA or DPIA you must be prepared to do so for all previous and future projects.</p>\n<h2 id=\"accountabilityandgovernance\">Accountability and governance</h2>\n<p>Both accountability and governance are key concepts in GDPR (Article 5, 24, and 25 speak to them) and are required if you wish to be compliant. Having an appointed CISO and thus a Security Steering Committee Policy is a great start as it provides a level of accountability and governance however a well considered ISMS or alternatively seek standards accreditation such as ISO/IEC 27014:2013 are assurances well considered also.</p>\n<h2 id=\"consent\">Consent</h2>\n<p>In all cases where privacy data is collected you should now be seeking consent from the individual not only that you may use their data but exactly how you intend to use the data now and into the future. If you intend to share their data with any other entities this must also be consented too. And if any of these change you must obtain consent anew.</p>\n<p>The GDPR includes a new definition of consent, which states that it must be freely given, specific, informed, and an unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing (Article 4(11)).</p>\n<p>Consent is not freely given if the individual has no genuine or free choice or is unable to refuse or withdraw consent at any time (Article 7 and recital 42). Businesses also need to make the withdrawal of consent as easy as giving consent, and, before individuals give consent, must inform individuals about this right to withdraw consent (Article 7(3)).</p>\n<p>Specific requirements apply in relation to children’s consent if an individual below 16 years.</p>\n<h2 id=\"breachnotifications\">Breach Notifications</h2>\n<p>At the time of writing; in four days, Australia enters into a new era where mandatory data breach notifications are enforcible.<br>\nHaving been lawful legislation for 12 months, Australian businesses have enjoyed a grace period for which they hopefully spent becoming compliant. Which is actually a simple process to start; by creating a data breach response plan and ongoing practice/drills to remain relevant.</p>\n<p>So it is no extra effort for Australian businesses to comply with GDPR as the breach notification obligations are aligned. Outside Australia however you may still need to establish a data breach response plan if you wish to be complaint with GDPR.</p>\n<h2 id=\"therighttoerasure\">The right to erasure</h2>\n<p>Known as the <code>right to be forgotten</code>, where the individual withdraws their consent you must delete their data. Additionally where the information is no longer necessary for the purpose for which it was collected and there is no other legal ground for processing their data, you must also delete the individuals data.</p>\n<p>There is no equivalent &quot;right to erasure&quot; under the Privacy Act in Australia, however APP 11.2 requires an APP entity that holds personal information to take reasonable steps to destroy the information or to ensure it is de-identified if the information is no longer needed for any purpose permitted under the Privacy Act; which is aligned to the second right of GDPR. If you have already demonstrated your compliance to APP 11.2 and have also provided the means for individuals to withdraw consent, then you are already aligned to this obligation of GDPR.</p>\n<h1 id=\"sanctions\">Sanctions</h1>\n<p>The GDPR gives supervisory authorities the power to impose administrative fines for contraventions, with fines of up to €20 million or 4% of annual worldwide turnover, whichever is greater for certain types of contraventions (Article 83(5)).</p>\n<p>Basically, you don't want to be unprepared. Many businesses such as online advertisers, recruitment agencies, and sales consultants should take a moment and consider their practices - any individual may be protected by European laws and therefore you are responsible for your businesses compliance to GDPR.</p>\n<h1 id=\"wherecanigethelpormoreinformation\">Where can I get help or more information?</h1>\n<ul>\n<li>Find a GRC Information Security Consultant (GRC = Governance, Regulation, and Compliance) like myself to guide you through the process and provide assistance to complete each step with you and your business objectives.</li>\n<li><a href=\"https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation\">OAIC Consultation draft for Australian businesses to comply with GDPR</a></li>\n<li><a href=\"https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/\">UK ICO website GDPR guidance</a></li>\n<li><a href=\"http://ec.europa.eu/justice/data-protection/reform/index_en.htm\">European Commission</a></li>\n</ul>\n<p>Follow me on Twitter <a href=\"https://twitter.com/chrisdlangton\">@chrisdlangton</a></p>\n<p>Or connect with me <a href=\"https://www.linkedin.com/in/chrisdlangton/\">on LinkedIn</a></p>\n<!--kg-card-end: markdown-->","comment_id":"5a66a4a1c6913d057a6dcb43","plaintext":"GDPR comes into effect in May 2018 and one of the buzz phrases you might have\nheard is\n\n> the right to be forgotten\n\n\nBut there is much more you need to know and the effects are far reaching beyond\nEurope.\n\nWhat Assessments are needed?\nData protection impact assessments (DPIAs) or Privacy Impact Assessment (PIAs)\nare required by GDPR.\n\nA D/PIA must be carried out prior to the implementation of the technology,\nproject, activity or process and ideally as early as practical in the design\nprocess.\n\nThe D/PIA will also need to be updated and/or steps repeated as the process\ndevelops, particularly if issues are identified which may affect the severity or\nlikelihood of risk to the data protection rights of affected individuals.\n\nGDPR Assessment Requirements\nGDPR article 35(7) lists the minimum requirements a DPIA must provide and\ncontain:\n\n * a systematic description of the envisaged processing operations and the\n   purposes of the processing, including, where applicable, the legitimate\n   interest pursued by the controller\n * an assessment of the necessity and proportionality of the processing\n   operations in relation to the purposes\n * an assessment of the risks to the rights and freedoms of data subjects\n * the measures envisaged to address the risks, including safeguards, security\n   measures and mechanisms to ensure the protection of personal data and to\n   demonstrate compliance with this Regulation taking into account the rights\n   and legitimate interests of data subjects and other persons concerned.\n\nA DPIA report must also contain\n\n * purposes of processing\n * the stakeholders\n * categories and types of private data processed in the system\n * characterization of types of data flows (e.g. is the data transferred?)\n\nThese descriptions should be clear. Clarity is valued in DPIA. Do not take any\nopportunity to document ambiguity for business benefit, it will only work\nagainst your efforts to be deemed compliant.\n\nAustralian business considerations\nYou should ensure that the entity fits into these GDPR scopes before starting a\nDPIA.\n\nAccording to OAIC: \nconsultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation\n[https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation]\n\nAustralian businesses that may be covered include:\n\n * an Australian business with an office in the EU\n * recital 23 - an Australian business whose website enables EU customers to\n   order goods or services in a European language (other than English) or\n   enables payment in euros\n * recital 23 - an Australian business whose website mentions customers or users\n   in the EU\n * recital 24 - an Australian business that tracks individuals in the EU on the\n   Internet and uses data processing techniques to profile individuals to\n   analyse and predict personal preferences, behaviours, and attitudes -\n   (section 6)\n\nIf the entity fits into these GDPR scopes you would need to consider the whole\nOAIC guidance.\n\nTo check whether you need to also comply with the Privacy Act, you can complete\nthe privacy checklist for small business found on the OAIC website here\n[http://www.oaic.gov.au/privacy/privacy-resources/privacy-business-resources/privacy-business-resource-10]\n.\n\nYou may not need to do anything\nSo you've identified that your business must comply but what exactly does that\nmean?\n\nIt may mean you need to complete a few additional tasks, or none at all. It all\ndepends on your current privacy posture.\n\nAssessments\nIf you currently complete a PIA or DPIA on a project by project basis to meet\nthe privacy obligations of your country (Such is the case in Australia), you do\nnot need to change a thing about this practice.\nHowever to meet the obligations of GDPR and you do not currently complete either\na PIA or DPIA you must be prepared to do so for all previous and future\nprojects.\n\nAccountability and governance\nBoth accountability and governance are key concepts in GDPR (Article 5, 24, and\n25 speak to them) and are required if you wish to be compliant. Having an\nappointed CISO and thus a Security Steering Committee Policy is a great start as\nit provides a level of accountability and governance however a well considered\nISMS or alternatively seek standards accreditation such as ISO/IEC 27014:2013\nare assurances well considered also.\n\nConsent\nIn all cases where privacy data is collected you should now be seeking consent\nfrom the individual not only that you may use their data but exactly how you\nintend to use the data now and into the future. If you intend to share their\ndata with any other entities this must also be consented too. And if any of\nthese change you must obtain consent anew.\n\nThe GDPR includes a new definition of consent, which states that it must be\nfreely given, specific, informed, and an unambiguous indication of the data\nsubject's wishes by which he or she, by a statement or by a clear affirmative\naction, signifies agreement to the processing (Article 4(11)).\n\nConsent is not freely given if the individual has no genuine or free choice or\nis unable to refuse or withdraw consent at any time (Article 7 and recital 42).\nBusinesses also need to make the withdrawal of consent as easy as giving\nconsent, and, before individuals give consent, must inform individuals about\nthis right to withdraw consent (Article 7(3)).\n\nSpecific requirements apply in relation to children’s consent if an individual\nbelow 16 years.\n\nBreach Notifications\nAt the time of writing; in four days, Australia enters into a new era where\nmandatory data breach notifications are enforcible.\nHaving been lawful legislation for 12 months, Australian businesses have enjoyed\na grace period for which they hopefully spent becoming compliant. Which is\nactually a simple process to start; by creating a data breach response plan and\nongoing practice/drills to remain relevant.\n\nSo it is no extra effort for Australian businesses to comply with GDPR as the\nbreach notification obligations are aligned. Outside Australia however you may\nstill need to establish a data breach response plan if you wish to be complaint\nwith GDPR.\n\nThe right to erasure\nKnown as the right to be forgotten, where the individual withdraws their consent\nyou must delete their data. Additionally where the information is no longer\nnecessary for the purpose for which it was collected and there is no other legal\nground for processing their data, you must also delete the individuals data.\n\nThere is no equivalent \"right to erasure\" under the Privacy Act in Australia,\nhowever APP 11.2 requires an APP entity that holds personal information to take\nreasonable steps to destroy the information or to ensure it is de-identified if\nthe information is no longer needed for any purpose permitted under the Privacy\nAct; which is aligned to the second right of GDPR. If you have already\ndemonstrated your compliance to APP 11.2 and have also provided the means for\nindividuals to withdraw consent, then you are already aligned to this obligation\nof GDPR.\n\nSanctions\nThe GDPR gives supervisory authorities the power to impose administrative fines\nfor contraventions, with fines of up to €20 million or 4% of annual worldwide\nturnover, whichever is greater for certain types of contraventions (Article\n83(5)).\n\nBasically, you don't want to be unprepared. Many businesses such as online\nadvertisers, recruitment agencies, and sales consultants should take a moment\nand consider their practices - any individual may be protected by European laws\nand therefore you are responsible for your businesses compliance to GDPR.\n\nWhere can I get help or more information?\n * Find a GRC Information Security Consultant (GRC = Governance, Regulation, and\n   Compliance) like myself to guide you through the process and provide\n   assistance to complete each step with you and your business objectives.\n * OAIC Consultation draft for Australian businesses to comply with GDPR\n   [https://www.oaic.gov.au/engage-with-us/consultations/australian-businesses-and-the-eu-general-data-protection-regulation/consultation-draft-australian-businesses-and-the-eu-general-data-protection-regulation]\n * UK ICO website GDPR guidance\n   [https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/]\n * European Commission\n   [http://ec.europa.eu/justice/data-protection/reform/index_en.htm]\n\nFollow me on Twitter @chrisdlangton [https://twitter.com/chrisdlangton]\n\nOr connect with me on LinkedIn [https://www.linkedin.com/in/chrisdlangton/]","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/02/gdpr.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-01-23 02:57:37","created_by":"1","updated_at":"2021-03-31 14:10:42","updated_by":"1","published_at":"2018-02-18 12:36:45","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb5","uuid":"af645a04-aedd-4951-a171-a7d87009eadf","title":"Misunderstood Business Continuity risk domain","slug":"misunderstood-business-continuity-risk-domain","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Business Continuity Management (BCM); The process that is responsible for Disaster Recovery Plan (DRP) and Business Continuity Plan (BCP).\\n\\nMy personal favorite security and risk domain area, one that has broad reaching and often misunderstood scope but can be the most valuable aspect of any information security management system (ISMS).\\n\\nCommonly constrained to just Operational Resilience, which is likely because a lot of us think technically. BCP is as much about contractural arrangements, process, policy and ritual than perhaps just the technical implementations we all focus on when the topic comes up.\\n\\n## Over-arching process\\n\\nOne of the most famous examples of BCP comes from the NIST SP800-34 which has focus on process and policy but the guidance lacks in clear actionable steps.\\n\\nIn fact if you look at everyones favorite inspiration; the CSA Security, Trust & Assurance Registry (STAR) framework. you'll find one of the 16 domains is Business Continuity Management & Operational Resilience bundled together in such a way that comes across in terms most of us should understand.\\n\\nSo what goals are we discussing?\\n\\n- Determine criticality\\n- Estimate max downtime\\n- Evaluate internal and external resource requirements\\n\\nLets break these down into a process, to answer each we will need to;\\n\\n- Gather information\\n- Analyse information\\n- Perform threat analysis\\n- Document results and present recommendations\\n\\nOnce you have developed the continuity planning policy, identified preventive controls, have a recovery strategy and contingency plan - you'll need to test the plan and conduct training and exercises to ensure you can maintain the plan.\\n\\n## Business Impact Analysis (BIA)\\n\\nOne of the most important first steps in the planning development. Qualitative and quantitative data on the business impact of a disaster need to be gathered, analyzed, interpreted, and presented to management.\\n\\nOne of the tools to determine criticality and effectively report on threats is called a Business Impact Analysis. A BIA involves the selection of individuals to interview for data gathering through surveys or questionnaires (remember to gather evidence to support your conclusions).\\nUse the learnings of this exercise to identify the company's critical business functions and the resources these functions depend on, and ensure you gather the data needed so you can calculate how long these functions can survive without these resources.\\n\\nIt is important to also perform threat and risk analysis (TRA) for each of these functions, however I will deep dive on this subject in a future post.\\n\\n> Formula: Risk = Threat * Impact * Probability\\n\\n## Disaster Recovery Plan (DRP)\\n\\nCarried out when everything is going to still be suffering from the effects of the disaster.\\n\\nManagement should be involved in setting the overall goals in continuity planning so before going ahead with any work ensure you have senior management support.\\n\\nThe goal of disaster recovery is to minimize the effects of a disaster whereas the goal of business continuity is to resume normal business operations as quickly as possible with the least amount of resources necessary to do so.\\n\\n![disaster-recovery-planning](https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/disaster-recovery-planning.jpg)\\n\\nYour acceptable amount of data loss measured in time is called a Recovery Point Objective (RPO).\\nThe earliest time period and a service level within which a business process must be restored after a disaster. Your Recovery Time Objective (RTO) value is smaller than the Maximum Tolerable Down-time (MTD) because the MTD represents the time after which an inability to recover will mean severe damage to the business's reputation and/or the bottom line. \\nThe RTO assumes there is a period of acceptable down-time. Setting a Work Recovery Time (WRT) allows for the remainder of the overall MTD value.\\n\\n> MTD, RTO and RPO values are derived during the BIA\\n\\n## Business Continuity Plan (BCP)\\n\\nUsed to return the business to normal operations.\\n\\nBCP contains strategy documents that provide detailed procedures that ensure critical business functions are maintained. A BCP provides procedures for emergency responses, extended backup operations, and post-disaster recovery.\\n\\nTypes of organisational changes that should be identified and included in the manual include staffing and key clients/vendors/suppliers as well as some specialised technical resources that include anti-virus (AV), patching, hardware, and data applications.\\n\\nTesting and verification of recovery procedures are important because as work processes change, previous recovery procedures may no longer be suitable. Ensure all work processes for critical functions are documented before changes reach production so if the systems used for critical functions changed there are documented work checklists that are meaningful and accurate so the business is capable of performing recovery tasks and supporting disaster recovery infrastructure within the predetermined recovery time objective.\\n\\nMean Time To Failures (MTTF) and Mean Time To Repair (MTTR) are two important KPI's, senior management will ask for both but at a technical level you only need to measure MTTF while putting focus on reducing MTTR.\\n\\n> Embrace failure\\n\\nWhen you start ignoring MTTF by only measuring it you might be blamed for allowing constant failures to continue, but by reducing the MTTR close to zero you're proving that any single failure or no failure is as impactful as double the most failures you ever experienced.\\n\\n> Mean Time To Failures = (Total up time) / (number of breakdowns)\\n\\n> Mean Time To Repair = (Total down time) / (number of breakdowns)\\n\\nMTTF is literally the average time elapsed from one failure to the next.  Usually people think of it as the average time that something works until it fails and needs to be repaired.\\n\\n## How to achieve resiliency\\n\\nI talked about embracing failure and allowing them to occur while focusing on that MTTR, this is what is referred to as being resilient.\\n\\nResiliency is the ability of a system to recover from failures and continue to function. It's not about avoiding failures, but responding to failures in a way that avoids downtime or data loss.\\n\\nTwo important aspects of resiliency are High Availability (HA) and Disaster Recovery (DR) where HA is the ability of the application to continue running in a healthy state and DR is the ability to recover from non-transient or wide-scale failures.\\n\\nBy HA we must retain \\\"healthy state\\\" which simply put means the application is responsive, and users can connect to the application and interact with it. To achieve this you must load balance users across multiple geographic areas, preferably three regions is HA because having only two is better defined as being a fail-over or blue-green strategy rather than HA.\\n\\nData backup is a critical part of DR because if data is lost the system can't return to a stable state. Data must be backed up in a different geographical region to enable HA. Backup is distinct from data replication however for the purposes of HA you may design for either or both however replication won't protect against human error. If data gets corrupted because of human error, the corrupted data just gets copied to the replicas and to have DR you must rely on backups still.\\n\\n## Conclusion\\n\\nEveryone likes numbers and percentages so like this post, the organisation should have a 80/20 percent split of 80% process 20% technical for Business Continuity management but that is only an upfront investment to what will be 100% embracing the chaos.\\n\\nRitual; or rather practice; will play key roles in your ability to execute during live incidents. The goal of the whole organisation (not just the technical people) should be the ability to automatically recover from all failures, failures should be permitted but only occur in a way that is of no consequence to the business.\\n\\nHowever to get any of this done in reality you should execute proper planning and document everything. Policy reinforces practice, analysis appeases auditors, and process enables humans. Investing in planning is profit for enterprise.\\n\\n> Some people must persevere the process so that others may fully embrace the chaos.\\n\\n## Honorary DevOps shout-out\\n\\nSkip if you came for the serious content.\\n\\nSome of you may find the end goal to be familiar if you've been exposed to DevOps.\\n\\nContrary to popular modern hype of DevOps, the roots of what DevOps is and to most of us from that time it always has been, is the ability and _freedom_ to move fast, experiment, and learn from breaking things. \\n\\nWe could have only ever achieved such luxury in enterprise if we are capable of doing so within the legal and contractural constraints of the business - which is to say we ensure critical systems do not suffer our foolery.\\n\\nToday; DevOps is a bit of a joke. Rather, it is misunderstood and often due to the recruitment process it has been transformed into some glorified playground for millennials to take contract after contract building bespoke glittery unicorn systems without any regard for what the business needs.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Business Continuity Management (BCM); The process that is responsible for Disaster Recovery Plan (DRP) and Business Continuity Plan (BCP).</p>\n<p>My personal favorite security and risk domain area, one that has broad reaching and often misunderstood scope but can be the most valuable aspect of any information security management system (ISMS).</p>\n<p>Commonly constrained to just Operational Resilience, which is likely because a lot of us think technically. BCP is as much about contractural arrangements, process, policy and ritual than perhaps just the technical implementations we all focus on when the topic comes up.</p>\n<h2 id=\"overarchingprocess\">Over-arching process</h2>\n<p>One of the most famous examples of BCP comes from the NIST SP800-34 which has focus on process and policy but the guidance lacks in clear actionable steps.</p>\n<p>In fact if you look at everyones favorite inspiration; the CSA Security, Trust &amp; Assurance Registry (STAR) framework. you'll find one of the 16 domains is Business Continuity Management &amp; Operational Resilience bundled together in such a way that comes across in terms most of us should understand.</p>\n<p>So what goals are we discussing?</p>\n<ul>\n<li>Determine criticality</li>\n<li>Estimate max downtime</li>\n<li>Evaluate internal and external resource requirements</li>\n</ul>\n<p>Lets break these down into a process, to answer each we will need to;</p>\n<ul>\n<li>Gather information</li>\n<li>Analyse information</li>\n<li>Perform threat analysis</li>\n<li>Document results and present recommendations</li>\n</ul>\n<p>Once you have developed the continuity planning policy, identified preventive controls, have a recovery strategy and contingency plan - you'll need to test the plan and conduct training and exercises to ensure you can maintain the plan.</p>\n<h2 id=\"businessimpactanalysisbia\">Business Impact Analysis (BIA)</h2>\n<p>One of the most important first steps in the planning development. Qualitative and quantitative data on the business impact of a disaster need to be gathered, analyzed, interpreted, and presented to management.</p>\n<p>One of the tools to determine criticality and effectively report on threats is called a Business Impact Analysis. A BIA involves the selection of individuals to interview for data gathering through surveys or questionnaires (remember to gather evidence to support your conclusions).<br>\nUse the learnings of this exercise to identify the company's critical business functions and the resources these functions depend on, and ensure you gather the data needed so you can calculate how long these functions can survive without these resources.</p>\n<p>It is important to also perform threat and risk analysis (TRA) for each of these functions, however I will deep dive on this subject in a future post.</p>\n<blockquote>\n<p>Formula: Risk = Threat * Impact * Probability</p>\n</blockquote>\n<h2 id=\"disasterrecoveryplandrp\">Disaster Recovery Plan (DRP)</h2>\n<p>Carried out when everything is going to still be suffering from the effects of the disaster.</p>\n<p>Management should be involved in setting the overall goals in continuity planning so before going ahead with any work ensure you have senior management support.</p>\n<p>The goal of disaster recovery is to minimize the effects of a disaster whereas the goal of business continuity is to resume normal business operations as quickly as possible with the least amount of resources necessary to do so.</p>\n<p><img src=\"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/disaster-recovery-planning.jpg\" alt=\"disaster-recovery-planning\" loading=\"lazy\"></p>\n<p>Your acceptable amount of data loss measured in time is called a Recovery Point Objective (RPO).<br>\nThe earliest time period and a service level within which a business process must be restored after a disaster. Your Recovery Time Objective (RTO) value is smaller than the Maximum Tolerable Down-time (MTD) because the MTD represents the time after which an inability to recover will mean severe damage to the business's reputation and/or the bottom line.<br>\nThe RTO assumes there is a period of acceptable down-time. Setting a Work Recovery Time (WRT) allows for the remainder of the overall MTD value.</p>\n<blockquote>\n<p>MTD, RTO and RPO values are derived during the BIA</p>\n</blockquote>\n<h2 id=\"businesscontinuityplanbcp\">Business Continuity Plan (BCP)</h2>\n<p>Used to return the business to normal operations.</p>\n<p>BCP contains strategy documents that provide detailed procedures that ensure critical business functions are maintained. A BCP provides procedures for emergency responses, extended backup operations, and post-disaster recovery.</p>\n<p>Types of organisational changes that should be identified and included in the manual include staffing and key clients/vendors/suppliers as well as some specialised technical resources that include anti-virus (AV), patching, hardware, and data applications.</p>\n<p>Testing and verification of recovery procedures are important because as work processes change, previous recovery procedures may no longer be suitable. Ensure all work processes for critical functions are documented before changes reach production so if the systems used for critical functions changed there are documented work checklists that are meaningful and accurate so the business is capable of performing recovery tasks and supporting disaster recovery infrastructure within the predetermined recovery time objective.</p>\n<p>Mean Time To Failures (MTTF) and Mean Time To Repair (MTTR) are two important KPI's, senior management will ask for both but at a technical level you only need to measure MTTF while putting focus on reducing MTTR.</p>\n<blockquote>\n<p>Embrace failure</p>\n</blockquote>\n<p>When you start ignoring MTTF by only measuring it you might be blamed for allowing constant failures to continue, but by reducing the MTTR close to zero you're proving that any single failure or no failure is as impactful as double the most failures you ever experienced.</p>\n<blockquote>\n<p>Mean Time To Failures = (Total up time) / (number of breakdowns)</p>\n</blockquote>\n<blockquote>\n<p>Mean Time To Repair = (Total down time) / (number of breakdowns)</p>\n</blockquote>\n<p>MTTF is literally the average time elapsed from one failure to the next.  Usually people think of it as the average time that something works until it fails and needs to be repaired.</p>\n<h2 id=\"howtoachieveresiliency\">How to achieve resiliency</h2>\n<p>I talked about embracing failure and allowing them to occur while focusing on that MTTR, this is what is referred to as being resilient.</p>\n<p>Resiliency is the ability of a system to recover from failures and continue to function. It's not about avoiding failures, but responding to failures in a way that avoids downtime or data loss.</p>\n<p>Two important aspects of resiliency are High Availability (HA) and Disaster Recovery (DR) where HA is the ability of the application to continue running in a healthy state and DR is the ability to recover from non-transient or wide-scale failures.</p>\n<p>By HA we must retain &quot;healthy state&quot; which simply put means the application is responsive, and users can connect to the application and interact with it. To achieve this you must load balance users across multiple geographic areas, preferably three regions is HA because having only two is better defined as being a fail-over or blue-green strategy rather than HA.</p>\n<p>Data backup is a critical part of DR because if data is lost the system can't return to a stable state. Data must be backed up in a different geographical region to enable HA. Backup is distinct from data replication however for the purposes of HA you may design for either or both however replication won't protect against human error. If data gets corrupted because of human error, the corrupted data just gets copied to the replicas and to have DR you must rely on backups still.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Everyone likes numbers and percentages so like this post, the organisation should have a 80/20 percent split of 80% process 20% technical for Business Continuity management but that is only an upfront investment to what will be 100% embracing the chaos.</p>\n<p>Ritual; or rather practice; will play key roles in your ability to execute during live incidents. The goal of the whole organisation (not just the technical people) should be the ability to automatically recover from all failures, failures should be permitted but only occur in a way that is of no consequence to the business.</p>\n<p>However to get any of this done in reality you should execute proper planning and document everything. Policy reinforces practice, analysis appeases auditors, and process enables humans. Investing in planning is profit for enterprise.</p>\n<blockquote>\n<p>Some people must persevere the process so that others may fully embrace the chaos.</p>\n</blockquote>\n<h2 id=\"honorarydevopsshoutout\">Honorary DevOps shout-out</h2>\n<p>Skip if you came for the serious content.</p>\n<p>Some of you may find the end goal to be familiar if you've been exposed to DevOps.</p>\n<p>Contrary to popular modern hype of DevOps, the roots of what DevOps is and to most of us from that time it always has been, is the ability and <em>freedom</em> to move fast, experiment, and learn from breaking things.</p>\n<p>We could have only ever achieved such luxury in enterprise if we are capable of doing so within the legal and contractural constraints of the business - which is to say we ensure critical systems do not suffer our foolery.</p>\n<p>Today; DevOps is a bit of a joke. Rather, it is misunderstood and often due to the recruitment process it has been transformed into some glorified playground for millennials to take contract after contract building bespoke glittery unicorn systems without any regard for what the business needs.</p>\n<!--kg-card-end: markdown-->","comment_id":"5a66a508c6913d057a6dcb44","plaintext":"Business Continuity Management (BCM); The process that is responsible for\nDisaster Recovery Plan (DRP) and Business Continuity Plan (BCP).\n\nMy personal favorite security and risk domain area, one that has broad reaching\nand often misunderstood scope but can be the most valuable aspect of any\ninformation security management system (ISMS).\n\nCommonly constrained to just Operational Resilience, which is likely because a\nlot of us think technically. BCP is as much about contractural arrangements,\nprocess, policy and ritual than perhaps just the technical implementations we\nall focus on when the topic comes up.\n\nOver-arching process\nOne of the most famous examples of BCP comes from the NIST SP800-34 which has\nfocus on process and policy but the guidance lacks in clear actionable steps.\n\nIn fact if you look at everyones favorite inspiration; the CSA Security, Trust &\nAssurance Registry (STAR) framework. you'll find one of the 16 domains is\nBusiness Continuity Management & Operational Resilience bundled together in such\na way that comes across in terms most of us should understand.\n\nSo what goals are we discussing?\n\n * Determine criticality\n * Estimate max downtime\n * Evaluate internal and external resource requirements\n\nLets break these down into a process, to answer each we will need to;\n\n * Gather information\n * Analyse information\n * Perform threat analysis\n * Document results and present recommendations\n\nOnce you have developed the continuity planning policy, identified preventive\ncontrols, have a recovery strategy and contingency plan - you'll need to test\nthe plan and conduct training and exercises to ensure you can maintain the plan.\n\nBusiness Impact Analysis (BIA)\nOne of the most important first steps in the planning development. Qualitative\nand quantitative data on the business impact of a disaster need to be gathered,\nanalyzed, interpreted, and presented to management.\n\nOne of the tools to determine criticality and effectively report on threats is\ncalled a Business Impact Analysis. A BIA involves the selection of individuals\nto interview for data gathering through surveys or questionnaires (remember to\ngather evidence to support your conclusions).\nUse the learnings of this exercise to identify the company's critical business\nfunctions and the resources these functions depend on, and ensure you gather the\ndata needed so you can calculate how long these functions can survive without\nthese resources.\n\nIt is important to also perform threat and risk analysis (TRA) for each of these\nfunctions, however I will deep dive on this subject in a future post.\n\n> Formula: Risk = Threat * Impact * Probability\n\n\nDisaster Recovery Plan (DRP)\nCarried out when everything is going to still be suffering from the effects of\nthe disaster.\n\nManagement should be involved in setting the overall goals in continuity\nplanning so before going ahead with any work ensure you have senior management\nsupport.\n\nThe goal of disaster recovery is to minimize the effects of a disaster whereas\nthe goal of business continuity is to resume normal business operations as\nquickly as possible with the least amount of resources necessary to do so.\n\n\n\nYour acceptable amount of data loss measured in time is called a Recovery Point\nObjective (RPO).\nThe earliest time period and a service level within which a business process\nmust be restored after a disaster. Your Recovery Time Objective (RTO) value is\nsmaller than the Maximum Tolerable Down-time (MTD) because the MTD represents\nthe time after which an inability to recover will mean severe damage to the\nbusiness's reputation and/or the bottom line.\nThe RTO assumes there is a period of acceptable down-time. Setting a Work\nRecovery Time (WRT) allows for the remainder of the overall MTD value.\n\n> MTD, RTO and RPO values are derived during the BIA\n\n\nBusiness Continuity Plan (BCP)\nUsed to return the business to normal operations.\n\nBCP contains strategy documents that provide detailed procedures that ensure\ncritical business functions are maintained. A BCP provides procedures for\nemergency responses, extended backup operations, and post-disaster recovery.\n\nTypes of organisational changes that should be identified and included in the\nmanual include staffing and key clients/vendors/suppliers as well as some\nspecialised technical resources that include anti-virus (AV), patching,\nhardware, and data applications.\n\nTesting and verification of recovery procedures are important because as work\nprocesses change, previous recovery procedures may no longer be suitable. Ensure\nall work processes for critical functions are documented before changes reach\nproduction so if the systems used for critical functions changed there are\ndocumented work checklists that are meaningful and accurate so the business is\ncapable of performing recovery tasks and supporting disaster recovery\ninfrastructure within the predetermined recovery time objective.\n\nMean Time To Failures (MTTF) and Mean Time To Repair (MTTR) are two important\nKPI's, senior management will ask for both but at a technical level you only\nneed to measure MTTF while putting focus on reducing MTTR.\n\n> Embrace failure\n\n\nWhen you start ignoring MTTF by only measuring it you might be blamed for\nallowing constant failures to continue, but by reducing the MTTR close to zero\nyou're proving that any single failure or no failure is as impactful as double\nthe most failures you ever experienced.\n\n> Mean Time To Failures = (Total up time) / (number of breakdowns)\n\n\n> Mean Time To Repair = (Total down time) / (number of breakdowns)\n\n\nMTTF is literally the average time elapsed from one failure to the next. Usually\npeople think of it as the average time that something works until it fails and\nneeds to be repaired.\n\nHow to achieve resiliency\nI talked about embracing failure and allowing them to occur while focusing on\nthat MTTR, this is what is referred to as being resilient.\n\nResiliency is the ability of a system to recover from failures and continue to\nfunction. It's not about avoiding failures, but responding to failures in a way\nthat avoids downtime or data loss.\n\nTwo important aspects of resiliency are High Availability (HA) and Disaster\nRecovery (DR) where HA is the ability of the application to continue running in\na healthy state and DR is the ability to recover from non-transient or\nwide-scale failures.\n\nBy HA we must retain \"healthy state\" which simply put means the application is\nresponsive, and users can connect to the application and interact with it. To\nachieve this you must load balance users across multiple geographic areas,\npreferably three regions is HA because having only two is better defined as\nbeing a fail-over or blue-green strategy rather than HA.\n\nData backup is a critical part of DR because if data is lost the system can't\nreturn to a stable state. Data must be backed up in a different geographical\nregion to enable HA. Backup is distinct from data replication however for the\npurposes of HA you may design for either or both however replication won't\nprotect against human error. If data gets corrupted because of human error, the\ncorrupted data just gets copied to the replicas and to have DR you must rely on\nbackups still.\n\nConclusion\nEveryone likes numbers and percentages so like this post, the organisation\nshould have a 80/20 percent split of 80% process 20% technical for Business\nContinuity management but that is only an upfront investment to what will be\n100% embracing the chaos.\n\nRitual; or rather practice; will play key roles in your ability to execute\nduring live incidents. The goal of the whole organisation (not just the\ntechnical people) should be the ability to automatically recover from all\nfailures, failures should be permitted but only occur in a way that is of no\nconsequence to the business.\n\nHowever to get any of this done in reality you should execute proper planning\nand document everything. Policy reinforces practice, analysis appeases auditors,\nand process enables humans. Investing in planning is profit for enterprise.\n\n> Some people must persevere the process so that others may fully embrace the\nchaos.\n\n\nHonorary DevOps shout-out\nSkip if you came for the serious content.\n\nSome of you may find the end goal to be familiar if you've been exposed to\nDevOps.\n\nContrary to popular modern hype of DevOps, the roots of what DevOps is and to\nmost of us from that time it always has been, is the ability and freedom to move\nfast, experiment, and learn from breaking things.\n\nWe could have only ever achieved such luxury in enterprise if we are capable of\ndoing so within the legal and contractural constraints of the business - which\nis to say we ensure critical systems do not suffer our foolery.\n\nToday; DevOps is a bit of a joke. Rather, it is misunderstood and often due to\nthe recruitment process it has been transformed into some glorified playground\nfor millennials to take contract after contract building bespoke glittery\nunicorn systems without any regard for what the business needs.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/01/vulnerability.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-01-23 02:59:20","created_by":"1","updated_at":"2021-03-31 14:10:55","updated_by":"1","published_at":"2018-01-23 12:15:00","published_by":"1","custom_excerpt":"A look at Business Continuity Management (BCM); The process that is responsible for Disaster Recovery Plan (DRP) and Business Continuity Plan (BCP) using Business Impact Analysis (BIA) enabling Operational Resilience through HA and DR principles","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb6","uuid":"d1caa222-590d-445e-86c7-ff4eb073b70d","title":"Vendor attestations prove nothing about your systems","slug":"vendor-attestations-prove-nothing-about-your-systems","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Over the years I've been tasked to implement controls as a developer or self-assess and design controls that meet the requirements of contractual, compliance, regulatory, and legislative obligations.\\n\\nOne of the continuing misconceptions I've seen is most business believe (and often publicly claim) a vendors security and compliance attestations will automatically give them the same level of security and meet the same obligations as outlined in the vendor provided attestation - this is wrong for many reasons and couldn't be further from reality.\\n\\n> Vendor Security and Compliance Attestations *_never_* cover your obligations\\n\\n## Amazon AWS Lambda example\\n\\nAWS added [Lambda to PCI DSS their compliance](https://aws.amazon.com/blogs/security/aws-adds-12-more-services-to-its-pci-dss-compliance-program/) attestations known as AWS Artefact. \\n\\n> Ref: AWS [Services in Scope](https://aws.amazon.com/compliance/services-in-scope/)\\n\\nAWS approach compliance in a very _minimal_ way, and services audited are scoped strictly to the [shared responsibility model](https://aws.amazon.com/compliance/shared-responsibility-model/) and in practice as part of the compliance exercise Amazon [offer customers guidance](https://d1.awsstatic.com/whitepapers/compliance/AWS_Anitian_Workbook_PCI_Cloud_Compliance.pdf) so that they may achieve PCI DSS on their usage on top of AWS - this is known as applying the carve-out method where Amazon will be audited for a small portion of the whole PCI DSS requirements with the understanding the end-customer applied the remainder. In practice you as the end-customer are ultimately obligated to meet _all of the requirements_ not just the remainder left over by AWS, thought it is the discretion of the auditor to consider the AWS attestation when auditing your organisation for PCI compliance.\\n\\nTo go deeper on this point, the QSA (Qualified Security Auditor) applied [PCI DSS testing procedures](https://www.pcisecuritystandards.org/.../PCI-DSS-v3_2-ROC-Reporting-Template.pdf) on what AWS have implemented under-the-hood.\\nAll requirements that are able to have the end-customer implement the control *AWS are not tested* by the QSA at all.\\n\\nIn terms of Lambda, a service where you do not have control of any infrastructure, access to the operating system, or any ability to configure anything at all beyond what the AWS abstraction provides - you don't even have access to the programming language execution environment configuration, how your code is initialised, or in some cases what the inputs are. You only have your source code in the form of a function which is executed on your behalf with limited or no ability to control more than the function output.\\n\\nThat is to say, for PCI DSS requirements around encryption at-rest, Lambda's disk-level storage is not encrypted, i.e. The encryption at-rest option presented will only encrypt the Lambda package AWS receives from you, but it does not provide encryption at-rest of the execution environment of Lambda itself.\\n\\nThe mentioned exception is if you pursue the PCI DSS Compensating Controls Worksheet, which must not only meet but exceed the original intent of the requirement you could not meet. More then that, compliance is at the full discretion of the QSA whether or not you've implemented sufficient controls that exceed the requirements original intent.\\nFor encryption at-rest requirements you will need compensating controls that apply sufficient controls for confidentiality, data integrity, data being rendered useless upon breach or disclosure to name a few intents of encryption at-rest. \\n\\nRemember, to achieve PCI DSS using the Compensating COntrols Worksheet you must exceed the original intent in the eyes of the QSA, so don't expect the audit will go smoothly when using this technique.\\n\\n> Important side note: Lambda disk storage is both multi-tenanted and _eventually_ ephemeral, it is reused/shared across multiple Lambda invocations that are perceived to be distinct/sandbox execution environments but are not.\\n\\nThis means a end-customer can not achieve PCI DSS if AWS Lambda is PCI in-scope, with 1 exception I'll describe in a moment. You cannot apply the requirements for encryption at-rest and AWS have not provided this as an option for you to configure yourself, and AWS has not provided you with any attestation that they use any hardware-encrypted storage.\\n\\nGenerally, if you are an MSSP, providing hosting as part of your service offering to your customers on top of AWS (or any other MSSP) you are fully responsible for protecting your customers CHD (card holder data) and secure the CDE (card data environment), none of vendor supplied attestations can be provided to your customers as attestation of your security posture, you must obtain attestations of your own for each compliance or standard you supply-chain provides to you.\\n\\nPutting it bluntly, if you do not achieve PCI DSS then none of your own customers can.\\n\\nIn summary, a vendors attestation is proof of their security not yours, attestations are not transferable to their customers in any way.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Over the years I've been tasked to implement controls as a developer or self-assess and design controls that meet the requirements of contractual, compliance, regulatory, and legislative obligations.</p>\n<p>One of the continuing misconceptions I've seen is most business believe (and often publicly claim) a vendors security and compliance attestations will automatically give them the same level of security and meet the same obligations as outlined in the vendor provided attestation - this is wrong for many reasons and couldn't be further from reality.</p>\n<blockquote>\n<p>Vendor Security and Compliance Attestations <em><em>never</em></em> cover your obligations</p>\n</blockquote>\n<h2 id=\"amazonawslambdaexample\">Amazon AWS Lambda example</h2>\n<p>AWS added <a href=\"https://aws.amazon.com/blogs/security/aws-adds-12-more-services-to-its-pci-dss-compliance-program/\">Lambda to PCI DSS their compliance</a> attestations known as AWS Artefact.</p>\n<blockquote>\n<p>Ref: AWS <a href=\"https://aws.amazon.com/compliance/services-in-scope/\">Services in Scope</a></p>\n</blockquote>\n<p>AWS approach compliance in a very <em>minimal</em> way, and services audited are scoped strictly to the <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">shared responsibility model</a> and in practice as part of the compliance exercise Amazon <a href=\"https://d1.awsstatic.com/whitepapers/compliance/AWS_Anitian_Workbook_PCI_Cloud_Compliance.pdf\">offer customers guidance</a> so that they may achieve PCI DSS on their usage on top of AWS - this is known as applying the carve-out method where Amazon will be audited for a small portion of the whole PCI DSS requirements with the understanding the end-customer applied the remainder. In practice you as the end-customer are ultimately obligated to meet <em>all of the requirements</em> not just the remainder left over by AWS, thought it is the discretion of the auditor to consider the AWS attestation when auditing your organisation for PCI compliance.</p>\n<p>To go deeper on this point, the QSA (Qualified Security Auditor) applied <a href=\"https://www.pcisecuritystandards.org/.../PCI-DSS-v3_2-ROC-Reporting-Template.pdf\">PCI DSS testing procedures</a> on what AWS have implemented under-the-hood.<br>\nAll requirements that are able to have the end-customer implement the control <em>AWS are not tested</em> by the QSA at all.</p>\n<p>In terms of Lambda, a service where you do not have control of any infrastructure, access to the operating system, or any ability to configure anything at all beyond what the AWS abstraction provides - you don't even have access to the programming language execution environment configuration, how your code is initialised, or in some cases what the inputs are. You only have your source code in the form of a function which is executed on your behalf with limited or no ability to control more than the function output.</p>\n<p>That is to say, for PCI DSS requirements around encryption at-rest, Lambda's disk-level storage is not encrypted, i.e. The encryption at-rest option presented will only encrypt the Lambda package AWS receives from you, but it does not provide encryption at-rest of the execution environment of Lambda itself.</p>\n<p>The mentioned exception is if you pursue the PCI DSS Compensating Controls Worksheet, which must not only meet but exceed the original intent of the requirement you could not meet. More then that, compliance is at the full discretion of the QSA whether or not you've implemented sufficient controls that exceed the requirements original intent.<br>\nFor encryption at-rest requirements you will need compensating controls that apply sufficient controls for confidentiality, data integrity, data being rendered useless upon breach or disclosure to name a few intents of encryption at-rest.</p>\n<p>Remember, to achieve PCI DSS using the Compensating COntrols Worksheet you must exceed the original intent in the eyes of the QSA, so don't expect the audit will go smoothly when using this technique.</p>\n<blockquote>\n<p>Important side note: Lambda disk storage is both multi-tenanted and <em>eventually</em> ephemeral, it is reused/shared across multiple Lambda invocations that are perceived to be distinct/sandbox execution environments but are not.</p>\n</blockquote>\n<p>This means a end-customer can not achieve PCI DSS if AWS Lambda is PCI in-scope, with 1 exception I'll describe in a moment. You cannot apply the requirements for encryption at-rest and AWS have not provided this as an option for you to configure yourself, and AWS has not provided you with any attestation that they use any hardware-encrypted storage.</p>\n<p>Generally, if you are an MSSP, providing hosting as part of your service offering to your customers on top of AWS (or any other MSSP) you are fully responsible for protecting your customers CHD (card holder data) and secure the CDE (card data environment), none of vendor supplied attestations can be provided to your customers as attestation of your security posture, you must obtain attestations of your own for each compliance or standard you supply-chain provides to you.</p>\n<p>Putting it bluntly, if you do not achieve PCI DSS then none of your own customers can.</p>\n<p>In summary, a vendors attestation is proof of their security not yours, attestations are not transferable to their customers in any way.</p>\n<!--kg-card-end: markdown-->","comment_id":"5a851277c6913d057a6dcb55","plaintext":"Over the years I've been tasked to implement controls as a developer or\nself-assess and design controls that meet the requirements of contractual,\ncompliance, regulatory, and legislative obligations.\n\nOne of the continuing misconceptions I've seen is most business believe (and\noften publicly claim) a vendors security and compliance attestations will\nautomatically give them the same level of security and meet the same obligations\nas outlined in the vendor provided attestation - this is wrong for many reasons\nand couldn't be further from reality.\n\n> Vendor Security and Compliance Attestations never cover your obligations\n\n\nAmazon AWS Lambda example\nAWS added Lambda to PCI DSS their compliance\n[https://aws.amazon.com/blogs/security/aws-adds-12-more-services-to-its-pci-dss-compliance-program/] \nattestations known as AWS Artefact.\n\n> Ref: AWS Services in Scope\n[https://aws.amazon.com/compliance/services-in-scope/]\n\n\nAWS approach compliance in a very minimal way, and services audited are scoped\nstrictly to the shared responsibility model\n[https://aws.amazon.com/compliance/shared-responsibility-model/] and in practice\nas part of the compliance exercise Amazon offer customers guidance\n[https://d1.awsstatic.com/whitepapers/compliance/AWS_Anitian_Workbook_PCI_Cloud_Compliance.pdf] \nso that they may achieve PCI DSS on their usage on top of AWS - this is known as\napplying the carve-out method where Amazon will be audited for a small portion\nof the whole PCI DSS requirements with the understanding the end-customer\napplied the remainder. In practice you as the end-customer are ultimately\nobligated to meet all of the requirements not just the remainder left over by\nAWS, thought it is the discretion of the auditor to consider the AWS attestation\nwhen auditing your organisation for PCI compliance.\n\nTo go deeper on this point, the QSA (Qualified Security Auditor) applied PCI\nDSS\ntesting procedures\n[https://www.pcisecuritystandards.org/.../PCI-DSS-v3_2-ROC-Reporting-Template.pdf] \non what AWS have implemented under-the-hood.\nAll requirements that are able to have the end-customer implement the control \nAWS are not tested by the QSA at all.\n\nIn terms of Lambda, a service where you do not have control of any\ninfrastructure, access to the operating system, or any ability to configure\nanything at all beyond what the AWS abstraction provides - you don't even have\naccess to the programming language execution environment configuration, how your\ncode is initialised, or in some cases what the inputs are. You only have your\nsource code in the form of a function which is executed on your behalf with\nlimited or no ability to control more than the function output.\n\nThat is to say, for PCI DSS requirements around encryption at-rest, Lambda's\ndisk-level storage is not encrypted, i.e. The encryption at-rest option\npresented will only encrypt the Lambda package AWS receives from you, but it\ndoes not provide encryption at-rest of the execution environment of Lambda\nitself.\n\nThe mentioned exception is if you pursue the PCI DSS Compensating Controls\nWorksheet, which must not only meet but exceed the original intent of the\nrequirement you could not meet. More then that, compliance is at the full\ndiscretion of the QSA whether or not you've implemented sufficient controls that\nexceed the requirements original intent.\nFor encryption at-rest requirements you will need compensating controls that\napply sufficient controls for confidentiality, data integrity, data being\nrendered useless upon breach or disclosure to name a few intents of encryption\nat-rest.\n\nRemember, to achieve PCI DSS using the Compensating COntrols Worksheet you must\nexceed the original intent in the eyes of the QSA, so don't expect the audit\nwill go smoothly when using this technique.\n\n> Important side note: Lambda disk storage is both multi-tenanted and eventually \nephemeral, it is reused/shared across multiple Lambda invocations that are\nperceived to be distinct/sandbox execution environments but are not.\n\n\nThis means a end-customer can not achieve PCI DSS if AWS Lambda is PCI in-scope,\nwith 1 exception I'll describe in a moment. You cannot apply the requirements\nfor encryption at-rest and AWS have not provided this as an option for you to\nconfigure yourself, and AWS has not provided you with any attestation that they\nuse any hardware-encrypted storage.\n\nGenerally, if you are an MSSP, providing hosting as part of your service\noffering to your customers on top of AWS (or any other MSSP) you are fully\nresponsible for protecting your customers CHD (card holder data) and secure the\nCDE (card data environment), none of vendor supplied attestations can be\nprovided to your customers as attestation of your security posture, you must\nobtain attestations of your own for each compliance or standard you supply-chain\nprovides to you.\n\nPutting it bluntly, if you do not achieve PCI DSS then none of your own\ncustomers can.\n\nIn summary, a vendors attestation is proof of their security not yours,\nattestations are not transferable to their customers in any way.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/what_is_attestation.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-02-15 04:54:15","created_by":"1","updated_at":"2021-03-31 14:00:45","updated_by":"1","published_at":"2018-08-04 06:09:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb7","uuid":"632f16a0-47aa-4839-8688-e3de349532eb","title":"How to fix sudo must be owned by uid 0 and have the setuid bit set in WSL","slug":"how-to-fix-sudo-must-be-owned-by-uid-0-and-have-the-setuid-bit-set","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"So I recently took up the challenge of turning my PC with only Ubuntu KDE installed into a dual-boot Windows 10 PC. I've been on Ubuntu since 2008 and after a decade free of Windows at home I decided to take up the challenge of using WSL (Windows subsystem for Linux).\\n\\nHowever after developing and running all my favorite cli tools for nearly 3 months I ran into `/usr/bin/sudo must be owned by uid 0 and have the setuid bit set` error.\\n\\nNormally on a Linux host you'd login to a root shell and fix things but WSL didn't appear to have a way to login as or switch to root from the scope of `bash.exe`.\\n\\nDare I venture into the land of PowerShell? Seems inevitable.\\n\\nI've been running WSL since before the Fall Creators Update so the `lx subsystem` environment is managed with `lxrun` (can't possibly be that tool from the 90's).\\nAppears that switching the user is as simple as running `lxrun /setdefaultuser root` in PowerShell, then you can go into a new `bash.exe` session and fix the following like you would on a Linux host;\\n\\n- Fix the owner of sudo `chown root:root /usr/bin/sudo`\\n- Fix permissions of sudo `chmod 4755 /usr/bin/sudo`\\n\\nExit the session and go back to PowerShell and run `lxrun /setdefaultuser ubuntu` to set the user back.\\n\\nStart a new `bash.exe` session with sudo fixed.\\n\\nNow if you are running WSL installed after the Fall Creators Update I believe the PowerShell commands are now different. You can try `ubuntu config --default-user root` first then `ubuntu config --default-user ubuntu` to revert, but I have no way to verifying this without breaking my _now_ working environment.\\n\\n\\n\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>So I recently took up the challenge of turning my PC with only Ubuntu KDE installed into a dual-boot Windows 10 PC. I've been on Ubuntu since 2008 and after a decade free of Windows at home I decided to take up the challenge of using WSL (Windows subsystem for Linux).</p>\n<p>However after developing and running all my favorite cli tools for nearly 3 months I ran into <code>/usr/bin/sudo must be owned by uid 0 and have the setuid bit set</code> error.</p>\n<p>Normally on a Linux host you'd login to a root shell and fix things but WSL didn't appear to have a way to login as or switch to root from the scope of <code>bash.exe</code>.</p>\n<p>Dare I venture into the land of PowerShell? Seems inevitable.</p>\n<p>I've been running WSL since before the Fall Creators Update so the <code>lx subsystem</code> environment is managed with <code>lxrun</code> (can't possibly be that tool from the 90's).<br>\nAppears that switching the user is as simple as running <code>lxrun /setdefaultuser root</code> in PowerShell, then you can go into a new <code>bash.exe</code> session and fix the following like you would on a Linux host;</p>\n<ul>\n<li>Fix the owner of sudo <code>chown root:root /usr/bin/sudo</code></li>\n<li>Fix permissions of sudo <code>chmod 4755 /usr/bin/sudo</code></li>\n</ul>\n<p>Exit the session and go back to PowerShell and run <code>lxrun /setdefaultuser ubuntu</code> to set the user back.</p>\n<p>Start a new <code>bash.exe</code> session with sudo fixed.</p>\n<p>Now if you are running WSL installed after the Fall Creators Update I believe the PowerShell commands are now different. You can try <code>ubuntu config --default-user root</code> first then <code>ubuntu config --default-user ubuntu</code> to revert, but I have no way to verifying this without breaking my <em>now</em> working environment.</p>\n<!--kg-card-end: markdown-->","comment_id":"5aab8101c6913d057a6dcb5d","plaintext":"So I recently took up the challenge of turning my PC with only Ubuntu KDE\ninstalled into a dual-boot Windows 10 PC. I've been on Ubuntu since 2008 and\nafter a decade free of Windows at home I decided to take up the challenge of\nusing WSL (Windows subsystem for Linux).\n\nHowever after developing and running all my favorite cli tools for nearly 3\nmonths I ran into /usr/bin/sudo must be owned by uid 0 and have the setuid bit\nset error.\n\nNormally on a Linux host you'd login to a root shell and fix things but WSL\ndidn't appear to have a way to login as or switch to root from the scope of \nbash.exe.\n\nDare I venture into the land of PowerShell? Seems inevitable.\n\nI've been running WSL since before the Fall Creators Update so the lx subsystem \nenvironment is managed with lxrun (can't possibly be that tool from the 90's).\nAppears that switching the user is as simple as running lxrun /setdefaultuser\nroot in PowerShell, then you can go into a new bash.exe session and fix the\nfollowing like you would on a Linux host;\n\n * Fix the owner of sudo chown root:root /usr/bin/sudo\n * Fix permissions of sudo chmod 4755 /usr/bin/sudo\n\nExit the session and go back to PowerShell and run lxrun /setdefaultuser ubuntu \nto set the user back.\n\nStart a new bash.exe session with sudo fixed.\n\nNow if you are running WSL installed after the Fall Creators Update I believe\nthe PowerShell commands are now different. You can try ubuntu config\n--default-user root first then ubuntu config --default-user ubuntu to revert,\nbut I have no way to verifying this without breaking my now working environment.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/05/Capture-2.JPG","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-03-16 08:32:01","created_by":"1","updated_at":"2021-03-31 14:02:19","updated_by":"1","published_at":"2018-03-20 03:46:06","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb8","uuid":"5a5e2d23-7a93-4962-a84b-6a4ae81c8c52","title":"How-to convince colleagues to accept new processes","slug":"how-to-convince-colleagues-to-accept-new-processes","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Let me share with you a technique taught to me that I've used and refined, it's always had an impact that can help convince even the most remiss, ignorant, or doubtful opposition to a well argued and rationalised new process.\\n\\nMore than 10yrs ago when I was in my 20s I already had several years experience and was being groomed to be a leader. At 23 I was supervising teams on the floor of exhibitions and events, organising hundreds of exhibitors, freight transports, all the logistics of a show the size of Australia's Avalon Air Show (yes the one with tanks, and crazy red bull plane demonstrations). Most of the time I'd be working on at least 5 (and up to 20) events simultaneously.\\nObviously that sounds incredibly difficult, but that was the job and there isn't much more than the organisation and interpersonal skills needed. So my mentors helped me through this teaching me the skills they've used for decades, one particular lesson was where my technique came from, and it was simple;\\n\\n1) Give the subject some _seemingly_ arbitrary information upfront\\n2) Discuss some merits\\n3) Ask questions you asked during your own journey to learn your conclusion (yet to be delivered)\\n4) Deliver your ultimate message\\n\\nNow let's dig deeper into this;\\n\\n1) Give the subject some _seemingly_ arbitrary information upfront but do not dwell on it. It is key to delivering your desired message and you want it in their subconscious early but not be over bearing.\\n2) Discuss some merits and try be extremely open to all arguments they present but be cautious to avoid any deep discussion on them. You've spent more time on this then most and have done you're own assessments on all these arguments. So although you have justified responses now do not indulge these arguments yet.\\n3) Your desired message is likely to be one that anybody can conclude given they had asked the same questions and followed similar investigations to your own, so be sure to lead your audience along your journey by presenting them simple questions in a well thought out order.\\n4) The delivery of your ultimate message, the one you desire your reluctant audience  to accept, should feel as though it was their idea and the most natural conclusion. In reality they simply took a condensed version of the same path you took to get to your conclusion.\\n\\nToday however, I do not follow this strategy. Though mostly effective it is actually very hard to go against my nature until the end. I intrinsically aim to share my knowledge which makes me susceptible to debate or give in to argument before realising my audience/adversary is not yet open or ready to accept the information being delivered.\\n\\nAlso I find the above technique to be border-line manipulative and that doesn't sit right with me.\\n\\nThe lessons in that method are people need to not just be told answers they need a _realisation_ moment. My mentor always suggested the answers should come to them as though they had thought of it, but i disagree. People are more mature I find, and you can still deliver the idea to them so long as you ensure you've delivered that moment of realisation prior. This is very important.\\n\\nSo here is an example of how I deliver _realisation_.\\n\\nHypothetically you are a new coder in a large organisation where there are a lot of more senior coders, architects, managers, and all the diverse rolls of a software development focused organisation. You learn that all code is stored in shared networked file server (ahem Dropbox) and you are horrified.\\nInstead of running screaming like a sane person, you were looking forward to working with these great people on a bleeding edge code base so you stay.\\n\\nYour first attempts to propose source control (git to the rescue, yes this is only in last decade) was met with sarcasm and dismissive jest, how could you possibly know how to manage such a large foreign code base as a young newcomer?\\nOne day you intend to be a consultant that works on new and exciting technologies with many companies so if you can't convince this one team to adopt a superior technology that will save them time, effort, and money - then how will you ever reach your goals?\\n\\nThis scenario is to establish a once common situation which we now deem a novel decision on any new project.\\n\\nTaking for granted today's development environment, source control enables you to do a lot of other related things that were impossible or very hard to do prior. Consider;\\n\\n- New starters setting up a development environment\\n- Switching between multiple development environments\\n- Software configuration\\n- Concurrent incompatible development\\n- Having the same state of code everywhere\\n- Being able to view past states of code\\n\\nLets just focus on the key topics you need to communicate;\\n\\n- What problem does this solve\\n- Time and cost, both savings and loss when switching\\n\\nIt's not always clear but you should always cover _all_ pro's as a single idea, it is the desired message you'll want to deliver. This case it is **Change and release management** that is the problem you are solving and it so happens to sum up all the pro's above as well. Be sure to prepare appropriate time and cost analysis associated but hold it back until after you've delivered the main message.\\n\\nLet's focus on the _realisation_ moment now. \\n\\nAssuming you secured a short time-slot to deliver your proposal, everyone is in the room and before you start you instruct everyone to;\\n1. Write on the paper you provided 6 four digit number and their own name below the numbers\\n2. Ensure that they have personal meaning to you, because you should memorise them all.\\n3. Swap paper with someone so that everyone has someone else's 6 numbers\\n4. Read all 6 numbers, strike out 2 of the 6 numbers, and write your name on the back, and hand in to you (the presenter)\\n\\nYou go on to present the proposal and make no mention of the exercise.\\n\\nAt the end you randomly choose a paper, ask the named person from the back of the paper to recall the 4 numbers they did not strike out. It is most likely they remember only the ones they had striked-out, and if they can recall all you'll have to start again with someone new because you likely chose someone who has a photographic memory!\\nNow ask the person who decided the numbers to recall them all, they likely recall each one perfectly because they have special meaning.\\n\\nThis exercise is designed to demonstrate;\\n\\n- A six step simple release plan\\n- The person who decided the numbers know the plan well, they designed it!\\n- When someone gets new information it is extremely hard to instantly relate to all of it at once so you focus on certain details that allow you to relate this new information to your current knowledge. Striking out 2 numbers demonstrates this\\n- The time period between the exercise and the test is filled with new information (your presentation) which demonstrates BAU in a workplace\\n\\nAll 6 steps are crucial, and without proper change and release management things break down fast.\\n\\nNo one has perfect memory recall for long periods of time not spent recalling the information, so for accuracy and consistency sake we should have systems and process in place to do the accurate memory recall for us.\\n\\nYou've demonstrated a simple analogy that everyone can relate to with your exercise which engaged and hopefully impacts the audience in preparation for you to deliver the desired message.\\n\\nHow could they now deny the benefits!?\"}]],\"markups\":[],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Let me share with you a technique taught to me that I've used and refined, it's always had an impact that can help convince even the most remiss, ignorant, or doubtful opposition to a well argued and rationalised new process.</p>\n<p>More than 10yrs ago when I was in my 20s I already had several years experience and was being groomed to be a leader. At 23 I was supervising teams on the floor of exhibitions and events, organising hundreds of exhibitors, freight transports, all the logistics of a show the size of Australia's Avalon Air Show (yes the one with tanks, and crazy red bull plane demonstrations). Most of the time I'd be working on at least 5 (and up to 20) events simultaneously.<br>\nObviously that sounds incredibly difficult, but that was the job and there isn't much more than the organisation and interpersonal skills needed. So my mentors helped me through this teaching me the skills they've used for decades, one particular lesson was where my technique came from, and it was simple;</p>\n<ol>\n<li>Give the subject some <em>seemingly</em> arbitrary information upfront</li>\n<li>Discuss some merits</li>\n<li>Ask questions you asked during your own journey to learn your conclusion (yet to be delivered)</li>\n<li>Deliver your ultimate message</li>\n</ol>\n<p>Now let's dig deeper into this;</p>\n<ol>\n<li>Give the subject some <em>seemingly</em> arbitrary information upfront but do not dwell on it. It is key to delivering your desired message and you want it in their subconscious early but not be over bearing.</li>\n<li>Discuss some merits and try be extremely open to all arguments they present but be cautious to avoid any deep discussion on them. You've spent more time on this then most and have done you're own assessments on all these arguments. So although you have justified responses now do not indulge these arguments yet.</li>\n<li>Your desired message is likely to be one that anybody can conclude given they had asked the same questions and followed similar investigations to your own, so be sure to lead your audience along your journey by presenting them simple questions in a well thought out order.</li>\n<li>The delivery of your ultimate message, the one you desire your reluctant audience  to accept, should feel as though it was their idea and the most natural conclusion. In reality they simply took a condensed version of the same path you took to get to your conclusion.</li>\n</ol>\n<p>Today however, I do not follow this strategy. Though mostly effective it is actually very hard to go against my nature until the end. I intrinsically aim to share my knowledge which makes me susceptible to debate or give in to argument before realising my audience/adversary is not yet open or ready to accept the information being delivered.</p>\n<p>Also I find the above technique to be border-line manipulative and that doesn't sit right with me.</p>\n<p>The lessons in that method are people need to not just be told answers they need a <em>realisation</em> moment. My mentor always suggested the answers should come to them as though they had thought of it, but i disagree. People are more mature I find, and you can still deliver the idea to them so long as you ensure you've delivered that moment of realisation prior. This is very important.</p>\n<p>So here is an example of how I deliver <em>realisation</em>.</p>\n<p>Hypothetically you are a new coder in a large organisation where there are a lot of more senior coders, architects, managers, and all the diverse rolls of a software development focused organisation. You learn that all code is stored in shared networked file server (ahem Dropbox) and you are horrified.<br>\nInstead of running screaming like a sane person, you were looking forward to working with these great people on a bleeding edge code base so you stay.</p>\n<p>Your first attempts to propose source control (git to the rescue, yes this is only in last decade) was met with sarcasm and dismissive jest, how could you possibly know how to manage such a large foreign code base as a young newcomer?<br>\nOne day you intend to be a consultant that works on new and exciting technologies with many companies so if you can't convince this one team to adopt a superior technology that will save them time, effort, and money - then how will you ever reach your goals?</p>\n<p>This scenario is to establish a once common situation which we now deem a novel decision on any new project.</p>\n<p>Taking for granted today's development environment, source control enables you to do a lot of other related things that were impossible or very hard to do prior. Consider;</p>\n<ul>\n<li>New starters setting up a development environment</li>\n<li>Switching between multiple development environments</li>\n<li>Software configuration</li>\n<li>Concurrent incompatible development</li>\n<li>Having the same state of code everywhere</li>\n<li>Being able to view past states of code</li>\n</ul>\n<p>Lets just focus on the key topics you need to communicate;</p>\n<ul>\n<li>What problem does this solve</li>\n<li>Time and cost, both savings and loss when switching</li>\n</ul>\n<p>It's not always clear but you should always cover <em>all</em> pro's as a single idea, it is the desired message you'll want to deliver. This case it is <strong>Change and release management</strong> that is the problem you are solving and it so happens to sum up all the pro's above as well. Be sure to prepare appropriate time and cost analysis associated but hold it back until after you've delivered the main message.</p>\n<p>Let's focus on the <em>realisation</em> moment now.</p>\n<p>Assuming you secured a short time-slot to deliver your proposal, everyone is in the room and before you start you instruct everyone to;</p>\n<ol>\n<li>Write on the paper you provided 6 four digit number and their own name below the numbers</li>\n<li>Ensure that they have personal meaning to you, because you should memorise them all.</li>\n<li>Swap paper with someone so that everyone has someone else's 6 numbers</li>\n<li>Read all 6 numbers, strike out 2 of the 6 numbers, and write your name on the back, and hand in to you (the presenter)</li>\n</ol>\n<p>You go on to present the proposal and make no mention of the exercise.</p>\n<p>At the end you randomly choose a paper, ask the named person from the back of the paper to recall the 4 numbers they did not strike out. It is most likely they remember only the ones they had striked-out, and if they can recall all you'll have to start again with someone new because you likely chose someone who has a photographic memory!<br>\nNow ask the person who decided the numbers to recall them all, they likely recall each one perfectly because they have special meaning.</p>\n<p>This exercise is designed to demonstrate;</p>\n<ul>\n<li>A six step simple release plan</li>\n<li>The person who decided the numbers know the plan well, they designed it!</li>\n<li>When someone gets new information it is extremely hard to instantly relate to all of it at once so you focus on certain details that allow you to relate this new information to your current knowledge. Striking out 2 numbers demonstrates this</li>\n<li>The time period between the exercise and the test is filled with new information (your presentation) which demonstrates BAU in a workplace</li>\n</ul>\n<p>All 6 steps are crucial, and without proper change and release management things break down fast.</p>\n<p>No one has perfect memory recall for long periods of time not spent recalling the information, so for accuracy and consistency sake we should have systems and process in place to do the accurate memory recall for us.</p>\n<p>You've demonstrated a simple analogy that everyone can relate to with your exercise which engaged and hopefully impacts the audience in preparation for you to deliver the desired message.</p>\n<p>How could they now deny the benefits!?</p>\n<!--kg-card-end: markdown-->","comment_id":"5acac7a7d918b0058e125fb5","plaintext":"Let me share with you a technique taught to me that I've used and refined, it's\nalways had an impact that can help convince even the most remiss, ignorant, or\ndoubtful opposition to a well argued and rationalised new process.\n\nMore than 10yrs ago when I was in my 20s I already had several years experience\nand was being groomed to be a leader. At 23 I was supervising teams on the floor\nof exhibitions and events, organising hundreds of exhibitors, freight\ntransports, all the logistics of a show the size of Australia's Avalon Air Show\n(yes the one with tanks, and crazy red bull plane demonstrations). Most of the\ntime I'd be working on at least 5 (and up to 20) events simultaneously.\nObviously that sounds incredibly difficult, but that was the job and there isn't\nmuch more than the organisation and interpersonal skills needed. So my mentors\nhelped me through this teaching me the skills they've used for decades, one\nparticular lesson was where my technique came from, and it was simple;\n\n 1. Give the subject some seemingly arbitrary information upfront\n 2. Discuss some merits\n 3. Ask questions you asked during your own journey to learn your conclusion\n    (yet to be delivered)\n 4. Deliver your ultimate message\n\nNow let's dig deeper into this;\n\n 1. Give the subject some seemingly arbitrary information upfront but do not\n    dwell on it. It is key to delivering your desired message and you want it in\n    their subconscious early but not be over bearing.\n 2. Discuss some merits and try be extremely open to all arguments they present\n    but be cautious to avoid any deep discussion on them. You've spent more time\n    on this then most and have done you're own assessments on all these\n    arguments. So although you have justified responses now do not indulge these\n    arguments yet.\n 3. Your desired message is likely to be one that anybody can conclude given\n    they had asked the same questions and followed similar investigations to\n    your own, so be sure to lead your audience along your journey by presenting\n    them simple questions in a well thought out order.\n 4. The delivery of your ultimate message, the one you desire your reluctant\n    audience to accept, should feel as though it was their idea and the most\n    natural conclusion. In reality they simply took a condensed version of the\n    same path you took to get to your conclusion.\n\nToday however, I do not follow this strategy. Though mostly effective it is\nactually very hard to go against my nature until the end. I intrinsically aim to\nshare my knowledge which makes me susceptible to debate or give in to argument\nbefore realising my audience/adversary is not yet open or ready to accept the\ninformation being delivered.\n\nAlso I find the above technique to be border-line manipulative and that doesn't\nsit right with me.\n\nThe lessons in that method are people need to not just be told answers they need\na realisation moment. My mentor always suggested the answers should come to them\nas though they had thought of it, but i disagree. People are more mature I find,\nand you can still deliver the idea to them so long as you ensure you've\ndelivered that moment of realisation prior. This is very important.\n\nSo here is an example of how I deliver realisation.\n\nHypothetically you are a new coder in a large organisation where there are a lot\nof more senior coders, architects, managers, and all the diverse rolls of a\nsoftware development focused organisation. You learn that all code is stored in\nshared networked file server (ahem Dropbox) and you are horrified.\nInstead of running screaming like a sane person, you were looking forward to\nworking with these great people on a bleeding edge code base so you stay.\n\nYour first attempts to propose source control (git to the rescue, yes this is\nonly in last decade) was met with sarcasm and dismissive jest, how could you\npossibly know how to manage such a large foreign code base as a young newcomer?\nOne day you intend to be a consultant that works on new and exciting\ntechnologies with many companies so if you can't convince this one team to adopt\na superior technology that will save them time, effort, and money - then how\nwill you ever reach your goals?\n\nThis scenario is to establish a once common situation which we now deem a novel\ndecision on any new project.\n\nTaking for granted today's development environment, source control enables you\nto do a lot of other related things that were impossible or very hard to do\nprior. Consider;\n\n * New starters setting up a development environment\n * Switching between multiple development environments\n * Software configuration\n * Concurrent incompatible development\n * Having the same state of code everywhere\n * Being able to view past states of code\n\nLets just focus on the key topics you need to communicate;\n\n * What problem does this solve\n * Time and cost, both savings and loss when switching\n\nIt's not always clear but you should always cover all pro's as a single idea, it\nis the desired message you'll want to deliver. This case it is Change and\nrelease management that is the problem you are solving and it so happens to sum\nup all the pro's above as well. Be sure to prepare appropriate time and cost\nanalysis associated but hold it back until after you've delivered the main\nmessage.\n\nLet's focus on the realisation moment now.\n\nAssuming you secured a short time-slot to deliver your proposal, everyone is in\nthe room and before you start you instruct everyone to;\n\n 1. Write on the paper you provided 6 four digit number and their own name below\n    the numbers\n 2. Ensure that they have personal meaning to you, because you should memorise\n    them all.\n 3. Swap paper with someone so that everyone has someone else's 6 numbers\n 4. Read all 6 numbers, strike out 2 of the 6 numbers, and write your name on\n    the back, and hand in to you (the presenter)\n\nYou go on to present the proposal and make no mention of the exercise.\n\nAt the end you randomly choose a paper, ask the named person from the back of\nthe paper to recall the 4 numbers they did not strike out. It is most likely\nthey remember only the ones they had striked-out, and if they can recall all\nyou'll have to start again with someone new because you likely chose someone who\nhas a photographic memory!\nNow ask the person who decided the numbers to recall them all, they likely\nrecall each one perfectly because they have special meaning.\n\nThis exercise is designed to demonstrate;\n\n * A six step simple release plan\n * The person who decided the numbers know the plan well, they designed it!\n * When someone gets new information it is extremely hard to instantly relate to\n   all of it at once so you focus on certain details that allow you to relate\n   this new information to your current knowledge. Striking out 2 numbers\n   demonstrates this\n * The time period between the exercise and the test is filled with new\n   information (your presentation) which demonstrates BAU in a workplace\n\nAll 6 steps are crucial, and without proper change and release management things\nbreak down fast.\n\nNo one has perfect memory recall for long periods of time not spent recalling\nthe information, so for accuracy and consistency sake we should have systems and\nprocess in place to do the accurate memory recall for us.\n\nYou've demonstrated a simple analogy that everyone can relate to with your\nexercise which engaged and hopefully impacts the audience in preparation for you\nto deliver the desired message.\n\nHow could they now deny the benefits!?","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/04/BAU.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-04-09 01:53:43","created_by":"1","updated_at":"2021-03-31 14:04:33","updated_by":"1","published_at":"2018-04-07 01:56:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcb9","uuid":"1e0c3959-8e96-4783-ac92-68f3f17d2649","title":"Software Engineers guide to AWS Solution Architecture","slug":"software-engineers-guide-to-aws-solution-architecture","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"So you're a developer or operations engineer (or both, DevOps) and work in a small team that either has no access to a Solution Architect.\\n\\n> or you do but expected to re-architect solutions for security and reliabilty\\n\\nGet to know the [AWS Well-Architected Framework](https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf) and the new [Operational Excellence Pillar](https://d0.awsstatic.com/whitepapers/architecture/AWS-Operational-Excellence-Pillar.pdf) white papers well. Do a review of your apps individually against this framework, and don't worry if you find serious issues because it's well understood within AWS that even the AWS Solution Architects 'always' find critical problems in every workplace they visit to review.\\n\\nIn your architecture diagrams be sure to include the consistently forgotten critical infrastructure components such as;\\n\\n## How AWS Accounts connect\\n\\nDefine these properly, show each account ID _visually_, and show not just VPC but also the availability zones and their subnets and endpoints.\\n\\nIf a RDS is multi-az, show this as 2 or 3 RDS instances not one, same for EC2 Auto-scale groups, show the EC2 instance in each AZ.\\nFor Lambda it helps to know which subnet it deploys too and if there are VPC endpoints for things like S3, because accessing S3 over-the-internet is sometimes not ideal\\n\\n## Host OS agents\\n\\nAWS CloudWatch logs is extremely easy to setup/install and will eliminate the need to ever SSH into an instance again. Once a log group or stream is created you can easily set an expiry so the archived logs are reliably purged via a 1-time cli command or in the console.\\n\\nThe SSM agent is another great tool to allow easy patching and provide reporting on compliance with AWS Config and Inspector.\\n\\nAnd don't forget any 3rd party intrusion prevention or endpoint protection agents such as Trend Micro Deep Security.\\n\\n## Single-Tenant vs Multi-tenant \\n\\nMake sure you show tenancy aspects of the solution design.\\nThis is key for any ISO (Information Security Officer) or auditor to know from you eventually anyway, so be sure to properly understand your choices earlier in the planning stage rather than have to resort to compensating controls much later in the implementation - which usually come at the highest developer time/cost when there is little or no time left on the project.\\n\\n## Secrets management\\n\\nBe sure to identify and visually represent SSM Parameter Store for encrypted secret strings such as tokens used by 3rd party integrations, and Secrets Manager for managing database passwords seamlessly.\\n\\nKMS or CloudHSM and the requirements of Key management such as revokation and rotation that developers will be required to implement should be clear visually represented and such requirements annotated as these will likely require some additional infrastructure (Lambda) to achieve.\\n\\n## Events and Integration Hooks\\n\\nJust like in the key management point, we have many used and unused CloudWatch Events, S3 lifecycle, and service specific events (like Kinesis and AWS Glue).\\n\\nThese events are usually of very high business value, under utilised, and extremely easy for a developer to leverage in most cases. But if a requirement never makes it's way to a developer because a business unit was not aware of the capability - then as an architect you've pretty much failed to identify and produce value from an essentially free resource that has potential for some ground breaking user experiences and statistical or behavioural data.\\n\\n## Alerts\\n\\nMonitoring is usually an afterthought.\\n\\nBut alerting capabilities are often never given a first thought.\\n\\nSNS Topics, Pager Duty, and Raygun.io are all great, but what I am talking about is CloudWatch Rules. Just like some of the events mentioned earlier, these can alert you for actions that CloudWatch Alarms cannot. One of my favorite uses for a CloudWatch Rule is with Glue Data Catalog because the Apache Hive Metadata store generated is useful for EMR, Athena, Redshift Spectrum, and Glue ETL - and with a CloudWatch Rule I am notified when interesting things happen like a Database schema change or a failed crawl of S3 so I can automate a retry or changes to services downstream in my Athena SQL or Models running in EMR.\\n\\n## Tips by service\\n\\n### AWS Shield\\n\\n- Free service\\n- Protects: ELB, CloudFront, Route 53\\n    - SYN/UDP floods\\n    - Reflection attacks  \\n    - Layer ¾ attacks\\n\\n### Pen Testing\\n\\n- Can only be carried out on certain services;\\n    - EC2, RDS, Aurora, CloudFront, API Gateway, Lambda, Lightsail, DNS Zone Walking.\\n    - Small or micro RDS instance not permitted.\\n    - m1.small, t1.micro or t2.nano EC2 not permitted\\n\\n### AWS Certificate Manager\\n\\n- Cannot export\\n- Only for Route53 registered domains\\n- Works with CloudFront and Load Balancers\\n\\n### API Gateway\\n\\n- Always throttles requests (default max 10k per sec)\\n- If burst-limit exceeds 429 Too Many Request is returned\\n- Use TTL caching to mitigate, max 3600 sec\\n\\n### Systems Manager\\n\\n- Parameter Store: EC2 (RunCommand), Lambda, CloudFormation, API\\n- RunCommand works on instances or Tags, and execute as root\\n\\n### S3\\n\\n- Replication uses SSL by default, versioning must be enabled at both ends\\n- Object DELETE are replicated, versioned DELETE are not\\n- Enforce SSL access using Condition aws:SecureTransport bucket policy\\n- Pre-signed URL default 1hr, you can pre-sign PutObject data too\\n\\n### CloudTrail\\n\\n- Delivered to S3 every 5mins, with 15min delay\\n- Only logs API, does not record instance-data, ssh or rdp\\n- Logs include api request metadata, identity, time, sourceIP, parameters, and response elements\\n- Use DIGEST for integrity validation of logs using SHA-256 hashing with RSA for signing\\n\\n### CloudWatch\\n\\n- Events are near real-time of: resource changes, CloudTrail, scheduled, or custom from code\\n- Rules match incoming Events, Rule Targets can be Lambda, SNS, SQS, Kinesis\\n\\n### EC2\\n\\n- Dedicated instances are account locked and may share hardware with other instances that are not dedicated if they are of the same account\\n- AWS Staff have access to Hosts and Hypervisors, they cannot access guest operating systems\\n- RAM and storage are securely scrubbed before delivery to customers\\n\\n### AWS Config\\n\\n- Resource inventory, configuration history, with change notification\\n- Trigger periodic, or filtered snapshots\\n\\n### AWS Inspector\\n\\n- Need EC2 agents installed on the assessment target\\n- Create assessment templates to run and verify rules against findings\\n- Detects common vulnerabilities, CIS benchmark, and runtime behaviour analysis of network, file, and process as well as advises remediation\\n\\n### Trusted Advisor\\n\\n- Cost optimisation, performance, availability, and limited security\\n- Requires business/enterprise support for unlocked features\\n\\n### CloudHSM\\n\\n- Only CloudHSM (full control of keys) meet level 3 FIPS 140-2, KMS does not. \\n- CloudHSM offers asymmetric encryption, KMS only has symmetric available. \\n- CloudHSM is single tenanted, KMS is multi-tenanted.\\n- 4 main user type for keys;\\n    - PRECO – Precrypto officer\\n        - User and password management\\n    - PCO or CO -- Crypto officer\\n        - All Key management, key access issuing, material creation, signing, verifying, chaining\\n    - CU – Crypto User\\n        - Basic key management (create, rotate), allowed key exporting, signing, verifying\\n    - AU – Appliance User\\n        - Can perform cloning and sync operations in a cluster\\n\\n### KMS\\n\\n- KMS is region specific\\n- KMS cannot be used for EC2 key/pairs, these are asymmetric, and allowing AWS to generate these would allow AWS access into the EC2 instances\\n- KMS integrates with EBS, S3, Redshift, Elastic Transcoder, WorkMail, RDS, more\\n    - Keys include; alias, create date, desc, state, material\\n    - Cannot be exported\\n- AWS-managed CMK have no material\\n- Customer-managed CMK need symmetric 256-bit key material provided\\n    - You can also build extra resiliency by storing the key yourself outside of AWS\\n    - Avoids 7-30day wait when deleting keys\\n    - No automatic key rotation\\n\\n### AWS Shield\\n\\n- On by default, $3000 a month for advanced options\\n    - Incident response team\\n    - in-depth reporting\\n    - payment reductions when victim of an attack\\n\\n### AWS WAF\\n\\n- Regionally integrate with load balancers or associate with cloudfront distribution  for global\\n- IPv6 is supported, and CIDR blocks /8, /16, /24, /32\\n- Allow or Block all except specified\\n- Count requests based on properties matched\\n- Properties: IP, Country, header values, body length, SQL with known exploits, scripts with known XSS\\n\\n### AWS Marketplace\\n\\n- AWS Partners and Authorised appliances may be used to conduct testing\\n- Firewalls, Hardened OS, WAF, Antivirus, Security monitoring, etc\\n- CIS Benchmarked OS\\n\\n### VPC\\n\\n- Ensure default route table has no public route out to the internet so new subnets that are associated by default are secure by default (AWS default route tables are not secure by default)\\n    - The same applies to the NACL allowing all inbound and outbound traffic\\n- NAT instances must be behind a security group and in a public subnet, disabled source/destination checks, are not patched, is not scalable, is single AZ, and single subnet.\\n    - Prefer NAT Gateways that are patched by AWS, scalable to 10Gbps, no security groups associated, no need to disable source/destination checks, apply to a route table, and get a public IP by default\\n- ALBs need at least 2 public subnets\\n- Flow logs cannot be tagged, and the IAM role or configuration cannot be edited\\n- VPC Peered Flow logs only work when both VPCs are in the same account\\n- Flow logs do not monitor DHCP, DNS, instance metadata address 169.254.169.254, Windows activation, or the AWS reserved IP addresses\\n- VPC Endpoints are either;\\n    - Interface: single ENI\\n    - Gateways: more durable (not single ENI)\\n\\n### General Security\\n\\n- CIA: confidentiality, integrity, availability\\n- AAA: authentication, authorization, accounting\\n- Non-repudiation = cannot deny a fact\\n- Shared Responsibility model changed per Infrastructure, container, abstracted levels;\\n    - Infrastructure (EC2, VPC, EBS)\\n    - Container (RDS, EMR)\\n    - Abstracted (S3, Dynamo, SQS)\\n- ECDHE for Perfect Forward Secrecy; Elliptic Curve DHE (Diffie-Hellman Ephemeral) key exchange.\\n\\n## Links\\n- https://aws.amazon.com/architecture/well-architected/\\n- https://aws.amazon.com/blogs/architecture/on-architecture-and-the-state-of-the-art/\\n- https://aws.amazon.com/blogs/architecture/well-architected-lens-focus-on-specific-workload-types/\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>So you're a developer or operations engineer (or both, DevOps) and work in a small team that either has no access to a Solution Architect.</p>\n<blockquote>\n<p>or you do but expected to re-architect solutions for security and reliabilty</p>\n</blockquote>\n<p>Get to know the <a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf\">AWS Well-Architected Framework</a> and the new <a href=\"https://d0.awsstatic.com/whitepapers/architecture/AWS-Operational-Excellence-Pillar.pdf\">Operational Excellence Pillar</a> white papers well. Do a review of your apps individually against this framework, and don't worry if you find serious issues because it's well understood within AWS that even the AWS Solution Architects 'always' find critical problems in every workplace they visit to review.</p>\n<p>In your architecture diagrams be sure to include the consistently forgotten critical infrastructure components such as;</p>\n<h2 id=\"howawsaccountsconnect\">How AWS Accounts connect</h2>\n<p>Define these properly, show each account ID <em>visually</em>, and show not just VPC but also the availability zones and their subnets and endpoints.</p>\n<p>If a RDS is multi-az, show this as 2 or 3 RDS instances not one, same for EC2 Auto-scale groups, show the EC2 instance in each AZ.<br>\nFor Lambda it helps to know which subnet it deploys too and if there are VPC endpoints for things like S3, because accessing S3 over-the-internet is sometimes not ideal</p>\n<h2 id=\"hostosagents\">Host OS agents</h2>\n<p>AWS CloudWatch logs is extremely easy to setup/install and will eliminate the need to ever SSH into an instance again. Once a log group or stream is created you can easily set an expiry so the archived logs are reliably purged via a 1-time cli command or in the console.</p>\n<p>The SSM agent is another great tool to allow easy patching and provide reporting on compliance with AWS Config and Inspector.</p>\n<p>And don't forget any 3rd party intrusion prevention or endpoint protection agents such as Trend Micro Deep Security.</p>\n<h2 id=\"singletenantvsmultitenant\">Single-Tenant vs Multi-tenant</h2>\n<p>Make sure you show tenancy aspects of the solution design.<br>\nThis is key for any ISO (Information Security Officer) or auditor to know from you eventually anyway, so be sure to properly understand your choices earlier in the planning stage rather than have to resort to compensating controls much later in the implementation - which usually come at the highest developer time/cost when there is little or no time left on the project.</p>\n<h2 id=\"secretsmanagement\">Secrets management</h2>\n<p>Be sure to identify and visually represent SSM Parameter Store for encrypted secret strings such as tokens used by 3rd party integrations, and Secrets Manager for managing database passwords seamlessly.</p>\n<p>KMS or CloudHSM and the requirements of Key management such as revokation and rotation that developers will be required to implement should be clear visually represented and such requirements annotated as these will likely require some additional infrastructure (Lambda) to achieve.</p>\n<h2 id=\"eventsandintegrationhooks\">Events and Integration Hooks</h2>\n<p>Just like in the key management point, we have many used and unused CloudWatch Events, S3 lifecycle, and service specific events (like Kinesis and AWS Glue).</p>\n<p>These events are usually of very high business value, under utilised, and extremely easy for a developer to leverage in most cases. But if a requirement never makes it's way to a developer because a business unit was not aware of the capability - then as an architect you've pretty much failed to identify and produce value from an essentially free resource that has potential for some ground breaking user experiences and statistical or behavioural data.</p>\n<h2 id=\"alerts\">Alerts</h2>\n<p>Monitoring is usually an afterthought.</p>\n<p>But alerting capabilities are often never given a first thought.</p>\n<p>SNS Topics, Pager Duty, and Raygun.io are all great, but what I am talking about is CloudWatch Rules. Just like some of the events mentioned earlier, these can alert you for actions that CloudWatch Alarms cannot. One of my favorite uses for a CloudWatch Rule is with Glue Data Catalog because the Apache Hive Metadata store generated is useful for EMR, Athena, Redshift Spectrum, and Glue ETL - and with a CloudWatch Rule I am notified when interesting things happen like a Database schema change or a failed crawl of S3 so I can automate a retry or changes to services downstream in my Athena SQL or Models running in EMR.</p>\n<h2 id=\"tipsbyservice\">Tips by service</h2>\n<h3 id=\"awsshield\">AWS Shield</h3>\n<ul>\n<li>Free service</li>\n<li>Protects: ELB, CloudFront, Route 53\n<ul>\n<li>SYN/UDP floods</li>\n<li>Reflection attacks</li>\n<li>Layer ¾ attacks</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"pentesting\">Pen Testing</h3>\n<ul>\n<li>Can only be carried out on certain services;\n<ul>\n<li>EC2, RDS, Aurora, CloudFront, API Gateway, Lambda, Lightsail, DNS Zone Walking.</li>\n<li>Small or micro RDS instance not permitted.</li>\n<li>m1.small, t1.micro or t2.nano EC2 not permitted</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"awscertificatemanager\">AWS Certificate Manager</h3>\n<ul>\n<li>Cannot export</li>\n<li>Only for Route53 registered domains</li>\n<li>Works with CloudFront and Load Balancers</li>\n</ul>\n<h3 id=\"apigateway\">API Gateway</h3>\n<ul>\n<li>Always throttles requests (default max 10k per sec)</li>\n<li>If burst-limit exceeds 429 Too Many Request is returned</li>\n<li>Use TTL caching to mitigate, max 3600 sec</li>\n</ul>\n<h3 id=\"systemsmanager\">Systems Manager</h3>\n<ul>\n<li>Parameter Store: EC2 (RunCommand), Lambda, CloudFormation, API</li>\n<li>RunCommand works on instances or Tags, and execute as root</li>\n</ul>\n<h3 id=\"s3\">S3</h3>\n<ul>\n<li>Replication uses SSL by default, versioning must be enabled at both ends</li>\n<li>Object DELETE are replicated, versioned DELETE are not</li>\n<li>Enforce SSL access using Condition aws:SecureTransport bucket policy</li>\n<li>Pre-signed URL default 1hr, you can pre-sign PutObject data too</li>\n</ul>\n<h3 id=\"cloudtrail\">CloudTrail</h3>\n<ul>\n<li>Delivered to S3 every 5mins, with 15min delay</li>\n<li>Only logs API, does not record instance-data, ssh or rdp</li>\n<li>Logs include api request metadata, identity, time, sourceIP, parameters, and response elements</li>\n<li>Use DIGEST for integrity validation of logs using SHA-256 hashing with RSA for signing</li>\n</ul>\n<h3 id=\"cloudwatch\">CloudWatch</h3>\n<ul>\n<li>Events are near real-time of: resource changes, CloudTrail, scheduled, or custom from code</li>\n<li>Rules match incoming Events, Rule Targets can be Lambda, SNS, SQS, Kinesis</li>\n</ul>\n<h3 id=\"ec2\">EC2</h3>\n<ul>\n<li>Dedicated instances are account locked and may share hardware with other instances that are not dedicated if they are of the same account</li>\n<li>AWS Staff have access to Hosts and Hypervisors, they cannot access guest operating systems</li>\n<li>RAM and storage are securely scrubbed before delivery to customers</li>\n</ul>\n<h3 id=\"awsconfig\">AWS Config</h3>\n<ul>\n<li>Resource inventory, configuration history, with change notification</li>\n<li>Trigger periodic, or filtered snapshots</li>\n</ul>\n<h3 id=\"awsinspector\">AWS Inspector</h3>\n<ul>\n<li>Need EC2 agents installed on the assessment target</li>\n<li>Create assessment templates to run and verify rules against findings</li>\n<li>Detects common vulnerabilities, CIS benchmark, and runtime behaviour analysis of network, file, and process as well as advises remediation</li>\n</ul>\n<h3 id=\"trustedadvisor\">Trusted Advisor</h3>\n<ul>\n<li>Cost optimisation, performance, availability, and limited security</li>\n<li>Requires business/enterprise support for unlocked features</li>\n</ul>\n<h3 id=\"cloudhsm\">CloudHSM</h3>\n<ul>\n<li>Only CloudHSM (full control of keys) meet level 3 FIPS 140-2, KMS does not.</li>\n<li>CloudHSM offers asymmetric encryption, KMS only has symmetric available.</li>\n<li>CloudHSM is single tenanted, KMS is multi-tenanted.</li>\n<li>4 main user type for keys;\n<ul>\n<li>PRECO – Precrypto officer\n<ul>\n<li>User and password management</li>\n</ul>\n</li>\n<li>PCO or CO -- Crypto officer\n<ul>\n<li>All Key management, key access issuing, material creation, signing, verifying, chaining</li>\n</ul>\n</li>\n<li>CU – Crypto User\n<ul>\n<li>Basic key management (create, rotate), allowed key exporting, signing, verifying</li>\n</ul>\n</li>\n<li>AU – Appliance User\n<ul>\n<li>Can perform cloning and sync operations in a cluster</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"kms\">KMS</h3>\n<ul>\n<li>KMS is region specific</li>\n<li>KMS cannot be used for EC2 key/pairs, these are asymmetric, and allowing AWS to generate these would allow AWS access into the EC2 instances</li>\n<li>KMS integrates with EBS, S3, Redshift, Elastic Transcoder, WorkMail, RDS, more\n<ul>\n<li>Keys include; alias, create date, desc, state, material</li>\n<li>Cannot be exported</li>\n</ul>\n</li>\n<li>AWS-managed CMK have no material</li>\n<li>Customer-managed CMK need symmetric 256-bit key material provided\n<ul>\n<li>You can also build extra resiliency by storing the key yourself outside of AWS</li>\n<li>Avoids 7-30day wait when deleting keys</li>\n<li>No automatic key rotation</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"awsshield\">AWS Shield</h3>\n<ul>\n<li>On by default, $3000 a month for advanced options\n<ul>\n<li>Incident response team</li>\n<li>in-depth reporting</li>\n<li>payment reductions when victim of an attack</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"awswaf\">AWS WAF</h3>\n<ul>\n<li>Regionally integrate with load balancers or associate with cloudfront distribution  for global</li>\n<li>IPv6 is supported, and CIDR blocks /8, /16, /24, /32</li>\n<li>Allow or Block all except specified</li>\n<li>Count requests based on properties matched</li>\n<li>Properties: IP, Country, header values, body length, SQL with known exploits, scripts with known XSS</li>\n</ul>\n<h3 id=\"awsmarketplace\">AWS Marketplace</h3>\n<ul>\n<li>AWS Partners and Authorised appliances may be used to conduct testing</li>\n<li>Firewalls, Hardened OS, WAF, Antivirus, Security monitoring, etc</li>\n<li>CIS Benchmarked OS</li>\n</ul>\n<h3 id=\"vpc\">VPC</h3>\n<ul>\n<li>Ensure default route table has no public route out to the internet so new subnets that are associated by default are secure by default (AWS default route tables are not secure by default)\n<ul>\n<li>The same applies to the NACL allowing all inbound and outbound traffic</li>\n</ul>\n</li>\n<li>NAT instances must be behind a security group and in a public subnet, disabled source/destination checks, are not patched, is not scalable, is single AZ, and single subnet.\n<ul>\n<li>Prefer NAT Gateways that are patched by AWS, scalable to 10Gbps, no security groups associated, no need to disable source/destination checks, apply to a route table, and get a public IP by default</li>\n</ul>\n</li>\n<li>ALBs need at least 2 public subnets</li>\n<li>Flow logs cannot be tagged, and the IAM role or configuration cannot be edited</li>\n<li>VPC Peered Flow logs only work when both VPCs are in the same account</li>\n<li>Flow logs do not monitor DHCP, DNS, instance metadata address 169.254.169.254, Windows activation, or the AWS reserved IP addresses</li>\n<li>VPC Endpoints are either;\n<ul>\n<li>Interface: single ENI</li>\n<li>Gateways: more durable (not single ENI)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"generalsecurity\">General Security</h3>\n<ul>\n<li>CIA: confidentiality, integrity, availability</li>\n<li>AAA: authentication, authorization, accounting</li>\n<li>Non-repudiation = cannot deny a fact</li>\n<li>Shared Responsibility model changed per Infrastructure, container, abstracted levels;\n<ul>\n<li>Infrastructure (EC2, VPC, EBS)</li>\n<li>Container (RDS, EMR)</li>\n<li>Abstracted (S3, Dynamo, SQS)</li>\n</ul>\n</li>\n<li>ECDHE for Perfect Forward Secrecy; Elliptic Curve DHE (Diffie-Hellman Ephemeral) key exchange.</li>\n</ul>\n<h2 id=\"links\">Links</h2>\n<ul>\n<li><a href=\"https://aws.amazon.com/architecture/well-architected/\">https://aws.amazon.com/architecture/well-architected/</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/architecture/on-architecture-and-the-state-of-the-art/\">https://aws.amazon.com/blogs/architecture/on-architecture-and-the-state-of-the-art/</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/architecture/well-architected-lens-focus-on-specific-workload-types/\">https://aws.amazon.com/blogs/architecture/well-architected-lens-focus-on-specific-workload-types/</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"5af85f9bf8bf5505b1893eb8","plaintext":"So you're a developer or operations engineer (or both, DevOps) and work in a\nsmall team that either has no access to a Solution Architect.\n\n> or you do but expected to re-architect solutions for security and reliabilty\n\n\nGet to know the AWS Well-Architected Framework\n[https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf] \nand the new Operational Excellence Pillar\n[https://d0.awsstatic.com/whitepapers/architecture/AWS-Operational-Excellence-Pillar.pdf] \nwhite papers well. Do a review of your apps individually against this framework,\nand don't worry if you find serious issues because it's well understood within\nAWS that even the AWS Solution Architects 'always' find critical problems in\nevery workplace they visit to review.\n\nIn your architecture diagrams be sure to include the consistently forgotten\ncritical infrastructure components such as;\n\nHow AWS Accounts connect\nDefine these properly, show each account ID visually, and show not just VPC but\nalso the availability zones and their subnets and endpoints.\n\nIf a RDS is multi-az, show this as 2 or 3 RDS instances not one, same for EC2\nAuto-scale groups, show the EC2 instance in each AZ.\nFor Lambda it helps to know which subnet it deploys too and if there are VPC\nendpoints for things like S3, because accessing S3 over-the-internet is\nsometimes not ideal\n\nHost OS agents\nAWS CloudWatch logs is extremely easy to setup/install and will eliminate the\nneed to ever SSH into an instance again. Once a log group or stream is created\nyou can easily set an expiry so the archived logs are reliably purged via a\n1-time cli command or in the console.\n\nThe SSM agent is another great tool to allow easy patching and provide reporting\non compliance with AWS Config and Inspector.\n\nAnd don't forget any 3rd party intrusion prevention or endpoint protection\nagents such as Trend Micro Deep Security.\n\nSingle-Tenant vs Multi-tenant\nMake sure you show tenancy aspects of the solution design.\nThis is key for any ISO (Information Security Officer) or auditor to know from\nyou eventually anyway, so be sure to properly understand your choices earlier in\nthe planning stage rather than have to resort to compensating controls much\nlater in the implementation - which usually come at the highest developer\ntime/cost when there is little or no time left on the project.\n\nSecrets management\nBe sure to identify and visually represent SSM Parameter Store for encrypted\nsecret strings such as tokens used by 3rd party integrations, and Secrets\nManager for managing database passwords seamlessly.\n\nKMS or CloudHSM and the requirements of Key management such as revokation and\nrotation that developers will be required to implement should be clear visually\nrepresented and such requirements annotated as these will likely require some\nadditional infrastructure (Lambda) to achieve.\n\nEvents and Integration Hooks\nJust like in the key management point, we have many used and unused CloudWatch\nEvents, S3 lifecycle, and service specific events (like Kinesis and AWS Glue).\n\nThese events are usually of very high business value, under utilised, and\nextremely easy for a developer to leverage in most cases. But if a requirement\nnever makes it's way to a developer because a business unit was not aware of the\ncapability - then as an architect you've pretty much failed to identify and\nproduce value from an essentially free resource that has potential for some\nground breaking user experiences and statistical or behavioural data.\n\nAlerts\nMonitoring is usually an afterthought.\n\nBut alerting capabilities are often never given a first thought.\n\nSNS Topics, Pager Duty, and Raygun.io are all great, but what I am talking about\nis CloudWatch Rules. Just like some of the events mentioned earlier, these can\nalert you for actions that CloudWatch Alarms cannot. One of my favorite uses for\na CloudWatch Rule is with Glue Data Catalog because the Apache Hive Metadata\nstore generated is useful for EMR, Athena, Redshift Spectrum, and Glue ETL - and\nwith a CloudWatch Rule I am notified when interesting things happen like a\nDatabase schema change or a failed crawl of S3 so I can automate a retry or\nchanges to services downstream in my Athena SQL or Models running in EMR.\n\nTips by service\nAWS Shield\n * Free service\n * Protects: ELB, CloudFront, Route 53 * SYN/UDP floods\n    * Reflection attacks\n    * Layer ¾ attacks\n   \n   \n\nPen Testing\n * Can only be carried out on certain services; * EC2, RDS, Aurora, CloudFront,\n      API Gateway, Lambda, Lightsail, DNS Zone Walking.\n    * Small or micro RDS instance\n      not permitted.\n    * m1.small, t1.micro or t2.nano\n      EC2 not permitted\n   \n   \n\nAWS Certificate Manager\n * Cannot export\n * Only for Route53 registered domains\n * Works with CloudFront and Load Balancers\n\nAPI Gateway\n * Always throttles requests (default max 10k per sec)\n * If burst-limit exceeds 429 Too Many Request is returned\n * Use TTL caching to mitigate, max 3600 sec\n\nSystems Manager\n * Parameter Store: EC2 (RunCommand), Lambda, CloudFormation, API\n * RunCommand works on instances or Tags, and execute as root\n\nS3\n * Replication uses SSL by default, versioning must be enabled at both ends\n * Object DELETE are replicated, versioned DELETE are not\n * Enforce SSL access using Condition aws:SecureTransport bucket policy\n * Pre-signed URL default 1hr, you can pre-sign PutObject data too\n\nCloudTrail\n * Delivered to S3 every 5mins, with 15min delay\n * Only logs API, does not record instance-data, ssh or rdp\n * Logs include api request metadata, identity, time, sourceIP, parameters, and\n   response elements\n * Use DIGEST for integrity validation of logs using SHA-256 hashing with RSA\n   for signing\n\nCloudWatch\n * Events are near real-time of: resource changes, CloudTrail, scheduled, or\n   custom from code\n * Rules match incoming Events, Rule Targets can be Lambda, SNS, SQS, Kinesis\n\nEC2\n * Dedicated instances are account locked and may share hardware with other\n   instances that are not dedicated if they are of the same account\n * AWS Staff have access to Hosts and Hypervisors, they cannot access guest\n   operating systems\n * RAM and storage are securely scrubbed before delivery to customers\n\nAWS Config\n * Resource inventory, configuration history, with change notification\n * Trigger periodic, or filtered snapshots\n\nAWS Inspector\n * Need EC2 agents installed on the assessment target\n * Create assessment templates to run and verify rules against findings\n * Detects common vulnerabilities, CIS benchmark, and runtime behaviour analysis\n   of network, file, and process as well as advises remediation\n\nTrusted Advisor\n * Cost optimisation, performance, availability, and limited security\n * Requires business/enterprise support for unlocked features\n\nCloudHSM\n * Only CloudHSM (full control of keys) meet level 3 FIPS 140-2, KMS does not.\n * CloudHSM offers asymmetric encryption, KMS only has symmetric available.\n * CloudHSM is single tenanted, KMS is multi-tenanted.\n * 4 main user type for keys; * PRECO – Precrypto officer * User and password management\n      \n      \n    * PCO or CO -- Crypto officer * All Key management, key access issuing,\n         material creation, signing, verifying, chaining\n      \n      \n    * CU – Crypto User * Basic key management (create, rotate), allowed key\n         exporting, signing, verifying\n      \n      \n    * AU – Appliance User * Can perform cloning and sync operations in a cluster\n      \n      \n   \n   \n\nKMS\n * KMS is region specific\n * KMS cannot be used for EC2 key/pairs, these are asymmetric, and allowing AWS\n   to generate these would allow AWS access into the EC2 instances\n * KMS integrates with EBS, S3, Redshift, Elastic Transcoder, WorkMail, RDS,\n   more * Keys include; alias, create date, desc, state, material\n    * Cannot be exported\n   \n   \n * AWS-managed CMK have no material\n * Customer-managed CMK need symmetric 256-bit key material provided * You can\n      also build extra resiliency by storing the key yourself outside of AWS\n    * Avoids\n      7-30day wait when deleting keys\n    * No\n      automatic key rotation\n   \n   \n\nAWS Shield\n * On by default, $3000 a month for advanced options * Incident response team\n    * in-depth reporting\n    * payment reductions when\n      victim of an attack\n   \n   \n\nAWS WAF\n * Regionally integrate with load balancers or associate with cloudfront\n   distribution for global\n * IPv6 is supported, and CIDR blocks /8, /16, /24, /32\n * Allow or Block all except specified\n * Count requests based on properties matched\n * Properties: IP, Country, header values, body length, SQL with known exploits,\n   scripts with known XSS\n\nAWS Marketplace\n * AWS Partners and Authorised appliances may be used to conduct testing\n * Firewalls, Hardened OS, WAF, Antivirus, Security monitoring, etc\n * CIS Benchmarked OS\n\nVPC\n * Ensure default route table has no public route out to the internet so new\n   subnets that are associated by default are secure by default (AWS default\n   route tables are not secure by default) * The same applies to the NACL\n      allowing all inbound and outbound traffic\n   \n   \n * NAT instances must be behind a security group and in a public subnet,\n   disabled source/destination checks, are not patched, is not scalable, is\n   single AZ, and single subnet. * Prefer NAT Gateways that are patched by AWS,\n      scalable to 10Gbps, no security groups associated, no need to disable\n      source/destination checks, apply to a route table, and get a public IP by\n      default\n   \n   \n * ALBs need at least 2 public subnets\n * Flow logs cannot be tagged, and the IAM role or configuration cannot be\n   edited\n * VPC Peered Flow logs only work when both VPCs are in the same account\n * Flow logs do not monitor DHCP, DNS, instance metadata address\n   169.254.169.254, Windows activation, or the AWS reserved IP addresses\n * VPC Endpoints are either; * Interface: single ENI\n    * Gateways: more durable (not single ENI)\n   \n   \n\nGeneral Security\n * CIA: confidentiality, integrity, availability\n * AAA: authentication, authorization, accounting\n * Non-repudiation = cannot deny a fact\n * Shared Responsibility model changed per Infrastructure, container, abstracted\n   levels; * Infrastructure (EC2, VPC, EBS)\n    * Container (RDS, EMR)\n    * Abstracted (S3, Dynamo, SQS)\n   \n   \n * ECDHE for Perfect Forward Secrecy; Elliptic Curve DHE (Diffie-Hellman\n   Ephemeral) key exchange.\n\nLinks\n * https://aws.amazon.com/architecture/well-architected/\n * https://aws.amazon.com/blogs/architecture/on-architecture-and-the-state-of-the-art/\n * https://aws.amazon.com/blogs/architecture/well-architected-lens-focus-on-specific-workload-types/","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/05/aws-this-is-my-architecture-header.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-05-13 15:54:03","created_by":"1","updated_at":"2021-03-31 14:02:09","updated_by":"1","published_at":"2018-05-12 03:17:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcba","uuid":"be2b163a-9b2c-4360-939f-3ccf2f71dba8","title":"PCI DSS - Are AWS KMS and CloudHSM suitable?","slug":"pci-dss-and-aws-kms-or-cloudhsm","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A look into the suitability of AWS KMS and CloudHSM for use with workloads in-scope of PCI DSS.\\n\\n## Who owns my encryption key in AWS?\\n\\nBy _owning_ we might think of ownership as who has potential to access the key to decrypt the data. I like to think of ownership being based on who creates the keys used for encryption.\\nOthers may also consider ownership being anyone with the ability to create and revoke keys on-demand, but who has the ability to decrypt the data, for me, is far more synonymous with ownership.\\n\\n> Owning can mean different things based on different audit requirements, or the frameworks and standards you implement\\n\\nIf we look at ownership in AWS, Amazon staff technically can access your systems. We have enough evidence to _trust_ that AWS have done all they can to ensure that staff will not access our operating systems, and data.\\n\\nEvidence provided in the form of third-party attestations in AWS Artefact.\\n\\n> Important: get familiar with [how AWS decided](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk) to use the term CMK in their documentation\\n\\nI will delineate between \\\"Customer-managed CMK\\\" and \\\"AWS-managed CMK\\\" in this article, which in my opinion is an oxymoron given \\\"AWS\\\" _managed_ \\\"Customer\\\" master keys are not actually keys distinct in any way related to a \\\"customer\\\". Only when a Customer provides a key material to a CMK is there any change to _ownership_ but both scenarios where you do and do not provide key material are still called a \\\"Customer-managed CMK\\\".\\n\\n> Are you confused? I was, and most people are\\n\\nDon't worry, most people aren't aware there is even an option for a customer to provide key material to a \\\"Customer-managed CMK\\\", let alone any of the differences between customer or AWS managed CMKs.\\n\\n## What do I need for PCI DSS?\\n\\nBefore we look at suitability, just what are the PCI DSS requirements that effect these service directly?\\n\\nIn the current PCI DSS version 3.2.1 there is one particular requirement group;\\n- *3.5* Protect cryptographic keys used for encryption of card holder data against both disclosure and misuse\\n\\nSpecifically in 3.5 there are these two important requirements;\\n- *3.5.1* restrict access to cryptographic keys to the fewest number of custodians necessary\\n- *3.5.2* store cryptographic keys securely in the fewest possible locations and forms\\n\\nThere are several other service that must meet these requirements also, such as CodeCommit or EC2, KMS, CloudHSM, and ACM.\\n\\nIt is important to understand this does not apply to public keys in a key/pair scenario like RSA used with SSH, as they are intended for public use encrypt only, not decrypt at all.\\n\\nFor other PCI requirements related to these service, see the [Automatic deficiencies](#automaticdeficiencies) section for a detailed analysis of PCI requirements related to other AWS services used when KMS and CloudHSM are in-scope of PCI.\\n\\n> The AWS Artefact collateral will inform you these requirements are the customer, or shared, responsibility\\n\\nIf we look deeper into PCI to find what type of cryptography should we use?  Requirement 3.5.3 says secret and private keys used to encrypt/decrypt card holder data in one or more of the following forms at all times;\\n\\n1. Encrypted with a key-encrypting key that is at least as strong as the data encrypting key, and that is stored separately from the data-encrypting key\\n2. Within a secure cryptographic device, such as a hardware (host) security module (HSM) or [PTS-approved](https://www.pcisecuritystandards.org/assessors_and_solutions/pin_transaction_devices) point-of-interaction device\\n3. As at least two full-length key components or key shares, in accordance with an industry accepted method\\n\\nFirstly, the statement \\\"stored separately from the data-encrypting key\\\" referring to a \\\"key-encrypting key\\\" (KEK) must work directly with requirement 3.6.6 also because;\\n - the data-encrypting key (DEK) provided by KMS includes clear-text\\n - the KEK simply put is a CMK, and for ownership we will be generating the CMK with key material. If material generation is not done via CloudHSM it would be done outside of AWS\\n\\nTherefore having KMS responsible for both DEK and KEK, both concerned with clear-text, does not satisfy this requirement.\\n\\nNext, looking at the PTS-approved point-of-interaction device list by company name, there are no AWS offerings at the time of writing, leaving the HSM as a prescriptive requirement.\\n\\nLastly, \\\"industry accepted method\\\" and \\\"full-length\\\" are not prescriptive, and not quantifiable, so refer to your QSA to clarify the requirement at the time of the audit.\\n\\n## CloudHSM suitability\\n\\nOnly CloudHSM provides full control of keys.\\n\\nNot PCI requirements, however still interesting;\\n- CloudHSM meets level 3 FIPS 140-2, ~~KMS does not~~ [KMS also meets this now](https://aws.amazon.com/blogs/security/aws-key-management-service-now-offers-fips-140-2-validated-cryptographic-modules-enabling-easier-adoption-of-the-service-for-regulated-workloads/).\\n- CloudHSM offers asymmetric encryption, KMS operates symmetric encryption\\n- CloudHSM is single tenanted, KMS is multi-tenanted\\n- CloudHSM is limited to a single VPC, therefore requires a VPC and applications must be able to route to the IP of all HSMs in your cluster.\\n- The control plane of CloudHSM (the service) is the AWS Query API (public internet), with no VPC Endpoint available, whereas KMS no offers a VPC Endpoint.\\n\\n## KMS suitability\\n\\nWhile KMS is entirely suitable overall, it requires far more effort to achieve a state of compliance.\\n\\nKMS requires you to, when providing key material to a \\\"Customer-managed CMK\\\", to apply additional workstation security for managing key material generation.\\n\\nAn employee physical and logical access comes into scope, and device hardware with wireless interfaces or radios described in requirement 9 can also be a challenge.\\n\\nThe largest concern for you to address is whether or not you will use a Customer-managed CMK with provided key material that is not generated by CloudHSM.\\n\\nGenerating key material outside of AWS will inherently broaden the scope of PCI to employee workstations.\\n\\nNote: at the time of writing there are no known Certificate Authority (CA) that will integrate with the KMS endpoints to supply key material directly. The majority of CAs do not have programmatic access to the key material either, needed so that you might automate the process.\\n\\n> Using any HSM (external to AWS) is an alternative.\\n\\nTo mitigate some concern when broadening the scope outside AWS, you might already apply appropriate governance and process to manage encryption keys, and use a HSM of your own. If not, It is recommended to use the CloudHSM.\\n\\nYou can also simply choose not to utilise Customer-managed CMK with provided key materials at all. It is an option, but that option would require you to complete an entry in the Compensating Controls Workbook (CCW) to meet requirement 3.5 beyond the prescriptive intent and to the satisfaction of the QSA's discretion.\\n\\nSome key considerations for KMS;\\n\\n- KMS cannot be used for EC2 key/pairs, these are asymmetric\\n- No automatic key rotation for a customer-managed CMK when providing key materials\\n- For CMKs where you do not provide key material, AWS generates key material for you, potentially allowing malicious insiders access to decrypt through vectors such as factorisation, derive the secrets with additional knowledge, or other implanted vulnerabilities. A risk you may chose is acceptable given the attestations.\\n- It is common to set KEY_MATERIAL_DOES_NOT_EXPIRE for CMKs\\n- [AWS documentation informs you to use SHA1](https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html) for the wrapping key, specifically RSAES OAEP with SHA1 - in 2005 a paper (ISBN 978-3-540-31870-5) demonstrated weaknesses in SHA1, followed by an RSA conference talk by Daniel R. L. Brown in 2007 that demonstrated secure RSAES OAEP implementations and SHA1 should not be used\\n\\nNot PCI requirements, however still interesting;\\n\\n-  Keys cannot be \\\"exported\\\", which is to say both the key-signing keys and the private key are secret.\\n- AWS-Managed CMK cannot be shared cross-account, meaning anything encrypted by one cannot be migrated as-is because the target account would not be able to decrypt. So using a Customer-managed CMK is best from a security and operation use perspective.\\n- Key material for Customer-managed CMKs require a symmetric 256-bit key material to be provided. Note: a 3072-bit RSA key is equivelent to a 128-bit symmetric key\\n\\n\\n## Great parts of KMS\\n\\n- Effortless to use CMKs when not providing key material\\n- KMS is region specific, whereas ClousHSM is limited to a VPC\\n- Keys can be aliased\\n- AWS-managed CMK require nothing more than a configuration value, great if you just want to encrypt things (outside of your usual obligations)\\n\\n## Automatic deficiencies\\n\\nThere are several deficiencies you should consider when using AWS, these are due to the inherent nature of the services AWS provide and their implementation detail that is by-design.\\n\\nSome PCI requirements automatically require an entry in the CCW, a detailed analysis of how we should reason with these concerns can be seen the table of specific PCI requirements below;\\n\\n| # | Requirement | Reason |\\n| --- |--- | --- |\\n| 1.3 | Prohibit direct public access between the Internet and any system component in the cardholder data environment | All AWS APIs are inherently public internet addressable, and services such as KMS and CLoudHSM offer no controls to avoid this. While a VPC Endpoint can be used for your implementation, the inherent public nature is unchanged and communications cannot be restricted to be only via the VPC Endpoint. |\\n| 2.3  | Encrypt all non-console administrative access using strong cryptography | While this point is contentious due to the AWS Query request APIs (the main HTTP API) itself provides you the ability to enforce the prescriptive controls, the AWS CLI built in to EC2 by default allows SSL/early TLS and can not currently support Perfect Forward Secrecy (PFS) at all, due to it's reliance on boto3/botocore python libraries  ([I'm trying to fix this](https://github.com/boto/botocore/pull/1823)). Additionally, most SDKs do not allow configurations that can be configured to restrict AWS Query API requests _directly_, therefore these SDKs defaults, while varied, are also mostly allowing SSL/early TLS. Use of SDKs may require you to address the requirements in Appendix A2 |\\n| 3.5 | Document and implement procedures to protect keys used to secure stored cardholder data against disclosure and misuse | This applies in 2 scenarios; 1) KEK conditionally to the use of customer provided key material for Customer-managed CMKs when not integrated with CloudHSM. 2) DEK providign clear-text provided by the AWS APIs traverse the public internet and require a VPC Endpoint. An entry to the CCW is needed for this which is a trivial exercise if all requesters are AWS services, non-trivial if any requester is a bespoke or external to AWS implementation |\\n| 3.6.4 | Cryptographic key changes for keys that have reached the end of their crypto period (for example, after a defined period of time has passed and/or after a certain amount of cipher-text has been produced by a given key), as defined by the associated application vendor or key owner, and based on industry best practices and guidelines (for example, NIST Special Publication 800-57) | In KMS there is an option, not the default, to never expire a CMK. For this reason AWS do not meet the requirement and it is the customer responsibility to ensure this requirement is met |\\n| 3.6.5 | Retirement or replacement (for example, archiving, destruction, and/or revocation) of keys as deemed necessary when the integrity of the key has been weakened (for example, departure of an employee with knowledge of a clear-text key component), or keys are suspected of being compromised | KMS automatic key rotation has no knowledge of your staff off-boarding process, therefore automatic key rotation is not sufficient alone, all data encrypted using keys the ex-employee has access to must be initiated for key rotation, including backups |\\n| 3.6.6 | If manual clear-text cryptographic key-management operations are used, these operations must be managed using split knowledge and dual control. Note: Examples of manual key-management operations include, but are not limited to: key generation, transmission, loading, storage and destruction. | Applies when choosing to provide key material to Customer-managed CMKs in KMS (without integration to CloudHSM) due to the necessity of having an external to KMS process to provide the key material which is inherently clear-text file key generation and transmission operation |\\n| 4.1 | Use strong cryptography and security protocols to safeguard sensitive cardholder data during transmission over open, public networks | as 2.3 above |\\n| 8.1.6 | Limit repeated access attempts by locking out the user ID after not more than six attempts | Not an available feature of the KMS API |\\n| 8.1.8 | If a session has been idle for more than 15 minutes, require the user to re-authenticate to re-activate the terminal or session | AWS provides configurations of the total session duration only, there is no idle settings available |\\n| 8.2 | In addition to assigning a unique ID, ensure proper user-authentication management for non-consumer users and administrators on all system components by employing at least one of the following methods to authenticate all users | While MFA is available to IAM Users, it is not the default, and IAM Users are not the best practice access pattern |\\n| 8.3 | Secure all individual non-console administrative access and all remote access to the CDE using multi-factor authentication | as 8.2 above |\\n| 8.5 | Do not use group, shared, or generic IDs, passwords, or other authentication methods | the default and general best practice access pattern is to use federated or assumed role based access which is violation of this prescriptive requirement |\\n| 10.2 | Implement automated audit trails for all system components to reconstruct the specific events | Both KMS and CloudHSM rely on CloudTrail to meet this requirement which is not available by default |\\n| 10.3 | Record at least the following audit trail entries for all system components for each event | as 10.2 above |\\n| 10.5 | Secure audit trails so they cannot be altered | While CloudTrail is not used by default, when it is in use the digest feature is required to meet this requirement and it is also not a default. Additionally S3 log delivery must be configured with controls in place to prevent tampering of the S3 Objects that represent both the log data and digests |\\n| 10.7 | Retain audit trail history for at least one year, with a minimum of three months immediately available for analysis (for example, online, archived, or restorable from backup) | While not used by default, S3 life-cycle events and glacier storage classes are available to meet these requirements |\\n| 11.5 | Deploy a change-detection mechanism (for example, file-integrity monitoring tools) to alert personnel to unauthorized modification (including changes, additions and deletions) of critical system files, configuration files, or content files; and configure the software to perform critical file comparisons at least weekly | as 10.7 above while also considering CloudWatch usage for monitoring activities |\\n\\nThe above considerations are particularly critical when you are operating a Level 1 CDE.\\nYou may chose to focus more on PCI requirement 11 if you operate a Level 2-5 CDE, leaving the SAQ and CCW efforts relatively relaxed. This would of course also be depending on your overall risk appetite, or other obligations outside PCI.\\n\\n## Conclusion\\n\\nBasically, PCI DSS is a prescriptive standard but has two key facts which seem to be unknown to many organisations;\\n\\n1. The QSA has full discretionary decision making power\\n2. There is the Compensating Controls Worksheet (CCW) that is available to you if you do not implement any of the prescriptive requirements.\\n\\n> Yes, you read these correctly.\\n\\nYou can actually achieve PCI DSS without meeting a single _prescriptive_ requirement as set out in the PCI guidance material, if the QSA agrees you provided evidence of compensating controls in the CCW that meets or exceeds the intent of the requirements themselves.\\n\\nSo the verdict is; to have a quick audit without many questions from the QSA you want to avoid the CCW.\\nIn context to this article, using the CloudHSM provides the highest degree of control and assurance to the QSA while also meeting the PCI prescriptive requirements for its feature set.\\n\\n# Final thoughts\\n\\nPCI is not all about requirement 3, Encryption.\\nBut while we are on the topic; ensure you apply appropriate key management as briefly mentioned above.\\n\\nThere are three important factors to consider here;\\n1. All keys are single workload use, so revoking a key won't take down more than one workload\\n2. Have separate access patterns for key encryption, and key decryption. So that the key creators cannot also the read data. this is good for operations also due to a separation of concerns\\n3. Be careful of scope increase due to access controls. This is commonly where access to the CDE is federated, so there might be too many users with permissions to access the CDE due to groups and time. Another is direct programmatic access being permitted to the CDE from devices which are not locked down to access the CDE alone. That is to say an employee workstation on wifi accessing the CDE is going to effect scope if they have the ability to read-only any cardholder data (which is transfered to the workstation to be viewed).\\n\\nIf you are not applying this level of completeness, prepare to spend a far longer time with your QSA addressing every detail of every one of the PCI requirements, or the entries in the CCW. They will scrutinise over all details for linked collateral entered into the CCW also, because it is their personal reputation on the line if they use their discretionary decision making power poorly.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>A look into the suitability of AWS KMS and CloudHSM for use with workloads in-scope of PCI DSS.</p>\n<h2 id=\"whoownsmyencryptionkeyinaws\">Who owns my encryption key in AWS?</h2>\n<p>By <em>owning</em> we might think of ownership as who has potential to access the key to decrypt the data. I like to think of ownership being based on who creates the keys used for encryption.<br>\nOthers may also consider ownership being anyone with the ability to create and revoke keys on-demand, but who has the ability to decrypt the data, for me, is far more synonymous with ownership.</p>\n<blockquote>\n<p>Owning can mean different things based on different audit requirements, or the frameworks and standards you implement</p>\n</blockquote>\n<p>If we look at ownership in AWS, Amazon staff technically can access your systems. We have enough evidence to <em>trust</em> that AWS have done all they can to ensure that staff will not access our operating systems, and data.</p>\n<p>Evidence provided in the form of third-party attestations in AWS Artefact.</p>\n<blockquote>\n<p>Important: get familiar with <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk\">how AWS decided</a> to use the term CMK in their documentation</p>\n</blockquote>\n<p>I will delineate between &quot;Customer-managed CMK&quot; and &quot;AWS-managed CMK&quot; in this article, which in my opinion is an oxymoron given &quot;AWS&quot; <em>managed</em> &quot;Customer&quot; master keys are not actually keys distinct in any way related to a &quot;customer&quot;. Only when a Customer provides a key material to a CMK is there any change to <em>ownership</em> but both scenarios where you do and do not provide key material are still called a &quot;Customer-managed CMK&quot;.</p>\n<blockquote>\n<p>Are you confused? I was, and most people are</p>\n</blockquote>\n<p>Don't worry, most people aren't aware there is even an option for a customer to provide key material to a &quot;Customer-managed CMK&quot;, let alone any of the differences between customer or AWS managed CMKs.</p>\n<h2 id=\"whatdoineedforpcidss\">What do I need for PCI DSS?</h2>\n<p>Before we look at suitability, just what are the PCI DSS requirements that effect these service directly?</p>\n<p>In the current PCI DSS version 3.2.1 there is one particular requirement group;</p>\n<ul>\n<li><em>3.5</em> Protect cryptographic keys used for encryption of card holder data against both disclosure and misuse</li>\n</ul>\n<p>Specifically in 3.5 there are these two important requirements;</p>\n<ul>\n<li><em>3.5.1</em> restrict access to cryptographic keys to the fewest number of custodians necessary</li>\n<li><em>3.5.2</em> store cryptographic keys securely in the fewest possible locations and forms</li>\n</ul>\n<p>There are several other service that must meet these requirements also, such as CodeCommit or EC2, KMS, CloudHSM, and ACM.</p>\n<p>It is important to understand this does not apply to public keys in a key/pair scenario like RSA used with SSH, as they are intended for public use encrypt only, not decrypt at all.</p>\n<p>For other PCI requirements related to these service, see the <a href=\"#automaticdeficiencies\">Automatic deficiencies</a> section for a detailed analysis of PCI requirements related to other AWS services used when KMS and CloudHSM are in-scope of PCI.</p>\n<blockquote>\n<p>The AWS Artefact collateral will inform you these requirements are the customer, or shared, responsibility</p>\n</blockquote>\n<p>If we look deeper into PCI to find what type of cryptography should we use?  Requirement 3.5.3 says secret and private keys used to encrypt/decrypt card holder data in one or more of the following forms at all times;</p>\n<ol>\n<li>Encrypted with a key-encrypting key that is at least as strong as the data encrypting key, and that is stored separately from the data-encrypting key</li>\n<li>Within a secure cryptographic device, such as a hardware (host) security module (HSM) or <a href=\"https://www.pcisecuritystandards.org/assessors_and_solutions/pin_transaction_devices\">PTS-approved</a> point-of-interaction device</li>\n<li>As at least two full-length key components or key shares, in accordance with an industry accepted method</li>\n</ol>\n<p>Firstly, the statement &quot;stored separately from the data-encrypting key&quot; referring to a &quot;key-encrypting key&quot; (KEK) must work directly with requirement 3.6.6 also because;</p>\n<ul>\n<li>the data-encrypting key (DEK) provided by KMS includes clear-text</li>\n<li>the KEK simply put is a CMK, and for ownership we will be generating the CMK with key material. If material generation is not done via CloudHSM it would be done outside of AWS</li>\n</ul>\n<p>Therefore having KMS responsible for both DEK and KEK, both concerned with clear-text, does not satisfy this requirement.</p>\n<p>Next, looking at the PTS-approved point-of-interaction device list by company name, there are no AWS offerings at the time of writing, leaving the HSM as a prescriptive requirement.</p>\n<p>Lastly, &quot;industry accepted method&quot; and &quot;full-length&quot; are not prescriptive, and not quantifiable, so refer to your QSA to clarify the requirement at the time of the audit.</p>\n<h2 id=\"cloudhsmsuitability\">CloudHSM suitability</h2>\n<p>Only CloudHSM provides full control of keys.</p>\n<p>Not PCI requirements, however still interesting;</p>\n<ul>\n<li>CloudHSM meets level 3 FIPS 140-2, <s>KMS does not</s> <a href=\"https://aws.amazon.com/blogs/security/aws-key-management-service-now-offers-fips-140-2-validated-cryptographic-modules-enabling-easier-adoption-of-the-service-for-regulated-workloads/\">KMS also meets this now</a>.</li>\n<li>CloudHSM offers asymmetric encryption, KMS operates symmetric encryption</li>\n<li>CloudHSM is single tenanted, KMS is multi-tenanted</li>\n<li>CloudHSM is limited to a single VPC, therefore requires a VPC and applications must be able to route to the IP of all HSMs in your cluster.</li>\n<li>The control plane of CloudHSM (the service) is the AWS Query API (public internet), with no VPC Endpoint available, whereas KMS no offers a VPC Endpoint.</li>\n</ul>\n<h2 id=\"kmssuitability\">KMS suitability</h2>\n<p>While KMS is entirely suitable overall, it requires far more effort to achieve a state of compliance.</p>\n<p>KMS requires you to, when providing key material to a &quot;Customer-managed CMK&quot;, to apply additional workstation security for managing key material generation.</p>\n<p>An employee physical and logical access comes into scope, and device hardware with wireless interfaces or radios described in requirement 9 can also be a challenge.</p>\n<p>The largest concern for you to address is whether or not you will use a Customer-managed CMK with provided key material that is not generated by CloudHSM.</p>\n<p>Generating key material outside of AWS will inherently broaden the scope of PCI to employee workstations.</p>\n<p>Note: at the time of writing there are no known Certificate Authority (CA) that will integrate with the KMS endpoints to supply key material directly. The majority of CAs do not have programmatic access to the key material either, needed so that you might automate the process.</p>\n<blockquote>\n<p>Using any HSM (external to AWS) is an alternative.</p>\n</blockquote>\n<p>To mitigate some concern when broadening the scope outside AWS, you might already apply appropriate governance and process to manage encryption keys, and use a HSM of your own. If not, It is recommended to use the CloudHSM.</p>\n<p>You can also simply choose not to utilise Customer-managed CMK with provided key materials at all. It is an option, but that option would require you to complete an entry in the Compensating Controls Workbook (CCW) to meet requirement 3.5 beyond the prescriptive intent and to the satisfaction of the QSA's discretion.</p>\n<p>Some key considerations for KMS;</p>\n<ul>\n<li>KMS cannot be used for EC2 key/pairs, these are asymmetric</li>\n<li>No automatic key rotation for a customer-managed CMK when providing key materials</li>\n<li>For CMKs where you do not provide key material, AWS generates key material for you, potentially allowing malicious insiders access to decrypt through vectors such as factorisation, derive the secrets with additional knowledge, or other implanted vulnerabilities. A risk you may chose is acceptable given the attestations.</li>\n<li>It is common to set KEY_MATERIAL_DOES_NOT_EXPIRE for CMKs</li>\n<li><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html\">AWS documentation informs you to use SHA1</a> for the wrapping key, specifically RSAES OAEP with SHA1 - in 2005 a paper (ISBN 978-3-540-31870-5) demonstrated weaknesses in SHA1, followed by an RSA conference talk by Daniel R. L. Brown in 2007 that demonstrated secure RSAES OAEP implementations and SHA1 should not be used</li>\n</ul>\n<p>Not PCI requirements, however still interesting;</p>\n<ul>\n<li>Keys cannot be &quot;exported&quot;, which is to say both the key-signing keys and the private key are secret.</li>\n<li>AWS-Managed CMK cannot be shared cross-account, meaning anything encrypted by one cannot be migrated as-is because the target account would not be able to decrypt. So using a Customer-managed CMK is best from a security and operation use perspective.</li>\n<li>Key material for Customer-managed CMKs require a symmetric 256-bit key material to be provided. Note: a 3072-bit RSA key is equivelent to a 128-bit symmetric key</li>\n</ul>\n<h2 id=\"greatpartsofkms\">Great parts of KMS</h2>\n<ul>\n<li>Effortless to use CMKs when not providing key material</li>\n<li>KMS is region specific, whereas ClousHSM is limited to a VPC</li>\n<li>Keys can be aliased</li>\n<li>AWS-managed CMK require nothing more than a configuration value, great if you just want to encrypt things (outside of your usual obligations)</li>\n</ul>\n<h2 id=\"automaticdeficiencies\">Automatic deficiencies</h2>\n<p>There are several deficiencies you should consider when using AWS, these are due to the inherent nature of the services AWS provide and their implementation detail that is by-design.</p>\n<p>Some PCI requirements automatically require an entry in the CCW, a detailed analysis of how we should reason with these concerns can be seen the table of specific PCI requirements below;</p>\n<table>\n<thead>\n<tr>\n<th>#</th>\n<th>Requirement</th>\n<th>Reason</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1.3</td>\n<td>Prohibit direct public access between the Internet and any system component in the cardholder data environment</td>\n<td>All AWS APIs are inherently public internet addressable, and services such as KMS and CLoudHSM offer no controls to avoid this. While a VPC Endpoint can be used for your implementation, the inherent public nature is unchanged and communications cannot be restricted to be only via the VPC Endpoint.</td>\n</tr>\n<tr>\n<td>2.3</td>\n<td>Encrypt all non-console administrative access using strong cryptography</td>\n<td>While this point is contentious due to the AWS Query request APIs (the main HTTP API) itself provides you the ability to enforce the prescriptive controls, the AWS CLI built in to EC2 by default allows SSL/early TLS and can not currently support Perfect Forward Secrecy (PFS) at all, due to it's reliance on boto3/botocore python libraries  (<a href=\"https://github.com/boto/botocore/pull/1823\">I'm trying to fix this</a>). Additionally, most SDKs do not allow configurations that can be configured to restrict AWS Query API requests <em>directly</em>, therefore these SDKs defaults, while varied, are also mostly allowing SSL/early TLS. Use of SDKs may require you to address the requirements in Appendix A2</td>\n</tr>\n<tr>\n<td>3.5</td>\n<td>Document and implement procedures to protect keys used to secure stored cardholder data against disclosure and misuse</td>\n<td>This applies in 2 scenarios; 1) KEK conditionally to the use of customer provided key material for Customer-managed CMKs when not integrated with CloudHSM. 2) DEK providign clear-text provided by the AWS APIs traverse the public internet and require a VPC Endpoint. An entry to the CCW is needed for this which is a trivial exercise if all requesters are AWS services, non-trivial if any requester is a bespoke or external to AWS implementation</td>\n</tr>\n<tr>\n<td>3.6.4</td>\n<td>Cryptographic key changes for keys that have reached the end of their crypto period (for example, after a defined period of time has passed and/or after a certain amount of cipher-text has been produced by a given key), as defined by the associated application vendor or key owner, and based on industry best practices and guidelines (for example, NIST Special Publication 800-57)</td>\n<td>In KMS there is an option, not the default, to never expire a CMK. For this reason AWS do not meet the requirement and it is the customer responsibility to ensure this requirement is met</td>\n</tr>\n<tr>\n<td>3.6.5</td>\n<td>Retirement or replacement (for example, archiving, destruction, and/or revocation) of keys as deemed necessary when the integrity of the key has been weakened (for example, departure of an employee with knowledge of a clear-text key component), or keys are suspected of being compromised</td>\n<td>KMS automatic key rotation has no knowledge of your staff off-boarding process, therefore automatic key rotation is not sufficient alone, all data encrypted using keys the ex-employee has access to must be initiated for key rotation, including backups</td>\n</tr>\n<tr>\n<td>3.6.6</td>\n<td>If manual clear-text cryptographic key-management operations are used, these operations must be managed using split knowledge and dual control. Note: Examples of manual key-management operations include, but are not limited to: key generation, transmission, loading, storage and destruction.</td>\n<td>Applies when choosing to provide key material to Customer-managed CMKs in KMS (without integration to CloudHSM) due to the necessity of having an external to KMS process to provide the key material which is inherently clear-text file key generation and transmission operation</td>\n</tr>\n<tr>\n<td>4.1</td>\n<td>Use strong cryptography and security protocols to safeguard sensitive cardholder data during transmission over open, public networks</td>\n<td>as 2.3 above</td>\n</tr>\n<tr>\n<td>8.1.6</td>\n<td>Limit repeated access attempts by locking out the user ID after not more than six attempts</td>\n<td>Not an available feature of the KMS API</td>\n</tr>\n<tr>\n<td>8.1.8</td>\n<td>If a session has been idle for more than 15 minutes, require the user to re-authenticate to re-activate the terminal or session</td>\n<td>AWS provides configurations of the total session duration only, there is no idle settings available</td>\n</tr>\n<tr>\n<td>8.2</td>\n<td>In addition to assigning a unique ID, ensure proper user-authentication management for non-consumer users and administrators on all system components by employing at least one of the following methods to authenticate all users</td>\n<td>While MFA is available to IAM Users, it is not the default, and IAM Users are not the best practice access pattern</td>\n</tr>\n<tr>\n<td>8.3</td>\n<td>Secure all individual non-console administrative access and all remote access to the CDE using multi-factor authentication</td>\n<td>as 8.2 above</td>\n</tr>\n<tr>\n<td>8.5</td>\n<td>Do not use group, shared, or generic IDs, passwords, or other authentication methods</td>\n<td>the default and general best practice access pattern is to use federated or assumed role based access which is violation of this prescriptive requirement</td>\n</tr>\n<tr>\n<td>10.2</td>\n<td>Implement automated audit trails for all system components to reconstruct the specific events</td>\n<td>Both KMS and CloudHSM rely on CloudTrail to meet this requirement which is not available by default</td>\n</tr>\n<tr>\n<td>10.3</td>\n<td>Record at least the following audit trail entries for all system components for each event</td>\n<td>as 10.2 above</td>\n</tr>\n<tr>\n<td>10.5</td>\n<td>Secure audit trails so they cannot be altered</td>\n<td>While CloudTrail is not used by default, when it is in use the digest feature is required to meet this requirement and it is also not a default. Additionally S3 log delivery must be configured with controls in place to prevent tampering of the S3 Objects that represent both the log data and digests</td>\n</tr>\n<tr>\n<td>10.7</td>\n<td>Retain audit trail history for at least one year, with a minimum of three months immediately available for analysis (for example, online, archived, or restorable from backup)</td>\n<td>While not used by default, S3 life-cycle events and glacier storage classes are available to meet these requirements</td>\n</tr>\n<tr>\n<td>11.5</td>\n<td>Deploy a change-detection mechanism (for example, file-integrity monitoring tools) to alert personnel to unauthorized modification (including changes, additions and deletions) of critical system files, configuration files, or content files; and configure the software to perform critical file comparisons at least weekly</td>\n<td>as 10.7 above while also considering CloudWatch usage for monitoring activities</td>\n</tr>\n</tbody>\n</table>\n<p>The above considerations are particularly critical when you are operating a Level 1 CDE.<br>\nYou may chose to focus more on PCI requirement 11 if you operate a Level 2-5 CDE, leaving the SAQ and CCW efforts relatively relaxed. This would of course also be depending on your overall risk appetite, or other obligations outside PCI.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Basically, PCI DSS is a prescriptive standard but has two key facts which seem to be unknown to many organisations;</p>\n<ol>\n<li>The QSA has full discretionary decision making power</li>\n<li>There is the Compensating Controls Worksheet (CCW) that is available to you if you do not implement any of the prescriptive requirements.</li>\n</ol>\n<blockquote>\n<p>Yes, you read these correctly.</p>\n</blockquote>\n<p>You can actually achieve PCI DSS without meeting a single <em>prescriptive</em> requirement as set out in the PCI guidance material, if the QSA agrees you provided evidence of compensating controls in the CCW that meets or exceeds the intent of the requirements themselves.</p>\n<p>So the verdict is; to have a quick audit without many questions from the QSA you want to avoid the CCW.<br>\nIn context to this article, using the CloudHSM provides the highest degree of control and assurance to the QSA while also meeting the PCI prescriptive requirements for its feature set.</p>\n<h1 id=\"finalthoughts\">Final thoughts</h1>\n<p>PCI is not all about requirement 3, Encryption.<br>\nBut while we are on the topic; ensure you apply appropriate key management as briefly mentioned above.</p>\n<p>There are three important factors to consider here;</p>\n<ol>\n<li>All keys are single workload use, so revoking a key won't take down more than one workload</li>\n<li>Have separate access patterns for key encryption, and key decryption. So that the key creators cannot also the read data. this is good for operations also due to a separation of concerns</li>\n<li>Be careful of scope increase due to access controls. This is commonly where access to the CDE is federated, so there might be too many users with permissions to access the CDE due to groups and time. Another is direct programmatic access being permitted to the CDE from devices which are not locked down to access the CDE alone. That is to say an employee workstation on wifi accessing the CDE is going to effect scope if they have the ability to read-only any cardholder data (which is transfered to the workstation to be viewed).</li>\n</ol>\n<p>If you are not applying this level of completeness, prepare to spend a far longer time with your QSA addressing every detail of every one of the PCI requirements, or the entries in the CCW. They will scrutinise over all details for linked collateral entered into the CCW also, because it is their personal reputation on the line if they use their discretionary decision making power poorly.</p>\n<!--kg-card-end: markdown-->","comment_id":"5af8657aea08b405ba5b0648","plaintext":"A look into the suitability of AWS KMS and CloudHSM for use with workloads\nin-scope of PCI DSS.\n\nWho owns my encryption key in AWS?\nBy owning we might think of ownership as who has potential to access the key to\ndecrypt the data. I like to think of ownership being based on who creates the\nkeys used for encryption.\nOthers may also consider ownership being anyone with the ability to create and\nrevoke keys on-demand, but who has the ability to decrypt the data, for me, is\nfar more synonymous with ownership.\n\n> Owning can mean different things based on different audit requirements, or the\nframeworks and standards you implement\n\n\nIf we look at ownership in AWS, Amazon staff technically can access your\nsystems. We have enough evidence to trust that AWS have done all they can to\nensure that staff will not access our operating systems, and data.\n\nEvidence provided in the form of third-party attestations in AWS Artefact.\n\n> Important: get familiar with how AWS decided\n[https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk] \nto use the term CMK in their documentation\n\n\nI will delineate between \"Customer-managed CMK\" and \"AWS-managed CMK\" in this\narticle, which in my opinion is an oxymoron given \"AWS\" managed \"Customer\"\nmaster keys are not actually keys distinct in any way related to a \"customer\".\nOnly when a Customer provides a key material to a CMK is there any change to \nownership but both scenarios where you do and do not provide key material are\nstill called a \"Customer-managed CMK\".\n\n> Are you confused? I was, and most people are\n\n\nDon't worry, most people aren't aware there is even an option for a customer to\nprovide key material to a \"Customer-managed CMK\", let alone any of the\ndifferences between customer or AWS managed CMKs.\n\nWhat do I need for PCI DSS?\nBefore we look at suitability, just what are the PCI DSS requirements that\neffect these service directly?\n\nIn the current PCI DSS version 3.2.1 there is one particular requirement group;\n\n * 3.5 Protect cryptographic keys used for encryption of card holder data\n   against both disclosure and misuse\n\nSpecifically in 3.5 there are these two important requirements;\n\n * 3.5.1 restrict access to cryptographic keys to the fewest number of\n   custodians necessary\n * 3.5.2 store cryptographic keys securely in the fewest possible locations and\n   forms\n\nThere are several other service that must meet these requirements also, such as\nCodeCommit or EC2, KMS, CloudHSM, and ACM.\n\nIt is important to understand this does not apply to public keys in a key/pair\nscenario like RSA used with SSH, as they are intended for public use encrypt\nonly, not decrypt at all.\n\nFor other PCI requirements related to these service, see the Automatic\ndeficiencies section for a detailed analysis of PCI requirements related to\nother AWS services used when KMS and CloudHSM are in-scope of PCI.\n\n> The AWS Artefact collateral will inform you these requirements are the customer,\nor shared, responsibility\n\n\nIf we look deeper into PCI to find what type of cryptography should we use?\nRequirement 3.5.3 says secret and private keys used to encrypt/decrypt card\nholder data in one or more of the following forms at all times;\n\n 1. Encrypted with a key-encrypting key that is at least as strong as the data\n    encrypting key, and that is stored separately from the data-encrypting key\n 2. Within a secure cryptographic device, such as a hardware (host) security\n    module (HSM) or PTS-approved\n    [https://www.pcisecuritystandards.org/assessors_and_solutions/pin_transaction_devices] \n    point-of-interaction device\n 3. As at least two full-length key components or key shares, in accordance with\n    an industry accepted method\n\nFirstly, the statement \"stored separately from the data-encrypting key\"\nreferring to a \"key-encrypting key\" (KEK) must work directly with requirement\n3.6.6 also because;\n\n * the data-encrypting key (DEK) provided by KMS includes clear-text\n * the KEK simply put is a CMK, and for ownership we will be generating the CMK\n   with key material. If material generation is not done via CloudHSM it would\n   be done outside of AWS\n\nTherefore having KMS responsible for both DEK and KEK, both concerned with\nclear-text, does not satisfy this requirement.\n\nNext, looking at the PTS-approved point-of-interaction device list by company\nname, there are no AWS offerings at the time of writing, leaving the HSM as a\nprescriptive requirement.\n\nLastly, \"industry accepted method\" and \"full-length\" are not prescriptive, and\nnot quantifiable, so refer to your QSA to clarify the requirement at the time of\nthe audit.\n\nCloudHSM suitability\nOnly CloudHSM provides full control of keys.\n\nNot PCI requirements, however still interesting;\n\n * CloudHSM meets level 3 FIPS 140-2, KMS does not KMS also meets this now\n   [https://aws.amazon.com/blogs/security/aws-key-management-service-now-offers-fips-140-2-validated-cryptographic-modules-enabling-easier-adoption-of-the-service-for-regulated-workloads/]\n   .\n * CloudHSM offers asymmetric encryption, KMS operates symmetric encryption\n * CloudHSM is single tenanted, KMS is multi-tenanted\n * CloudHSM is limited to a single VPC, therefore requires a VPC and\n   applications must be able to route to the IP of all HSMs in your cluster.\n * The control plane of CloudHSM (the service) is the AWS Query API (public\n   internet), with no VPC Endpoint available, whereas KMS no offers a VPC\n   Endpoint.\n\nKMS suitability\nWhile KMS is entirely suitable overall, it requires far more effort to achieve a\nstate of compliance.\n\nKMS requires you to, when providing key material to a \"Customer-managed CMK\", to\napply additional workstation security for managing key material generation.\n\nAn employee physical and logical access comes into scope, and device hardware\nwith wireless interfaces or radios described in requirement 9 can also be a\nchallenge.\n\nThe largest concern for you to address is whether or not you will use a\nCustomer-managed CMK with provided key material that is not generated by\nCloudHSM.\n\nGenerating key material outside of AWS will inherently broaden the scope of PCI\nto employee workstations.\n\nNote: at the time of writing there are no known Certificate Authority (CA) that\nwill integrate with the KMS endpoints to supply key material directly. The\nmajority of CAs do not have programmatic access to the key material either,\nneeded so that you might automate the process.\n\n> Using any HSM (external to AWS) is an alternative.\n\n\nTo mitigate some concern when broadening the scope outside AWS, you might\nalready apply appropriate governance and process to manage encryption keys, and\nuse a HSM of your own. If not, It is recommended to use the CloudHSM.\n\nYou can also simply choose not to utilise Customer-managed CMK with provided key\nmaterials at all. It is an option, but that option would require you to complete\nan entry in the Compensating Controls Workbook (CCW) to meet requirement 3.5\nbeyond the prescriptive intent and to the satisfaction of the QSA's discretion.\n\nSome key considerations for KMS;\n\n * KMS cannot be used for EC2 key/pairs, these are asymmetric\n * No automatic key rotation for a customer-managed CMK when providing key\n   materials\n * For CMKs where you do not provide key material, AWS generates key material\n   for you, potentially allowing malicious insiders access to decrypt through\n   vectors such as factorisation, derive the secrets with additional knowledge,\n   or other implanted vulnerabilities. A risk you may chose is acceptable given\n   the attestations.\n * It is common to set KEY_MATERIAL_DOES_NOT_EXPIRE for CMKs\n * AWS documentation informs you to use SHA1\n   [https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html] \n   for the wrapping key, specifically RSAES OAEP with SHA1 - in 2005 a paper\n   (ISBN 978-3-540-31870-5) demonstrated weaknesses in SHA1, followed by an RSA\n   conference talk by Daniel R. L. Brown in 2007 that demonstrated secure RSAES\n   OAEP implementations and SHA1 should not be used\n\nNot PCI requirements, however still interesting;\n\n * Keys cannot be \"exported\", which is to say both the key-signing keys and the\n   private key are secret.\n * AWS-Managed CMK cannot be shared cross-account, meaning anything encrypted by\n   one cannot be migrated as-is because the target account would not be able to\n   decrypt. So using a Customer-managed CMK is best from a security and\n   operation use perspective.\n * Key material for Customer-managed CMKs require a symmetric 256-bit key\n   material to be provided. Note: a 3072-bit RSA key is equivelent to a 128-bit\n   symmetric key\n\nGreat parts of KMS\n * Effortless to use CMKs when not providing key material\n * KMS is region specific, whereas ClousHSM is limited to a VPC\n * Keys can be aliased\n * AWS-managed CMK require nothing more than a configuration value, great if you\n   just want to encrypt things (outside of your usual obligations)\n\nAutomatic deficiencies\nThere are several deficiencies you should consider when using AWS, these are due\nto the inherent nature of the services AWS provide and their implementation\ndetail that is by-design.\n\nSome PCI requirements automatically require an entry in the CCW, a detailed\nanalysis of how we should reason with these concerns can be seen the table of\nspecific PCI requirements below;\n\n#RequirementReason1.3Prohibit direct public access between the Internet and any\nsystem component in the cardholder data environmentAll AWS APIs are inherently\npublic internet addressable, and services such as KMS and CLoudHSM offer no\ncontrols to avoid this. While a VPC Endpoint can be used for your\nimplementation, the inherent public nature is unchanged and communications\ncannot be restricted to be only via the VPC Endpoint.2.3Encrypt all non-console\nadministrative access using strong cryptographyWhile this point is contentious\ndue to the AWS Query request APIs (the main HTTP API) itself provides you the\nability to enforce the prescriptive controls, the AWS CLI built in to EC2 by\ndefault allows SSL/early TLS and can not currently support Perfect Forward\nSecrecy (PFS) at all, due to it's reliance on boto3/botocore python libraries (\nI'm trying to fix this [https://github.com/boto/botocore/pull/1823]).\nAdditionally, most SDKs do not allow configurations that can be configured to\nrestrict AWS Query API requests directly, therefore these SDKs defaults, while\nvaried, are also mostly allowing SSL/early TLS. Use of SDKs may require you to\naddress the requirements in Appendix A23.5Document and implement procedures to\nprotect keys used to secure stored cardholder data against disclosure and misuse\nThis applies in 2 scenarios; 1) KEK conditionally to the use of customer\nprovided key material for Customer-managed CMKs when not integrated with\nCloudHSM. 2) DEK providign clear-text provided by the AWS APIs traverse the\npublic internet and require a VPC Endpoint. An entry to the CCW is needed for\nthis which is a trivial exercise if all requesters are AWS services, non-trivial\nif any requester is a bespoke or external to AWS implementation3.6.4\nCryptographic key changes for keys that have reached the end of their crypto\nperiod (for example, after a defined period of time has passed and/or after a\ncertain amount of cipher-text has been produced by a given key), as defined by\nthe associated application vendor or key owner, and based on industry best\npractices and guidelines (for example, NIST Special Publication 800-57)In KMS\nthere is an option, not the default, to never expire a CMK. For this reason AWS\ndo not meet the requirement and it is the customer responsibility to ensure this\nrequirement is met3.6.5Retirement or replacement (for example, archiving,\ndestruction, and/or revocation) of keys as deemed necessary when the integrity\nof the key has been weakened (for example, departure of an employee with\nknowledge of a clear-text key component), or keys are suspected of being\ncompromisedKMS automatic key rotation has no knowledge of your staff\noff-boarding process, therefore automatic key rotation is not sufficient alone,\nall data encrypted using keys the ex-employee has access to must be initiated\nfor key rotation, including backups3.6.6If manual clear-text cryptographic\nkey-management operations are used, these operations must be managed using split\nknowledge and dual control. Note: Examples of manual key-management operations\ninclude, but are not limited to: key generation, transmission, loading, storage\nand destruction.Applies when choosing to provide key material to\nCustomer-managed CMKs in KMS (without integration to CloudHSM) due to the\nnecessity of having an external to KMS process to provide the key material which\nis inherently clear-text file key generation and transmission operation4.1Use\nstrong cryptography and security protocols to safeguard sensitive cardholder\ndata during transmission over open, public networksas 2.3 above8.1.6Limit\nrepeated access attempts by locking out the user ID after not more than six\nattemptsNot an available feature of the KMS API8.1.8If a session has been idle\nfor more than 15 minutes, require the user to re-authenticate to re-activate the\nterminal or sessionAWS provides configurations of the total session duration\nonly, there is no idle settings available8.2In addition to assigning a unique\nID, ensure proper user-authentication management for non-consumer users and\nadministrators on all system components by employing at least one of the\nfollowing methods to authenticate all usersWhile MFA is available to IAM Users,\nit is not the default, and IAM Users are not the best practice access pattern8.3\nSecure all individual non-console administrative access and all remote access to\nthe CDE using multi-factor authenticationas 8.2 above8.5Do not use group,\nshared, or generic IDs, passwords, or other authentication methodsthe default\nand general best practice access pattern is to use federated or assumed role\nbased access which is violation of this prescriptive requirement10.2Implement\nautomated audit trails for all system components to reconstruct the specific\neventsBoth KMS and CloudHSM rely on CloudTrail to meet this requirement which is\nnot available by default10.3Record at least the following audit trail entries\nfor all system components for each eventas 10.2 above10.5Secure audit trails so\nthey cannot be alteredWhile CloudTrail is not used by default, when it is in use\nthe digest feature is required to meet this requirement and it is also not a\ndefault. Additionally S3 log delivery must be configured with controls in place\nto prevent tampering of the S3 Objects that represent both the log data and\ndigests10.7Retain audit trail history for at least one year, with a minimum of\nthree months immediately available for analysis (for example, online, archived,\nor restorable from backup)While not used by default, S3 life-cycle events and\nglacier storage classes are available to meet these requirements11.5Deploy a\nchange-detection mechanism (for example, file-integrity monitoring tools) to\nalert personnel to unauthorized modification (including changes, additions and\ndeletions) of critical system files, configuration files, or content files; and\nconfigure the software to perform critical file comparisons at least weeklyas\n10.7 above while also considering CloudWatch usage for monitoring activitiesThe\nabove considerations are particularly critical when you are operating a Level 1\nCDE.\nYou may chose to focus more on PCI requirement 11 if you operate a Level 2-5\nCDE, leaving the SAQ and CCW efforts relatively relaxed. This would of course\nalso be depending on your overall risk appetite, or other obligations outside\nPCI.\n\nConclusion\nBasically, PCI DSS is a prescriptive standard but has two key facts which seem\nto be unknown to many organisations;\n\n 1. The QSA has full discretionary decision making power\n 2. There is the Compensating Controls Worksheet (CCW) that is available to you\n    if you do not implement any of the prescriptive requirements.\n\n> Yes, you read these correctly.\n\n\nYou can actually achieve PCI DSS without meeting a single prescriptive \nrequirement as set out in the PCI guidance material, if the QSA agrees you\nprovided evidence of compensating controls in the CCW that meets or exceeds the\nintent of the requirements themselves.\n\nSo the verdict is; to have a quick audit without many questions from the QSA you\nwant to avoid the CCW.\nIn context to this article, using the CloudHSM provides the highest degree of\ncontrol and assurance to the QSA while also meeting the PCI prescriptive\nrequirements for its feature set.\n\nFinal thoughts\nPCI is not all about requirement 3, Encryption.\nBut while we are on the topic; ensure you apply appropriate key management as\nbriefly mentioned above.\n\nThere are three important factors to consider here;\n\n 1. All keys are single workload use, so revoking a key won't take down more\n    than one workload\n 2. Have separate access patterns for key encryption, and key decryption. So\n    that the key creators cannot also the read data. this is good for operations\n    also due to a separation of concerns\n 3. Be careful of scope increase due to access controls. This is commonly where\n    access to the CDE is federated, so there might be too many users with\n    permissions to access the CDE due to groups and time. Another is direct\n    programmatic access being permitted to the CDE from devices which are not\n    locked down to access the CDE alone. That is to say an employee workstation\n    on wifi accessing the CDE is going to effect scope if they have the ability\n    to read-only any cardholder data (which is transfered to the workstation to\n    be viewed).\n\nIf you are not applying this level of completeness, prepare to spend a far\nlonger time with your QSA addressing every detail of every one of the PCI\nrequirements, or the entries in the CCW. They will scrutinise over all details\nfor linked collateral entered into the CCW also, because it is their personal\nreputation on the line if they use their discretionary decision making power\npoorly.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/09/aws-pci-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-05-13 16:19:06","created_by":"1","updated_at":"2021-03-31 13:59:36","updated_by":"1","published_at":"2019-09-01 04:22:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcbb","uuid":"fbd7e4e1-6b30-4afd-a471-e327ac9d108c","title":"Machine Learning model training over time","slug":"machine-learning-model-training-over-time","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Do you train new models using new data, or do you re-train existing models with new samples?\\n\\n## Retraining a model\\n\\nFor the past few years I've been capturing network traffic and building various machine learning models around the data, not always security related but always design for a long lasting purpose.\\n\\nThis means I have many different models, using sometimes the same features, all off of the same full data set. Retraining models on this exponentially growing data became unmanagable last year, so i took a new approach.\\n\\n## Versioning Models\\n\\nTaking inspiration from my software engineering background I already employ a strict versioning strategy so that I can reason with changes in my data and how models evolve over months and years.\\n\\n## Multiple Partitioned Models\\n\\nDistinct from versioning, you should consider using multiple models that are trained on partitions of data. Each partition of data uses identical models and may also be of identical schema, but the benefit of using multiple partitioned models in Machine Learning is;\\n\\n> Economics\\n\\nThe economics of training models is a result of the inherent economics of the cloud infrastructure and time needed to train even a shallow decision tree on petabytes of data. So this is to say that by splitting out your data into partitions and training a model on each partition that is independent of the feature chosen for the partition, you can for example train a model on Auctions that ended in the past decade, deploy that model separately from the current decade that is currently being re-trained as new data arrives.\\n\\nBy separating models into partitions we are confident of the static model and this confidence is what we commonly call a Bayesian or probabilistic model, because we can make an probabilistic inference. \\n\\nAt first glance you may think I am describing in simplistic terms cascaded classification or regression, these are distinct in that partitioning your models is a technique not an algorithm. Many times when explaining this technique some organizations I've worked with immediately jump to describe their multi-stage classification or regression models because these are the algorithms they have memorized, but these partitioned Models are not belonging to any specific category, you can partition your models for regression or not.\\n\\n### Applications for Partitioning\\n\\nInformation Security has many vendors that market features such as behavior analysis using machine learning, and in many cases will not discuss their models, what samples are used for training, how much, or the algorithms they use - because;\\n\\n> Proprietary!\\n\\nOn occasion I have been able to speak to vendors at a conference that were willing to discuss maybe one or two of these points with me, I like to ask;\\n\\n> How are threats evaluated over time and represented in the models\\n\\nWhat I learned was troubling to me. I commonly receive a response that is a variation of some cyber threat scoring and model retraining mixture of replies. If we have learned anything from GRC is that a threat score is not a risk assessment, more then that is that in data science we know that by changing context any assessment today could be either redundant or critical to the accuracy of a model.\\n\\nGiven the model is designed to provide probabilistic inference based on samples over time, the confidence score if the model was trained today will be far different then if the model was trained a decade ago on data at the time. However, inversely if the model is designed to infer point-in-time you would require that many models are trained and deployed and never retrained to retain that point-in-time accuracy when used as an input (or the inference as an engineered value) to your regression model.\\n\\n> Forking a model\\n\\nAnother great example in the real world is that threats are patched, and attackers move on to new vulnerabilities. So a critical and high likelihood threat today may because moderate and unlikely in future as defense-in-depth is deployed and attackers stop using the patched vulnerability. This scenario _requires_ retraining certain models to account for these changes over time. In this case a fork of a model may be required so as to retain accuracy of the point-in-time and real-time inference requirements.\\n\\n## Conclusion\\n\\nMachine Learning model training over time is not complex but it is domain specific and should be driven by purpose rather than opinionated or classical views taught in the past.\\n\\nTry to look at the problem at hand, think of what you need to know now for your current point-in-time and real-time null hypothesis, but also try to imagine the future point-in-time and real-time needs as these are never going to be identical to your current _now_ null hypothesis.\\n\\nThen use not one, but all of these as variations of the original intent when designing a solution.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Do you train new models using new data, or do you re-train existing models with new samples?</p>\n<h2 id=\"retrainingamodel\">Retraining a model</h2>\n<p>For the past few years I've been capturing network traffic and building various machine learning models around the data, not always security related but always design for a long lasting purpose.</p>\n<p>This means I have many different models, using sometimes the same features, all off of the same full data set. Retraining models on this exponentially growing data became unmanagable last year, so i took a new approach.</p>\n<h2 id=\"versioningmodels\">Versioning Models</h2>\n<p>Taking inspiration from my software engineering background I already employ a strict versioning strategy so that I can reason with changes in my data and how models evolve over months and years.</p>\n<h2 id=\"multiplepartitionedmodels\">Multiple Partitioned Models</h2>\n<p>Distinct from versioning, you should consider using multiple models that are trained on partitions of data. Each partition of data uses identical models and may also be of identical schema, but the benefit of using multiple partitioned models in Machine Learning is;</p>\n<blockquote>\n<p>Economics</p>\n</blockquote>\n<p>The economics of training models is a result of the inherent economics of the cloud infrastructure and time needed to train even a shallow decision tree on petabytes of data. So this is to say that by splitting out your data into partitions and training a model on each partition that is independent of the feature chosen for the partition, you can for example train a model on Auctions that ended in the past decade, deploy that model separately from the current decade that is currently being re-trained as new data arrives.</p>\n<p>By separating models into partitions we are confident of the static model and this confidence is what we commonly call a Bayesian or probabilistic model, because we can make an probabilistic inference.</p>\n<p>At first glance you may think I am describing in simplistic terms cascaded classification or regression, these are distinct in that partitioning your models is a technique not an algorithm. Many times when explaining this technique some organizations I've worked with immediately jump to describe their multi-stage classification or regression models because these are the algorithms they have memorized, but these partitioned Models are not belonging to any specific category, you can partition your models for regression or not.</p>\n<h3 id=\"applicationsforpartitioning\">Applications for Partitioning</h3>\n<p>Information Security has many vendors that market features such as behavior analysis using machine learning, and in many cases will not discuss their models, what samples are used for training, how much, or the algorithms they use - because;</p>\n<blockquote>\n<p>Proprietary!</p>\n</blockquote>\n<p>On occasion I have been able to speak to vendors at a conference that were willing to discuss maybe one or two of these points with me, I like to ask;</p>\n<blockquote>\n<p>How are threats evaluated over time and represented in the models</p>\n</blockquote>\n<p>What I learned was troubling to me. I commonly receive a response that is a variation of some cyber threat scoring and model retraining mixture of replies. If we have learned anything from GRC is that a threat score is not a risk assessment, more then that is that in data science we know that by changing context any assessment today could be either redundant or critical to the accuracy of a model.</p>\n<p>Given the model is designed to provide probabilistic inference based on samples over time, the confidence score if the model was trained today will be far different then if the model was trained a decade ago on data at the time. However, inversely if the model is designed to infer point-in-time you would require that many models are trained and deployed and never retrained to retain that point-in-time accuracy when used as an input (or the inference as an engineered value) to your regression model.</p>\n<blockquote>\n<p>Forking a model</p>\n</blockquote>\n<p>Another great example in the real world is that threats are patched, and attackers move on to new vulnerabilities. So a critical and high likelihood threat today may because moderate and unlikely in future as defense-in-depth is deployed and attackers stop using the patched vulnerability. This scenario <em>requires</em> retraining certain models to account for these changes over time. In this case a fork of a model may be required so as to retain accuracy of the point-in-time and real-time inference requirements.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Machine Learning model training over time is not complex but it is domain specific and should be driven by purpose rather than opinionated or classical views taught in the past.</p>\n<p>Try to look at the problem at hand, think of what you need to know now for your current point-in-time and real-time null hypothesis, but also try to imagine the future point-in-time and real-time needs as these are never going to be identical to your current <em>now</em> null hypothesis.</p>\n<p>Then use not one, but all of these as variations of the original intent when designing a solution.</p>\n<!--kg-card-end: markdown-->","comment_id":"5af8ea4eea08b405ba5b064a","plaintext":"Do you train new models using new data, or do you re-train existing models with\nnew samples?\n\nRetraining a model\nFor the past few years I've been capturing network traffic and building various\nmachine learning models around the data, not always security related but always\ndesign for a long lasting purpose.\n\nThis means I have many different models, using sometimes the same features, all\noff of the same full data set. Retraining models on this exponentially growing\ndata became unmanagable last year, so i took a new approach.\n\nVersioning Models\nTaking inspiration from my software engineering background I already employ a\nstrict versioning strategy so that I can reason with changes in my data and how\nmodels evolve over months and years.\n\nMultiple Partitioned Models\nDistinct from versioning, you should consider using multiple models that are\ntrained on partitions of data. Each partition of data uses identical models and\nmay also be of identical schema, but the benefit of using multiple partitioned\nmodels in Machine Learning is;\n\n> Economics\n\n\nThe economics of training models is a result of the inherent economics of the\ncloud infrastructure and time needed to train even a shallow decision tree on\npetabytes of data. So this is to say that by splitting out your data into\npartitions and training a model on each partition that is independent of the\nfeature chosen for the partition, you can for example train a model on Auctions\nthat ended in the past decade, deploy that model separately from the current\ndecade that is currently being re-trained as new data arrives.\n\nBy separating models into partitions we are confident of the static model and\nthis confidence is what we commonly call a Bayesian or probabilistic model,\nbecause we can make an probabilistic inference.\n\nAt first glance you may think I am describing in simplistic terms cascaded\nclassification or regression, these are distinct in that partitioning your\nmodels is a technique not an algorithm. Many times when explaining this\ntechnique some organizations I've worked with immediately jump to describe their\nmulti-stage classification or regression models because these are the algorithms\nthey have memorized, but these partitioned Models are not belonging to any\nspecific category, you can partition your models for regression or not.\n\nApplications for Partitioning\nInformation Security has many vendors that market features such as behavior\nanalysis using machine learning, and in many cases will not discuss their\nmodels, what samples are used for training, how much, or the algorithms they use\n- because;\n\n> Proprietary!\n\n\nOn occasion I have been able to speak to vendors at a conference that were\nwilling to discuss maybe one or two of these points with me, I like to ask;\n\n> How are threats evaluated over time and represented in the models\n\n\nWhat I learned was troubling to me. I commonly receive a response that is a\nvariation of some cyber threat scoring and model retraining mixture of replies.\nIf we have learned anything from GRC is that a threat score is not a risk\nassessment, more then that is that in data science we know that by changing\ncontext any assessment today could be either redundant or critical to the\naccuracy of a model.\n\nGiven the model is designed to provide probabilistic inference based on samples\nover time, the confidence score if the model was trained today will be far\ndifferent then if the model was trained a decade ago on data at the time.\nHowever, inversely if the model is designed to infer point-in-time you would\nrequire that many models are trained and deployed and never retrained to retain\nthat point-in-time accuracy when used as an input (or the inference as an\nengineered value) to your regression model.\n\n> Forking a model\n\n\nAnother great example in the real world is that threats are patched, and\nattackers move on to new vulnerabilities. So a critical and high likelihood\nthreat today may because moderate and unlikely in future as defense-in-depth is\ndeployed and attackers stop using the patched vulnerability. This scenario \nrequires retraining certain models to account for these changes over time. In\nthis case a fork of a model may be required so as to retain accuracy of the\npoint-in-time and real-time inference requirements.\n\nConclusion\nMachine Learning model training over time is not complex but it is domain\nspecific and should be driven by purpose rather than opinionated or classical\nviews taught in the past.\n\nTry to look at the problem at hand, think of what you need to know now for your\ncurrent point-in-time and real-time null hypothesis, but also try to imagine the\nfuture point-in-time and real-time needs as these are never going to be\nidentical to your current now null hypothesis.\n\nThen use not one, but all of these as variations of the original intent when\ndesigning a solution.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/05/1_kzmiuypmxgehhxx7slbp4w.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-05-14 01:45:50","created_by":"1","updated_at":"2021-03-31 14:01:25","updated_by":"1","published_at":"2018-05-13 01:45:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcbc","uuid":"6265ce2c-b911-44e5-b883-cd0308c15423","title":"Responding to a troll GDPR Subject Access Request - Australian/NZ Version","slug":"responding-to-a-gdpr-subject-access-request","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The General Data Protection Regulation (GDPR) guidance in this post is experience based and your own response should be reviewed by a law counselor. I'm no lawyer, and many who are do not have subject matter experience (yet), so a balanced approach is playing it safe in these early days.\\n\\nYou may have heard that a [sample SAR letter](https://www.linkedin.com/pulse/nightmare-letter-subject-access-request-under-gdpr-karbaliotis/) was posted and likely you've received a carbon-copy in your inbox, if so you've come to the right place. That sample letter has been widely used to harass businesses without motive other than mass disruption in most cases, Whether your business is targeted or not the SAR letter is considered legitimate (at least it is currently) and requires a response.\\n\\nThe following points are specific to the sample letter but can be used if you've received a similar variant, however a more sophisticated SAP letter, perhaps drafted by a lawyer, may require more thorough consultancy.\\n\\n# A real customer?\\n\\nAn obvious tell tale sign of the claimant using a copy of the sample is if it starts with (2nd sentence)\\n\\n> I am a customer of yours,\\n\\nAre they? Have they provided any way for you to verify this? Check the letter for any indications that this is a real customer and if you cannot find any records you might feel comfortable to disregard the rest of the content in the letter at this point and simply respond with a request for the individual to identify themselves as a customer.\\n\\nThe letter also includes\\n\\n> I am including a copy of documentation necessary to verify my identity\\n\\nSo this claimant _must_ have done this, right?\\n\\n# Getting an extension\\n\\nThis is a good time to mention that GDPR has a provision to extend the deadline with another two months if\\n\\n- you are overwhelmed with requests, or;\\n- if the request is overly complex\\n\\nA letter that has no way to identify themselves as a customer would not qualify for _\\\"overly complex\\\"_ but if you have many letters and limited resources to dedicate to addressing them you can mail back the claimant telling them their request will take up to three months (two months extension after the one month as required under Article 12) to process giving you some time to automate some of the elements.\\n\\n# Australian InfoSec\\nQuestion 8 specifically asks about policies and standards.\\n\\nMost Australian businesses should already adhere to the 13 APPs (Australian Privacy Principles) given they have been around for 30 years in Australian law.\\n\\n> Side note: New Zealand also have 12 Information Privacy Principles closely aligned to Australia's\\n\\nIf your business has not interacted with any company defined under \\\"agencies\\\" (an not an agency yourself) of the Privacy Act you might not have encountered the APPs or other Australian specific InfoSec initiatives such as;\\n\\n- Office of the Australian Information Commissioner (OAIC) Assessment\\n- Information Security Registered Assessors Program (iRAP) from the Australian Signals Directorate (ASD)\\n- Australian Prudential Regulation Authority (APRA) for financial systems\\n  - predecessor regulators were the Insurance and Superannuation Commission (ISC), the Reserve Bank of Australia (RBA), and the Australian Financial Institutions Commission (AFIC)\\n- The Protective Security Policy Framework (PSPF) from the Attorney-General's Department\\n- Australian Government Information Security Manual (ISM) from the Australian Signals Directorate (ASD)\\n- Restricted Authorised deposit-taking institution (ADI) Framework from Australian Prudential Regulation Authority (APRA)\\n\\nThere are also a lot of internationally recognised initiatives, some of the most notable;\\n\\n- ISO Standards\\n  - ISO 9001:2015\\n  - ISO/IEC 27001:2013\\n  - ISO/IEC 27002:2013\\n  - ISO/IEC 27017:2015\\n  - ISO/IEC 27018:2014\\n- PCI DSS 3.2 or PCI PA\\n- Sarbanes-Oxley (SOX)\\n- ISAE 3402\\n  - SOC 1 (SSAE 16)\\n  - SOC 2 (Trust Services and AT 101)\\n  - SOC 3 (alternate report for SOC 2 Type 2)\\n- The Committee of Sponsoring Organizations (COSO) \\n- Information Systems Audit and Control Association (ISACA) Control Objectives for Information and Related Technology (COBIT)\\n- IT Infrastructure Library (ITIL)\\n- Cloud Security Alliance (CSA) Security, Trust & Assurance Registry (STAR)\\n- Center of Internet Security (CIS) Benchmarks\\n- Cyber Essentials Plus\\n\\nThere are also some country specific initiatives that are highly regarded;\\n\\n- The NIST Cybersecurity Framework (NIST CSF) \\n- Federal Information Processing Standard (FIPS) 140-2 Level 4\\n- Health Insurance Portability and Accountability Act (HIPAA)\\n\\nIf you have no intersections with at least one of these, you may have a hard time with GDPR.\\n\\n# Quickly addressing questions\\n\\nThere are a lot of questions in the sample, however a lot can be addressed together.\\n\\n## Personal data processing\\n\\nThis is the first question int he sample and the general nature of the inquiry.\\n\\n1. If you do not collect, store, or process personal data of your end-users directly, make sure you have a GDPR compliant Privacy Policy and respond now directing to the policy.\\n\\n2. If you do process personal data but do not collect it from the user directly, you are what is called a sub-processor and you can direct the individual to the b2c entity that you to process the data on their behalf. Your business is subject to  a high volume of this kind of _fishing expedition_, so you might look at automating this type of response.\\n\\n3. You do collect and process personal data, so be prepared and document a [Privacy Impact Assessment (PIA)](https://www.oaic.gov.au/agencies-and-organisations/guides/guide-to-undertaking-privacy-impact-assessments#undertaking-a-pia) for the systems that process personal data if you do not already have one (you should), and address the claimants questions.\\n\\n## I want a copy of my data\\n\\nThe GDPR is the successor of the Data Protection Directive which was a European Union directive that didn't reach beyond Europe as the GDPR does. If you operated in Europe prior to GDPR you should already be able to comply with this request due to it being required under DPA.\\n\\nIf you've never had a need to provide an individuals data before it should be easy because there is no formatting requirements. So an export using whatever plain-text format (like CSV, SQL inserts, JSON, XML) your database exports to can be used unmodified with little effort. However it is advised you transform the export file format so as to not have your systems identifiable and compromised. \\n\\nThis is one of the core rights that users get under the GDPR (and already had, under the DPD) so it would be a priority for you to automate this now GDPR non-compliance is subject to legal action and financial impacts.\\n\\n## How do you use my data\\n\\nThis question comes in many forms throughout the sample letter.\\n\\nAssuming you have an existing Privacy Policy, make sure it outlines each past, current, and future uses of data processing you expect and describe time-frames you expect for these processes to operate. You must not store personal data past the disclosed periods so you may have certain changes to implement for your systems to be compliant.\\n\\n## Who do you share my data with\\n\\nIn most cases this is limited to law enforcement, or no entities at all. You may also have Master Service Agreements (MSA) or contract with another entity that you share private data with, which is the most obvious data sharing capability you might have.\\n\\nHowever a less obvious data sharing capability a lot of modern businesses provide is an application programming interface (API) as part of a standard offering. It may even be provided indirectly if you have a web browser based service/s like a website or an intranet, in which case you should look at what kind of private information is delivered over the API.\\n\\n## How do you safeguard my data\\n\\nFair question.\\n\\nHowever you have no obligation to disclose _details_ of any controls you implement to my knowledge, or knowledge of many who have scrutinise GDPR responsibilities as I have.\\n\\n## Have you disclosed my data in a breach?\\n\\nAustralian businesses were expected to disclose data breaches since 2017, before GDPR. In fact it became a requirement February 22nd this year under the OAIC's Notifiable Data Breaches (NDB) scheme introduced in 2015, and GDPR was enforcible only 2 weeks ago (May 25th).\\n\\nGiven it is the individual claimants responsibility to assess the risk of harm to themselves, and I hope you have no known but undisclosed breaches, so you could satisfy the claimant with an answer or you might decide to politely respond stating there are no undisclosed breaches.\\n\\n## Backups of my data\\n\\nYour Privacy Policy should indicate data storage including backups, and be careful about providing details of where data is stored or how it is secured because you may inadvertently disclose vulnerabilities to a bad actor posing as a customer.\\n\\n## Staff access to my data\\n\\nIn my professional experience this is a question that is not a common as I'd like. I use this question myself extensively when conducting risk assessments as it has a powerful reason to pry into service providers security practices.\\n\\nFor your response and perhaps an action for you to take if you don't already practice this, the best control here is that you ask all employee's and contractors to sign _data confidentiality_ agreements and upon termination you have them sign a _non-retention_ agreement.\\n\\n# Conclusion\\n\\nThere are a lot of examples of businesses being harassed under the guise of GDPR, and unfortunately there's free ammunition for both the legitimate and bad actors can use to disrupt your business. I fundamentally believe in the goals of our own APPs and the GDPR but know all too well the burden placed on businesses to achieve these goals, for which I obligate myself to make it as easy as possible for everyone to find useful and actionable information to reduce the burden of compliance.\\n\\nIf GDPR is still new to you, hopefully you've learned enough by reading my [previous post](__GHOST_URL__/gdpr-compliance-beyond-europe/) and here, but I strongly recommend you become familiar and seek formal legal advice before making any decision or taking action that might cause you significant impacts.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The General Data Protection Regulation (GDPR) guidance in this post is experience based and your own response should be reviewed by a law counselor. I'm no lawyer, and many who are do not have subject matter experience (yet), so a balanced approach is playing it safe in these early days.</p>\n<p>You may have heard that a <a href=\"https://www.linkedin.com/pulse/nightmare-letter-subject-access-request-under-gdpr-karbaliotis/\">sample SAR letter</a> was posted and likely you've received a carbon-copy in your inbox, if so you've come to the right place. That sample letter has been widely used to harass businesses without motive other than mass disruption in most cases, Whether your business is targeted or not the SAR letter is considered legitimate (at least it is currently) and requires a response.</p>\n<p>The following points are specific to the sample letter but can be used if you've received a similar variant, however a more sophisticated SAP letter, perhaps drafted by a lawyer, may require more thorough consultancy.</p>\n<h1 id=\"arealcustomer\">A real customer?</h1>\n<p>An obvious tell tale sign of the claimant using a copy of the sample is if it starts with (2nd sentence)</p>\n<blockquote>\n<p>I am a customer of yours,</p>\n</blockquote>\n<p>Are they? Have they provided any way for you to verify this? Check the letter for any indications that this is a real customer and if you cannot find any records you might feel comfortable to disregard the rest of the content in the letter at this point and simply respond with a request for the individual to identify themselves as a customer.</p>\n<p>The letter also includes</p>\n<blockquote>\n<p>I am including a copy of documentation necessary to verify my identity</p>\n</blockquote>\n<p>So this claimant <em>must</em> have done this, right?</p>\n<h1 id=\"gettinganextension\">Getting an extension</h1>\n<p>This is a good time to mention that GDPR has a provision to extend the deadline with another two months if</p>\n<ul>\n<li>you are overwhelmed with requests, or;</li>\n<li>if the request is overly complex</li>\n</ul>\n<p>A letter that has no way to identify themselves as a customer would not qualify for <em>&quot;overly complex&quot;</em> but if you have many letters and limited resources to dedicate to addressing them you can mail back the claimant telling them their request will take up to three months (two months extension after the one month as required under Article 12) to process giving you some time to automate some of the elements.</p>\n<h1 id=\"australianinfosec\">Australian InfoSec</h1>\n<p>Question 8 specifically asks about policies and standards.</p>\n<p>Most Australian businesses should already adhere to the 13 APPs (Australian Privacy Principles) given they have been around for 30 years in Australian law.</p>\n<blockquote>\n<p>Side note: New Zealand also have 12 Information Privacy Principles closely aligned to Australia's</p>\n</blockquote>\n<p>If your business has not interacted with any company defined under &quot;agencies&quot; (an not an agency yourself) of the Privacy Act you might not have encountered the APPs or other Australian specific InfoSec initiatives such as;</p>\n<ul>\n<li>Office of the Australian Information Commissioner (OAIC) Assessment</li>\n<li>Information Security Registered Assessors Program (iRAP) from the Australian Signals Directorate (ASD)</li>\n<li>Australian Prudential Regulation Authority (APRA) for financial systems\n<ul>\n<li>predecessor regulators were the Insurance and Superannuation Commission (ISC), the Reserve Bank of Australia (RBA), and the Australian Financial Institutions Commission (AFIC)</li>\n</ul>\n</li>\n<li>The Protective Security Policy Framework (PSPF) from the Attorney-General's Department</li>\n<li>Australian Government Information Security Manual (ISM) from the Australian Signals Directorate (ASD)</li>\n<li>Restricted Authorised deposit-taking institution (ADI) Framework from Australian Prudential Regulation Authority (APRA)</li>\n</ul>\n<p>There are also a lot of internationally recognised initiatives, some of the most notable;</p>\n<ul>\n<li>ISO Standards\n<ul>\n<li>ISO 9001:2015</li>\n<li>ISO/IEC 27001:2013</li>\n<li>ISO/IEC 27002:2013</li>\n<li>ISO/IEC 27017:2015</li>\n<li>ISO/IEC 27018:2014</li>\n</ul>\n</li>\n<li>PCI DSS 3.2 or PCI PA</li>\n<li>Sarbanes-Oxley (SOX)</li>\n<li>ISAE 3402\n<ul>\n<li>SOC 1 (SSAE 16)</li>\n<li>SOC 2 (Trust Services and AT 101)</li>\n<li>SOC 3 (alternate report for SOC 2 Type 2)</li>\n</ul>\n</li>\n<li>The Committee of Sponsoring Organizations (COSO)</li>\n<li>Information Systems Audit and Control Association (ISACA) Control Objectives for Information and Related Technology (COBIT)</li>\n<li>IT Infrastructure Library (ITIL)</li>\n<li>Cloud Security Alliance (CSA) Security, Trust &amp; Assurance Registry (STAR)</li>\n<li>Center of Internet Security (CIS) Benchmarks</li>\n<li>Cyber Essentials Plus</li>\n</ul>\n<p>There are also some country specific initiatives that are highly regarded;</p>\n<ul>\n<li>The NIST Cybersecurity Framework (NIST CSF)</li>\n<li>Federal Information Processing Standard (FIPS) 140-2 Level 4</li>\n<li>Health Insurance Portability and Accountability Act (HIPAA)</li>\n</ul>\n<p>If you have no intersections with at least one of these, you may have a hard time with GDPR.</p>\n<h1 id=\"quicklyaddressingquestions\">Quickly addressing questions</h1>\n<p>There are a lot of questions in the sample, however a lot can be addressed together.</p>\n<h2 id=\"personaldataprocessing\">Personal data processing</h2>\n<p>This is the first question int he sample and the general nature of the inquiry.</p>\n<ol>\n<li>\n<p>If you do not collect, store, or process personal data of your end-users directly, make sure you have a GDPR compliant Privacy Policy and respond now directing to the policy.</p>\n</li>\n<li>\n<p>If you do process personal data but do not collect it from the user directly, you are what is called a sub-processor and you can direct the individual to the b2c entity that you to process the data on their behalf. Your business is subject to  a high volume of this kind of <em>fishing expedition</em>, so you might look at automating this type of response.</p>\n</li>\n<li>\n<p>You do collect and process personal data, so be prepared and document a <a href=\"https://www.oaic.gov.au/agencies-and-organisations/guides/guide-to-undertaking-privacy-impact-assessments#undertaking-a-pia\">Privacy Impact Assessment (PIA)</a> for the systems that process personal data if you do not already have one (you should), and address the claimants questions.</p>\n</li>\n</ol>\n<h2 id=\"iwantacopyofmydata\">I want a copy of my data</h2>\n<p>The GDPR is the successor of the Data Protection Directive which was a European Union directive that didn't reach beyond Europe as the GDPR does. If you operated in Europe prior to GDPR you should already be able to comply with this request due to it being required under DPA.</p>\n<p>If you've never had a need to provide an individuals data before it should be easy because there is no formatting requirements. So an export using whatever plain-text format (like CSV, SQL inserts, JSON, XML) your database exports to can be used unmodified with little effort. However it is advised you transform the export file format so as to not have your systems identifiable and compromised.</p>\n<p>This is one of the core rights that users get under the GDPR (and already had, under the DPD) so it would be a priority for you to automate this now GDPR non-compliance is subject to legal action and financial impacts.</p>\n<h2 id=\"howdoyouusemydata\">How do you use my data</h2>\n<p>This question comes in many forms throughout the sample letter.</p>\n<p>Assuming you have an existing Privacy Policy, make sure it outlines each past, current, and future uses of data processing you expect and describe time-frames you expect for these processes to operate. You must not store personal data past the disclosed periods so you may have certain changes to implement for your systems to be compliant.</p>\n<h2 id=\"whodoyousharemydatawith\">Who do you share my data with</h2>\n<p>In most cases this is limited to law enforcement, or no entities at all. You may also have Master Service Agreements (MSA) or contract with another entity that you share private data with, which is the most obvious data sharing capability you might have.</p>\n<p>However a less obvious data sharing capability a lot of modern businesses provide is an application programming interface (API) as part of a standard offering. It may even be provided indirectly if you have a web browser based service/s like a website or an intranet, in which case you should look at what kind of private information is delivered over the API.</p>\n<h2 id=\"howdoyousafeguardmydata\">How do you safeguard my data</h2>\n<p>Fair question.</p>\n<p>However you have no obligation to disclose <em>details</em> of any controls you implement to my knowledge, or knowledge of many who have scrutinise GDPR responsibilities as I have.</p>\n<h2 id=\"haveyoudisclosedmydatainabreach\">Have you disclosed my data in a breach?</h2>\n<p>Australian businesses were expected to disclose data breaches since 2017, before GDPR. In fact it became a requirement February 22nd this year under the OAIC's Notifiable Data Breaches (NDB) scheme introduced in 2015, and GDPR was enforcible only 2 weeks ago (May 25th).</p>\n<p>Given it is the individual claimants responsibility to assess the risk of harm to themselves, and I hope you have no known but undisclosed breaches, so you could satisfy the claimant with an answer or you might decide to politely respond stating there are no undisclosed breaches.</p>\n<h2 id=\"backupsofmydata\">Backups of my data</h2>\n<p>Your Privacy Policy should indicate data storage including backups, and be careful about providing details of where data is stored or how it is secured because you may inadvertently disclose vulnerabilities to a bad actor posing as a customer.</p>\n<h2 id=\"staffaccesstomydata\">Staff access to my data</h2>\n<p>In my professional experience this is a question that is not a common as I'd like. I use this question myself extensively when conducting risk assessments as it has a powerful reason to pry into service providers security practices.</p>\n<p>For your response and perhaps an action for you to take if you don't already practice this, the best control here is that you ask all employee's and contractors to sign <em>data confidentiality</em> agreements and upon termination you have them sign a <em>non-retention</em> agreement.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>There are a lot of examples of businesses being harassed under the guise of GDPR, and unfortunately there's free ammunition for both the legitimate and bad actors can use to disrupt your business. I fundamentally believe in the goals of our own APPs and the GDPR but know all too well the burden placed on businesses to achieve these goals, for which I obligate myself to make it as easy as possible for everyone to find useful and actionable information to reduce the burden of compliance.</p>\n<p>If GDPR is still new to you, hopefully you've learned enough by reading my <a href=\"__GHOST_URL__/gdpr-compliance-beyond-europe/\">previous post</a> and here, but I strongly recommend you become familiar and seek formal legal advice before making any decision or taking action that might cause you significant impacts.</p>\n<!--kg-card-end: markdown-->","comment_id":"5b1292e9c4513d05ac959985","plaintext":"The General Data Protection Regulation (GDPR) guidance in this post is\nexperience based and your own response should be reviewed by a law counselor.\nI'm no lawyer, and many who are do not have subject matter experience (yet), so\na balanced approach is playing it safe in these early days.\n\nYou may have heard that a sample SAR letter\n[https://www.linkedin.com/pulse/nightmare-letter-subject-access-request-under-gdpr-karbaliotis/] \nwas posted and likely you've received a carbon-copy in your inbox, if so you've\ncome to the right place. That sample letter has been widely used to harass\nbusinesses without motive other than mass disruption in most cases, Whether your\nbusiness is targeted or not the SAR letter is considered legitimate (at least it\nis currently) and requires a response.\n\nThe following points are specific to the sample letter but can be used if you've\nreceived a similar variant, however a more sophisticated SAP letter, perhaps\ndrafted by a lawyer, may require more thorough consultancy.\n\nA real customer?\nAn obvious tell tale sign of the claimant using a copy of the sample is if it\nstarts with (2nd sentence)\n\n> I am a customer of yours,\n\n\nAre they? Have they provided any way for you to verify this? Check the letter\nfor any indications that this is a real customer and if you cannot find any\nrecords you might feel comfortable to disregard the rest of the content in the\nletter at this point and simply respond with a request for the individual to\nidentify themselves as a customer.\n\nThe letter also includes\n\n> I am including a copy of documentation necessary to verify my identity\n\n\nSo this claimant must have done this, right?\n\nGetting an extension\nThis is a good time to mention that GDPR has a provision to extend the deadline\nwith another two months if\n\n * you are overwhelmed with requests, or;\n * if the request is overly complex\n\nA letter that has no way to identify themselves as a customer would not qualify\nfor \"overly complex\" but if you have many letters and limited resources to\ndedicate to addressing them you can mail back the claimant telling them their\nrequest will take up to three months (two months extension after the one month\nas required under Article 12) to process giving you some time to automate some\nof the elements.\n\nAustralian InfoSec\nQuestion 8 specifically asks about policies and standards.\n\nMost Australian businesses should already adhere to the 13 APPs (Australian\nPrivacy Principles) given they have been around for 30 years in Australian law.\n\n> Side note: New Zealand also have 12 Information Privacy Principles closely\naligned to Australia's\n\n\nIf your business has not interacted with any company defined under \"agencies\"\n(an not an agency yourself) of the Privacy Act you might not have encountered\nthe APPs or other Australian specific InfoSec initiatives such as;\n\n * Office of the Australian Information Commissioner (OAIC) Assessment\n * Information Security Registered Assessors Program (iRAP) from the Australian\n   Signals Directorate (ASD)\n * Australian Prudential Regulation Authority (APRA) for financial systems * \n      predecessor regulators were the Insurance and Superannuation Commission\n      (ISC), the Reserve Bank of Australia (RBA), and the Australian Financial\n      Institutions Commission (AFIC)\n   \n   \n * The Protective Security Policy Framework (PSPF) from the Attorney-General's\n   Department\n * Australian Government Information Security Manual (ISM) from the Australian\n   Signals Directorate (ASD)\n * Restricted Authorised deposit-taking institution (ADI) Framework from\n   Australian Prudential Regulation Authority (APRA)\n\nThere are also a lot of internationally recognised initiatives, some of the most\nnotable;\n\n * ISO Standards * ISO 9001:2015\n    * ISO/IEC 27001:2013\n    * ISO/IEC 27002:2013\n    * ISO/IEC 27017:2015\n    * ISO/IEC 27018:2014\n   \n   \n * PCI DSS 3.2 or PCI PA\n * Sarbanes-Oxley (SOX)\n * ISAE 3402 * SOC 1 (SSAE 16)\n    * SOC 2 (Trust Services and AT 101)\n    * SOC 3 (alternate report for SOC 2 Type 2)\n   \n   \n * The Committee of Sponsoring Organizations (COSO)\n * Information Systems Audit and Control Association (ISACA) Control Objectives\n   for Information and Related Technology (COBIT)\n * IT Infrastructure Library (ITIL)\n * Cloud Security Alliance (CSA) Security, Trust & Assurance Registry (STAR)\n * Center of Internet Security (CIS) Benchmarks\n * Cyber Essentials Plus\n\nThere are also some country specific initiatives that are highly regarded;\n\n * The NIST Cybersecurity Framework (NIST CSF)\n * Federal Information Processing Standard (FIPS) 140-2 Level 4\n * Health Insurance Portability and Accountability Act (HIPAA)\n\nIf you have no intersections with at least one of these, you may have a hard\ntime with GDPR.\n\nQuickly addressing questions\nThere are a lot of questions in the sample, however a lot can be addressed\ntogether.\n\nPersonal data processing\nThis is the first question int he sample and the general nature of the inquiry.\n\n 1. If you do not collect, store, or process personal data of your end-users\n    directly, make sure you have a GDPR compliant Privacy Policy and respond now\n    directing to the policy.\n    \n    \n 2. If you do process personal data but do not collect it from the user\n    directly, you are what is called a sub-processor and you can direct the\n    individual to the b2c entity that you to process the data on their behalf.\n    Your business is subject to a high volume of this kind of fishing expedition\n    , so you might look at automating this type of response.\n    \n    \n 3. You do collect and process personal data, so be prepared and document a \n    Privacy Impact Assessment (PIA)\n    [https://www.oaic.gov.au/agencies-and-organisations/guides/guide-to-undertaking-privacy-impact-assessments#undertaking-a-pia] \n    for the systems that process personal data if you do not already have one\n    (you should), and address the claimants questions.\n    \n    \n\nI want a copy of my data\nThe GDPR is the successor of the Data Protection Directive which was a European\nUnion directive that didn't reach beyond Europe as the GDPR does. If you\noperated in Europe prior to GDPR you should already be able to comply with this\nrequest due to it being required under DPA.\n\nIf you've never had a need to provide an individuals data before it should be\neasy because there is no formatting requirements. So an export using whatever\nplain-text format (like CSV, SQL inserts, JSON, XML) your database exports to\ncan be used unmodified with little effort. However it is advised you transform\nthe export file format so as to not have your systems identifiable and\ncompromised.\n\nThis is one of the core rights that users get under the GDPR (and already had,\nunder the DPD) so it would be a priority for you to automate this now GDPR\nnon-compliance is subject to legal action and financial impacts.\n\nHow do you use my data\nThis question comes in many forms throughout the sample letter.\n\nAssuming you have an existing Privacy Policy, make sure it outlines each past,\ncurrent, and future uses of data processing you expect and describe time-frames\nyou expect for these processes to operate. You must not store personal data past\nthe disclosed periods so you may have certain changes to implement for your\nsystems to be compliant.\n\nWho do you share my data with\nIn most cases this is limited to law enforcement, or no entities at all. You may\nalso have Master Service Agreements (MSA) or contract with another entity that\nyou share private data with, which is the most obvious data sharing capability\nyou might have.\n\nHowever a less obvious data sharing capability a lot of modern businesses\nprovide is an application programming interface (API) as part of a standard\noffering. It may even be provided indirectly if you have a web browser based\nservice/s like a website or an intranet, in which case you should look at what\nkind of private information is delivered over the API.\n\nHow do you safeguard my data\nFair question.\n\nHowever you have no obligation to disclose details of any controls you implement\nto my knowledge, or knowledge of many who have scrutinise GDPR responsibilities\nas I have.\n\nHave you disclosed my data in a breach?\nAustralian businesses were expected to disclose data breaches since 2017, before\nGDPR. In fact it became a requirement February 22nd this year under the OAIC's\nNotifiable Data Breaches (NDB) scheme introduced in 2015, and GDPR was\nenforcible only 2 weeks ago (May 25th).\n\nGiven it is the individual claimants responsibility to assess the risk of harm\nto themselves, and I hope you have no known but undisclosed breaches, so you\ncould satisfy the claimant with an answer or you might decide to politely\nrespond stating there are no undisclosed breaches.\n\nBackups of my data\nYour Privacy Policy should indicate data storage including backups, and be\ncareful about providing details of where data is stored or how it is secured\nbecause you may inadvertently disclose vulnerabilities to a bad actor posing as\na customer.\n\nStaff access to my data\nIn my professional experience this is a question that is not a common as I'd\nlike. I use this question myself extensively when conducting risk assessments as\nit has a powerful reason to pry into service providers security practices.\n\nFor your response and perhaps an action for you to take if you don't already\npractice this, the best control here is that you ask all employee's and\ncontractors to sign data confidentiality agreements and upon termination you\nhave them sign a non-retention agreement.\n\nConclusion\nThere are a lot of examples of businesses being harassed under the guise of\nGDPR, and unfortunately there's free ammunition for both the legitimate and bad\nactors can use to disrupt your business. I fundamentally believe in the goals of\nour own APPs and the GDPR but know all too well the burden placed on businesses\nto achieve these goals, for which I obligate myself to make it as easy as\npossible for everyone to find useful and actionable information to reduce the\nburden of compliance.\n\nIf GDPR is still new to you, hopefully you've learned enough by reading my \nprevious post [__GHOST_URL__/gdpr-compliance-beyond-europe/] and here, but I strongly\nrecommend you become familiar and seek formal legal advice before making any\ndecision or taking action that might cause you significant impacts.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/06/GDPR.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-06-02 12:51:53","created_by":"1","updated_at":"2021-03-31 14:01:10","updated_by":"1","published_at":"2018-06-06 09:52:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcbd","uuid":"48d50dc7-7cf6-45e7-932f-2c1527d1aa9c","title":"On Tether Cryptocurrency and Ransomware","slug":"on-tether-cryptocurrency-and-ransomware","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Tether has become the defacto reserve currency as the hundreds of different cryptocurrencies are actually created and exchanged on it.\\n\\nI'll touch on Tether and go into Ransomware briefly, as these are the reason for the post. The I'll deep dive on Cryptocurrency because there is actually three totally separate concepts to address: the concept of the cryptocurrencies themselves, public blockchain, and then there's the concept of the private or permissions blockchain.\\n\\n## Tether and Bitcoin\\n\\nIf you look at Bitcoin trading volume most of it is not actually being exchanged on fiat but these notional cryptocurrencies (refer to crypto as notional cryptocurrencies) that are not [backed-up](https://www.investopedia.com/terms/b/back-up.asp), questionably legal, and not in the wider public interest (allow me to explain).\\n\\nIt is surprising to me that it has lasted this long, reached the market cap it has, and the blind average person will invest without question.\\n\\n> Scam artists have learned a few new tricks this decade.\\n\\nI can only imagine that in the next decade people will generally look back in disgust of themselves at the wasted investment and energy, worse then any time in history, or look back with victorious greed at how fantastically evil and clever they were.\\n\\n**I digress**\\n\\nThere is (or rather was) a [selection bias](https://en.wikipedia.org/wiki/Selection_bias) in crypto where it attracted only true believers like myself. Some people would look at it and think this is garbage and ignore it, or become true believers. There was no economic model for somebody to look at crypto and say it is a scam, but maybe keep looking at the field in case it shows promise (except a few academics maybe). You were a true believer or not interested.\\nThis was great for an immature and developing community.\\nHowever It's no longer harm-limited to a small population of self-inflicted believers, it is spilling over to the average person. Segue into Ransomware.\\n\\n# Ransomware\\n\\nAbout 4-5 years ago we had the first Ransomware epidemic and I believe it was a person name Giovanni who famously introduced it to us, he was able to somehow get control of one of these Ransomware server infrastructure and found at the time you can pay with the either Bitcoin or the greendot moneypak, and effectively everybody paid with greendot because you could walk into 7-Eleven buy a moneypak and get your data back.\\nThis ended up disappearing when the U.S. treasury forced greendot to clean up money laundering and now when you register greendot cards you must provide your Social Security number and there is no longer illegal harvesting of greendot cards for illegal value exchange. This disrupted ransomware for a while, but now it has come back with crypto solely. I don't really care about the drug dealers or any of that, Silk Road was entertaining to watch, but the ransomware epidemic is affecting real people who actually are innocent victims. \\n\\nFortunately and controversially I believe the nefarious parts of the cryptocurrency space can die with proper application of regulation because of how the regulations already are in most countries. \\n\\nIt's become important for me to advocate for the need to clean up this space as an early true believer and as an InfoSec professional. The current state of crypto doesn't provide benefit to society, just those who are interested in committing crimes with a very thin slither of an exception to the few off-the-grid purist privacy-minded who only benefit when mass-adoption is achieved or they have a less privacy concerned middle-man that takes payment as crypto from you to provide goods when you need from society (a compromise in-itself), so even such privacy people get no benefits of crypto unless we all adopt it (and they need no compromise) as our primary trade currency.\\n\\nSo we have 2 groups; criminals, and the rest of us who don't benefit yet regardless of our beliefs.\\n\\n## Private Blockchain\\n\\nSo Blockchains are distributed append only ledgers, but what is a private or permissioned blockchain?\\n\\nSimply it is still an append only data structure, just with a limited number of authorised writers. a.k.a. [a get archive or WORM](https://www.wikiwand.com/en/Write_once_read_many). \\nThere is nothing nothing fundamental in a private blockchain that's hasn't been understood in the field for 20+ years, and even the truly paranoid had [tarsnap for 10+ years now](https://www.wikiwand.com/en/Tarsnap) with the same characteristics, it's just that now crypto has a buzz word **Blockchain** that causes idiots to throw money at the problem.\\n\\n> Crypto has a buzz word Blockchain that causes idiots to throw money at the problem.\\n\\nSo if you see a private or permission blockchain project it means either one of two things; either it's a delusional piece techno-utopianism or somebody smart in IT knows that there real problems with what data you store, how you access it, data provenance and all this other stuff. and they have banded around this buzzword because idiots up management will now throw money at him to solve the said real interesting hard problems with a new label.\\n\\n## Public Blockchain\\n\\nSo the public blockchains are a global data structure where the idea is there's some sort of centralised point of trust but anybody can append to these systems, so not actually distributed as advertised. Distributed in a sense the data is duplicated to everyone but the trust design decisions are still a centralised point or you have no security, and what is security without trust?\\n\\nA little on the distributed topic, it is well covered that the Bitcoin blockchain is effectively controlled by only three entities, but in an attempt to be distributed somehow there's this religious notion that being a distributed trust is somehow good and in of itself. The result is systems that are either grossly insufficient or insecure.\\nThe biggest tool that's used for this is called proof of work (best described as proof-of-waste), the idea of POW is that for somebody to rewrite the history they have to do as much useless work as was done to create the history in the first place so as to make it too hard to do and that work is more economicly put to creating value legitimately. Although we have seen a lot of 51% attacks lately I won't digress into these, or how the crypto developers can wipe out ownership of thier blockchain if something they don't like occurs. I will talk about how POW is great if your goal is to do a lot of useless work, though inefficient. Though it is more interesting to look at how to make the system efficient and don't do a lot of useless work, you run into the problem of now you actually don't have any real protections any more. \\nFor example Bitcoin, proof of work is _\\\"paid for\\\"_ and ends up using as much power as many major cities and that's just an obscene waste of energy to claim there's any benefits worth that.\\nSo far a distributed public append only ledger has only been useful for cryptocurrencies regardless of any individual concern of security, privacy, distrubted ledgers, any of that, if it's public it's limited use period.\\n\\n## Cryptocurrency\\n\\nSo although called cryptocurrency, they don't actually work as _\\\"currency\\\"_, they are provably inferior and can never be superior to the alternative fiat or real world payments like VISA, unless you need what is known as **censorship resistance**, and then you might have an argument for crypto. So let's look at censorship resistance.\\n\\nThe only way to do a censorship resistant transaction without a cryptocurrencies cash, and cash requires physical proximity and mass (a million dollars in US dollars weighs 10KG). So for modern censorship resistant transactions, let's do a direct peer-to-peer payment system (make it public so as to appear legitimate) but make it so that there are no central intermediaries, but it must be solely electronically.\\nEnter cryptocurrencies.\\n\\nThis has been used quite practically for illegal purposes.\\n\\nBut if I, as an average law-abiding person, want to do any payments that is one that the central authorities will actually process, then crypto provably do not work as I have to turn my dollars into crypto because I don't want to always keep it in crypto because the prices jumping up and mostly down at the moment, that's expensive, then I transfer the crypto (that's relatively cheap right now but it's been upwards of $30 in the past even for sub-dollar transactions), and then the recipient on the other side has to convert the crypto back into dollars. You have these mandatory currency conversion steps for any real world transaction using crypto, and even the public companies that used to embraces cryptocurrency (most have backed out including the Bitcoin conferences) only keep a few hundred thousand dollars worth of cryptocurrency when they operated, they are mostly converting crypto to dollars themselves.\\n\\nCrypto does not work for legitimate purchases, but suppose you believe in the vision of the great Satoshi; you don't want to use crypto either because even his monetary policy is that crypto are designed to be deflationary ([JP Buntinx - Cryptocurrency Inflation vs Deflation](https://themerkle.com/cryptocurrency-inflation-vs-deflation/)). \\n> The first rule of a deflationary currency is never spend your deflationary currency!\\n\\nSo when you purchased that _pizza of regret_ for a 10k Bitcoin that's now worth more then your house. Is that FOMO then when you invested, or a result of hard work by the true believers? Neither it turns out. Tether is responsible. But first;\\n\\nThere is way to make a cryptocurrency work, you have to have an entity that takes dollars and give you crypto of equal value and vice versa (take the crypto and return you dollars). This is called a **bank** and the value exchanged are called **banknotes** and it's recreating the 18th century banking system digitally yet again. This can work but one of three things have to happen. Regardless of legality or country. these concepts are transferable, so either you;\\n\\n1. have regulation and enforcement money laundering laws (and everything else) in which case you have a system that ends up being no cheaper to operate than VISA or Paypal so whats the point\\n2. you can have what's known as a [wild cat bank](https://en.wikipedia.org/wiki/Wildcat_banking) which is classically a bank that print banknotes but the term can be modernized with any form of value, but wildcats are actually not [backed-up](https://www.investopedia.com/terms/b/back-up.asp), again, 18th century banking.\\n3. you can have something like an [liberty reserve](https://www.wikiwand.com/en/Liberty_Reserve) where they actually had a backup for their reserve which differentiates itself from the descendant (not backed-up) Crypto. They didn't follow the money laundering laws which is the reason they're not operating today, but you can learn from their mistake.\\n\\nSo now we've set the stage;\\n\\n## Tether\\n\\nTether is technically a Cryptocurrency, but unlike others it _promises_ to be backed by dollars. The problem is is this is almost certainly a wildcat bank because they manage to produce 2 billion dollars in the space of a few months and they are tied to a Bitcoin exchange that is otherwise cut off from banking.\\n\\nThe core reason why the Bitcoin price exploded was because this is what enables most of the Bitcoin exchanges to operate, very few (if any) of the crypto exchanges actually are connected to any fiat banking systems.\\nCoinbase and Gemini as the two well known exchanges that will take your fiat (how exactly they connect to a banking system is clouded in secrecy, smoke, and mirrors), the rest of them pretty much you can't actually transfer fiat money in or out of them, they use various investment methods have what is called a _\\\"Pay out currency\\\"_ which is just your desired crypto and it's wallet address, you don't get fiat money.\\n\\n## ICO\\n\\nEvery Initial Coin Offering (ICO) should be regarded as a scam, yes i said it, here's why.\\n\\nIn the  U.S. banking system and treasury there's this thing called a security that legally describe an ICO, in fact most countries recognise securities. However an ICO is an _unregistered_ security in legal terms, and there is so much precedence i don't have the space to link to them all.\\n\\nOkay we all fell for the ICO because we fell in love with the Ripple company, and then we figured out XRP [the ripple never-ending ICO we now hate](https://www.bloomberg.com/news/articles/2018-05-04/ripple-hit-with-class-action-suit-over-never-ending-ico) has nothing to do with Ripple the company, not even a little bit. The ICO was (is) essentially a money grab that perhaps a small amount of the proceeds were used for Ripple the company but this is entirely discretionary at a personal level with zero obligation apart from public pressure.\\n\\nOn the other side of the spectrum we have companies like Winding Tree, who before the February 2018 ICO I felt passionate about fundamentally, and now I fear the ICO folly will eventually come back to haunt them. To their credit they have addressed certain issues like fake hotels and fiat currency fluctuations among other concerns in their white papers at a high-level using pseudo-tech language that investors really like (but actual technical people find hard to swallow) so it just comes down to the fact Ethereum is subject to POWH, batch overflow bugs, flaws like the \\\"i accidentally killed it\\\" by devops199 or the DOW race condition, and more hard-forks for any arbitrary reason they want (because we can. i.e. not decentralised, not immutable).\\n\\nIf you're familiar smart contracts or not, this is arguably the most popular driver behind ICOs and a really bad idea.\\n\\n## Smart contracts\\n\\nWith regular contracts you have this exception handling mechanism called a judge and the legal system. If I can walk up to a smart contract and say give me all your money and it does, is that even called theft? In the real world it would but not for smart contracts. The \\\"i accidentally killed it\\\" flaw shows how it is possible to burn someone else's money without permission figuratively speaking, a highly illegal act. \\n\\nSmart contracts are intended to be stadardised in that they written in a formal language (normally this is called legalese) but they eliminate the exception handling mechanism and then requires that the code be bug free (the usual I can assure you that we've looked at the code and it's bug free statement applies here).\\nAnd the notion that code is law and there's no central authorities, no way to undo things, with basically revealed to be a transparent lie when it's their money on the line (DOW race condition).\\n\\nThere are so many issues with Parity Multisig Wallets like the \\\"i accidentally killed it\\\" flaw, means wallets should never be on internet connected devices, ever. And these are considered cryptographically strong, it's the Smart contract that was at fault. \\n\\nYou need Smart contracts to be one of two things; be perfect from the beginning (right...) or be upgradable (immutable?..) with trust the programmer/s (central authority?..) do not cause further damage and keep the values in their programming outcomes.\\n\\nBottom-line, if a central authority can destroy _\\\"hack created\\\"_ smart contracts, they have means to destroy or otherwise alter and fabricate value and you have given them your approval to do this to you inherently when you agreed to the cetralised trust model (bet you didn't think of that), irony? legal? We'll see.\\nArguably you can arrange what is poorly named a 51 percent attack to change things however you see fit, it is not actually an attack but actually a design flaw and doing so is not really a bug or an exploit but it certainly will make the central authority irate to the point they'll just hard-fork and you've wasted your effort.\\nUltimately all the power belongs to the select few at the top, all the profit also sits at the top with the earliest adopters. Does this sound familiar? It should because this describes precisely a pyramid-scheme.\\n\\n## Conclusion\\n\\nThis turned out to be a negative toned post.. That just shows how amateurish Crypto is currently.\\n\\nAs I mentioned I'm a true believer in Crypto, but the technology that embodies it's values hasn't been invent yet. What we have today is a utopia for scam artists, criminals, and privacy paranoid.\\n\\nIf implemented correctly we may one day realise a truly fair decentralised and secure virtual currency that is adopted throughout humanity, whether that is trust based like past attempts, privacy focused like some of us dream, or a perfect system from the start I doubt, but we can hope.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Tether has become the defacto reserve currency as the hundreds of different cryptocurrencies are actually created and exchanged on it.</p>\n<p>I'll touch on Tether and go into Ransomware briefly, as these are the reason for the post. The I'll deep dive on Cryptocurrency because there is actually three totally separate concepts to address: the concept of the cryptocurrencies themselves, public blockchain, and then there's the concept of the private or permissions blockchain.</p>\n<h2 id=\"tetherandbitcoin\">Tether and Bitcoin</h2>\n<p>If you look at Bitcoin trading volume most of it is not actually being exchanged on fiat but these notional cryptocurrencies (refer to crypto as notional cryptocurrencies) that are not <a href=\"https://www.investopedia.com/terms/b/back-up.asp\">backed-up</a>, questionably legal, and not in the wider public interest (allow me to explain).</p>\n<p>It is surprising to me that it has lasted this long, reached the market cap it has, and the blind average person will invest without question.</p>\n<blockquote>\n<p>Scam artists have learned a few new tricks this decade.</p>\n</blockquote>\n<p>I can only imagine that in the next decade people will generally look back in disgust of themselves at the wasted investment and energy, worse then any time in history, or look back with victorious greed at how fantastically evil and clever they were.</p>\n<p><strong>I digress</strong></p>\n<p>There is (or rather was) a <a href=\"https://en.wikipedia.org/wiki/Selection_bias\">selection bias</a> in crypto where it attracted only true believers like myself. Some people would look at it and think this is garbage and ignore it, or become true believers. There was no economic model for somebody to look at crypto and say it is a scam, but maybe keep looking at the field in case it shows promise (except a few academics maybe). You were a true believer or not interested.<br>\nThis was great for an immature and developing community.<br>\nHowever It's no longer harm-limited to a small population of self-inflicted believers, it is spilling over to the average person. Segue into Ransomware.</p>\n<h1 id=\"ransomware\">Ransomware</h1>\n<p>About 4-5 years ago we had the first Ransomware epidemic and I believe it was a person name Giovanni who famously introduced it to us, he was able to somehow get control of one of these Ransomware server infrastructure and found at the time you can pay with the either Bitcoin or the greendot moneypak, and effectively everybody paid with greendot because you could walk into 7-Eleven buy a moneypak and get your data back.<br>\nThis ended up disappearing when the U.S. treasury forced greendot to clean up money laundering and now when you register greendot cards you must provide your Social Security number and there is no longer illegal harvesting of greendot cards for illegal value exchange. This disrupted ransomware for a while, but now it has come back with crypto solely. I don't really care about the drug dealers or any of that, Silk Road was entertaining to watch, but the ransomware epidemic is affecting real people who actually are innocent victims.</p>\n<p>Fortunately and controversially I believe the nefarious parts of the cryptocurrency space can die with proper application of regulation because of how the regulations already are in most countries.</p>\n<p>It's become important for me to advocate for the need to clean up this space as an early true believer and as an InfoSec professional. The current state of crypto doesn't provide benefit to society, just those who are interested in committing crimes with a very thin slither of an exception to the few off-the-grid purist privacy-minded who only benefit when mass-adoption is achieved or they have a less privacy concerned middle-man that takes payment as crypto from you to provide goods when you need from society (a compromise in-itself), so even such privacy people get no benefits of crypto unless we all adopt it (and they need no compromise) as our primary trade currency.</p>\n<p>So we have 2 groups; criminals, and the rest of us who don't benefit yet regardless of our beliefs.</p>\n<h2 id=\"privateblockchain\">Private Blockchain</h2>\n<p>So Blockchains are distributed append only ledgers, but what is a private or permissioned blockchain?</p>\n<p>Simply it is still an append only data structure, just with a limited number of authorised writers. a.k.a. <a href=\"https://www.wikiwand.com/en/Write_once_read_many\">a get archive or WORM</a>.<br>\nThere is nothing nothing fundamental in a private blockchain that's hasn't been understood in the field for 20+ years, and even the truly paranoid had <a href=\"https://www.wikiwand.com/en/Tarsnap\">tarsnap for 10+ years now</a> with the same characteristics, it's just that now crypto has a buzz word <strong>Blockchain</strong> that causes idiots to throw money at the problem.</p>\n<blockquote>\n<p>Crypto has a buzz word Blockchain that causes idiots to throw money at the problem.</p>\n</blockquote>\n<p>So if you see a private or permission blockchain project it means either one of two things; either it's a delusional piece techno-utopianism or somebody smart in IT knows that there real problems with what data you store, how you access it, data provenance and all this other stuff. and they have banded around this buzzword because idiots up management will now throw money at him to solve the said real interesting hard problems with a new label.</p>\n<h2 id=\"publicblockchain\">Public Blockchain</h2>\n<p>So the public blockchains are a global data structure where the idea is there's some sort of centralised point of trust but anybody can append to these systems, so not actually distributed as advertised. Distributed in a sense the data is duplicated to everyone but the trust design decisions are still a centralised point or you have no security, and what is security without trust?</p>\n<p>A little on the distributed topic, it is well covered that the Bitcoin blockchain is effectively controlled by only three entities, but in an attempt to be distributed somehow there's this religious notion that being a distributed trust is somehow good and in of itself. The result is systems that are either grossly insufficient or insecure.<br>\nThe biggest tool that's used for this is called proof of work (best described as proof-of-waste), the idea of POW is that for somebody to rewrite the history they have to do as much useless work as was done to create the history in the first place so as to make it too hard to do and that work is more economicly put to creating value legitimately. Although we have seen a lot of 51% attacks lately I won't digress into these, or how the crypto developers can wipe out ownership of thier blockchain if something they don't like occurs. I will talk about how POW is great if your goal is to do a lot of useless work, though inefficient. Though it is more interesting to look at how to make the system efficient and don't do a lot of useless work, you run into the problem of now you actually don't have any real protections any more.<br>\nFor example Bitcoin, proof of work is <em>&quot;paid for&quot;</em> and ends up using as much power as many major cities and that's just an obscene waste of energy to claim there's any benefits worth that.<br>\nSo far a distributed public append only ledger has only been useful for cryptocurrencies regardless of any individual concern of security, privacy, distrubted ledgers, any of that, if it's public it's limited use period.</p>\n<h2 id=\"cryptocurrency\">Cryptocurrency</h2>\n<p>So although called cryptocurrency, they don't actually work as <em>&quot;currency&quot;</em>, they are provably inferior and can never be superior to the alternative fiat or real world payments like VISA, unless you need what is known as <strong>censorship resistance</strong>, and then you might have an argument for crypto. So let's look at censorship resistance.</p>\n<p>The only way to do a censorship resistant transaction without a cryptocurrencies cash, and cash requires physical proximity and mass (a million dollars in US dollars weighs 10KG). So for modern censorship resistant transactions, let's do a direct peer-to-peer payment system (make it public so as to appear legitimate) but make it so that there are no central intermediaries, but it must be solely electronically.<br>\nEnter cryptocurrencies.</p>\n<p>This has been used quite practically for illegal purposes.</p>\n<p>But if I, as an average law-abiding person, want to do any payments that is one that the central authorities will actually process, then crypto provably do not work as I have to turn my dollars into crypto because I don't want to always keep it in crypto because the prices jumping up and mostly down at the moment, that's expensive, then I transfer the crypto (that's relatively cheap right now but it's been upwards of $30 in the past even for sub-dollar transactions), and then the recipient on the other side has to convert the crypto back into dollars. You have these mandatory currency conversion steps for any real world transaction using crypto, and even the public companies that used to embraces cryptocurrency (most have backed out including the Bitcoin conferences) only keep a few hundred thousand dollars worth of cryptocurrency when they operated, they are mostly converting crypto to dollars themselves.</p>\n<p>Crypto does not work for legitimate purchases, but suppose you believe in the vision of the great Satoshi; you don't want to use crypto either because even his monetary policy is that crypto are designed to be deflationary (<a href=\"https://themerkle.com/cryptocurrency-inflation-vs-deflation/\">JP Buntinx - Cryptocurrency Inflation vs Deflation</a>).</p>\n<blockquote>\n<p>The first rule of a deflationary currency is never spend your deflationary currency!</p>\n</blockquote>\n<p>So when you purchased that <em>pizza of regret</em> for a 10k Bitcoin that's now worth more then your house. Is that FOMO then when you invested, or a result of hard work by the true believers? Neither it turns out. Tether is responsible. But first;</p>\n<p>There is way to make a cryptocurrency work, you have to have an entity that takes dollars and give you crypto of equal value and vice versa (take the crypto and return you dollars). This is called a <strong>bank</strong> and the value exchanged are called <strong>banknotes</strong> and it's recreating the 18th century banking system digitally yet again. This can work but one of three things have to happen. Regardless of legality or country. these concepts are transferable, so either you;</p>\n<ol>\n<li>have regulation and enforcement money laundering laws (and everything else) in which case you have a system that ends up being no cheaper to operate than VISA or Paypal so whats the point</li>\n<li>you can have what's known as a <a href=\"https://en.wikipedia.org/wiki/Wildcat_banking\">wild cat bank</a> which is classically a bank that print banknotes but the term can be modernized with any form of value, but wildcats are actually not <a href=\"https://www.investopedia.com/terms/b/back-up.asp\">backed-up</a>, again, 18th century banking.</li>\n<li>you can have something like an <a href=\"https://www.wikiwand.com/en/Liberty_Reserve\">liberty reserve</a> where they actually had a backup for their reserve which differentiates itself from the descendant (not backed-up) Crypto. They didn't follow the money laundering laws which is the reason they're not operating today, but you can learn from their mistake.</li>\n</ol>\n<p>So now we've set the stage;</p>\n<h2 id=\"tether\">Tether</h2>\n<p>Tether is technically a Cryptocurrency, but unlike others it <em>promises</em> to be backed by dollars. The problem is is this is almost certainly a wildcat bank because they manage to produce 2 billion dollars in the space of a few months and they are tied to a Bitcoin exchange that is otherwise cut off from banking.</p>\n<p>The core reason why the Bitcoin price exploded was because this is what enables most of the Bitcoin exchanges to operate, very few (if any) of the crypto exchanges actually are connected to any fiat banking systems.<br>\nCoinbase and Gemini as the two well known exchanges that will take your fiat (how exactly they connect to a banking system is clouded in secrecy, smoke, and mirrors), the rest of them pretty much you can't actually transfer fiat money in or out of them, they use various investment methods have what is called a <em>&quot;Pay out currency&quot;</em> which is just your desired crypto and it's wallet address, you don't get fiat money.</p>\n<h2 id=\"ico\">ICO</h2>\n<p>Every Initial Coin Offering (ICO) should be regarded as a scam, yes i said it, here's why.</p>\n<p>In the  U.S. banking system and treasury there's this thing called a security that legally describe an ICO, in fact most countries recognise securities. However an ICO is an <em>unregistered</em> security in legal terms, and there is so much precedence i don't have the space to link to them all.</p>\n<p>Okay we all fell for the ICO because we fell in love with the Ripple company, and then we figured out XRP <a href=\"https://www.bloomberg.com/news/articles/2018-05-04/ripple-hit-with-class-action-suit-over-never-ending-ico\">the ripple never-ending ICO we now hate</a> has nothing to do with Ripple the company, not even a little bit. The ICO was (is) essentially a money grab that perhaps a small amount of the proceeds were used for Ripple the company but this is entirely discretionary at a personal level with zero obligation apart from public pressure.</p>\n<p>On the other side of the spectrum we have companies like Winding Tree, who before the February 2018 ICO I felt passionate about fundamentally, and now I fear the ICO folly will eventually come back to haunt them. To their credit they have addressed certain issues like fake hotels and fiat currency fluctuations among other concerns in their white papers at a high-level using pseudo-tech language that investors really like (but actual technical people find hard to swallow) so it just comes down to the fact Ethereum is subject to POWH, batch overflow bugs, flaws like the &quot;i accidentally killed it&quot; by devops199 or the DOW race condition, and more hard-forks for any arbitrary reason they want (because we can. i.e. not decentralised, not immutable).</p>\n<p>If you're familiar smart contracts or not, this is arguably the most popular driver behind ICOs and a really bad idea.</p>\n<h2 id=\"smartcontracts\">Smart contracts</h2>\n<p>With regular contracts you have this exception handling mechanism called a judge and the legal system. If I can walk up to a smart contract and say give me all your money and it does, is that even called theft? In the real world it would but not for smart contracts. The &quot;i accidentally killed it&quot; flaw shows how it is possible to burn someone else's money without permission figuratively speaking, a highly illegal act.</p>\n<p>Smart contracts are intended to be stadardised in that they written in a formal language (normally this is called legalese) but they eliminate the exception handling mechanism and then requires that the code be bug free (the usual I can assure you that we've looked at the code and it's bug free statement applies here).<br>\nAnd the notion that code is law and there's no central authorities, no way to undo things, with basically revealed to be a transparent lie when it's their money on the line (DOW race condition).</p>\n<p>There are so many issues with Parity Multisig Wallets like the &quot;i accidentally killed it&quot; flaw, means wallets should never be on internet connected devices, ever. And these are considered cryptographically strong, it's the Smart contract that was at fault.</p>\n<p>You need Smart contracts to be one of two things; be perfect from the beginning (right...) or be upgradable (immutable?..) with trust the programmer/s (central authority?..) do not cause further damage and keep the values in their programming outcomes.</p>\n<p>Bottom-line, if a central authority can destroy <em>&quot;hack created&quot;</em> smart contracts, they have means to destroy or otherwise alter and fabricate value and you have given them your approval to do this to you inherently when you agreed to the cetralised trust model (bet you didn't think of that), irony? legal? We'll see.<br>\nArguably you can arrange what is poorly named a 51 percent attack to change things however you see fit, it is not actually an attack but actually a design flaw and doing so is not really a bug or an exploit but it certainly will make the central authority irate to the point they'll just hard-fork and you've wasted your effort.<br>\nUltimately all the power belongs to the select few at the top, all the profit also sits at the top with the earliest adopters. Does this sound familiar? It should because this describes precisely a pyramid-scheme.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This turned out to be a negative toned post.. That just shows how amateurish Crypto is currently.</p>\n<p>As I mentioned I'm a true believer in Crypto, but the technology that embodies it's values hasn't been invent yet. What we have today is a utopia for scam artists, criminals, and privacy paranoid.</p>\n<p>If implemented correctly we may one day realise a truly fair decentralised and secure virtual currency that is adopted throughout humanity, whether that is trust based like past attempts, privacy focused like some of us dream, or a perfect system from the start I doubt, but we can hope.</p>\n<!--kg-card-end: markdown-->","comment_id":"5b15df06d3e4d005b0d599c4","plaintext":"Tether has become the defacto reserve currency as the hundreds of different\ncryptocurrencies are actually created and exchanged on it.\n\nI'll touch on Tether and go into Ransomware briefly, as these are the reason for\nthe post. The I'll deep dive on Cryptocurrency because there is actually three\ntotally separate concepts to address: the concept of the cryptocurrencies\nthemselves, public blockchain, and then there's the concept of the private or\npermissions blockchain.\n\nTether and Bitcoin\nIf you look at Bitcoin trading volume most of it is not actually being exchanged\non fiat but these notional cryptocurrencies (refer to crypto as notional\ncryptocurrencies) that are not backed-up\n[https://www.investopedia.com/terms/b/back-up.asp], questionably legal, and not\nin the wider public interest (allow me to explain).\n\nIt is surprising to me that it has lasted this long, reached the market cap it\nhas, and the blind average person will invest without question.\n\n> Scam artists have learned a few new tricks this decade.\n\n\nI can only imagine that in the next decade people will generally look back in\ndisgust of themselves at the wasted investment and energy, worse then any time\nin history, or look back with victorious greed at how fantastically evil and\nclever they were.\n\nI digress\n\nThere is (or rather was) a selection bias\n[https://en.wikipedia.org/wiki/Selection_bias] in crypto where it attracted only\ntrue believers like myself. Some people would look at it and think this is\ngarbage and ignore it, or become true believers. There was no economic model for\nsomebody to look at crypto and say it is a scam, but maybe keep looking at the\nfield in case it shows promise (except a few academics maybe). You were a true\nbeliever or not interested.\nThis was great for an immature and developing community.\nHowever It's no longer harm-limited to a small population of self-inflicted\nbelievers, it is spilling over to the average person. Segue into Ransomware.\n\nRansomware\nAbout 4-5 years ago we had the first Ransomware epidemic and I believe it was a\nperson name Giovanni who famously introduced it to us, he was able to somehow\nget control of one of these Ransomware server infrastructure and found at the\ntime you can pay with the either Bitcoin or the greendot moneypak, and\neffectively everybody paid with greendot because you could walk into 7-Eleven\nbuy a moneypak and get your data back.\nThis ended up disappearing when the U.S. treasury forced greendot to clean up\nmoney laundering and now when you register greendot cards you must provide your\nSocial Security number and there is no longer illegal harvesting of greendot\ncards for illegal value exchange. This disrupted ransomware for a while, but now\nit has come back with crypto solely. I don't really care about the drug dealers\nor any of that, Silk Road was entertaining to watch, but the ransomware epidemic\nis affecting real people who actually are innocent victims.\n\nFortunately and controversially I believe the nefarious parts of the\ncryptocurrency space can die with proper application of regulation because of\nhow the regulations already are in most countries.\n\nIt's become important for me to advocate for the need to clean up this space as\nan early true believer and as an InfoSec professional. The current state of\ncrypto doesn't provide benefit to society, just those who are interested in\ncommitting crimes with a very thin slither of an exception to the few\noff-the-grid purist privacy-minded who only benefit when mass-adoption is\nachieved or they have a less privacy concerned middle-man that takes payment as\ncrypto from you to provide goods when you need from society (a compromise\nin-itself), so even such privacy people get no benefits of crypto unless we all\nadopt it (and they need no compromise) as our primary trade currency.\n\nSo we have 2 groups; criminals, and the rest of us who don't benefit yet\nregardless of our beliefs.\n\nPrivate Blockchain\nSo Blockchains are distributed append only ledgers, but what is a private or\npermissioned blockchain?\n\nSimply it is still an append only data structure, just with a limited number of\nauthorised writers. a.k.a. a get archive or WORM\n[https://www.wikiwand.com/en/Write_once_read_many].\nThere is nothing nothing fundamental in a private blockchain that's hasn't been\nunderstood in the field for 20+ years, and even the truly paranoid had tarsnap\nfor 10+ years now [https://www.wikiwand.com/en/Tarsnap] with the same\ncharacteristics, it's just that now crypto has a buzz word Blockchain that\ncauses idiots to throw money at the problem.\n\n> Crypto has a buzz word Blockchain that causes idiots to throw money at the\nproblem.\n\n\nSo if you see a private or permission blockchain project it means either one of\ntwo things; either it's a delusional piece techno-utopianism or somebody smart\nin IT knows that there real problems with what data you store, how you access\nit, data provenance and all this other stuff. and they have banded around this\nbuzzword because idiots up management will now throw money at him to solve the\nsaid real interesting hard problems with a new label.\n\nPublic Blockchain\nSo the public blockchains are a global data structure where the idea is there's\nsome sort of centralised point of trust but anybody can append to these systems,\nso not actually distributed as advertised. Distributed in a sense the data is\nduplicated to everyone but the trust design decisions are still a centralised\npoint or you have no security, and what is security without trust?\n\nA little on the distributed topic, it is well covered that the Bitcoin\nblockchain is effectively controlled by only three entities, but in an attempt\nto be distributed somehow there's this religious notion that being a distributed\ntrust is somehow good and in of itself. The result is systems that are either\ngrossly insufficient or insecure.\nThe biggest tool that's used for this is called proof of work (best described as\nproof-of-waste), the idea of POW is that for somebody to rewrite the history\nthey have to do as much useless work as was done to create the history in the\nfirst place so as to make it too hard to do and that work is more economicly put\nto creating value legitimately. Although we have seen a lot of 51% attacks\nlately I won't digress into these, or how the crypto developers can wipe out\nownership of thier blockchain if something they don't like occurs. I will talk\nabout how POW is great if your goal is to do a lot of useless work, though\ninefficient. Though it is more interesting to look at how to make the system\nefficient and don't do a lot of useless work, you run into the problem of now\nyou actually don't have any real protections any more.\nFor example Bitcoin, proof of work is \"paid for\" and ends up using as much power\nas many major cities and that's just an obscene waste of energy to claim there's\nany benefits worth that.\nSo far a distributed public append only ledger has only been useful for\ncryptocurrencies regardless of any individual concern of security, privacy,\ndistrubted ledgers, any of that, if it's public it's limited use period.\n\nCryptocurrency\nSo although called cryptocurrency, they don't actually work as \"currency\", they\nare provably inferior and can never be superior to the alternative fiat or real\nworld payments like VISA, unless you need what is known as censorship resistance\n, and then you might have an argument for crypto. So let's look at censorship\nresistance.\n\nThe only way to do a censorship resistant transaction without a cryptocurrencies\ncash, and cash requires physical proximity and mass (a million dollars in US\ndollars weighs 10KG). So for modern censorship resistant transactions, let's do\na direct peer-to-peer payment system (make it public so as to appear legitimate)\nbut make it so that there are no central intermediaries, but it must be solely\nelectronically.\nEnter cryptocurrencies.\n\nThis has been used quite practically for illegal purposes.\n\nBut if I, as an average law-abiding person, want to do any payments that is one\nthat the central authorities will actually process, then crypto provably do not\nwork as I have to turn my dollars into crypto because I don't want to always\nkeep it in crypto because the prices jumping up and mostly down at the moment,\nthat's expensive, then I transfer the crypto (that's relatively cheap right now\nbut it's been upwards of $30 in the past even for sub-dollar transactions), and\nthen the recipient on the other side has to convert the crypto back into\ndollars. You have these mandatory currency conversion steps for any real world\ntransaction using crypto, and even the public companies that used to embraces\ncryptocurrency (most have backed out including the Bitcoin conferences) only\nkeep a few hundred thousand dollars worth of cryptocurrency when they operated,\nthey are mostly converting crypto to dollars themselves.\n\nCrypto does not work for legitimate purchases, but suppose you believe in the\nvision of the great Satoshi; you don't want to use crypto either because even\nhis monetary policy is that crypto are designed to be deflationary (JP Buntinx\n-\nCryptocurrency Inflation vs Deflation\n[https://themerkle.com/cryptocurrency-inflation-vs-deflation/]).\n\n> The first rule of a deflationary currency is never spend your deflationary\ncurrency!\n\n\nSo when you purchased that pizza of regret for a 10k Bitcoin that's now worth\nmore then your house. Is that FOMO then when you invested, or a result of hard\nwork by the true believers? Neither it turns out. Tether is responsible. But\nfirst;\n\nThere is way to make a cryptocurrency work, you have to have an entity that\ntakes dollars and give you crypto of equal value and vice versa (take the crypto\nand return you dollars). This is called a bank and the value exchanged are\ncalled banknotes and it's recreating the 18th century banking system digitally\nyet again. This can work but one of three things have to happen. Regardless of\nlegality or country. these concepts are transferable, so either you;\n\n 1. have regulation and enforcement money laundering laws (and everything else)\n    in which case you have a system that ends up being no cheaper to operate\n    than VISA or Paypal so whats the point\n 2. you can have what's known as a wild cat bank\n    [https://en.wikipedia.org/wiki/Wildcat_banking] which is classically a bank\n    that print banknotes but the term can be modernized with any form of value,\n    but wildcats are actually not backed-up\n    [https://www.investopedia.com/terms/b/back-up.asp], again, 18th century\n    banking.\n 3. you can have something like an liberty reserve\n    [https://www.wikiwand.com/en/Liberty_Reserve] where they actually had a\n    backup for their reserve which differentiates itself from the descendant\n    (not backed-up) Crypto. They didn't follow the money laundering laws which\n    is the reason they're not operating today, but you can learn from their\n    mistake.\n\nSo now we've set the stage;\n\nTether\nTether is technically a Cryptocurrency, but unlike others it promises to be\nbacked by dollars. The problem is is this is almost certainly a wildcat bank\nbecause they manage to produce 2 billion dollars in the space of a few months\nand they are tied to a Bitcoin exchange that is otherwise cut off from banking.\n\nThe core reason why the Bitcoin price exploded was because this is what enables\nmost of the Bitcoin exchanges to operate, very few (if any) of the crypto\nexchanges actually are connected to any fiat banking systems.\nCoinbase and Gemini as the two well known exchanges that will take your fiat\n(how exactly they connect to a banking system is clouded in secrecy, smoke, and\nmirrors), the rest of them pretty much you can't actually transfer fiat money in\nor out of them, they use various investment methods have what is called a \"Pay\nout currency\" which is just your desired crypto and it's wallet address, you\ndon't get fiat money.\n\nICO\nEvery Initial Coin Offering (ICO) should be regarded as a scam, yes i said it,\nhere's why.\n\nIn the U.S. banking system and treasury there's this thing called a security\nthat legally describe an ICO, in fact most countries recognise securities.\nHowever an ICO is an unregistered security in legal terms, and there is so much\nprecedence i don't have the space to link to them all.\n\nOkay we all fell for the ICO because we fell in love with the Ripple company,\nand then we figured out XRP the ripple never-ending ICO we now hate\n[https://www.bloomberg.com/news/articles/2018-05-04/ripple-hit-with-class-action-suit-over-never-ending-ico] \nhas nothing to do with Ripple the company, not even a little bit. The ICO was\n(is) essentially a money grab that perhaps a small amount of the proceeds were\nused for Ripple the company but this is entirely discretionary at a personal\nlevel with zero obligation apart from public pressure.\n\nOn the other side of the spectrum we have companies like Winding Tree, who\nbefore the February 2018 ICO I felt passionate about fundamentally, and now I\nfear the ICO folly will eventually come back to haunt them. To their credit they\nhave addressed certain issues like fake hotels and fiat currency fluctuations\namong other concerns in their white papers at a high-level using pseudo-tech\nlanguage that investors really like (but actual technical people find hard to\nswallow) so it just comes down to the fact Ethereum is subject to POWH, batch\noverflow bugs, flaws like the \"i accidentally killed it\" by devops199 or the DOW\nrace condition, and more hard-forks for any arbitrary reason they want (because\nwe can. i.e. not decentralised, not immutable).\n\nIf you're familiar smart contracts or not, this is arguably the most popular\ndriver behind ICOs and a really bad idea.\n\nSmart contracts\nWith regular contracts you have this exception handling mechanism called a judge\nand the legal system. If I can walk up to a smart contract and say give me all\nyour money and it does, is that even called theft? In the real world it would\nbut not for smart contracts. The \"i accidentally killed it\" flaw shows how it is\npossible to burn someone else's money without permission figuratively speaking,\na highly illegal act.\n\nSmart contracts are intended to be stadardised in that they written in a formal\nlanguage (normally this is called legalese) but they eliminate the exception\nhandling mechanism and then requires that the code be bug free (the usual I can\nassure you that we've looked at the code and it's bug free statement applies\nhere).\nAnd the notion that code is law and there's no central authorities, no way to\nundo things, with basically revealed to be a transparent lie when it's their\nmoney on the line (DOW race condition).\n\nThere are so many issues with Parity Multisig Wallets like the \"i accidentally\nkilled it\" flaw, means wallets should never be on internet connected devices,\never. And these are considered cryptographically strong, it's the Smart contract\nthat was at fault.\n\nYou need Smart contracts to be one of two things; be perfect from the beginning\n(right...) or be upgradable (immutable?..) with trust the programmer/s (central\nauthority?..) do not cause further damage and keep the values in their\nprogramming outcomes.\n\nBottom-line, if a central authority can destroy \"hack created\" smart contracts,\nthey have means to destroy or otherwise alter and fabricate value and you have\ngiven them your approval to do this to you inherently when you agreed to the\ncetralised trust model (bet you didn't think of that), irony? legal? We'll see.\nArguably you can arrange what is poorly named a 51 percent attack to change\nthings however you see fit, it is not actually an attack but actually a design\nflaw and doing so is not really a bug or an exploit but it certainly will make\nthe central authority irate to the point they'll just hard-fork and you've\nwasted your effort.\nUltimately all the power belongs to the select few at the top, all the profit\nalso sits at the top with the earliest adopters. Does this sound familiar? It\nshould because this describes precisely a pyramid-scheme.\n\nConclusion\nThis turned out to be a negative toned post.. That just shows how amateurish\nCrypto is currently.\n\nAs I mentioned I'm a true believer in Crypto, but the technology that embodies\nit's values hasn't been invent yet. What we have today is a utopia for scam\nartists, criminals, and privacy paranoid.\n\nIf implemented correctly we may one day realise a truly fair decentralised and\nsecure virtual currency that is adopted throughout humanity, whether that is\ntrust based like past attempts, privacy focused like some of us dream, or a\nperfect system from the start I doubt, but we can hope.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/06/bitcoin-ethereum-ripple-tether.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-06-05 00:53:26","created_by":"1","updated_at":"2021-03-31 14:01:59","updated_by":"1","published_at":"2018-06-03 11:50:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcbe","uuid":"bdf26dcc-725b-4bf7-a166-ea01745e725b","title":"ASD Essential Eight Mitigation Strategies to Detect Cyber Security Incidents and Respond","slug":"asd-essential-eight-mitigation-strategies-to-detect-cyber-security-incidents-and-respond","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The Australian Cyber Security Centre (ACSC) has developed The Essential Eight which are mitigation strategies that organisation's can use to produce a modern risk profile and response plan for todays cyber threats.\\n\\nMy post will help you align these best practices to your solution as a baseline making it much harder for adversaries to compromise your systems.\\n\\n# Preperation\\n\\nBefore implementing any of the mitigation strategies, you should consider the following;\\n\\n### Identify which systems require protection\\n\\nWhich systems store, process, or communicate sensitive information or other information with a high availability requirement.\\n\\nThese Systems are usually in-scope for compliance to a standard such as ISO, PCI, or other regulatory obligations such as those in health care or insurance.\\n\\n## Identify which adversaries are most likely to target their systems\\n\\nThis is your typical Threat Matrix used in any Risk Assessment For Australian Agency or organisation that interacts with Agencies, it's likely you're already conducting Assessments these for compliance to the Privacy Act, APRA, or iRAP. If you've yet to complete a Threat Matrix or Risk Assessment I highly encourage you to now, as it will get you thinking about threats unique to your organisation.\\n\\n## Identify what level of protection is required\\n\\nSelecting mitigation strategies to implement based on the risks to business activities from specific cyber threats. This is at a high level rather than the actual controls that need to be implemented.\\n\\nThe Essential Eight will provide you the controls necessary, however the strategy to implement these will differ for each organisation, and may even change over time as your risk appetite changes and your security posture matures.\\n\\n# Essential Eight Explained\\n\\nThe Essential Eight are application whitelisting, patching applications, configuring macro settings, application hardening, restricting administrative privileges, patching operating systems, multi-factor authentication, and daily backups.\\n\\n## Mitigation strategies to prevent malware delivery and execution\\n\\nComputer programs designed to infiltrate and damage computers without the users consent. Malware is the general term covering all the different types of threats to your computer safety such as viruses, spyware, worms, trojans, and rootkits to name a few.\\n\\nTHe following controls from The Essential Eight are designed to protect your organisation from Malware.\\n\\n### Application Whitelisting\\n\\nWith this control all non-approved applications (including malicious code) are prevented from executing.\\n\\nApproved application whitelisting of trusted programs to prevent execution of unapproved/malicious programs including .exe, DLL, scripts (e.g. Windows Script Host, PowerShell and HTA) and installers.\\n\\n### Patching Applications\\n\\nUse the latest version of applications.\\nSecurity vulnerabilities in applications can be used to execute malicious code on systems.\\n\\nPatch applications e.g. Flash, web browsers, Microsoft Office, Java and PDF viewers. Patch/mitigate computers with _extreme risk_ vulnerabilities within 48 hours.\\n\\n### Configuring macro settings\\n\\nMicrosoft Office macros can be used to deliver and execute malicious code on systems.\\n\\nConfigure Microsoft Office macro settings to block macros from the Internet, and only allow vetted macros either in _trusted locations_ with limited write access or digitally signed with a trusted certificate.\\n\\n### Application Hardening\\n\\nFlash, ads and Java are popular ways to deliver and execute malicious code on systems.\\n\\nConfigure web browsers to block Flash (ideally uninstall it), ads and Java on the Internet. Disable unneeded features in Microsoft Office (e.g. OLE), web browsers and PDF viewers.\\n\\n## Mitigation strategies to limit the extent of cyber security incidents\\n\\nConstantly evolving cyber attacks challenge your organization to keep up with quickly emerging security gaps that leave you exposed to bad actors despite the safeguards and mitigation measures you may have in place.\\n\\nThe following controls are designed to minimise the impacts after an incident, and reduce the likelihood you are at risk of further threats.\\n\\n### Restricting Administrative Privileges\\n\\nAdmin accounts are the _keys to the kingdom_. Adversaries use these accounts to gain full access to information and systems.\\n\\nRestrict administrative privileges to operating systems and applications based on user duties. Regularly revalidate the need for privileges. Don't use privileged accounts for reading email and web browsing.\\n\\n### Patching Operating Systems\\n\\nSecurity vulnerabilities in operating systems can be used to further the compromise of systems.\\n\\nPatch operating systems to mitigate vulnerabilities within computers, including network devices with extreme risk.\\nUse the latest operating system version. Don't use unsupported versions. \\n\\n### Multi-factor Authentication\\n\\nStronger user authentication makes it harder for adversaries to access sensitive information and systems.\\n\\nMulti-factor authentication including for VPNs, RDP, SSH and other remote access, and for all users when they perform a privileged action or access an important, sensitive, high-availability data repository.\\n\\n## Mitigation strategies to recover data and system availability\\n\\nWhile you may have a mature Business Continuity Plan (BCP), when your systems are compromised this may work against you. When compromised systems are identified you'll want to have a well practiced Disaster Recovery (DR) strategy and follow an incident response plan.\\n\\nIf you are not yet familiar with incident response and you have an immediate need to implement one, here are the basics;\\n\\n1. **Do not** power down compromised systems, you will lose forensic evidence.\\n2. Immediately segregate the effected system\\n3. Start taking impromptu backups of any data, files, disks, that were accessible to the effected system and follow steps 1 & 2 for each system as they are identified.\\n4. Take a memory dump of each system\\n\\nAfter you've completed these steps effected systems may be powered off, however if you've segregated these successfully they pose no further risk and you may find later that you had them running still.\\n\\nIn AWS EC2 it's a good idea to Tag these to let other know they are compromised, use the _Name_ tag for best effect and notify all AWS users of the incident. Remove any associated IAM role, Security Group, and assign a new subnet specifically created to isolate the instance (it has no routes in or out).\\n\\n### Daily Backups\\n\\nTo ensure information can be accessed again following a cyber security incident (e.g. after a successful ransomware incident).\\n\\nDaily backups of important new/changed data, software and configuration settings, stored disconnected, retained for at least three months. Test restoration initially, annually and when IT infrastructure changes.\\n\\nThis is a baseline, your own DR strategy may require more granular approach.\\n\\n# Can your organisation answer these questions?\\n\\n## Event Logging\\n\\n**Have we configured workstations to log events to a central server? Could we provide, at a minimum, the last three months’ worth of these logs to incident response analysts?**\\n\\nProviding logging data will assist incident response analysts to establish the cause, extent and duration of the compromise.\\n\\n- **Workstation logs**\\nApplication whitelisting logs\\nEvent logs\\nAnti-virus logs\\nFirewall logs\\nAuthentication logs\\n\\n- **Network logs**\\nProxy logs\\nDHCP logs\\nDNS logs\\nVPN logs\\nFirewall logs\\nNetwork device logs\\n\\n- **Server logs**\\nMail server logs\\nAuthentication server\\nWeb server access\\nRemote access servers\\n\\n## Roles and responsibilities\\n\\n**Have we documented incident response policies and procedures for cyber security incident response?**\\n\\nDefining your policies and procedures, and making staff aware of them, will give you the best chance of a rapid and coordinated response.\\n\\n**Do our staff understand their incident response roles and responsibilities?**\\n\\nWhat good is having an incident response plan if it is not practiced and staff are aware of their responsibilities int he event of a breach.\\n\\n**Does our service provider understand its roles and responsibilities in the event of an incident?**\\n\\nThis is usually identified during Due Diligence or a Vendor Risk Assessment. When an incident occurs within the service provider outside of your own visibility, you have a mandatory obligation to complete a data breach notification (Privacy Act) for which the service provider must adhere to and assist you. If you do not already have this agreement in place and you are concerned that you'll be unable to meet your obligations due to a non-Australian service provider being unprepared to provide support to your organisation in the event of a data breach, now is the time to address your contract and urge them to agree to appropriate notification and forensic investigation support.\\n\\n## Contact details for your organisation\\n\\n**Does our organisation have a current OnSecure account with correct contact details for our Information Technology Security Adviser?**\\n\\nProviding up-to-date details will allow ASD to quickly contact the right person in your organisation. Furthermore, OnSecure is where ASD posts and publishes Alerts on significant threats as well as Protect publications and advice that your organisation will need to keep up to date with in order to respond to some cyber security incidents.\\n\\n## Initial incident treatment \\n\\n**How quickly can we identify, physically locate and isolate an infected machine on our network? Do we know what our baseline network traffic looks like? Do we have the ability to recognise and assess anomalies in network traffic? Would we pull all plugs on the identified machine, or ensure capture of volatile information for investigation?**\\n\\nA good understanding and sound documentation of your network and all workstations will assist when particular workstations need to be identified quickly. Understanding your network traffic, along with any anomalies when asked, will assist ASD to tailor incident response to your needs. Your organisation may choose to contain the identified machine. In this case, it is important to configure the machine for hibernation and then hibernate rather than fully shutting down the machine. This will preserve valuable volatile artefacts that will be used in investigation of the incident. \\n\\n## Assisting with investigations\\n\\n**Once identified, can our organisation effectively and safely isolate malware and provide it to the Cyber Security Operations Centre (CSOC)?**\\n\\nMalware provided to CSOC is used to prevent the reoccurrence of similar cyber security incidents across government. If you have not setup a CSOC you might need to augment your current capability with outside support, which is an opportunity to develop some repeatable processes you can implement yourself.\\n\\n# Conclusion\\n\\nOnce you have implemented the desired mitigation strategies to your organisation at an initial level, the next steps are to focus on increasing the maturity of the implementation such that you aim to eventually reach full alignment with the intent of each mitigation strategy.\\n\\nIf you're an Amazon Web Services customer [there is a great post here](https://aws.amazon.com/blogs/publicsector/aws-and-the-australian-signals-directorate-essential-eight/) to help you take this 1 step further and implement sane controls in your AWS accounts.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The Australian Cyber Security Centre (ACSC) has developed The Essential Eight which are mitigation strategies that organisation's can use to produce a modern risk profile and response plan for todays cyber threats.</p>\n<p>My post will help you align these best practices to your solution as a baseline making it much harder for adversaries to compromise your systems.</p>\n<h1 id=\"preperation\">Preperation</h1>\n<p>Before implementing any of the mitigation strategies, you should consider the following;</p>\n<h3 id=\"identifywhichsystemsrequireprotection\">Identify which systems require protection</h3>\n<p>Which systems store, process, or communicate sensitive information or other information with a high availability requirement.</p>\n<p>These Systems are usually in-scope for compliance to a standard such as ISO, PCI, or other regulatory obligations such as those in health care or insurance.</p>\n<h2 id=\"identifywhichadversariesaremostlikelytotargettheirsystems\">Identify which adversaries are most likely to target their systems</h2>\n<p>This is your typical Threat Matrix used in any Risk Assessment For Australian Agency or organisation that interacts with Agencies, it's likely you're already conducting Assessments these for compliance to the Privacy Act, APRA, or iRAP. If you've yet to complete a Threat Matrix or Risk Assessment I highly encourage you to now, as it will get you thinking about threats unique to your organisation.</p>\n<h2 id=\"identifywhatlevelofprotectionisrequired\">Identify what level of protection is required</h2>\n<p>Selecting mitigation strategies to implement based on the risks to business activities from specific cyber threats. This is at a high level rather than the actual controls that need to be implemented.</p>\n<p>The Essential Eight will provide you the controls necessary, however the strategy to implement these will differ for each organisation, and may even change over time as your risk appetite changes and your security posture matures.</p>\n<h1 id=\"essentialeightexplained\">Essential Eight Explained</h1>\n<p>The Essential Eight are application whitelisting, patching applications, configuring macro settings, application hardening, restricting administrative privileges, patching operating systems, multi-factor authentication, and daily backups.</p>\n<h2 id=\"mitigationstrategiestopreventmalwaredeliveryandexecution\">Mitigation strategies to prevent malware delivery and execution</h2>\n<p>Computer programs designed to infiltrate and damage computers without the users consent. Malware is the general term covering all the different types of threats to your computer safety such as viruses, spyware, worms, trojans, and rootkits to name a few.</p>\n<p>THe following controls from The Essential Eight are designed to protect your organisation from Malware.</p>\n<h3 id=\"applicationwhitelisting\">Application Whitelisting</h3>\n<p>With this control all non-approved applications (including malicious code) are prevented from executing.</p>\n<p>Approved application whitelisting of trusted programs to prevent execution of unapproved/malicious programs including .exe, DLL, scripts (e.g. Windows Script Host, PowerShell and HTA) and installers.</p>\n<h3 id=\"patchingapplications\">Patching Applications</h3>\n<p>Use the latest version of applications.<br>\nSecurity vulnerabilities in applications can be used to execute malicious code on systems.</p>\n<p>Patch applications e.g. Flash, web browsers, Microsoft Office, Java and PDF viewers. Patch/mitigate computers with <em>extreme risk</em> vulnerabilities within 48 hours.</p>\n<h3 id=\"configuringmacrosettings\">Configuring macro settings</h3>\n<p>Microsoft Office macros can be used to deliver and execute malicious code on systems.</p>\n<p>Configure Microsoft Office macro settings to block macros from the Internet, and only allow vetted macros either in <em>trusted locations</em> with limited write access or digitally signed with a trusted certificate.</p>\n<h3 id=\"applicationhardening\">Application Hardening</h3>\n<p>Flash, ads and Java are popular ways to deliver and execute malicious code on systems.</p>\n<p>Configure web browsers to block Flash (ideally uninstall it), ads and Java on the Internet. Disable unneeded features in Microsoft Office (e.g. OLE), web browsers and PDF viewers.</p>\n<h2 id=\"mitigationstrategiestolimittheextentofcybersecurityincidents\">Mitigation strategies to limit the extent of cyber security incidents</h2>\n<p>Constantly evolving cyber attacks challenge your organization to keep up with quickly emerging security gaps that leave you exposed to bad actors despite the safeguards and mitigation measures you may have in place.</p>\n<p>The following controls are designed to minimise the impacts after an incident, and reduce the likelihood you are at risk of further threats.</p>\n<h3 id=\"restrictingadministrativeprivileges\">Restricting Administrative Privileges</h3>\n<p>Admin accounts are the <em>keys to the kingdom</em>. Adversaries use these accounts to gain full access to information and systems.</p>\n<p>Restrict administrative privileges to operating systems and applications based on user duties. Regularly revalidate the need for privileges. Don't use privileged accounts for reading email and web browsing.</p>\n<h3 id=\"patchingoperatingsystems\">Patching Operating Systems</h3>\n<p>Security vulnerabilities in operating systems can be used to further the compromise of systems.</p>\n<p>Patch operating systems to mitigate vulnerabilities within computers, including network devices with extreme risk.<br>\nUse the latest operating system version. Don't use unsupported versions.</p>\n<h3 id=\"multifactorauthentication\">Multi-factor Authentication</h3>\n<p>Stronger user authentication makes it harder for adversaries to access sensitive information and systems.</p>\n<p>Multi-factor authentication including for VPNs, RDP, SSH and other remote access, and for all users when they perform a privileged action or access an important, sensitive, high-availability data repository.</p>\n<h2 id=\"mitigationstrategiestorecoverdataandsystemavailability\">Mitigation strategies to recover data and system availability</h2>\n<p>While you may have a mature Business Continuity Plan (BCP), when your systems are compromised this may work against you. When compromised systems are identified you'll want to have a well practiced Disaster Recovery (DR) strategy and follow an incident response plan.</p>\n<p>If you are not yet familiar with incident response and you have an immediate need to implement one, here are the basics;</p>\n<ol>\n<li><strong>Do not</strong> power down compromised systems, you will lose forensic evidence.</li>\n<li>Immediately segregate the effected system</li>\n<li>Start taking impromptu backups of any data, files, disks, that were accessible to the effected system and follow steps 1 &amp; 2 for each system as they are identified.</li>\n<li>Take a memory dump of each system</li>\n</ol>\n<p>After you've completed these steps effected systems may be powered off, however if you've segregated these successfully they pose no further risk and you may find later that you had them running still.</p>\n<p>In AWS EC2 it's a good idea to Tag these to let other know they are compromised, use the <em>Name</em> tag for best effect and notify all AWS users of the incident. Remove any associated IAM role, Security Group, and assign a new subnet specifically created to isolate the instance (it has no routes in or out).</p>\n<h3 id=\"dailybackups\">Daily Backups</h3>\n<p>To ensure information can be accessed again following a cyber security incident (e.g. after a successful ransomware incident).</p>\n<p>Daily backups of important new/changed data, software and configuration settings, stored disconnected, retained for at least three months. Test restoration initially, annually and when IT infrastructure changes.</p>\n<p>This is a baseline, your own DR strategy may require more granular approach.</p>\n<h1 id=\"canyourorganisationanswerthesequestions\">Can your organisation answer these questions?</h1>\n<h2 id=\"eventlogging\">Event Logging</h2>\n<p><strong>Have we configured workstations to log events to a central server? Could we provide, at a minimum, the last three months’ worth of these logs to incident response analysts?</strong></p>\n<p>Providing logging data will assist incident response analysts to establish the cause, extent and duration of the compromise.</p>\n<ul>\n<li>\n<p><strong>Workstation logs</strong><br>\nApplication whitelisting logs<br>\nEvent logs<br>\nAnti-virus logs<br>\nFirewall logs<br>\nAuthentication logs</p>\n</li>\n<li>\n<p><strong>Network logs</strong><br>\nProxy logs<br>\nDHCP logs<br>\nDNS logs<br>\nVPN logs<br>\nFirewall logs<br>\nNetwork device logs</p>\n</li>\n<li>\n<p><strong>Server logs</strong><br>\nMail server logs<br>\nAuthentication server<br>\nWeb server access<br>\nRemote access servers</p>\n</li>\n</ul>\n<h2 id=\"rolesandresponsibilities\">Roles and responsibilities</h2>\n<p><strong>Have we documented incident response policies and procedures for cyber security incident response?</strong></p>\n<p>Defining your policies and procedures, and making staff aware of them, will give you the best chance of a rapid and coordinated response.</p>\n<p><strong>Do our staff understand their incident response roles and responsibilities?</strong></p>\n<p>What good is having an incident response plan if it is not practiced and staff are aware of their responsibilities int he event of a breach.</p>\n<p><strong>Does our service provider understand its roles and responsibilities in the event of an incident?</strong></p>\n<p>This is usually identified during Due Diligence or a Vendor Risk Assessment. When an incident occurs within the service provider outside of your own visibility, you have a mandatory obligation to complete a data breach notification (Privacy Act) for which the service provider must adhere to and assist you. If you do not already have this agreement in place and you are concerned that you'll be unable to meet your obligations due to a non-Australian service provider being unprepared to provide support to your organisation in the event of a data breach, now is the time to address your contract and urge them to agree to appropriate notification and forensic investigation support.</p>\n<h2 id=\"contactdetailsforyourorganisation\">Contact details for your organisation</h2>\n<p><strong>Does our organisation have a current OnSecure account with correct contact details for our Information Technology Security Adviser?</strong></p>\n<p>Providing up-to-date details will allow ASD to quickly contact the right person in your organisation. Furthermore, OnSecure is where ASD posts and publishes Alerts on significant threats as well as Protect publications and advice that your organisation will need to keep up to date with in order to respond to some cyber security incidents.</p>\n<h2 id=\"initialincidenttreatment\">Initial incident treatment</h2>\n<p><strong>How quickly can we identify, physically locate and isolate an infected machine on our network? Do we know what our baseline network traffic looks like? Do we have the ability to recognise and assess anomalies in network traffic? Would we pull all plugs on the identified machine, or ensure capture of volatile information for investigation?</strong></p>\n<p>A good understanding and sound documentation of your network and all workstations will assist when particular workstations need to be identified quickly. Understanding your network traffic, along with any anomalies when asked, will assist ASD to tailor incident response to your needs. Your organisation may choose to contain the identified machine. In this case, it is important to configure the machine for hibernation and then hibernate rather than fully shutting down the machine. This will preserve valuable volatile artefacts that will be used in investigation of the incident.</p>\n<h2 id=\"assistingwithinvestigations\">Assisting with investigations</h2>\n<p><strong>Once identified, can our organisation effectively and safely isolate malware and provide it to the Cyber Security Operations Centre (CSOC)?</strong></p>\n<p>Malware provided to CSOC is used to prevent the reoccurrence of similar cyber security incidents across government. If you have not setup a CSOC you might need to augment your current capability with outside support, which is an opportunity to develop some repeatable processes you can implement yourself.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>Once you have implemented the desired mitigation strategies to your organisation at an initial level, the next steps are to focus on increasing the maturity of the implementation such that you aim to eventually reach full alignment with the intent of each mitigation strategy.</p>\n<p>If you're an Amazon Web Services customer <a href=\"https://aws.amazon.com/blogs/publicsector/aws-and-the-australian-signals-directorate-essential-eight/\">there is a great post here</a> to help you take this 1 step further and implement sane controls in your AWS accounts.</p>\n<!--kg-card-end: markdown-->","comment_id":"5b25c18ad3e4d005b0d599d8","plaintext":"The Australian Cyber Security Centre (ACSC) has developed The Essential Eight\nwhich are mitigation strategies that organisation's can use to produce a modern\nrisk profile and response plan for todays cyber threats.\n\nMy post will help you align these best practices to your solution as a baseline\nmaking it much harder for adversaries to compromise your systems.\n\nPreperation\nBefore implementing any of the mitigation strategies, you should consider the\nfollowing;\n\nIdentify which systems require protection\nWhich systems store, process, or communicate sensitive information or other\ninformation with a high availability requirement.\n\nThese Systems are usually in-scope for compliance to a standard such as ISO,\nPCI, or other regulatory obligations such as those in health care or insurance.\n\nIdentify which adversaries are most likely to target their systems\nThis is your typical Threat Matrix used in any Risk Assessment For Australian\nAgency or organisation that interacts with Agencies, it's likely you're already\nconducting Assessments these for compliance to the Privacy Act, APRA, or iRAP.\nIf you've yet to complete a Threat Matrix or Risk Assessment I highly encourage\nyou to now, as it will get you thinking about threats unique to your\norganisation.\n\nIdentify what level of protection is required\nSelecting mitigation strategies to implement based on the risks to business\nactivities from specific cyber threats. This is at a high level rather than the\nactual controls that need to be implemented.\n\nThe Essential Eight will provide you the controls necessary, however the\nstrategy to implement these will differ for each organisation, and may even\nchange over time as your risk appetite changes and your security posture\nmatures.\n\nEssential Eight Explained\nThe Essential Eight are application whitelisting, patching applications,\nconfiguring macro settings, application hardening, restricting administrative\nprivileges, patching operating systems, multi-factor authentication, and daily\nbackups.\n\nMitigation strategies to prevent malware delivery and execution\nComputer programs designed to infiltrate and damage computers without the users\nconsent. Malware is the general term covering all the different types of threats\nto your computer safety such as viruses, spyware, worms, trojans, and rootkits\nto name a few.\n\nTHe following controls from The Essential Eight are designed to protect your\norganisation from Malware.\n\nApplication Whitelisting\nWith this control all non-approved applications (including malicious code) are\nprevented from executing.\n\nApproved application whitelisting of trusted programs to prevent execution of\nunapproved/malicious programs including .exe, DLL, scripts (e.g. Windows Script\nHost, PowerShell and HTA) and installers.\n\nPatching Applications\nUse the latest version of applications.\nSecurity vulnerabilities in applications can be used to execute malicious code\non systems.\n\nPatch applications e.g. Flash, web browsers, Microsoft Office, Java and PDF\nviewers. Patch/mitigate computers with extreme risk vulnerabilities within 48\nhours.\n\nConfiguring macro settings\nMicrosoft Office macros can be used to deliver and execute malicious code on\nsystems.\n\nConfigure Microsoft Office macro settings to block macros from the Internet, and\nonly allow vetted macros either in trusted locations with limited write access\nor digitally signed with a trusted certificate.\n\nApplication Hardening\nFlash, ads and Java are popular ways to deliver and execute malicious code on\nsystems.\n\nConfigure web browsers to block Flash (ideally uninstall it), ads and Java on\nthe Internet. Disable unneeded features in Microsoft Office (e.g. OLE), web\nbrowsers and PDF viewers.\n\nMitigation strategies to limit the extent of cyber security incidents\nConstantly evolving cyber attacks challenge your organization to keep up with\nquickly emerging security gaps that leave you exposed to bad actors despite the\nsafeguards and mitigation measures you may have in place.\n\nThe following controls are designed to minimise the impacts after an incident,\nand reduce the likelihood you are at risk of further threats.\n\nRestricting Administrative Privileges\nAdmin accounts are the keys to the kingdom. Adversaries use these accounts to\ngain full access to information and systems.\n\nRestrict administrative privileges to operating systems and applications based\non user duties. Regularly revalidate the need for privileges. Don't use\nprivileged accounts for reading email and web browsing.\n\nPatching Operating Systems\nSecurity vulnerabilities in operating systems can be used to further the\ncompromise of systems.\n\nPatch operating systems to mitigate vulnerabilities within computers, including\nnetwork devices with extreme risk.\nUse the latest operating system version. Don't use unsupported versions.\n\nMulti-factor Authentication\nStronger user authentication makes it harder for adversaries to access sensitive\ninformation and systems.\n\nMulti-factor authentication including for VPNs, RDP, SSH and other remote\naccess, and for all users when they perform a privileged action or access an\nimportant, sensitive, high-availability data repository.\n\nMitigation strategies to recover data and system availability\nWhile you may have a mature Business Continuity Plan (BCP), when your systems\nare compromised this may work against you. When compromised systems are\nidentified you'll want to have a well practiced Disaster Recovery (DR) strategy\nand follow an incident response plan.\n\nIf you are not yet familiar with incident response and you have an immediate\nneed to implement one, here are the basics;\n\n 1. Do not power down compromised systems, you will lose forensic evidence.\n 2. Immediately segregate the effected system\n 3. Start taking impromptu backups of any data, files, disks, that were\n    accessible to the effected system and follow steps 1 & 2 for each system as\n    they are identified.\n 4. Take a memory dump of each system\n\nAfter you've completed these steps effected systems may be powered off, however\nif you've segregated these successfully they pose no further risk and you may\nfind later that you had them running still.\n\nIn AWS EC2 it's a good idea to Tag these to let other know they are compromised,\nuse the Name tag for best effect and notify all AWS users of the incident.\nRemove any associated IAM role, Security Group, and assign a new subnet\nspecifically created to isolate the instance (it has no routes in or out).\n\nDaily Backups\nTo ensure information can be accessed again following a cyber security incident\n(e.g. after a successful ransomware incident).\n\nDaily backups of important new/changed data, software and configuration\nsettings, stored disconnected, retained for at least three months. Test\nrestoration initially, annually and when IT infrastructure changes.\n\nThis is a baseline, your own DR strategy may require more granular approach.\n\nCan your organisation answer these questions?\nEvent Logging\nHave we configured workstations to log events to a central server? Could we\nprovide, at a minimum, the last three months’ worth of these logs to incident\nresponse analysts?\n\nProviding logging data will assist incident response analysts to establish the\ncause, extent and duration of the compromise.\n\n * Workstation logs\n   Application whitelisting logs\n   Event logs\n   Anti-virus logs\n   Firewall logs\n   Authentication logs\n   \n   \n * Network logs\n   Proxy logs\n   DHCP logs\n   DNS logs\n   VPN logs\n   Firewall logs\n   Network device logs\n   \n   \n * Server logs\n   Mail server logs\n   Authentication server\n   Web server access\n   Remote access servers\n   \n   \n\nRoles and responsibilities\nHave we documented incident response policies and procedures for cyber security\nincident response?\n\nDefining your policies and procedures, and making staff aware of them, will give\nyou the best chance of a rapid and coordinated response.\n\nDo our staff understand their incident response roles and responsibilities?\n\nWhat good is having an incident response plan if it is not practiced and staff\nare aware of their responsibilities int he event of a breach.\n\nDoes our service provider understand its roles and responsibilities in the event\nof an incident?\n\nThis is usually identified during Due Diligence or a Vendor Risk Assessment.\nWhen an incident occurs within the service provider outside of your own\nvisibility, you have a mandatory obligation to complete a data breach\nnotification (Privacy Act) for which the service provider must adhere to and\nassist you. If you do not already have this agreement in place and you are\nconcerned that you'll be unable to meet your obligations due to a non-Australian\nservice provider being unprepared to provide support to your organisation in the\nevent of a data breach, now is the time to address your contract and urge them\nto agree to appropriate notification and forensic investigation support.\n\nContact details for your organisation\nDoes our organisation have a current OnSecure account with correct contact\ndetails for our Information Technology Security Adviser?\n\nProviding up-to-date details will allow ASD to quickly contact the right person\nin your organisation. Furthermore, OnSecure is where ASD posts and publishes\nAlerts on significant threats as well as Protect publications and advice that\nyour organisation will need to keep up to date with in order to respond to some\ncyber security incidents.\n\nInitial incident treatment\nHow quickly can we identify, physically locate and isolate an infected machine\non our network? Do we know what our baseline network traffic looks like? Do we\nhave the ability to recognise and assess anomalies in network traffic? Would we\npull all plugs on the identified machine, or ensure capture of volatile\ninformation for investigation?\n\nA good understanding and sound documentation of your network and all\nworkstations will assist when particular workstations need to be identified\nquickly. Understanding your network traffic, along with any anomalies when\nasked, will assist ASD to tailor incident response to your needs. Your\norganisation may choose to contain the identified machine. In this case, it is\nimportant to configure the machine for hibernation and then hibernate rather\nthan fully shutting down the machine. This will preserve valuable volatile\nartefacts that will be used in investigation of the incident.\n\nAssisting with investigations\nOnce identified, can our organisation effectively and safely isolate malware and\nprovide it to the Cyber Security Operations Centre (CSOC)?\n\nMalware provided to CSOC is used to prevent the reoccurrence of similar cyber\nsecurity incidents across government. If you have not setup a CSOC you might\nneed to augment your current capability with outside support, which is an\nopportunity to develop some repeatable processes you can implement yourself.\n\nConclusion\nOnce you have implemented the desired mitigation strategies to your organisation\nat an initial level, the next steps are to focus on increasing the maturity of\nthe implementation such that you aim to eventually reach full alignment with the\nintent of each mitigation strategy.\n\nIf you're an Amazon Web Services customer there is a great post here\n[https://aws.amazon.com/blogs/publicsector/aws-and-the-australian-signals-directorate-essential-eight/] \nto help you take this 1 step further and implement sane controls in your AWS\naccounts.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/06/Essential-Eight-image-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-06-17 02:03:54","created_by":"1","updated_at":"2021-03-31 14:01:01","updated_by":"1","published_at":"2018-06-16 03:59:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcbf","uuid":"0976d8f5-db0f-4b35-b2d8-90ab27bc8d9e","title":"Information Security strategy tips for startups","slug":"information-security-strategy-tips-for-startups","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Information Security is a broad reaching area of concern for your business. In the enterprise world a large component of this is part of their GRC efforts which can be over-whelming for a smaller organisation.\\n\\n> Governance, Risk, and Compliance\\n\\nOne commonality between enterprise and startups is the reliance on vendors. \\nAt the enterprise level you may choose a vendor to take responsibility for certain business functions for various reasons but for startups it is a necessity that you don't reinvent the wheel.\\nAs a startup you must focus efforts on your value prospect, what differentiates you in whatever market you're competing in.\\n\\nFor the purposes of this post I'll use a fictional FinTech startup.\\n\\nI'll also use the lens of Information Security with consideration for Solution Architecture on AWS because\\n> Cloud!\\n\\nOk, so this is essentially DevSecOps for those up-to-date with buzzwords.\\n\\n# Vendor selection\\n\\nCommonly called Software Selection interchangeably.\\n\\nWhile a business process for Vendor selection may include request for information, quotation, proposal, pricing etc, as well as proposal evaluation. These and a boring list of other business oriented activities.\\n\\nIt's up to you if you complete these though I strongly suggest the startup at-least complete the business requirements statement, preferably a formal business case report that conveys a clear message from stakeholders and directors.\\n\\nFor us concerned with Information Security, specifically due diligence, the following activities are required;\\n\\n1. Discover the Vendors legal name and business registration status.\\n   - In Australia [search ASIC's registers](https://asic.gov.au/online-services/search-asics-registers/) and [notices](https://insolvencynotices.asic.gov.au/browsesearch-notices/).\\n2. Do a thorough web and social media search (OSINT) on the company, products, and services\\n3. Perform an analysis on the directors and stakeholders for conflicts of interest, and discover the employees whom you will be working with. Some organisations will supply you the CV of personnel upon request\\n4. Review the vendors public MSA, Privacy Policy, and any legally binding agreements on conduct that may provide additional powers to the vendor with restrictions to your business and its operations\\n\\nSome of these tasks are time consuming so the logical order plays a role here. By the time you get to the point where you are reviewing the legal documents any red flags should have sent you back to your superiors to see if you should short-circuit this due diligence exercise.\\n\\nAdditionally, the review of the legal documents should remain high level. You are conducting due diligence and should avoid the more thorough legal scrutiny yourself.\\nBe sure you seek legal advice before any commitments are made to a vendor regardless of what you identify, however you should review them yourself as part of due diligence as not all red flags are legal oriented, you may find incompatibilities with your business requirements and operations that legal council will not identify, such as a domain registrar having the right to take down your site if they detect user content that violates their policy whether or not you moderate.\\n\\n# Qualitative analysis\\n\\nFor those not familiar with the terminology, put simply this is a process where you look at the qualities of something without applying calculations, which is good for analysing features that are inherently unmeasurable.\\n\\nActivities that will help you document a report are;\\n\\n- Analysis of software functionality or vendor services rendered.\\n- Maintenance and support\\n- SLA (Service Level Agreement)\\n- Interoperability (not just for exit strategy)\\n\\nWhile the findings may appear obvious to you, and in most cases you've done this many times and never needed to create documentation for it. The real benefit recording these activities is you've demonstrated that you applied a high level of analysis to the selection process and transparently followed a business process which applies the business interest. Your employees will also benefit from your efforts as a lot of these tasks are done, documented, and often would need to be done in future after the fact of not documented the first time doubling your efforts overall.\\n\\nAs a FinTech start up you are obligated to present a case to regulatory authority (APRA in Australia), or you may be obligated to pursue a PCI Audit. In both cases you will be required to demonstrate due diligence and risk assessment capability through many types of collateral as evidence.\\n\\n# Risk Assessments\\n\\nA crucial process, but one that is often over looked by small businesses and startups. Once again this seems like a massive effort but it doesn't have to be.\\n\\nThis particular section could mean the difference between success and failure in more areas then you first expect. Besides the benefits for Information Security, demonstrating risk analysis is valuable to your investors and a skill you can use to evaluate future partnerships.\\n\\nThe key to a quicker risk assessment is to apply a widely accepted framework rather than develop your own processes. There are many frameworks and guides but the efficient choice I recommend for small to medium business is the [CIS RAM (Risk Assessment Method)](https://www.cisecurity.org/white-papers/cis-ram-risk-assessment-method/). \\n\\nIf you wish to develop a more mature process beyond the CIS RAM the next easiest step is combining the [CSA CCM (Cloud Security Alliance Cloud Controls Matrix)](https://cloudsecurityalliance.org/working-groups/cloud-controls-matrix/) as a guide on inherent risk analysis, apply effort in [Threat Modeling](https://www.owasp.org/index.php/Threat_Modeling_Cheat_Sheet), and your own [Risk Vulnerability Model](https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:A/AC:H/PR:H/UI:R/S:U/C:N/I:N/A:L). At this stage you might also be interested in the [NIST guides](https://nvd.nist.gov/800-53/Rev4/family/Risk%20Assessment).\\n\\n# Controls Implementation\\nHopefully you've applied a the inherent risk assessment to save you time coming up with a concise set of controls appropriate to your circumstance. It's overkill and wasted effort to apply every control that exists if they're not prioritised based on the threat model.\\nIt is hard for anyone to come up with controls regardless of their experience, so I recommend you check with the controls offered by [CIS](https://www.cisecurity.org) to get started. If you feel the controls aren't meeting your expectations (and you have time) it is also useful to review the various standards and compliance reports to get the brain working and help you come up with more appropriate controls, a good reference is the [PCI DSS RoC](https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2-ROC-Reporting-Template.pdf).\\n\\n# Controls Evaluation\\nControls are the whole reason for your Information Security strategy to exist. It is not worth the effort in implementing them if you have no idea of their effectiveness.\\n\\nA typical evaluation is quite manual however there are tools that automate the task such as [Cloud Conformity](https://www.cloudconformity.com/) and [Qualys](https://www.qualys.com/forms/freescan/) from the outside looking in, or my personal favorite [Lynis](https://cisofy.com/lynis/) when looking at a host.\\n\\nYour controls evaluation report should be conducted periodically, some compliance obligations are prescriptive in this case but if you have none of these obligations you might find it useful to do annually or as a task for a new hire in a senior level position as it is a good way for them to become familiar with your systems and encourage a culture of security.\\n\\nThe goals for documenting the Controls Evaluation are to address each control identified and implemented with a testing procedure and that tests result, a fairly simple process but one that provides a great deal of assurance to the controls effectiveness and your companies security posture.\\n\\nKeep the Controls Evaluation as high level as possible, don't allow any scope-creep to bloat the report for 2 reasons;\\n\\n1. This is the ideal document to provide confidentially to business partners or investors.\\n2. With greater complexity auditors will apply a higher degree of scrutiny.\\n\\nKeeping it simple also makes the task itself more approachable for non-security trained staff, new employees, or for the team member to annually conduct as it should be much less complicated then the risk assessment or due diligence processes.\\n\\n# Conclusion\\n\\nThe formal process is not as important as the businesses ability to provide implementation detail evidence of their security posture and risk appetite.\\n\\nYou aren't going to be faulted for demonstrating through evidence that is showing the results of your efforts. However if you have processes but cannot produce any collateral of them being followed will do more harm to your security posture then if you had no formal processes at all.\\n\\nAs a lean startup, skipping formal processes is unlikely to hold you back from achieving the licensing or compliance you need. If you can show a higher than expected degree of risk mitigation through evidence, then you're practically more secure then most.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Information Security is a broad reaching area of concern for your business. In the enterprise world a large component of this is part of their GRC efforts which can be over-whelming for a smaller organisation.</p>\n<blockquote>\n<p>Governance, Risk, and Compliance</p>\n</blockquote>\n<p>One commonality between enterprise and startups is the reliance on vendors.<br>\nAt the enterprise level you may choose a vendor to take responsibility for certain business functions for various reasons but for startups it is a necessity that you don't reinvent the wheel.<br>\nAs a startup you must focus efforts on your value prospect, what differentiates you in whatever market you're competing in.</p>\n<p>For the purposes of this post I'll use a fictional FinTech startup.</p>\n<p>I'll also use the lens of Information Security with consideration for Solution Architecture on AWS because</p>\n<blockquote>\n<p>Cloud!</p>\n</blockquote>\n<p>Ok, so this is essentially DevSecOps for those up-to-date with buzzwords.</p>\n<h1 id=\"vendorselection\">Vendor selection</h1>\n<p>Commonly called Software Selection interchangeably.</p>\n<p>While a business process for Vendor selection may include request for information, quotation, proposal, pricing etc, as well as proposal evaluation. These and a boring list of other business oriented activities.</p>\n<p>It's up to you if you complete these though I strongly suggest the startup at-least complete the business requirements statement, preferably a formal business case report that conveys a clear message from stakeholders and directors.</p>\n<p>For us concerned with Information Security, specifically due diligence, the following activities are required;</p>\n<ol>\n<li>Discover the Vendors legal name and business registration status.\n<ul>\n<li>In Australia <a href=\"https://asic.gov.au/online-services/search-asics-registers/\">search ASIC's registers</a> and <a href=\"https://insolvencynotices.asic.gov.au/browsesearch-notices/\">notices</a>.</li>\n</ul>\n</li>\n<li>Do a thorough web and social media search (OSINT) on the company, products, and services</li>\n<li>Perform an analysis on the directors and stakeholders for conflicts of interest, and discover the employees whom you will be working with. Some organisations will supply you the CV of personnel upon request</li>\n<li>Review the vendors public MSA, Privacy Policy, and any legally binding agreements on conduct that may provide additional powers to the vendor with restrictions to your business and its operations</li>\n</ol>\n<p>Some of these tasks are time consuming so the logical order plays a role here. By the time you get to the point where you are reviewing the legal documents any red flags should have sent you back to your superiors to see if you should short-circuit this due diligence exercise.</p>\n<p>Additionally, the review of the legal documents should remain high level. You are conducting due diligence and should avoid the more thorough legal scrutiny yourself.<br>\nBe sure you seek legal advice before any commitments are made to a vendor regardless of what you identify, however you should review them yourself as part of due diligence as not all red flags are legal oriented, you may find incompatibilities with your business requirements and operations that legal council will not identify, such as a domain registrar having the right to take down your site if they detect user content that violates their policy whether or not you moderate.</p>\n<h1 id=\"qualitativeanalysis\">Qualitative analysis</h1>\n<p>For those not familiar with the terminology, put simply this is a process where you look at the qualities of something without applying calculations, which is good for analysing features that are inherently unmeasurable.</p>\n<p>Activities that will help you document a report are;</p>\n<ul>\n<li>Analysis of software functionality or vendor services rendered.</li>\n<li>Maintenance and support</li>\n<li>SLA (Service Level Agreement)</li>\n<li>Interoperability (not just for exit strategy)</li>\n</ul>\n<p>While the findings may appear obvious to you, and in most cases you've done this many times and never needed to create documentation for it. The real benefit recording these activities is you've demonstrated that you applied a high level of analysis to the selection process and transparently followed a business process which applies the business interest. Your employees will also benefit from your efforts as a lot of these tasks are done, documented, and often would need to be done in future after the fact of not documented the first time doubling your efforts overall.</p>\n<p>As a FinTech start up you are obligated to present a case to regulatory authority (APRA in Australia), or you may be obligated to pursue a PCI Audit. In both cases you will be required to demonstrate due diligence and risk assessment capability through many types of collateral as evidence.</p>\n<h1 id=\"riskassessments\">Risk Assessments</h1>\n<p>A crucial process, but one that is often over looked by small businesses and startups. Once again this seems like a massive effort but it doesn't have to be.</p>\n<p>This particular section could mean the difference between success and failure in more areas then you first expect. Besides the benefits for Information Security, demonstrating risk analysis is valuable to your investors and a skill you can use to evaluate future partnerships.</p>\n<p>The key to a quicker risk assessment is to apply a widely accepted framework rather than develop your own processes. There are many frameworks and guides but the efficient choice I recommend for small to medium business is the <a href=\"https://www.cisecurity.org/white-papers/cis-ram-risk-assessment-method/\">CIS RAM (Risk Assessment Method)</a>.</p>\n<p>If you wish to develop a more mature process beyond the CIS RAM the next easiest step is combining the <a href=\"https://cloudsecurityalliance.org/working-groups/cloud-controls-matrix/\">CSA CCM (Cloud Security Alliance Cloud Controls Matrix)</a> as a guide on inherent risk analysis, apply effort in <a href=\"https://www.owasp.org/index.php/Threat_Modeling_Cheat_Sheet\">Threat Modeling</a>, and your own <a href=\"https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:A/AC:H/PR:H/UI:R/S:U/C:N/I:N/A:L\">Risk Vulnerability Model</a>. At this stage you might also be interested in the <a href=\"https://nvd.nist.gov/800-53/Rev4/family/Risk%20Assessment\">NIST guides</a>.</p>\n<h1 id=\"controlsimplementation\">Controls Implementation</h1>\n<p>Hopefully you've applied a the inherent risk assessment to save you time coming up with a concise set of controls appropriate to your circumstance. It's overkill and wasted effort to apply every control that exists if they're not prioritised based on the threat model.<br>\nIt is hard for anyone to come up with controls regardless of their experience, so I recommend you check with the controls offered by <a href=\"https://www.cisecurity.org\">CIS</a> to get started. If you feel the controls aren't meeting your expectations (and you have time) it is also useful to review the various standards and compliance reports to get the brain working and help you come up with more appropriate controls, a good reference is the <a href=\"https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2-ROC-Reporting-Template.pdf\">PCI DSS RoC</a>.</p>\n<h1 id=\"controlsevaluation\">Controls Evaluation</h1>\n<p>Controls are the whole reason for your Information Security strategy to exist. It is not worth the effort in implementing them if you have no idea of their effectiveness.</p>\n<p>A typical evaluation is quite manual however there are tools that automate the task such as <a href=\"https://www.cloudconformity.com/\">Cloud Conformity</a> and <a href=\"https://www.qualys.com/forms/freescan/\">Qualys</a> from the outside looking in, or my personal favorite <a href=\"https://cisofy.com/lynis/\">Lynis</a> when looking at a host.</p>\n<p>Your controls evaluation report should be conducted periodically, some compliance obligations are prescriptive in this case but if you have none of these obligations you might find it useful to do annually or as a task for a new hire in a senior level position as it is a good way for them to become familiar with your systems and encourage a culture of security.</p>\n<p>The goals for documenting the Controls Evaluation are to address each control identified and implemented with a testing procedure and that tests result, a fairly simple process but one that provides a great deal of assurance to the controls effectiveness and your companies security posture.</p>\n<p>Keep the Controls Evaluation as high level as possible, don't allow any scope-creep to bloat the report for 2 reasons;</p>\n<ol>\n<li>This is the ideal document to provide confidentially to business partners or investors.</li>\n<li>With greater complexity auditors will apply a higher degree of scrutiny.</li>\n</ol>\n<p>Keeping it simple also makes the task itself more approachable for non-security trained staff, new employees, or for the team member to annually conduct as it should be much less complicated then the risk assessment or due diligence processes.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>The formal process is not as important as the businesses ability to provide implementation detail evidence of their security posture and risk appetite.</p>\n<p>You aren't going to be faulted for demonstrating through evidence that is showing the results of your efforts. However if you have processes but cannot produce any collateral of them being followed will do more harm to your security posture then if you had no formal processes at all.</p>\n<p>As a lean startup, skipping formal processes is unlikely to hold you back from achieving the licensing or compliance you need. If you can show a higher than expected degree of risk mitigation through evidence, then you're practically more secure then most.</p>\n<!--kg-card-end: markdown-->","comment_id":"5b765c15c4513d05ac9599db","plaintext":"Information Security is a broad reaching area of concern for your business. In\nthe enterprise world a large component of this is part of their GRC efforts\nwhich can be over-whelming for a smaller organisation.\n\n> Governance, Risk, and Compliance\n\n\nOne commonality between enterprise and startups is the reliance on vendors.\nAt the enterprise level you may choose a vendor to take responsibility for\ncertain business functions for various reasons but for startups it is a\nnecessity that you don't reinvent the wheel.\nAs a startup you must focus efforts on your value prospect, what differentiates\nyou in whatever market you're competing in.\n\nFor the purposes of this post I'll use a fictional FinTech startup.\n\nI'll also use the lens of Information Security with consideration for Solution\nArchitecture on AWS because\n\n> Cloud!\n\n\nOk, so this is essentially DevSecOps for those up-to-date with buzzwords.\n\nVendor selection\nCommonly called Software Selection interchangeably.\n\nWhile a business process for Vendor selection may include request for\ninformation, quotation, proposal, pricing etc, as well as proposal evaluation.\nThese and a boring list of other business oriented activities.\n\nIt's up to you if you complete these though I strongly suggest the startup\nat-least complete the business requirements statement, preferably a formal\nbusiness case report that conveys a clear message from stakeholders and\ndirectors.\n\nFor us concerned with Information Security, specifically due diligence, the\nfollowing activities are required;\n\n 1. Discover the Vendors legal name and business registration status. * In\n       Australia search ASIC's registers\n       [https://asic.gov.au/online-services/search-asics-registers/] and notices\n       [https://insolvencynotices.asic.gov.au/browsesearch-notices/].\n    \n    \n 2. Do a thorough web and social media search (OSINT) on the company, products,\n    and services\n 3. Perform an analysis on the directors and stakeholders for conflicts of\n    interest, and discover the employees whom you will be working with. Some\n    organisations will supply you the CV of personnel upon request\n 4. Review the vendors public MSA, Privacy Policy, and any legally binding\n    agreements on conduct that may provide additional powers to the vendor with\n    restrictions to your business and its operations\n\nSome of these tasks are time consuming so the logical order plays a role here.\nBy the time you get to the point where you are reviewing the legal documents any\nred flags should have sent you back to your superiors to see if you should\nshort-circuit this due diligence exercise.\n\nAdditionally, the review of the legal documents should remain high level. You\nare conducting due diligence and should avoid the more thorough legal scrutiny\nyourself.\nBe sure you seek legal advice before any commitments are made to a vendor\nregardless of what you identify, however you should review them yourself as part\nof due diligence as not all red flags are legal oriented, you may find\nincompatibilities with your business requirements and operations that legal\ncouncil will not identify, such as a domain registrar having the right to take\ndown your site if they detect user content that violates their policy whether or\nnot you moderate.\n\nQualitative analysis\nFor those not familiar with the terminology, put simply this is a process where\nyou look at the qualities of something without applying calculations, which is\ngood for analysing features that are inherently unmeasurable.\n\nActivities that will help you document a report are;\n\n * Analysis of software functionality or vendor services rendered.\n * Maintenance and support\n * SLA (Service Level Agreement)\n * Interoperability (not just for exit strategy)\n\nWhile the findings may appear obvious to you, and in most cases you've done this\nmany times and never needed to create documentation for it. The real benefit\nrecording these activities is you've demonstrated that you applied a high level\nof analysis to the selection process and transparently followed a business\nprocess which applies the business interest. Your employees will also benefit\nfrom your efforts as a lot of these tasks are done, documented, and often would\nneed to be done in future after the fact of not documented the first time\ndoubling your efforts overall.\n\nAs a FinTech start up you are obligated to present a case to regulatory\nauthority (APRA in Australia), or you may be obligated to pursue a PCI Audit. In\nboth cases you will be required to demonstrate due diligence and risk assessment\ncapability through many types of collateral as evidence.\n\nRisk Assessments\nA crucial process, but one that is often over looked by small businesses and\nstartups. Once again this seems like a massive effort but it doesn't have to be.\n\nThis particular section could mean the difference between success and failure in\nmore areas then you first expect. Besides the benefits for Information Security,\ndemonstrating risk analysis is valuable to your investors and a skill you can\nuse to evaluate future partnerships.\n\nThe key to a quicker risk assessment is to apply a widely accepted framework\nrather than develop your own processes. There are many frameworks and guides but\nthe efficient choice I recommend for small to medium business is the CIS RAM\n(Risk Assessment Method)\n[https://www.cisecurity.org/white-papers/cis-ram-risk-assessment-method/].\n\nIf you wish to develop a more mature process beyond the CIS RAM the next easiest\nstep is combining the CSA CCM (Cloud Security Alliance Cloud Controls Matrix)\n[https://cloudsecurityalliance.org/working-groups/cloud-controls-matrix/] as a\nguide on inherent risk analysis, apply effort in Threat Modeling\n[https://www.owasp.org/index.php/Threat_Modeling_Cheat_Sheet], and your own \nRisk\nVulnerability Model\n[https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:A/AC:H/PR:H/UI:R/S:U/C:N/I:N/A:L]\n. At this stage you might also be interested in the NIST guides\n[https://nvd.nist.gov/800-53/Rev4/family/Risk%20Assessment].\n\nControls Implementation\nHopefully you've applied a the inherent risk assessment to save you time coming\nup with a concise set of controls appropriate to your circumstance. It's\noverkill and wasted effort to apply every control that exists if they're not\nprioritised based on the threat model.\nIt is hard for anyone to come up with controls regardless of their experience,\nso I recommend you check with the controls offered by CIS\n[https://www.cisecurity.org] to get started. If you feel the controls aren't\nmeeting your expectations (and you have time) it is also useful to review the\nvarious standards and compliance reports to get the brain working and help you\ncome up with more appropriate controls, a good reference is the PCI DSS RoC\n[https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2-ROC-Reporting-Template.pdf]\n.\n\nControls Evaluation\nControls are the whole reason for your Information Security strategy to exist.\nIt is not worth the effort in implementing them if you have no idea of their\neffectiveness.\n\nA typical evaluation is quite manual however there are tools that automate the\ntask such as Cloud Conformity [https://www.cloudconformity.com/] and Qualys\n[https://www.qualys.com/forms/freescan/] from the outside looking in, or my\npersonal favorite Lynis [https://cisofy.com/lynis/] when looking at a host.\n\nYour controls evaluation report should be conducted periodically, some\ncompliance obligations are prescriptive in this case but if you have none of\nthese obligations you might find it useful to do annually or as a task for a new\nhire in a senior level position as it is a good way for them to become familiar\nwith your systems and encourage a culture of security.\n\nThe goals for documenting the Controls Evaluation are to address each control\nidentified and implemented with a testing procedure and that tests result, a\nfairly simple process but one that provides a great deal of assurance to the\ncontrols effectiveness and your companies security posture.\n\nKeep the Controls Evaluation as high level as possible, don't allow any\nscope-creep to bloat the report for 2 reasons;\n\n 1. This is the ideal document to provide confidentially to business partners or\n    investors.\n 2. With greater complexity auditors will apply a higher degree of scrutiny.\n\nKeeping it simple also makes the task itself more approachable for non-security\ntrained staff, new employees, or for the team member to annually conduct as it\nshould be much less complicated then the risk assessment or due diligence\nprocesses.\n\nConclusion\nThe formal process is not as important as the businesses ability to provide\nimplementation detail evidence of their security posture and risk appetite.\n\nYou aren't going to be faulted for demonstrating through evidence that is\nshowing the results of your efforts. However if you have processes but cannot\nproduce any collateral of them being followed will do more harm to your security\nposture then if you had no formal processes at all.\n\nAs a lean startup, skipping formal processes is unlikely to hold you back from\nachieving the licensing or compliance you need. If you can show a higher than\nexpected degree of risk mitigation through evidence, then you're practically\nmore secure then most.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/risk-return.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2018-08-17 05:24:37","created_by":"1","updated_at":"2021-03-31 14:00:36","updated_by":"1","published_at":"2018-10-06 00:31:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc0","uuid":"3e651192-b98c-4835-94c6-ec3562f3b69e","title":"Preparing for Independent Penetration Testing","slug":"preparing-for-independent-penetration-testing","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# White box\\n\\nCommonly used by organisations after a black box pentest to validate controls, or as an assurance to the business for audit purposes. A white box pentest is prescriptive in nature and is the only type of pentest that provides an attestation of compliance.\\n\\n## What to provide\\n\\nPreparing for an effective whitebox pentest is key to it's success.\\n\\n-   Formal engagement agreement\\n-   Reduction Analysis\\n-   System Architecture Diagram (SAD)\\n-   GNS3 validated network topology\\n-   Whitelist Inventory (complete list of domains, software, and dependencies)\\n-   Blacklist Inventory\\n\\n## Optionally provide\\n\\n-   Credentials: 2 or more per each RBAC or ABAC or other access levels\\n-   Dictionary of terms, unique to your proprietary business, sector, or partners\\n-   Source code (often in place of Inventory)\\n\\nThe quality of findings and coverage are directly proportionate to the level of detail disclosed to the tester.\\n\\n## Redundant Efforts\\n\\n- Open Source Intelligence (OSINT) gathering\\n- Remote System Discovery\\n- Social Engineering (obtain credentials, physical access, etc)\\n\\n## Advantages\\n\\nThe organisation defines the attack surface which can be appropriate for various reasons such as fragile systems' availability, Material data integrity, or maintain confidentiality of trade secrets. \\n\\nThe level of insight allows testers to better understand intended use to find and exploit undesirable side effects the organisation may be unaware of. This also translates into concise recommendations for remediation and controls.\\n\\nAutomation of white box tests can also be achieved to avoid regression into the future and improve coverage and effectiveness of future testing efforts.\\n\\nCompleted on a time-cost basis appropriate to your budget and risk appetite, as these tests represent an infinite ongoing opportunity you chose the amount of effort you can afford or need.\\n\\nA complete white box test report provides a timed attestation on the security posture of your organisation for the scope of test procedures performed. A good tester provides both a public appropriate report that you might distribute to customers and one that is comprehensive and useful for your security strategy and auditors.\\n\\n## Disadvantages\\n\\nTesters often find themselves having a much higher barrier to entry for any given white box test as they are expected to have an intimate knowledge of the systems, beyond the developers of the systems themselves. This limits the pentesting organisation to use only resources with the experience and ability unique to the target rather than the experience of only being a pentester.\\n\\nDue to the scope and complexity of systems some conditions will initially be left untested, it is the goal of continued testing, and the automation of prior testing, to reach a broader coverage of conditions tested over time.\\n\\nThe prescriptive nature of this testing excludes the tester to explore any unknown unknowns, meaning the tests are performed based on the functionality that exists at the time, and is known through the disclosure of the target to the tester.\\n\\nThe targeted nature of these tests can cause system availability, integrity, or confidentiality issues. Ensure that the testers are capable of providing detailed tracing reports helping you identify and validate the effects of the tests performed. Therefore, ensure your BCP and DR strategy is tested and validated, internal and external parties are aware in advance of possible disruptions, and you do a complete audit of system integrity before assuming a BAU posture.\\n\\n# Black box\\n\\nOften described as requirements based or specification based testing, where an ethical attacker simulates an external malicious attacker with no prior knowledge of the targeted systems.\\n\\n## What to provide\\n\\n-   Formal engagement agreement defining the rules of engagement (RoE)\\n-   Copy of the legal permit to conduct the tests from regulatory, prudential, compliance, or any other relevant obligations such as contractual with service providers and downstream partners\\n-   Upfront status report and meeting expectations\\n\\n## Advantages\\n\\nOrganisations with trade secrets and patents (or any other legal expectation of secrets) often find the black box tests extremely useful to find public sources of confidential information and better understand the threats to their secrets. This may lead to legal action to remove or control any such findings, however the intention of the tester is to find and demonstrate the threats only.\\n\\nWhile internal systems (including those that transit the world wide internet) are likely being monitored already, it is scrutiny of the wider public internet that a tester performs that can give you additional ongoing monitoring capabilities. Such as monitoring pastebin or other code dumping repositories, identifying things you weren't aware of that are actually internal to your business such as employee social media with company branding, public 3rd party tools used by employees, directories that aggregate your brand in some way.\\n\\n## Disadvantages\\n\\nOrganisations feel a false-sense-of-security that the path the ethical attacker took is the same paths available to malicious attackers - this is fundamentally untrue as the ethical attacker abides by an RoE and applicable laws whereas the true threats lie outside these boundaries.\\n\\nThe findings report shows you where identified vulnerabilities exist, it is not comprehensive to be sufficient for your security strategy alone, and it is not collateral useful for public disclosure or to be useful for auditors (it is not considered an attestation of any kind)\\n\\nThis type of testing is highly likely to cause availability, integrity, or confidentiality issues. Therefore ensure your BCP and DR strategy is tested and validated, internal and external parties are aware in advance of expected disruptions, and you do a complete audit of system integrity before assuming a BAU posture\\n\\n# Grey box\\n\\nThe most common tests performed, often misunderstood by organisations as being one of either the white or black box types after all the engagement requirements have been discussed and decided. While grey box testing gives you the best of both worlds on the surface, it is actually the opposite as it makes most advantages of both cancel the other out.\\n\\nYou may wish the testers to simulate scenarios such as a malicious insider threat where specific information is disclosed to a competitor so as to identify the risks to the business.\\n\\n## What to provide\\n\\nThe reason grey box testing exists is that it combines any variation of white and black box testing.\\n\\nIt is common that successful grey box testing is done with a defined scenario, otherwise testers will be burdened with disparate and conflicting goals.\\n\\n## Advantages\\n\\nGrey box scenario can be extremely useful to identify various insider threats, such as system misuse (accidental or malicious), information disclosure, rouge technology, and leaked credentials.\\n\\nFindings can also help decide on various educational initiatives such as senior staff or executives conducting themselves in a high risk manner.\\n\\nDetect the existence of 'godmode'. This is where an employee with legitimate access to perform high risk actions does so for reasons outside tier role or not in the business interest. For example a manager giving another manager access to bypass controls that are inconvenient.\\n\\n## Disadvantages\\n\\nA grey box test report may include the same structure and depth of content as a white box test report however it can not be used as an attestation as the requirements were not disclosed up front.\\n\\nInsider knowledge of the grey box testing procedures negates the threat scenarios that simulate external malicious attackers.\\n\\n# Conclusion\\n\\nThe unfortunate reality is most organisations commission either (or both) the white or black box tests and get neither, instead the tests better represent a grey box test. This can be due to the maturity of the testing organisation, or a lack of knowledge on the side of the target organisation when defining requirements. To avoid falling into grey box unintentionally an organisation should first understand their requirements and split efforts of black, white, and grey box testing across distinct testing organisations, and in future engagements avoid using the same testing organisation for a different test type in case they reuse knowledge from past tests. This will encourage the testers to perform tasks within expected boundaries, and provide assurance to the organisation and auditors that tests are performed without bias and within the defined requirements.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h1 id=\"whitebox\">White box</h1>\n<p>Commonly used by organisations after a black box pentest to validate controls, or as an assurance to the business for audit purposes. A white box pentest is prescriptive in nature and is the only type of pentest that provides an attestation of compliance.</p>\n<h2 id=\"whattoprovide\">What to provide</h2>\n<p>Preparing for an effective whitebox pentest is key to it's success.</p>\n<ul>\n<li>Formal engagement agreement</li>\n<li>Reduction Analysis</li>\n<li>System Architecture Diagram (SAD)</li>\n<li>GNS3 validated network topology</li>\n<li>Whitelist Inventory (complete list of domains, software, and dependencies)</li>\n<li>Blacklist Inventory</li>\n</ul>\n<h2 id=\"optionallyprovide\">Optionally provide</h2>\n<ul>\n<li>Credentials: 2 or more per each RBAC or ABAC or other access levels</li>\n<li>Dictionary of terms, unique to your proprietary business, sector, or partners</li>\n<li>Source code (often in place of Inventory)</li>\n</ul>\n<p>The quality of findings and coverage are directly proportionate to the level of detail disclosed to the tester.</p>\n<h2 id=\"redundantefforts\">Redundant Efforts</h2>\n<ul>\n<li>Open Source Intelligence (OSINT) gathering</li>\n<li>Remote System Discovery</li>\n<li>Social Engineering (obtain credentials, physical access, etc)</li>\n</ul>\n<h2 id=\"advantages\">Advantages</h2>\n<p>The organisation defines the attack surface which can be appropriate for various reasons such as fragile systems' availability, Material data integrity, or maintain confidentiality of trade secrets.</p>\n<p>The level of insight allows testers to better understand intended use to find and exploit undesirable side effects the organisation may be unaware of. This also translates into concise recommendations for remediation and controls.</p>\n<p>Automation of white box tests can also be achieved to avoid regression into the future and improve coverage and effectiveness of future testing efforts.</p>\n<p>Completed on a time-cost basis appropriate to your budget and risk appetite, as these tests represent an infinite ongoing opportunity you chose the amount of effort you can afford or need.</p>\n<p>A complete white box test report provides a timed attestation on the security posture of your organisation for the scope of test procedures performed. A good tester provides both a public appropriate report that you might distribute to customers and one that is comprehensive and useful for your security strategy and auditors.</p>\n<h2 id=\"disadvantages\">Disadvantages</h2>\n<p>Testers often find themselves having a much higher barrier to entry for any given white box test as they are expected to have an intimate knowledge of the systems, beyond the developers of the systems themselves. This limits the pentesting organisation to use only resources with the experience and ability unique to the target rather than the experience of only being a pentester.</p>\n<p>Due to the scope and complexity of systems some conditions will initially be left untested, it is the goal of continued testing, and the automation of prior testing, to reach a broader coverage of conditions tested over time.</p>\n<p>The prescriptive nature of this testing excludes the tester to explore any unknown unknowns, meaning the tests are performed based on the functionality that exists at the time, and is known through the disclosure of the target to the tester.</p>\n<p>The targeted nature of these tests can cause system availability, integrity, or confidentiality issues. Ensure that the testers are capable of providing detailed tracing reports helping you identify and validate the effects of the tests performed. Therefore, ensure your BCP and DR strategy is tested and validated, internal and external parties are aware in advance of possible disruptions, and you do a complete audit of system integrity before assuming a BAU posture.</p>\n<h1 id=\"blackbox\">Black box</h1>\n<p>Often described as requirements based or specification based testing, where an ethical attacker simulates an external malicious attacker with no prior knowledge of the targeted systems.</p>\n<h2 id=\"whattoprovide\">What to provide</h2>\n<ul>\n<li>Formal engagement agreement defining the rules of engagement (RoE)</li>\n<li>Copy of the legal permit to conduct the tests from regulatory, prudential, compliance, or any other relevant obligations such as contractual with service providers and downstream partners</li>\n<li>Upfront status report and meeting expectations</li>\n</ul>\n<h2 id=\"advantages\">Advantages</h2>\n<p>Organisations with trade secrets and patents (or any other legal expectation of secrets) often find the black box tests extremely useful to find public sources of confidential information and better understand the threats to their secrets. This may lead to legal action to remove or control any such findings, however the intention of the tester is to find and demonstrate the threats only.</p>\n<p>While internal systems (including those that transit the world wide internet) are likely being monitored already, it is scrutiny of the wider public internet that a tester performs that can give you additional ongoing monitoring capabilities. Such as monitoring pastebin or other code dumping repositories, identifying things you weren't aware of that are actually internal to your business such as employee social media with company branding, public 3rd party tools used by employees, directories that aggregate your brand in some way.</p>\n<h2 id=\"disadvantages\">Disadvantages</h2>\n<p>Organisations feel a false-sense-of-security that the path the ethical attacker took is the same paths available to malicious attackers - this is fundamentally untrue as the ethical attacker abides by an RoE and applicable laws whereas the true threats lie outside these boundaries.</p>\n<p>The findings report shows you where identified vulnerabilities exist, it is not comprehensive to be sufficient for your security strategy alone, and it is not collateral useful for public disclosure or to be useful for auditors (it is not considered an attestation of any kind)</p>\n<p>This type of testing is highly likely to cause availability, integrity, or confidentiality issues. Therefore ensure your BCP and DR strategy is tested and validated, internal and external parties are aware in advance of expected disruptions, and you do a complete audit of system integrity before assuming a BAU posture</p>\n<h1 id=\"greybox\">Grey box</h1>\n<p>The most common tests performed, often misunderstood by organisations as being one of either the white or black box types after all the engagement requirements have been discussed and decided. While grey box testing gives you the best of both worlds on the surface, it is actually the opposite as it makes most advantages of both cancel the other out.</p>\n<p>You may wish the testers to simulate scenarios such as a malicious insider threat where specific information is disclosed to a competitor so as to identify the risks to the business.</p>\n<h2 id=\"whattoprovide\">What to provide</h2>\n<p>The reason grey box testing exists is that it combines any variation of white and black box testing.</p>\n<p>It is common that successful grey box testing is done with a defined scenario, otherwise testers will be burdened with disparate and conflicting goals.</p>\n<h2 id=\"advantages\">Advantages</h2>\n<p>Grey box scenario can be extremely useful to identify various insider threats, such as system misuse (accidental or malicious), information disclosure, rouge technology, and leaked credentials.</p>\n<p>Findings can also help decide on various educational initiatives such as senior staff or executives conducting themselves in a high risk manner.</p>\n<p>Detect the existence of 'godmode'. This is where an employee with legitimate access to perform high risk actions does so for reasons outside tier role or not in the business interest. For example a manager giving another manager access to bypass controls that are inconvenient.</p>\n<h2 id=\"disadvantages\">Disadvantages</h2>\n<p>A grey box test report may include the same structure and depth of content as a white box test report however it can not be used as an attestation as the requirements were not disclosed up front.</p>\n<p>Insider knowledge of the grey box testing procedures negates the threat scenarios that simulate external malicious attackers.</p>\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>The unfortunate reality is most organisations commission either (or both) the white or black box tests and get neither, instead the tests better represent a grey box test. This can be due to the maturity of the testing organisation, or a lack of knowledge on the side of the target organisation when defining requirements. To avoid falling into grey box unintentionally an organisation should first understand their requirements and split efforts of black, white, and grey box testing across distinct testing organisations, and in future engagements avoid using the same testing organisation for a different test type in case they reuse knowledge from past tests. This will encourage the testers to perform tasks within expected boundaries, and provide assurance to the organisation and auditors that tests are performed without bias and within the defined requirements.</p>\n<!--kg-card-end: markdown-->","comment_id":"5c3006980b905706a2ab0b22","plaintext":"White box\nCommonly used by organisations after a black box pentest to validate controls,\nor as an assurance to the business for audit purposes. A white box pentest is\nprescriptive in nature and is the only type of pentest that provides an\nattestation of compliance.\n\nWhat to provide\nPreparing for an effective whitebox pentest is key to it's success.\n\n * Formal engagement agreement\n * Reduction Analysis\n * System Architecture Diagram (SAD)\n * GNS3 validated network topology\n * Whitelist Inventory (complete list of domains, software, and dependencies)\n * Blacklist Inventory\n\nOptionally provide\n * Credentials: 2 or more per each RBAC or ABAC or other access levels\n * Dictionary of terms, unique to your proprietary business, sector, or partners\n * Source code (often in place of Inventory)\n\nThe quality of findings and coverage are directly proportionate to the level of\ndetail disclosed to the tester.\n\nRedundant Efforts\n * Open Source Intelligence (OSINT) gathering\n * Remote System Discovery\n * Social Engineering (obtain credentials, physical access, etc)\n\nAdvantages\nThe organisation defines the attack surface which can be appropriate for various\nreasons such as fragile systems' availability, Material data integrity, or\nmaintain confidentiality of trade secrets.\n\nThe level of insight allows testers to better understand intended use to find\nand exploit undesirable side effects the organisation may be unaware of. This\nalso translates into concise recommendations for remediation and controls.\n\nAutomation of white box tests can also be achieved to avoid regression into the\nfuture and improve coverage and effectiveness of future testing efforts.\n\nCompleted on a time-cost basis appropriate to your budget and risk appetite, as\nthese tests represent an infinite ongoing opportunity you chose the amount of\neffort you can afford or need.\n\nA complete white box test report provides a timed attestation on the security\nposture of your organisation for the scope of test procedures performed. A good\ntester provides both a public appropriate report that you might distribute to\ncustomers and one that is comprehensive and useful for your security strategy\nand auditors.\n\nDisadvantages\nTesters often find themselves having a much higher barrier to entry for any\ngiven white box test as they are expected to have an intimate knowledge of the\nsystems, beyond the developers of the systems themselves. This limits the\npentesting organisation to use only resources with the experience and ability\nunique to the target rather than the experience of only being a pentester.\n\nDue to the scope and complexity of systems some conditions will initially be\nleft untested, it is the goal of continued testing, and the automation of prior\ntesting, to reach a broader coverage of conditions tested over time.\n\nThe prescriptive nature of this testing excludes the tester to explore any\nunknown unknowns, meaning the tests are performed based on the functionality\nthat exists at the time, and is known through the disclosure of the target to\nthe tester.\n\nThe targeted nature of these tests can cause system availability, integrity, or\nconfidentiality issues. Ensure that the testers are capable of providing\ndetailed tracing reports helping you identify and validate the effects of the\ntests performed. Therefore, ensure your BCP and DR strategy is tested and\nvalidated, internal and external parties are aware in advance of possible\ndisruptions, and you do a complete audit of system integrity before assuming a\nBAU posture.\n\nBlack box\nOften described as requirements based or specification based testing, where an\nethical attacker simulates an external malicious attacker with no prior\nknowledge of the targeted systems.\n\nWhat to provide\n * Formal engagement agreement defining the rules of engagement (RoE)\n * Copy of the legal permit to conduct the tests from regulatory, prudential,\n   compliance, or any other relevant obligations such as contractual with\n   service providers and downstream partners\n * Upfront status report and meeting expectations\n\nAdvantages\nOrganisations with trade secrets and patents (or any other legal expectation of\nsecrets) often find the black box tests extremely useful to find public sources\nof confidential information and better understand the threats to their secrets.\nThis may lead to legal action to remove or control any such findings, however\nthe intention of the tester is to find and demonstrate the threats only.\n\nWhile internal systems (including those that transit the world wide internet)\nare likely being monitored already, it is scrutiny of the wider public internet\nthat a tester performs that can give you additional ongoing monitoring\ncapabilities. Such as monitoring pastebin or other code dumping repositories,\nidentifying things you weren't aware of that are actually internal to your\nbusiness such as employee social media with company branding, public 3rd party\ntools used by employees, directories that aggregate your brand in some way.\n\nDisadvantages\nOrganisations feel a false-sense-of-security that the path the ethical attacker\ntook is the same paths available to malicious attackers - this is fundamentally\nuntrue as the ethical attacker abides by an RoE and applicable laws whereas the\ntrue threats lie outside these boundaries.\n\nThe findings report shows you where identified vulnerabilities exist, it is not\ncomprehensive to be sufficient for your security strategy alone, and it is not\ncollateral useful for public disclosure or to be useful for auditors (it is not\nconsidered an attestation of any kind)\n\nThis type of testing is highly likely to cause availability, integrity, or\nconfidentiality issues. Therefore ensure your BCP and DR strategy is tested and\nvalidated, internal and external parties are aware in advance of expected\ndisruptions, and you do a complete audit of system integrity before assuming a\nBAU posture\n\nGrey box\nThe most common tests performed, often misunderstood by organisations as being\none of either the white or black box types after all the engagement requirements\nhave been discussed and decided. While grey box testing gives you the best of\nboth worlds on the surface, it is actually the opposite as it makes most\nadvantages of both cancel the other out.\n\nYou may wish the testers to simulate scenarios such as a malicious insider\nthreat where specific information is disclosed to a competitor so as to identify\nthe risks to the business.\n\nWhat to provide\nThe reason grey box testing exists is that it combines any variation of white\nand black box testing.\n\nIt is common that successful grey box testing is done with a defined scenario,\notherwise testers will be burdened with disparate and conflicting goals.\n\nAdvantages\nGrey box scenario can be extremely useful to identify various insider threats,\nsuch as system misuse (accidental or malicious), information disclosure, rouge\ntechnology, and leaked credentials.\n\nFindings can also help decide on various educational initiatives such as senior\nstaff or executives conducting themselves in a high risk manner.\n\nDetect the existence of 'godmode'. This is where an employee with legitimate\naccess to perform high risk actions does so for reasons outside tier role or not\nin the business interest. For example a manager giving another manager access to\nbypass controls that are inconvenient.\n\nDisadvantages\nA grey box test report may include the same structure and depth of content as a\nwhite box test report however it can not be used as an attestation as the\nrequirements were not disclosed up front.\n\nInsider knowledge of the grey box testing procedures negates the threat\nscenarios that simulate external malicious attackers.\n\nConclusion\nThe unfortunate reality is most organisations commission either (or both) the\nwhite or black box tests and get neither, instead the tests better represent a\ngrey box test. This can be due to the maturity of the testing organisation, or a\nlack of knowledge on the side of the target organisation when defining\nrequirements. To avoid falling into grey box unintentionally an organisation\nshould first understand their requirements and split efforts of black, white,\nand grey box testing across distinct testing organisations, and in future\nengagements avoid using the same testing organisation for a different test type\nin case they reuse knowledge from past tests. This will encourage the testers to\nperform tasks within expected boundaries, and provide assurance to the\norganisation and auditors that tests are performed without bias and within the\ndefined requirements.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/white_grey_black_hat.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2019-01-05 01:21:28","created_by":"1","updated_at":"2021-03-31 14:00:23","updated_by":"1","published_at":"2019-01-04 01:21:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc1","uuid":"8f9ce77d-ec82-4535-b0b9-e25e192247e9","title":"Exploiting Orphaned Webserver Files","slug":"exploiting-orphaned-webserver-files","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"> Detect as a means to defend\\n\\nThe idea of this attack is to identify old dependencies with known exploits.\\n\\nEven some of the most secure clients, that have excellent patching practices, are still vulnerable years after they assume they patched a vulnerability.\\n\\nMany of the competent website developers are now adopting the immutable server paradigm meaning this type of attack is mostly mitigated. If the server is rebuilt often as the website is updated you will unlikely retain any old dependencies that make you vulnerable.\\nA CDN (Content Delivery Network) approach may also be a good mitigation here as the files are not actually on your servers, however a strict CSP (Content Security Policy) may not allow you to use a CDN, or maybe it does but not all dependencies are loaded from the CDN are they?\\n\\nHowever immutable servers, patching, and CDN's were not always applied, and this attack can make you vulnerable to threats you long since believed you've avoided.\\n\\n## Exploit Tool Chain\\n\\nThe following tools will be used;\\n\\n- [parallel](http://www.gnu.org/s/parallel)\\n- [retire.js](https://www.npmjs.com/package/retire)\\n- [waybackurls](https://github.com/tomnomnom/waybackurls)\\n- [searchsploit](https://www.exploit-db.com/searchsploit#installgit)\\n\\nIf you haven't already installed Node.js: https://github.com/nodejs/help/wiki/Installation\\n\\nSame for Go: https://golang.org/doc/install\\n\\nInstall parallel on Debian/Ubuntu: `apt install parallel`\\nInstall parallel on macos: First get [brew](https://docs.brew.sh/Installation) if you haven't already then simply `brew install parallel`\\n\\nBe a good GNU consumer: `parallel --citation`\\n\\nInstall retire.js: `npm install --save-dev retire`\\n\\nMake sure the retire binary is in your PATH;\\n```bash\\nif [[ $PATH != *$(npm bin)* ]]; then\\n  export PATH=\\\"$PATH:$(npm bin)\\\"\\nfi\\n```\\n\\nInstall waybackurls: `go get github.com/tomnomnom/waybackurls`\\n\\nAgain make sure the binary is in your PATH;\\n```bash\\nif [[ -d \\\"$HOME/go/bin\\\" ]] && [[ $PATH != *\\\"$HOME/go/bin\\\"* ]] ; then\\n  export PATH=\\\"$PATH:$HOME/go/bin\\\"\\nfi\\n```\\n\\nLastly, you should be able to run `searchsploit -h` if you followed the installation instructions.\\n\\n## Gathering a list of URLs\\n\\nIf you're familiar with OSINT you will likely have a bunch of techniques to find _active_ or _current_ asset of your target, this tool can find _possible_ forgotten assets, cached by the way back machine.\\n\\nFor this example we will just look at JavaScript, which are commonly used for XSS attacks. But with this method you can identity all sorts of files with threats.\\n\\n```bash\\nwaybackurls target.hostname.tld | grep \\\"\\\\.js\\\" | uniq | sort >>urls_file.tmp\\n```\\n\\nNow this is just a list of URLs that might have existed at some time in the past, lets filter that to files that still exist;\\n\\n```bash\\ncat urls_file.tmp | \\\\\\n  parallel -X HEAD -j50 -q curl -Lw 'Status:%{http_code}\\\\t Size:%{size_download}\\\\t %{url_effective}\\\\n' -o /dev/null -sk | \\\\\\n  grep 'Status:200' | \\\\\\n  egrep -o 'https?://[^ ]+' >>urls_file\\nrm ${urls_file}.tmp\\n```\\n\\nNow we will have only valid URLs targets so let's download them (notice we did a HEAD last time).\\n\\n```bash\\nmkdir -p target_files; cd target_files\\ncat ${urls_file} | xargs wget\\n```\\n\\nNow simply running `retire` here to find out if any of our targets old dependencies have any common known vulnerabilities.\\n\\nAssuming you found jQuery upload in there, you can check for an exploit to launch to verify you are vulnerable with `searchsploit -w jquery upload` which might show you;\\n```\\nblueimp's jQuery 9.22.0 - (Arbitrary) File Upload (Metasploit) | https://www.exploit-db.com/exploits/45790/\\njQuery Uploadify 2.1.0 - Arbitrary File Upload                 | https://www.exploit-db.com/exploits/11218/\\njQuery-File-Upload 9.22.0 - Arbitrary File Upload              | https://www.exploit-db.com/exploits/45584/\\n```\\n\\nNow go to one of the links in the results to inspect the exploit.\\n\\nHere is a Gist of the entire scan;\\n<script src=\\\"https://gist.github.com/chrisdlangton/4b385e168d8ea251c478c552b1828be5.js\\\"></script>\\n\\n## Defending\\n\\nAs previously discussed, you might want to implement immutable web servers, so each patch and code update will purge all unused code. This is constrained to a certain type of infrastructure and might be possible for you, so an alternative to is purge all assets from the persistent storage periodically and rebuild assets from a known and most importantly current inventory, allowing you to also verify that what you are putting into production is at least safe at the time.\\n\\nAnother excellent idea it to periodically perform static analysis of the assets stored persistently, not just on the next code deployment candidate. If you have any findings cross reference your inventory (you have one of these right?) and risk register in case the risk has been assessed already.\\n\\nLastly, automate scanning tools similar to what is in this post. There are a lot of ways you can stay ahead of attackers, at the very least try to scan your own assets frequently with the common tools attackers are using to make sure you're not the easy target.\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>Detect as a means to defend</p>\n</blockquote>\n<p>The idea of this attack is to identify old dependencies with known exploits.</p>\n<p>Even some of the most secure clients, that have excellent patching practices, are still vulnerable years after they assume they patched a vulnerability.</p>\n<p>Many of the competent website developers are now adopting the immutable server paradigm meaning this type of attack is mostly mitigated. If the server is rebuilt often as the website is updated you will unlikely retain any old dependencies that make you vulnerable.<br>\nA CDN (Content Delivery Network) approach may also be a good mitigation here as the files are not actually on your servers, however a strict CSP (Content Security Policy) may not allow you to use a CDN, or maybe it does but not all dependencies are loaded from the CDN are they?</p>\n<p>However immutable servers, patching, and CDN's were not always applied, and this attack can make you vulnerable to threats you long since believed you've avoided.</p>\n<h2 id=\"exploittoolchain\">Exploit Tool Chain</h2>\n<p>The following tools will be used;</p>\n<ul>\n<li><a href=\"http://www.gnu.org/s/parallel\">parallel</a></li>\n<li><a href=\"https://www.npmjs.com/package/retire\">retire.js</a></li>\n<li><a href=\"https://github.com/tomnomnom/waybackurls\">waybackurls</a></li>\n<li><a href=\"https://www.exploit-db.com/searchsploit#installgit\">searchsploit</a></li>\n</ul>\n<p>If you haven't already installed Node.js: <a href=\"https://github.com/nodejs/help/wiki/Installation\">https://github.com/nodejs/help/wiki/Installation</a></p>\n<p>Same for Go: <a href=\"https://golang.org/doc/install\">https://golang.org/doc/install</a></p>\n<p>Install parallel on Debian/Ubuntu: <code>apt install parallel</code><br>\nInstall parallel on macos: First get <a href=\"https://docs.brew.sh/Installation\">brew</a> if you haven't already then simply <code>brew install parallel</code></p>\n<p>Be a good GNU consumer: <code>parallel --citation</code></p>\n<p>Install retire.js: <code>npm install --save-dev retire</code></p>\n<p>Make sure the retire binary is in your PATH;</p>\n<pre><code class=\"language-bash\">if [[ $PATH != *$(npm bin)* ]]; then\n  export PATH=&quot;$PATH:$(npm bin)&quot;\nfi\n</code></pre>\n<p>Install waybackurls: <code>go get github.com/tomnomnom/waybackurls</code></p>\n<p>Again make sure the binary is in your PATH;</p>\n<pre><code class=\"language-bash\">if [[ -d &quot;$HOME/go/bin&quot; ]] &amp;&amp; [[ $PATH != *&quot;$HOME/go/bin&quot;* ]] ; then\n  export PATH=&quot;$PATH:$HOME/go/bin&quot;\nfi\n</code></pre>\n<p>Lastly, you should be able to run <code>searchsploit -h</code> if you followed the installation instructions.</p>\n<h2 id=\"gatheringalistofurls\">Gathering a list of URLs</h2>\n<p>If you're familiar with OSINT you will likely have a bunch of techniques to find <em>active</em> or <em>current</em> asset of your target, this tool can find <em>possible</em> forgotten assets, cached by the way back machine.</p>\n<p>For this example we will just look at JavaScript, which are commonly used for XSS attacks. But with this method you can identity all sorts of files with threats.</p>\n<pre><code class=\"language-bash\">waybackurls target.hostname.tld | grep &quot;\\.js&quot; | uniq | sort &gt;&gt;urls_file.tmp\n</code></pre>\n<p>Now this is just a list of URLs that might have existed at some time in the past, lets filter that to files that still exist;</p>\n<pre><code class=\"language-bash\">cat urls_file.tmp | \\\n  parallel -X HEAD -j50 -q curl -Lw 'Status:%{http_code}\\t Size:%{size_download}\\t %{url_effective}\\n' -o /dev/null -sk | \\\n  grep 'Status:200' | \\\n  egrep -o 'https?://[^ ]+' &gt;&gt;urls_file\nrm ${urls_file}.tmp\n</code></pre>\n<p>Now we will have only valid URLs targets so let's download them (notice we did a HEAD last time).</p>\n<pre><code class=\"language-bash\">mkdir -p target_files; cd target_files\ncat ${urls_file} | xargs wget\n</code></pre>\n<p>Now simply running <code>retire</code> here to find out if any of our targets old dependencies have any common known vulnerabilities.</p>\n<p>Assuming you found jQuery upload in there, you can check for an exploit to launch to verify you are vulnerable with <code>searchsploit -w jquery upload</code> which might show you;</p>\n<pre><code>blueimp's jQuery 9.22.0 - (Arbitrary) File Upload (Metasploit) | https://www.exploit-db.com/exploits/45790/\njQuery Uploadify 2.1.0 - Arbitrary File Upload                 | https://www.exploit-db.com/exploits/11218/\njQuery-File-Upload 9.22.0 - Arbitrary File Upload              | https://www.exploit-db.com/exploits/45584/\n</code></pre>\n<p>Now go to one of the links in the results to inspect the exploit.</p>\n<p>Here is a Gist of the entire scan;</p>\n<script src=\"https://gist.github.com/chrisdlangton/4b385e168d8ea251c478c552b1828be5.js\"></script>\n<h2 id=\"defending\">Defending</h2>\n<p>As previously discussed, you might want to implement immutable web servers, so each patch and code update will purge all unused code. This is constrained to a certain type of infrastructure and might be possible for you, so an alternative to is purge all assets from the persistent storage periodically and rebuild assets from a known and most importantly current inventory, allowing you to also verify that what you are putting into production is at least safe at the time.</p>\n<p>Another excellent idea it to periodically perform static analysis of the assets stored persistently, not just on the next code deployment candidate. If you have any findings cross reference your inventory (you have one of these right?) and risk register in case the risk has been assessed already.</p>\n<p>Lastly, automate scanning tools similar to what is in this post. There are a lot of ways you can stay ahead of attackers, at the very least try to scan your own assets frequently with the common tools attackers are using to make sure you're not the easy target.</p>\n<!--kg-card-end: markdown-->","comment_id":"5c32c9c90b905706a2ab0b29","plaintext":"> Detect as a means to defend\n\n\nThe idea of this attack is to identify old dependencies with known exploits.\n\nEven some of the most secure clients, that have excellent patching practices,\nare still vulnerable years after they assume they patched a vulnerability.\n\nMany of the competent website developers are now adopting the immutable server\nparadigm meaning this type of attack is mostly mitigated. If the server is\nrebuilt often as the website is updated you will unlikely retain any old\ndependencies that make you vulnerable.\nA CDN (Content Delivery Network) approach may also be a good mitigation here as\nthe files are not actually on your servers, however a strict CSP (Content\nSecurity Policy) may not allow you to use a CDN, or maybe it does but not all\ndependencies are loaded from the CDN are they?\n\nHowever immutable servers, patching, and CDN's were not always applied, and this\nattack can make you vulnerable to threats you long since believed you've\navoided.\n\nExploit Tool Chain\nThe following tools will be used;\n\n * parallel [http://www.gnu.org/s/parallel]\n * retire.js [https://www.npmjs.com/package/retire]\n * waybackurls [https://github.com/tomnomnom/waybackurls]\n * searchsploit [https://www.exploit-db.com/searchsploit#installgit]\n\nIf you haven't already installed Node.js: \nhttps://github.com/nodejs/help/wiki/Installation\n\nSame for Go: https://golang.org/doc/install\n\nInstall parallel on Debian/Ubuntu: apt install parallel\nInstall parallel on macos: First get brew [https://docs.brew.sh/Installation] if\nyou haven't already then simply brew install parallel\n\nBe a good GNU consumer: parallel --citation\n\nInstall retire.js: npm install --save-dev retire\n\nMake sure the retire binary is in your PATH;\n\nif [[ $PATH != *$(npm bin)* ]]; then\n  export PATH=\"$PATH:$(npm bin)\"\nfi\n\n\nInstall waybackurls: go get github.com/tomnomnom/waybackurls\n\nAgain make sure the binary is in your PATH;\n\nif [[ -d \"$HOME/go/bin\" ]] && [[ $PATH != *\"$HOME/go/bin\"* ]] ; then\n  export PATH=\"$PATH:$HOME/go/bin\"\nfi\n\n\nLastly, you should be able to run searchsploit -h if you followed the\ninstallation instructions.\n\nGathering a list of URLs\nIf you're familiar with OSINT you will likely have a bunch of techniques to find \nactive or current asset of your target, this tool can find possible forgotten\nassets, cached by the way back machine.\n\nFor this example we will just look at JavaScript, which are commonly used for\nXSS attacks. But with this method you can identity all sorts of files with\nthreats.\n\nwaybackurls target.hostname.tld | grep \"\\.js\" | uniq | sort >>urls_file.tmp\n\n\nNow this is just a list of URLs that might have existed at some time in the\npast, lets filter that to files that still exist;\n\ncat urls_file.tmp | \\\n  parallel -X HEAD -j50 -q curl -Lw 'Status:%{http_code}\\t Size:%{size_download}\\t %{url_effective}\\n' -o /dev/null -sk | \\\n  grep 'Status:200' | \\\n  egrep -o 'https?://[^ ]+' >>urls_file\nrm ${urls_file}.tmp\n\n\nNow we will have only valid URLs targets so let's download them (notice we did a\nHEAD last time).\n\nmkdir -p target_files; cd target_files\ncat ${urls_file} | xargs wget\n\n\nNow simply running retire here to find out if any of our targets old\ndependencies have any common known vulnerabilities.\n\nAssuming you found jQuery upload in there, you can check for an exploit to\nlaunch to verify you are vulnerable with searchsploit -w jquery upload which\nmight show you;\n\nblueimp's jQuery 9.22.0 - (Arbitrary) File Upload (Metasploit) | https://www.exploit-db.com/exploits/45790/\njQuery Uploadify 2.1.0 - Arbitrary File Upload                 | https://www.exploit-db.com/exploits/11218/\njQuery-File-Upload 9.22.0 - Arbitrary File Upload              | https://www.exploit-db.com/exploits/45584/\n\n\nNow go to one of the links in the results to inspect the exploit.\n\nHere is a Gist of the entire scan;\n\nDefending\nAs previously discussed, you might want to implement immutable web servers, so\neach patch and code update will purge all unused code. This is constrained to a\ncertain type of infrastructure and might be possible for you, so an alternative\nto is purge all assets from the persistent storage periodically and rebuild\nassets from a known and most importantly current inventory, allowing you to also\nverify that what you are putting into production is at least safe at the time.\n\nAnother excellent idea it to periodically perform static analysis of the assets\nstored persistently, not just on the next code deployment candidate. If you have\nany findings cross reference your inventory (you have one of these right?) and\nrisk register in case the risk has been assessed already.\n\nLastly, automate scanning tools similar to what is in this post. There are a lot\nof ways you can stay ahead of attackers, at the very least try to scan your own\nassets frequently with the common tools attackers are using to make sure you're\nnot the easy target.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/ancient-troll-statue.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2019-01-07 03:38:49","created_by":"1","updated_at":"2021-03-31 14:00:12","updated_by":"1","published_at":"2019-01-07 05:30:53","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc2","uuid":"670ae5bd-1864-43ea-b65a-e469cb6ffdcf","title":"Software Supply Chain Risk Mitigation","slug":"software-supply-chain-risk-mitigation","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Inventory\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Inventory</p>\n<!--kg-card-end: markdown-->","comment_id":"5c35cfe4ca804b06bb5dc773","plaintext":"Inventory","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2019-01-09 10:41:40","created_by":"1","updated_at":"2021-03-31 14:20:00","updated_by":"1","published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc3","uuid":"e4c7fe0d-690b-424e-94b7-2d77da49e65b","title":"Australia - Where Compliance and Regulation Obligations are Unlawful","slug":"australia-where-compliance-and-regulation-obligations-are-unlawful","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The Australian Government's controversial encryption bill [passed the Senate](https://www.abc.net.au/news/2018-12-06/labor-backdown-federal-government-to-pass-greater-surveillance/10591944) last month and will undoubtedly be law soon.\\n\\nThe bill proposes three key powers;\\n\\n**A technical assistance request (TAR):** Police ask a company to \\\"voluntarily\\\" help, such as give technical details about the development of a new online service\\n\\n**A technical assistance notice (TAN):** A company is required to give assistance. For example, if they can decrypt a specific communication, they must or face fines\\n\\n**A technical capability notice (TCN):** The company must build a new function to help police get at a suspect's data, or face fines\\n\\n### Sound familiar?\\n\\n#### The Clipper Chip\\n\\nThe Clipper Chip is a cryptographic device purportedly intended to protect private communications while at the same time permitting government agents to obtain the \\\"keys\\\" upon presentation of what has been vaguely characterized as \\\"legal authorization.\\\"\\n\\nIn 1994, Matt Blaze published the paper _Protocol Failure in the Escrowed Encryption Standard_ which explained that any back door to encryption is as good as a closed **unlocked** home front door.\\n\\n**_We know this was impossible in 1994!_**\\n**_in ==1994==.._**\\n\\nFor many business entities in Australia there aren't too many Compliance and Regulation Obligations that must be met that are considered under law, the Privacy Act is the best example of obligations under law. However that said, there are certainly a lot of other Compliance and Regulation Obligations for you to consider if you want to avoid fines or other unpleasant outcomes in Australia.\\n\\n### Australian specific Compliance and Regulation Obligations\\n\\nThe following list is the obligations I personally encounter for my day job, it may be missing something that you're aware of, and is specifically in the lens of the Australian Government so excludes many other obligations that may apply to Australian business entities which are more broadly accepted outside of the Australian borders.\\n\\n- [The Australian Prudential Regulation Authority (APRA)](https://www.apra.gov.au/file/11771) CPG-234 Management of Security Risk in Information and Information Technology\\n- ACSC Publication [Cloud Computing Security for Tenants](https://acsc.gov.au/publications/protect/cloud-security-tenants.htm)\\n- [Protective Security Policy Framework (PSPF)](https://www.protectivesecurity.gov.au/information/sensitive-classified-information/Pages/default.aspx)\\n- IRAP (Information Security Registered Assessors Program ) Assessment (document 5) version 0.04\\n- [ACSC Australian Government Information Security Manual (ISM)](https://acsc.gov.au/infosec/ism/index.htm)\\n- [Australasian Information Security Evaluation Program (AISEP)](https://acsc.gov.au/infosec/aisep/crypto.htm)\\n- [OAIC guidance](https://www.oaic.gov.au/images/documents/privacy/privacy-guides/information-security-guide-2013_WEB.pdf)\\n\\n> What a list\\n\\nSo let's go through each and why we should care.\\n\\n### APRA\\n\\nIn general terms this is the body that regulates banks, insurers, and companies that take fiat currency deposits, so as to protect _consumers_ as the main goal. I emphasise _consumers_ because we are mostly going to be looking at obligations where the concerned party is not the consumer, and APRA is a special case for this topic.\\n\\n### OAIC\\n\\nThe entity that is responsible for Privacy Act related matters and enforcements, notably the mandatory notifiable data breach law introduced (before GDPR) that makes it unlawful for organisations to keep data breaches to themselves in Australia.\\n\\n### ACSC\\n\\nThe ACSC is interesting in that there isn't anything in law specifically that it enforces, but rather it is responsible for many initiatives that help **Agency** organisation become more _secure_. The **Agency** reference carries with it special meaning in Australia, and to explain it properly would take up more then this whole post. In terms of _secure_ we are making a distinction that we are not referring to privacy of Australians, or any concerns of the public at all, Agency security is in context of government interests alone.\\n\\n### IRAP\\n\\nWhile not an obligation required by law, it is however a requirement for any business to interact technologically with the government, any of it's agencies, or in some cases if you operate in areas of public interest where some other regulatory body like APRA has not been established. For example the consultancy I currently work with are not considered to be in any of the above categories however we often work with our clients to achieve IRAP certification where we are the MSP, so we are in the scope of IRAP also.\\n\\n> Failing to meet IRAP requirements can make or break a business\\n\\n### ISM / PSPF / AISEP\\n\\nSimilar to IRAP these are not obligation required under law for the public sector, however an **Agency** is expected to meet the obligations for each. As described with ACSC, Agency security is in context of government interests alone.\\nThe public sector is _encouraged_ to align their business to the obligations set out for Agencies if they do not implement their own.\\n\\n## Baseline Security Posture is Encryption\\n\\nWith the mandatory notifiable data breach law, the expectation for all Australian businesses is to have a fairly high standard of Information Security, through the implementation of controls set out in many of the above as a preventative strategy.\\n\\nRendering data unreadable through cryptographic or hashing means in case of a data breach is by law the baseline, so why then is it also enforcible by law that an organisation must produce in plain text private or sensitive data via TAN or TCN from law enforcement?\\n\\n## The Requirements and Controls\\n\\n#### IRAP\\n\\nWhile controls are not uniquely IRAP, they are PSPF or ISM, some are Core Security Requirements (CSRs) that are interesting to note here;\\n\\n- The SRMP **must** protect the personal and sensitive information of subscribers, authentication credentials including transaction histories,\\nbacked up and archived information. This wording contradicts all 3 enforcements of the new bill.\\n- The Applicant **must** immediately revoke digital certificates and associated keys that have been compromised. Are these enforcements a compromise to an individuals privacy or security?\\n- The Applicant **should** immediately suspend digital certificates and associated keys that are suspected by being compromised. Will the new data custodians respect an individuals privacy or security?\\n\\nAn official example of critical Non Compliance states;\\n\\n> The inappropriate storage of cryptographic keys, digital certificates, or\\npassphrases will be classified as a critical Non Compliance.\\n\\nThough not prescriptive it does detail that any control that states **must** will result in critical Non Compliance if not met, meaning in practice an organisation cannot achieve IRAP certification unless they adhere to the controls around encryption stated above, in the PSPF and ISM below.\\n\\n#### PSPF\\n\\nWhere encryption is defined in the PSPF a citation for information on cryptography, you're directed to the ISM. Some PSPF specific controls;\\n\\n-  PSPF43: use the encryption standards identified in the ISM for information transmitted over public network infrastructure. The encryption will sufficiently protect the information to allow it to be transmitted on an unclassified network. Basically apply the encryption standards identified in the ISM to protect information on their network infrastructure in unsecured areas.\\n- PSPF33: Do not allow information to be accessed merely because it would be convenient for personnel to know or because of their status, position, rank or level of authorised access. This is comically appropriate for the new bill, i literally couldn't make up a better control to conflict with the new bill.\\n-  PSPF01: Ensure that personnel who access Australian Government resources are eligible to have access,have had their identity established, are suitable to have access, and agree to comply with the Government’s policies, standards, protocols and guidelines that safeguard resources from harm. This control ties directly with my recommendations for if/when you encounter a possible enforcement under the new bill.\\n\\n#### ISM\\n\\nThe ISM has a section on _Guidelines for using cryptography_ where it states;\\n\\n> An Australian Signals Directorate (ASD) Cryptographic Evaluation (ACE) requirements supplement this document and where conflicts occur take precedence.\\n\\nSimply put, if you're subject to achieving ACE do so first then fulfill any outstanding requirements in the ISM.\\n\\nKey ISM take aways;\\n\\n- FIPS 140-x is the standard only when concerned with cryptographic modules (HACE) compliance to the ISM.\\n- Security Control: 0455; Revision: 2; You **Must** provide the means to derive the plaintext recovery of data from the ciphertext without the use of the data encryption key used.\\n- Security Control: 1162; Revision: 3; **Must** Use encryption in-transit when sensitive data is exchanged over untrusted networks\\n- ASD Approved Cryptographic Algorithms, apart from the expected DH / ECDH / RSA / AES / 3DES etc, include SHA-2 hashing! Security Control: 1054; Revision: 4;\\n- ASD Approved Cryptographic Protocols, apart from the expected WPA2 / TLS / SSH / IPsec, it also approves OpenPGP specifically as well as S/MIME (which when used on its own is a shock). Security Control: 0490; Revision: 3;\\n- Security Control: 0142; Revision: 2, Control: 0143; Revision: 7, and Control: 1091; Revision: 4; All discuss _compromised_ keys and what you must do, but are keys considered compromised after being subject to a TAN or TCN?\\n- Security Control 0571; Where users send email from outside their network, an authenticated and encrypted channel **must** be configured to allow email to be sent via the centralised email gateway.\\n- Security Control  0486; Applicants that allow passphrase authentication **must** use techniques to block brute force attempts against the passphrase.\\n\\n\\n#### ASD ACE / AISEP\\n\\nAn ASD Cryptographic Evaluation test may include packet sniffing, black box testing, source code review, key management analysis, and Random Number Generation (RNG) evaluation.\\n\\n> Cryptographic evaluations conducted in other nations such as CAPS, FIPS-140, and CMVP are not a replacement for an ASD Cryptographic Evaluation for Australian government agencies.\\n\\nKey take aways;\\n\\n- ASD-approved cryptographic algorithms and ASD-approved cryptographic protocols defined in the ISM.\\n- Some products on the EPL do not have a consumer guide. They're not recommended for the use of cryptographic security or products that do not contain cryptographic security.\\n- An ASD-approved Protection Profile covers the necessary security functionality expected of the evaluated product and known security threats\\n- FPT_KST_EXT.2.1 from Protection Profile for Mobile Device. The TOE Security Functionality (TSF) shall not transmit any plaintext key material outside the security boundary of the target of evaluation (TOE). A TAN or TCN conflicts with this fundamentally.- FCS_CKM_EXT.4 Cryptographic Key Zeroization from Protection Profile for IPsec clients; requirements to ensure plaintext sensitive data such as key material and the plaintext secrets being protected have no known way to be recovered. This is of particular relevance in the context of TAN and TCN as it is in undisputed conflict. \\n\\n#### APRA\\n\\nRemember, APRA exists to protect the consumer.\\n\\nHere are some interesting take aways from the CPG-234;\\n- APRA envisages that a regulated institution would select algorithms from the population of well established and proven international standards that have been subjected to rigorous public scrutiny and verification of effectiveness (e.g. Triple DES38, AES39 etc.). The length of a cryptographic key would typically be selected to render a brute force attacks impractical (i.e. would require an extremely long period of time to breach using current computing capabilities).\\n- segregation of duties, with no single individual having knowledge of the entire cryptographic key (i.e. two-person controls) or having access to all the components making up these keys.\\n- A regulated institution would typically utilise tamper resistant devices to store and generate cryptographic keys, generate PINs and perform encryption and decryption. In most cases this would involve the use of Hardware Security Modules (HSMs) or similarly secured devices. These devices would be appropriately secured both physically and logically.\\n\\nBasically if any APRA regulated entity received a TCN it must find a deliberately insecure HSM, one that the rest of the world would never use and might only exist in fantasy, and don't forget that the TCN would essentially make the APRA regulated entity lose it's license for no other reason then it's inability to meet CPG-234 F.7.\\n\\n#### ACSC\\n\\nHonorable mention. \\nThe ACSC release guidance in certain area where clarity is needed, a great example most organisations will be falimiar with is the Cloud Computing Security for Tenants guidance. This and most others will reference back to the ASD-approved cryptographic controls or ISM with few exceptions. \\nIn the case of this CSP guidance the exception is control 12;\\n> Perform up-to-date encrypted backups in a format avoiding CSP lock-in, stored offline at the tenants premises or at a second CSP requiring multi-factor authentication to modify/delete data. Annually test the recovery process.\\n\\nWhile not so different from the ISM, there is specific reference to multi-factor authentication which hasn't been all that common until late 2018.\\n\\n#### OAIC\\n\\nSeveral investigations and case studies provide us with a clear understanding of how encryption is considered in the context of a data breach and any legal proceedings that may follow.\\n\\n1. Case study: Cupid Media Pty Ltd\\n> my investigation found that, at the time of the incident, Cupid did not have password encryption processes in place. Password encryption is a basic security strategy that may prevent unauthorised access to user accounts. Cupid insecurely stored passwords in plain text, and I found that to be a failure to take reasonable security steps, as required under the Privacy Act.\\n\\nStatement of common situations\\n> The loss of portable devices containing personal information. Laptops, external hard drives of USB sticks are often left in taxis, on trains, or misplaced in the office. This is a known risk, however often these devices are not encrypted\\n\\nAgain encryption is the recommendation.\\n\\nThe result;\\n> Password encryption strategies such as hashing and salting are basic security steps that were available to Cupid at the time of the data breach that may have prevented unauthorised access to user accounts. The Commissioner therefore found Cupid’s storage of passwords in plain text to be a failure to take reasonable security steps for the purpose of NPP 4.1. \\n\\n2. Another example; Adobe\\n\\n> While Adobe generally took a sophisticated and layered approach to information security and the protection of its IT systems, it failed to implement consistently strong security measures across its various internal systems. In particular, a backup server stored a database of unencrypted credential information (email addresses and password hints) of over 1.7 million Australian users, directly linked to the encrypted password for each user. The type of encryption used (block cipher), together with plaintext password hints, allowed security experts with to the database\\n\\nThe verdict;\\n\\n> The Commissioner found that Adobe breached NPP 4 by failing to take reasonable steps to protect the personal information it held from misuse and loss and from unauthorised access, modification or disclosure\\n\\nWhat is the OAIC view on the new bill?\\n\\n> The OAIC acknowledges that the power to issue TANs and TCNs is subject to certain limitations, including that a TAN or TCN must not have the effect of requiring a systemic weakness or vulnerability to be built into a form of electronic protection. However, it will be necessary to ensure that the measures proposed in Schedule 1 do not, in practice, introduce unintended exploitable weaknesses into a telecommunications environment that fundamentally relies on strong and robust security settings. While the OAIC acknowledges that the Bill and explanatory memorandum before the Committee addresses some of the recommendations in the OAIC’s earlier submission, privacy risks remain.\\n\\nBasically, that opportunity to make a submission to the Department of Home Affairs lasted single digit weeks. What a ruse. The Senate passed the bill in contempt of any submissions made and bypassing any conclusions the committee might have made thereafter. So the OAIC statement above is essentially meaningless.\\n\\nThe OAIC themselves made a submission to the committee with 10 recommendations that were not addressed by the bill passed through Senate, but the public is assured that maybe one of the recommendations _define the terms ‘systemic weakness’ and ‘systemic vulnerability’ in s 317ZG_ might be addressed in typical post-truth politics. I digress.\\n\\nBasically, so long as the 13 legally binding APPs are far from prescriptive, subject to the discretion of the privacy commissioner, the new bill will inevitably supersede privacy, so as to not make the government appear contradictory or any more contemptuous to the public then it is already perceived.\\n\\n### Final thoughts\\n\\nBasically if your privacy or security is compromised from an APRA regulated company and data was decrypted, it is fair to state there were certainly more than 1 key custodian responsible for the breach and one might assume the act of disclosure was not an accident.\\n\\nThe decision-making criteria that apply to TAN and TCN, and the matters to be consider when applying those criteria, should also be extended to TAR (not part of the bill). A compliance to a TAR should include legitimate expectations relating to privacy and cybersecurity, such as reasonableness, proportionality, practicability, and technical feasibility. The subject of the TAR will more likely comply out of fear of legal proceedings and make mistakes that compromise individuals privacy and security from a lack of knowledge and lack of formal process or decision-making, which is far more damaging then the current state and not in the intent of the bill.\\n\\nIn terms of the AISEP you'll fail the ASD ACE by complying with a TAN or TCN, looking at 2 of dozens of protection profiles, one for mobile device communications and the other for IPSec. Simply doing a quick search I identified a fundamental conflict in both, further analysis must yield a few more. Did the people writing the anti-encryption bill even consider their own existing rules? They mustn't be aware of the ASD, or are intentionally ignorant knowing full-well the conflict in an attempt to avoid the inevitable analysis that would make them look like fools.\\n\\nFinally; Considering the Notifiable Data Breaches scheme under Part IIIC of the Privacy Act. The fact that personal information is encrypted may reduce the likelihood of serious harm in the event of a data breach and therefore avoid the requirement to notify the OAIC or affected individuals entirely. Did the committee or Senate consider this new NDB Law or the older Privacy Act? Obviously not, or the payoffs were high enough they simply don't care.\\n\\nAnd the best part, the PSPF33 control spells out that data shouldn't be accessed just because the person wanting access can get that access. This is in context stating that a warrant should be provided or how can a business possibly know if an external individual or entity should get access to what they are asking for and not just asking because they can ask (which violates PSPF33).\\n\\n## What to do if you get an enforcement\\n\\nAt a minimum, do the following:\\n\\n- Identity verification, If they identify as law enforcement call the appropriate law enforcement office to make sure it is not a bad actor.\\n- Criminal record check. This is probably not possible, but if the individual is not identified as law enforcement and you have the means to conduct a Criminal record check i suggest you do so immediately with their consent.\\n- Eligibility checks, consult with your legal advisor, make sure the requested data and circumstances are all legal at the time of the enforcement. You are in your legal right to check if your own actions are legally appropriate.\\n- Do not perform any proposed actions outside normal business operations without first consulting your legal advisor specifically about the action.\\n\\nSpeaking from the point of view of a penetration tester i can see the use of these enforcement letters being a fantastic tool to test my target, not posing as law enforcement of course but a convincing TAR might just trick most organisations.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The Australian Government's controversial encryption bill <a href=\"https://www.abc.net.au/news/2018-12-06/labor-backdown-federal-government-to-pass-greater-surveillance/10591944\">passed the Senate</a> last month and will undoubtedly be law soon.</p>\n<p>The bill proposes three key powers;</p>\n<p><strong>A technical assistance request (TAR):</strong> Police ask a company to &quot;voluntarily&quot; help, such as give technical details about the development of a new online service</p>\n<p><strong>A technical assistance notice (TAN):</strong> A company is required to give assistance. For example, if they can decrypt a specific communication, they must or face fines</p>\n<p><strong>A technical capability notice (TCN):</strong> The company must build a new function to help police get at a suspect's data, or face fines</p>\n<h3 id=\"soundfamiliar\">Sound familiar?</h3>\n<h4 id=\"theclipperchip\">The Clipper Chip</h4>\n<p>The Clipper Chip is a cryptographic device purportedly intended to protect private communications while at the same time permitting government agents to obtain the &quot;keys&quot; upon presentation of what has been vaguely characterized as &quot;legal authorization.&quot;</p>\n<p>In 1994, Matt Blaze published the paper <em>Protocol Failure in the Escrowed Encryption Standard</em> which explained that any back door to encryption is as good as a closed <strong>unlocked</strong> home front door.</p>\n<p><strong><em>We know this was impossible in 1994!</em></strong><br>\n<strong><em>in <mark>1994</mark>..</em></strong></p>\n<p>For many business entities in Australia there aren't too many Compliance and Regulation Obligations that must be met that are considered under law, the Privacy Act is the best example of obligations under law. However that said, there are certainly a lot of other Compliance and Regulation Obligations for you to consider if you want to avoid fines or other unpleasant outcomes in Australia.</p>\n<h3 id=\"australianspecificcomplianceandregulationobligations\">Australian specific Compliance and Regulation Obligations</h3>\n<p>The following list is the obligations I personally encounter for my day job, it may be missing something that you're aware of, and is specifically in the lens of the Australian Government so excludes many other obligations that may apply to Australian business entities which are more broadly accepted outside of the Australian borders.</p>\n<ul>\n<li><a href=\"https://www.apra.gov.au/file/11771\">The Australian Prudential Regulation Authority (APRA)</a> CPG-234 Management of Security Risk in Information and Information Technology</li>\n<li>ACSC Publication <a href=\"https://acsc.gov.au/publications/protect/cloud-security-tenants.htm\">Cloud Computing Security for Tenants</a></li>\n<li><a href=\"https://www.protectivesecurity.gov.au/information/sensitive-classified-information/Pages/default.aspx\">Protective Security Policy Framework (PSPF)</a></li>\n<li>IRAP (Information Security Registered Assessors Program ) Assessment (document 5) version 0.04</li>\n<li><a href=\"https://acsc.gov.au/infosec/ism/index.htm\">ACSC Australian Government Information Security Manual (ISM)</a></li>\n<li><a href=\"https://acsc.gov.au/infosec/aisep/crypto.htm\">Australasian Information Security Evaluation Program (AISEP)</a></li>\n<li><a href=\"https://www.oaic.gov.au/images/documents/privacy/privacy-guides/information-security-guide-2013_WEB.pdf\">OAIC guidance</a></li>\n</ul>\n<blockquote>\n<p>What a list</p>\n</blockquote>\n<p>So let's go through each and why we should care.</p>\n<h3 id=\"apra\">APRA</h3>\n<p>In general terms this is the body that regulates banks, insurers, and companies that take fiat currency deposits, so as to protect <em>consumers</em> as the main goal. I emphasise <em>consumers</em> because we are mostly going to be looking at obligations where the concerned party is not the consumer, and APRA is a special case for this topic.</p>\n<h3 id=\"oaic\">OAIC</h3>\n<p>The entity that is responsible for Privacy Act related matters and enforcements, notably the mandatory notifiable data breach law introduced (before GDPR) that makes it unlawful for organisations to keep data breaches to themselves in Australia.</p>\n<h3 id=\"acsc\">ACSC</h3>\n<p>The ACSC is interesting in that there isn't anything in law specifically that it enforces, but rather it is responsible for many initiatives that help <strong>Agency</strong> organisation become more <em>secure</em>. The <strong>Agency</strong> reference carries with it special meaning in Australia, and to explain it properly would take up more then this whole post. In terms of <em>secure</em> we are making a distinction that we are not referring to privacy of Australians, or any concerns of the public at all, Agency security is in context of government interests alone.</p>\n<h3 id=\"irap\">IRAP</h3>\n<p>While not an obligation required by law, it is however a requirement for any business to interact technologically with the government, any of it's agencies, or in some cases if you operate in areas of public interest where some other regulatory body like APRA has not been established. For example the consultancy I currently work with are not considered to be in any of the above categories however we often work with our clients to achieve IRAP certification where we are the MSP, so we are in the scope of IRAP also.</p>\n<blockquote>\n<p>Failing to meet IRAP requirements can make or break a business</p>\n</blockquote>\n<h3 id=\"ismpspfaisep\">ISM / PSPF / AISEP</h3>\n<p>Similar to IRAP these are not obligation required under law for the public sector, however an <strong>Agency</strong> is expected to meet the obligations for each. As described with ACSC, Agency security is in context of government interests alone.<br>\nThe public sector is <em>encouraged</em> to align their business to the obligations set out for Agencies if they do not implement their own.</p>\n<h2 id=\"baselinesecuritypostureisencryption\">Baseline Security Posture is Encryption</h2>\n<p>With the mandatory notifiable data breach law, the expectation for all Australian businesses is to have a fairly high standard of Information Security, through the implementation of controls set out in many of the above as a preventative strategy.</p>\n<p>Rendering data unreadable through cryptographic or hashing means in case of a data breach is by law the baseline, so why then is it also enforcible by law that an organisation must produce in plain text private or sensitive data via TAN or TCN from law enforcement?</p>\n<h2 id=\"therequirementsandcontrols\">The Requirements and Controls</h2>\n<h4 id=\"irap\">IRAP</h4>\n<p>While controls are not uniquely IRAP, they are PSPF or ISM, some are Core Security Requirements (CSRs) that are interesting to note here;</p>\n<ul>\n<li>The SRMP <strong>must</strong> protect the personal and sensitive information of subscribers, authentication credentials including transaction histories,<br>\nbacked up and archived information. This wording contradicts all 3 enforcements of the new bill.</li>\n<li>The Applicant <strong>must</strong> immediately revoke digital certificates and associated keys that have been compromised. Are these enforcements a compromise to an individuals privacy or security?</li>\n<li>The Applicant <strong>should</strong> immediately suspend digital certificates and associated keys that are suspected by being compromised. Will the new data custodians respect an individuals privacy or security?</li>\n</ul>\n<p>An official example of critical Non Compliance states;</p>\n<blockquote>\n<p>The inappropriate storage of cryptographic keys, digital certificates, or<br>\npassphrases will be classified as a critical Non Compliance.</p>\n</blockquote>\n<p>Though not prescriptive it does detail that any control that states <strong>must</strong> will result in critical Non Compliance if not met, meaning in practice an organisation cannot achieve IRAP certification unless they adhere to the controls around encryption stated above, in the PSPF and ISM below.</p>\n<h4 id=\"pspf\">PSPF</h4>\n<p>Where encryption is defined in the PSPF a citation for information on cryptography, you're directed to the ISM. Some PSPF specific controls;</p>\n<ul>\n<li>PSPF43: use the encryption standards identified in the ISM for information transmitted over public network infrastructure. The encryption will sufficiently protect the information to allow it to be transmitted on an unclassified network. Basically apply the encryption standards identified in the ISM to protect information on their network infrastructure in unsecured areas.</li>\n<li>PSPF33: Do not allow information to be accessed merely because it would be convenient for personnel to know or because of their status, position, rank or level of authorised access. This is comically appropriate for the new bill, i literally couldn't make up a better control to conflict with the new bill.</li>\n<li>PSPF01: Ensure that personnel who access Australian Government resources are eligible to have access,have had their identity established, are suitable to have access, and agree to comply with the Government’s policies, standards, protocols and guidelines that safeguard resources from harm. This control ties directly with my recommendations for if/when you encounter a possible enforcement under the new bill.</li>\n</ul>\n<h4 id=\"ism\">ISM</h4>\n<p>The ISM has a section on <em>Guidelines for using cryptography</em> where it states;</p>\n<blockquote>\n<p>An Australian Signals Directorate (ASD) Cryptographic Evaluation (ACE) requirements supplement this document and where conflicts occur take precedence.</p>\n</blockquote>\n<p>Simply put, if you're subject to achieving ACE do so first then fulfill any outstanding requirements in the ISM.</p>\n<p>Key ISM take aways;</p>\n<ul>\n<li>FIPS 140-x is the standard only when concerned with cryptographic modules (HACE) compliance to the ISM.</li>\n<li>Security Control: 0455; Revision: 2; You <strong>Must</strong> provide the means to derive the plaintext recovery of data from the ciphertext without the use of the data encryption key used.</li>\n<li>Security Control: 1162; Revision: 3; <strong>Must</strong> Use encryption in-transit when sensitive data is exchanged over untrusted networks</li>\n<li>ASD Approved Cryptographic Algorithms, apart from the expected DH / ECDH / RSA / AES / 3DES etc, include SHA-2 hashing! Security Control: 1054; Revision: 4;</li>\n<li>ASD Approved Cryptographic Protocols, apart from the expected WPA2 / TLS / SSH / IPsec, it also approves OpenPGP specifically as well as S/MIME (which when used on its own is a shock). Security Control: 0490; Revision: 3;</li>\n<li>Security Control: 0142; Revision: 2, Control: 0143; Revision: 7, and Control: 1091; Revision: 4; All discuss <em>compromised</em> keys and what you must do, but are keys considered compromised after being subject to a TAN or TCN?</li>\n<li>Security Control 0571; Where users send email from outside their network, an authenticated and encrypted channel <strong>must</strong> be configured to allow email to be sent via the centralised email gateway.</li>\n<li>Security Control  0486; Applicants that allow passphrase authentication <strong>must</strong> use techniques to block brute force attempts against the passphrase.</li>\n</ul>\n<h4 id=\"asdaceaisep\">ASD ACE / AISEP</h4>\n<p>An ASD Cryptographic Evaluation test may include packet sniffing, black box testing, source code review, key management analysis, and Random Number Generation (RNG) evaluation.</p>\n<blockquote>\n<p>Cryptographic evaluations conducted in other nations such as CAPS, FIPS-140, and CMVP are not a replacement for an ASD Cryptographic Evaluation for Australian government agencies.</p>\n</blockquote>\n<p>Key take aways;</p>\n<ul>\n<li>ASD-approved cryptographic algorithms and ASD-approved cryptographic protocols defined in the ISM.</li>\n<li>Some products on the EPL do not have a consumer guide. They're not recommended for the use of cryptographic security or products that do not contain cryptographic security.</li>\n<li>An ASD-approved Protection Profile covers the necessary security functionality expected of the evaluated product and known security threats</li>\n<li>FPT_KST_EXT.2.1 from Protection Profile for Mobile Device. The TOE Security Functionality (TSF) shall not transmit any plaintext key material outside the security boundary of the target of evaluation (TOE). A TAN or TCN conflicts with this fundamentally.- FCS_CKM_EXT.4 Cryptographic Key Zeroization from Protection Profile for IPsec clients; requirements to ensure plaintext sensitive data such as key material and the plaintext secrets being protected have no known way to be recovered. This is of particular relevance in the context of TAN and TCN as it is in undisputed conflict.</li>\n</ul>\n<h4 id=\"apra\">APRA</h4>\n<p>Remember, APRA exists to protect the consumer.</p>\n<p>Here are some interesting take aways from the CPG-234;</p>\n<ul>\n<li>APRA envisages that a regulated institution would select algorithms from the population of well established and proven international standards that have been subjected to rigorous public scrutiny and verification of effectiveness (e.g. Triple DES38, AES39 etc.). The length of a cryptographic key would typically be selected to render a brute force attacks impractical (i.e. would require an extremely long period of time to breach using current computing capabilities).</li>\n<li>segregation of duties, with no single individual having knowledge of the entire cryptographic key (i.e. two-person controls) or having access to all the components making up these keys.</li>\n<li>A regulated institution would typically utilise tamper resistant devices to store and generate cryptographic keys, generate PINs and perform encryption and decryption. In most cases this would involve the use of Hardware Security Modules (HSMs) or similarly secured devices. These devices would be appropriately secured both physically and logically.</li>\n</ul>\n<p>Basically if any APRA regulated entity received a TCN it must find a deliberately insecure HSM, one that the rest of the world would never use and might only exist in fantasy, and don't forget that the TCN would essentially make the APRA regulated entity lose it's license for no other reason then it's inability to meet CPG-234 F.7.</p>\n<h4 id=\"acsc\">ACSC</h4>\n<p>Honorable mention.<br>\nThe ACSC release guidance in certain area where clarity is needed, a great example most organisations will be falimiar with is the Cloud Computing Security for Tenants guidance. This and most others will reference back to the ASD-approved cryptographic controls or ISM with few exceptions.<br>\nIn the case of this CSP guidance the exception is control 12;</p>\n<blockquote>\n<p>Perform up-to-date encrypted backups in a format avoiding CSP lock-in, stored offline at the tenants premises or at a second CSP requiring multi-factor authentication to modify/delete data. Annually test the recovery process.</p>\n</blockquote>\n<p>While not so different from the ISM, there is specific reference to multi-factor authentication which hasn't been all that common until late 2018.</p>\n<h4 id=\"oaic\">OAIC</h4>\n<p>Several investigations and case studies provide us with a clear understanding of how encryption is considered in the context of a data breach and any legal proceedings that may follow.</p>\n<ol>\n<li>Case study: Cupid Media Pty Ltd</li>\n</ol>\n<blockquote>\n<p>my investigation found that, at the time of the incident, Cupid did not have password encryption processes in place. Password encryption is a basic security strategy that may prevent unauthorised access to user accounts. Cupid insecurely stored passwords in plain text, and I found that to be a failure to take reasonable security steps, as required under the Privacy Act.</p>\n</blockquote>\n<p>Statement of common situations</p>\n<blockquote>\n<p>The loss of portable devices containing personal information. Laptops, external hard drives of USB sticks are often left in taxis, on trains, or misplaced in the office. This is a known risk, however often these devices are not encrypted</p>\n</blockquote>\n<p>Again encryption is the recommendation.</p>\n<p>The result;</p>\n<blockquote>\n<p>Password encryption strategies such as hashing and salting are basic security steps that were available to Cupid at the time of the data breach that may have prevented unauthorised access to user accounts. The Commissioner therefore found Cupid’s storage of passwords in plain text to be a failure to take reasonable security steps for the purpose of NPP 4.1.</p>\n</blockquote>\n<ol start=\"2\">\n<li>Another example; Adobe</li>\n</ol>\n<blockquote>\n<p>While Adobe generally took a sophisticated and layered approach to information security and the protection of its IT systems, it failed to implement consistently strong security measures across its various internal systems. In particular, a backup server stored a database of unencrypted credential information (email addresses and password hints) of over 1.7 million Australian users, directly linked to the encrypted password for each user. The type of encryption used (block cipher), together with plaintext password hints, allowed security experts with to the database</p>\n</blockquote>\n<p>The verdict;</p>\n<blockquote>\n<p>The Commissioner found that Adobe breached NPP 4 by failing to take reasonable steps to protect the personal information it held from misuse and loss and from unauthorised access, modification or disclosure</p>\n</blockquote>\n<p>What is the OAIC view on the new bill?</p>\n<blockquote>\n<p>The OAIC acknowledges that the power to issue TANs and TCNs is subject to certain limitations, including that a TAN or TCN must not have the effect of requiring a systemic weakness or vulnerability to be built into a form of electronic protection. However, it will be necessary to ensure that the measures proposed in Schedule 1 do not, in practice, introduce unintended exploitable weaknesses into a telecommunications environment that fundamentally relies on strong and robust security settings. While the OAIC acknowledges that the Bill and explanatory memorandum before the Committee addresses some of the recommendations in the OAIC’s earlier submission, privacy risks remain.</p>\n</blockquote>\n<p>Basically, that opportunity to make a submission to the Department of Home Affairs lasted single digit weeks. What a ruse. The Senate passed the bill in contempt of any submissions made and bypassing any conclusions the committee might have made thereafter. So the OAIC statement above is essentially meaningless.</p>\n<p>The OAIC themselves made a submission to the committee with 10 recommendations that were not addressed by the bill passed through Senate, but the public is assured that maybe one of the recommendations <em>define the terms ‘systemic weakness’ and ‘systemic vulnerability’ in s 317ZG</em> might be addressed in typical post-truth politics. I digress.</p>\n<p>Basically, so long as the 13 legally binding APPs are far from prescriptive, subject to the discretion of the privacy commissioner, the new bill will inevitably supersede privacy, so as to not make the government appear contradictory or any more contemptuous to the public then it is already perceived.</p>\n<h3 id=\"finalthoughts\">Final thoughts</h3>\n<p>Basically if your privacy or security is compromised from an APRA regulated company and data was decrypted, it is fair to state there were certainly more than 1 key custodian responsible for the breach and one might assume the act of disclosure was not an accident.</p>\n<p>The decision-making criteria that apply to TAN and TCN, and the matters to be consider when applying those criteria, should also be extended to TAR (not part of the bill). A compliance to a TAR should include legitimate expectations relating to privacy and cybersecurity, such as reasonableness, proportionality, practicability, and technical feasibility. The subject of the TAR will more likely comply out of fear of legal proceedings and make mistakes that compromise individuals privacy and security from a lack of knowledge and lack of formal process or decision-making, which is far more damaging then the current state and not in the intent of the bill.</p>\n<p>In terms of the AISEP you'll fail the ASD ACE by complying with a TAN or TCN, looking at 2 of dozens of protection profiles, one for mobile device communications and the other for IPSec. Simply doing a quick search I identified a fundamental conflict in both, further analysis must yield a few more. Did the people writing the anti-encryption bill even consider their own existing rules? They mustn't be aware of the ASD, or are intentionally ignorant knowing full-well the conflict in an attempt to avoid the inevitable analysis that would make them look like fools.</p>\n<p>Finally; Considering the Notifiable Data Breaches scheme under Part IIIC of the Privacy Act. The fact that personal information is encrypted may reduce the likelihood of serious harm in the event of a data breach and therefore avoid the requirement to notify the OAIC or affected individuals entirely. Did the committee or Senate consider this new NDB Law or the older Privacy Act? Obviously not, or the payoffs were high enough they simply don't care.</p>\n<p>And the best part, the PSPF33 control spells out that data shouldn't be accessed just because the person wanting access can get that access. This is in context stating that a warrant should be provided or how can a business possibly know if an external individual or entity should get access to what they are asking for and not just asking because they can ask (which violates PSPF33).</p>\n<h2 id=\"whattodoifyougetanenforcement\">What to do if you get an enforcement</h2>\n<p>At a minimum, do the following:</p>\n<ul>\n<li>Identity verification, If they identify as law enforcement call the appropriate law enforcement office to make sure it is not a bad actor.</li>\n<li>Criminal record check. This is probably not possible, but if the individual is not identified as law enforcement and you have the means to conduct a Criminal record check i suggest you do so immediately with their consent.</li>\n<li>Eligibility checks, consult with your legal advisor, make sure the requested data and circumstances are all legal at the time of the enforcement. You are in your legal right to check if your own actions are legally appropriate.</li>\n<li>Do not perform any proposed actions outside normal business operations without first consulting your legal advisor specifically about the action.</li>\n</ul>\n<p>Speaking from the point of view of a penetration tester i can see the use of these enforcement letters being a fantastic tool to test my target, not posing as law enforcement of course but a convincing TAR might just trick most organisations.</p>\n<!--kg-card-end: markdown-->","comment_id":"5c3d78bbca804b06bb5dc779","plaintext":"The Australian Government's controversial encryption bill passed the Senate\n[https://www.abc.net.au/news/2018-12-06/labor-backdown-federal-government-to-pass-greater-surveillance/10591944] \nlast month and will undoubtedly be law soon.\n\nThe bill proposes three key powers;\n\nA technical assistance request (TAR): Police ask a company to \"voluntarily\"\nhelp, such as give technical details about the development of a new online\nservice\n\nA technical assistance notice (TAN): A company is required to give assistance.\nFor example, if they can decrypt a specific communication, they must or face\nfines\n\nA technical capability notice (TCN): The company must build a new function to\nhelp police get at a suspect's data, or face fines\n\nSound familiar?\nThe Clipper Chip\nThe Clipper Chip is a cryptographic device purportedly intended to protect\nprivate communications while at the same time permitting government agents to\nobtain the \"keys\" upon presentation of what has been vaguely characterized as\n\"legal authorization.\"\n\nIn 1994, Matt Blaze published the paper Protocol Failure in the Escrowed\nEncryption Standard which explained that any back door to encryption is as good\nas a closed unlocked home front door.\n\nWe know this was impossible in 1994!\nin 1994..\n\nFor many business entities in Australia there aren't too many Compliance and\nRegulation Obligations that must be met that are considered under law, the\nPrivacy Act is the best example of obligations under law. However that said,\nthere are certainly a lot of other Compliance and Regulation Obligations for you\nto consider if you want to avoid fines or other unpleasant outcomes in\nAustralia.\n\nAustralian specific Compliance and Regulation Obligations\nThe following list is the obligations I personally encounter for my day job, it\nmay be missing something that you're aware of, and is specifically in the lens\nof the Australian Government so excludes many other obligations that may apply\nto Australian business entities which are more broadly accepted outside of the\nAustralian borders.\n\n * The Australian Prudential Regulation Authority (APRA)\n   [https://www.apra.gov.au/file/11771] CPG-234 Management of Security Risk in\n   Information and Information Technology\n * ACSC Publication Cloud Computing Security for Tenants\n   [https://acsc.gov.au/publications/protect/cloud-security-tenants.htm]\n * Protective Security Policy Framework (PSPF)\n   [https://www.protectivesecurity.gov.au/information/sensitive-classified-information/Pages/default.aspx]\n * IRAP (Information Security Registered Assessors Program ) Assessment\n   (document 5) version 0.04\n * ACSC Australian Government Information Security Manual (ISM)\n   [https://acsc.gov.au/infosec/ism/index.htm]\n * Australasian Information Security Evaluation Program (AISEP)\n   [https://acsc.gov.au/infosec/aisep/crypto.htm]\n * OAIC guidance\n   [https://www.oaic.gov.au/images/documents/privacy/privacy-guides/information-security-guide-2013_WEB.pdf]\n\n> What a list\n\n\nSo let's go through each and why we should care.\n\nAPRA\nIn general terms this is the body that regulates banks, insurers, and companies\nthat take fiat currency deposits, so as to protect consumers as the main goal. I\nemphasise consumers because we are mostly going to be looking at obligations\nwhere the concerned party is not the consumer, and APRA is a special case for\nthis topic.\n\nOAIC\nThe entity that is responsible for Privacy Act related matters and enforcements,\nnotably the mandatory notifiable data breach law introduced (before GDPR) that\nmakes it unlawful for organisations to keep data breaches to themselves in\nAustralia.\n\nACSC\nThe ACSC is interesting in that there isn't anything in law specifically that it\nenforces, but rather it is responsible for many initiatives that help Agency \norganisation become more secure. The Agency reference carries with it special\nmeaning in Australia, and to explain it properly would take up more then this\nwhole post. In terms of secure we are making a distinction that we are not\nreferring to privacy of Australians, or any concerns of the public at all,\nAgency security is in context of government interests alone.\n\nIRAP\nWhile not an obligation required by law, it is however a requirement for any\nbusiness to interact technologically with the government, any of it's agencies,\nor in some cases if you operate in areas of public interest where some other\nregulatory body like APRA has not been established. For example the consultancy\nI currently work with are not considered to be in any of the above categories\nhowever we often work with our clients to achieve IRAP certification where we\nare the MSP, so we are in the scope of IRAP also.\n\n> Failing to meet IRAP requirements can make or break a business\n\n\nISM / PSPF / AISEP\nSimilar to IRAP these are not obligation required under law for the public\nsector, however an Agency is expected to meet the obligations for each. As\ndescribed with ACSC, Agency security is in context of government interests\nalone.\nThe public sector is encouraged to align their business to the obligations set\nout for Agencies if they do not implement their own.\n\nBaseline Security Posture is Encryption\nWith the mandatory notifiable data breach law, the expectation for all\nAustralian businesses is to have a fairly high standard of Information Security,\nthrough the implementation of controls set out in many of the above as a\npreventative strategy.\n\nRendering data unreadable through cryptographic or hashing means in case of a\ndata breach is by law the baseline, so why then is it also enforcible by law\nthat an organisation must produce in plain text private or sensitive data via\nTAN or TCN from law enforcement?\n\nThe Requirements and Controls\nIRAP\nWhile controls are not uniquely IRAP, they are PSPF or ISM, some are Core\nSecurity Requirements (CSRs) that are interesting to note here;\n\n * The SRMP must protect the personal and sensitive information of subscribers,\n   authentication credentials including transaction histories,\n   backed up and archived information. This wording contradicts all 3\n   enforcements of the new bill.\n * The Applicant must immediately revoke digital certificates and associated\n   keys that have been compromised. Are these enforcements a compromise to an\n   individuals privacy or security?\n * The Applicant should immediately suspend digital certificates and associated\n   keys that are suspected by being compromised. Will the new data custodians\n   respect an individuals privacy or security?\n\nAn official example of critical Non Compliance states;\n\n> The inappropriate storage of cryptographic keys, digital certificates, or\npassphrases will be classified as a critical Non Compliance.\n\n\nThough not prescriptive it does detail that any control that states must will\nresult in critical Non Compliance if not met, meaning in practice an\norganisation cannot achieve IRAP certification unless they adhere to the\ncontrols around encryption stated above, in the PSPF and ISM below.\n\nPSPF\nWhere encryption is defined in the PSPF a citation for information on\ncryptography, you're directed to the ISM. Some PSPF specific controls;\n\n * PSPF43: use the encryption standards identified in the ISM for information\n   transmitted over public network infrastructure. The encryption will\n   sufficiently protect the information to allow it to be transmitted on an\n   unclassified network. Basically apply the encryption standards identified in\n   the ISM to protect information on their network infrastructure in unsecured\n   areas.\n * PSPF33: Do not allow information to be accessed merely because it would be\n   convenient for personnel to know or because of their status, position, rank\n   or level of authorised access. This is comically appropriate for the new\n   bill, i literally couldn't make up a better control to conflict with the new\n   bill.\n * PSPF01: Ensure that personnel who access Australian Government resources are\n   eligible to have access,have had their identity established, are suitable to\n   have access, and agree to comply with the Government’s policies, standards,\n   protocols and guidelines that safeguard resources from harm. This control\n   ties directly with my recommendations for if/when you encounter a possible\n   enforcement under the new bill.\n\nISM\nThe ISM has a section on Guidelines for using cryptography where it states;\n\n> An Australian Signals Directorate (ASD) Cryptographic Evaluation (ACE)\nrequirements supplement this document and where conflicts occur take precedence.\n\n\nSimply put, if you're subject to achieving ACE do so first then fulfill any\noutstanding requirements in the ISM.\n\nKey ISM take aways;\n\n * FIPS 140-x is the standard only when concerned with cryptographic modules\n   (HACE) compliance to the ISM.\n * Security Control: 0455; Revision: 2; You Must provide the means to derive the\n   plaintext recovery of data from the ciphertext without the use of the data\n   encryption key used.\n * Security Control: 1162; Revision: 3; Must Use encryption in-transit when\n   sensitive data is exchanged over untrusted networks\n * ASD Approved Cryptographic Algorithms, apart from the expected DH / ECDH /\n   RSA / AES / 3DES etc, include SHA-2 hashing! Security Control: 1054;\n   Revision: 4;\n * ASD Approved Cryptographic Protocols, apart from the expected WPA2 / TLS /\n   SSH / IPsec, it also approves OpenPGP specifically as well as S/MIME (which\n   when used on its own is a shock). Security Control: 0490; Revision: 3;\n * Security Control: 0142; Revision: 2, Control: 0143; Revision: 7, and Control:\n   1091; Revision: 4; All discuss compromised keys and what you must do, but are\n   keys considered compromised after being subject to a TAN or TCN?\n * Security Control 0571; Where users send email from outside their network, an\n   authenticated and encrypted channel must be configured to allow email to be\n   sent via the centralised email gateway.\n * Security Control 0486; Applicants that allow passphrase authentication must \n   use techniques to block brute force attempts against the passphrase.\n\nASD ACE / AISEP\nAn ASD Cryptographic Evaluation test may include packet sniffing, black box\ntesting, source code review, key management analysis, and Random Number\nGeneration (RNG) evaluation.\n\n> Cryptographic evaluations conducted in other nations such as CAPS, FIPS-140, and\nCMVP are not a replacement for an ASD Cryptographic Evaluation for Australian\ngovernment agencies.\n\n\nKey take aways;\n\n * ASD-approved cryptographic algorithms and ASD-approved cryptographic\n   protocols defined in the ISM.\n * Some products on the EPL do not have a consumer guide. They're not\n   recommended for the use of cryptographic security or products that do not\n   contain cryptographic security.\n * An ASD-approved Protection Profile covers the necessary security\n   functionality expected of the evaluated product and known security threats\n * FPT_KST_EXT.2.1 from Protection Profile for Mobile Device. The TOE Security\n   Functionality (TSF) shall not transmit any plaintext key material outside the\n   security boundary of the target of evaluation (TOE). A TAN or TCN conflicts\n   with this fundamentally.- FCS_CKM_EXT.4 Cryptographic Key Zeroization from\n   Protection Profile for IPsec clients; requirements to ensure plaintext\n   sensitive data such as key material and the plaintext secrets being protected\n   have no known way to be recovered. This is of particular relevance in the\n   context of TAN and TCN as it is in undisputed conflict.\n\nAPRA\nRemember, APRA exists to protect the consumer.\n\nHere are some interesting take aways from the CPG-234;\n\n * APRA envisages that a regulated institution would select algorithms from the\n   population of well established and proven international standards that have\n   been subjected to rigorous public scrutiny and verification of effectiveness\n   (e.g. Triple DES38, AES39 etc.). The length of a cryptographic key would\n   typically be selected to render a brute force attacks impractical (i.e. would\n   require an extremely long period of time to breach using current computing\n   capabilities).\n * segregation of duties, with no single individual having knowledge of the\n   entire cryptographic key (i.e. two-person controls) or having access to all\n   the components making up these keys.\n * A regulated institution would typically utilise tamper resistant devices to\n   store and generate cryptographic keys, generate PINs and perform encryption\n   and decryption. In most cases this would involve the use of Hardware Security\n   Modules (HSMs) or similarly secured devices. These devices would be\n   appropriately secured both physically and logically.\n\nBasically if any APRA regulated entity received a TCN it must find a\ndeliberately insecure HSM, one that the rest of the world would never use and\nmight only exist in fantasy, and don't forget that the TCN would essentially\nmake the APRA regulated entity lose it's license for no other reason then it's\ninability to meet CPG-234 F.7.\n\nACSC\nHonorable mention.\nThe ACSC release guidance in certain area where clarity is needed, a great\nexample most organisations will be falimiar with is the Cloud Computing Security\nfor Tenants guidance. This and most others will reference back to the\nASD-approved cryptographic controls or ISM with few exceptions.\nIn the case of this CSP guidance the exception is control 12;\n\n> Perform up-to-date encrypted backups in a format avoiding CSP lock-in, stored\noffline at the tenants premises or at a second CSP requiring multi-factor\nauthentication to modify/delete data. Annually test the recovery process.\n\n\nWhile not so different from the ISM, there is specific reference to multi-factor\nauthentication which hasn't been all that common until late 2018.\n\nOAIC\nSeveral investigations and case studies provide us with a clear understanding of\nhow encryption is considered in the context of a data breach and any legal\nproceedings that may follow.\n\n 1. Case study: Cupid Media Pty Ltd\n\n> my investigation found that, at the time of the incident, Cupid did not have\npassword encryption processes in place. Password encryption is a basic security\nstrategy that may prevent unauthorised access to user accounts. Cupid insecurely\nstored passwords in plain text, and I found that to be a failure to take\nreasonable security steps, as required under the Privacy Act.\n\n\nStatement of common situations\n\n> The loss of portable devices containing personal information. Laptops, external\nhard drives of USB sticks are often left in taxis, on trains, or misplaced in\nthe office. This is a known risk, however often these devices are not encrypted\n\n\nAgain encryption is the recommendation.\n\nThe result;\n\n> Password encryption strategies such as hashing and salting are basic security\nsteps that were available to Cupid at the time of the data breach that may have\nprevented unauthorised access to user accounts. The Commissioner therefore found\nCupid’s storage of passwords in plain text to be a failure to take reasonable\nsecurity steps for the purpose of NPP 4.1.\n\n\n 2. Another example; Adobe\n\n> While Adobe generally took a sophisticated and layered approach to information\nsecurity and the protection of its IT systems, it failed to implement\nconsistently strong security measures across its various internal systems. In\nparticular, a backup server stored a database of unencrypted credential\ninformation (email addresses and password hints) of over 1.7 million Australian\nusers, directly linked to the encrypted password for each user. The type of\nencryption used (block cipher), together with plaintext password hints, allowed\nsecurity experts with to the database\n\n\nThe verdict;\n\n> The Commissioner found that Adobe breached NPP 4 by failing to take reasonable\nsteps to protect the personal information it held from misuse and loss and from\nunauthorised access, modification or disclosure\n\n\nWhat is the OAIC view on the new bill?\n\n> The OAIC acknowledges that the power to issue TANs and TCNs is subject to\ncertain limitations, including that a TAN or TCN must not have the effect of\nrequiring a systemic weakness or vulnerability to be built into a form of\nelectronic protection. However, it will be necessary to ensure that the measures\nproposed in Schedule 1 do not, in practice, introduce unintended exploitable\nweaknesses into a telecommunications environment that fundamentally relies on\nstrong and robust security settings. While the OAIC acknowledges that the Bill\nand explanatory memorandum before the Committee addresses some of the\nrecommendations in the OAIC’s earlier submission, privacy risks remain.\n\n\nBasically, that opportunity to make a submission to the Department of Home\nAffairs lasted single digit weeks. What a ruse. The Senate passed the bill in\ncontempt of any submissions made and bypassing any conclusions the committee\nmight have made thereafter. So the OAIC statement above is essentially\nmeaningless.\n\nThe OAIC themselves made a submission to the committee with 10 recommendations\nthat were not addressed by the bill passed through Senate, but the public is\nassured that maybe one of the recommendations define the terms ‘systemic\nweakness’ and ‘systemic vulnerability’ in s 317ZG might be addressed in typical\npost-truth politics. I digress.\n\nBasically, so long as the 13 legally binding APPs are far from prescriptive,\nsubject to the discretion of the privacy commissioner, the new bill will\ninevitably supersede privacy, so as to not make the government appear\ncontradictory or any more contemptuous to the public then it is already\nperceived.\n\nFinal thoughts\nBasically if your privacy or security is compromised from an APRA regulated\ncompany and data was decrypted, it is fair to state there were certainly more\nthan 1 key custodian responsible for the breach and one might assume the act of\ndisclosure was not an accident.\n\nThe decision-making criteria that apply to TAN and TCN, and the matters to be\nconsider when applying those criteria, should also be extended to TAR (not part\nof the bill). A compliance to a TAR should include legitimate expectations\nrelating to privacy and cybersecurity, such as reasonableness, proportionality,\npracticability, and technical feasibility. The subject of the TAR will more\nlikely comply out of fear of legal proceedings and make mistakes that compromise\nindividuals privacy and security from a lack of knowledge and lack of formal\nprocess or decision-making, which is far more damaging then the current state\nand not in the intent of the bill.\n\nIn terms of the AISEP you'll fail the ASD ACE by complying with a TAN or TCN,\nlooking at 2 of dozens of protection profiles, one for mobile device\ncommunications and the other for IPSec. Simply doing a quick search I identified\na fundamental conflict in both, further analysis must yield a few more. Did the\npeople writing the anti-encryption bill even consider their own existing rules?\nThey mustn't be aware of the ASD, or are intentionally ignorant knowing\nfull-well the conflict in an attempt to avoid the inevitable analysis that would\nmake them look like fools.\n\nFinally; Considering the Notifiable Data Breaches scheme under Part IIIC of the\nPrivacy Act. The fact that personal information is encrypted may reduce the\nlikelihood of serious harm in the event of a data breach and therefore avoid the\nrequirement to notify the OAIC or affected individuals entirely. Did the\ncommittee or Senate consider this new NDB Law or the older Privacy Act?\nObviously not, or the payoffs were high enough they simply don't care.\n\nAnd the best part, the PSPF33 control spells out that data shouldn't be accessed\njust because the person wanting access can get that access. This is in context\nstating that a warrant should be provided or how can a business possibly know if\nan external individual or entity should get access to what they are asking for\nand not just asking because they can ask (which violates PSPF33).\n\nWhat to do if you get an enforcement\nAt a minimum, do the following:\n\n * Identity verification, If they identify as law enforcement call the\n   appropriate law enforcement office to make sure it is not a bad actor.\n * Criminal record check. This is probably not possible, but if the individual\n   is not identified as law enforcement and you have the means to conduct a\n   Criminal record check i suggest you do so immediately with their consent.\n * Eligibility checks, consult with your legal advisor, make sure the requested\n   data and circumstances are all legal at the time of the enforcement. You are\n   in your legal right to check if your own actions are legally appropriate.\n * Do not perform any proposed actions outside normal business operations\n   without first consulting your legal advisor specifically about the action.\n\nSpeaking from the point of view of a penetration tester i can see the use of\nthese enforcement letters being a fantastic tool to test my target, not posing\nas law enforcement of course but a convincing TAR might just trick most\norganisations.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/assistance-and-access-bill.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2019-01-15 06:07:55","created_by":"1","updated_at":"2021-03-31 14:00:00","updated_by":"1","published_at":"2019-01-15 14:49:14","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc4","uuid":"ff3ec64d-8ac2-4440-8c23-5b5f41cd5e04","title":"Risk Assessment The Right Way","slug":"risk-assessment-the-right-way","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Stop wasting effort on risk assessments that raise more questions than they address, get it right the first time.\\n\\n1. [ One-time preparation. ](#s1)\\n    - [ Define the Process ](#s1c1)\\n    - [ Risk Appetite ](#s1c2)\\n    - [ Risk Modeling ](#s1c3)\\n        - [ Risk Matrix ](#s1c4)\\n        - [ Likelihood ](#s1c5)\\n        - [ Impact ](#s1c6)\\n    - [ Data Classification ](#s1c7)\\n2. [ Report Preparations ](#s2)\\n    - [ Compliance ](#s2c1)\\n    - [ Threat Modeling ](#s2c2)\\n    - [ Pricing Model ](#s2c3)\\n    - [ Capabilities and Limitations ](#s2c4)\\n    - [ Monitoring ](#s2c4)\\n    - [ Access Control ](#s2c5)\\n3. [ Assessment Analysis ](#s3)\\n    - [ Risk Domains ](#s3c1)\\n    - [ Workbook ](#s3c2)\\n    - [ Inherent Risk ](#s3c3)\\n    - [ Threat Actors and Motivation ](#s3c4)\\n    - [ Controls ](#s3c5)\\n    - [ Residual Risk ](#s3c6)\\n4. [ Reporting ](#s4)\\n5. [ Conclusion ](#s5)\\n\\n<a name=\\\"s1\\\"></a>\\n## One-time Preparation\\n\\nThe items in this section will be re-usable for each report and save a lot of time. Hopefully the business already provides resources that can help you rapidly develop these resources, otherwise you are in for a few long interviews with your colleagues to learn, and likely let them define for you, each of the following.\\n\\n<a name=\\\"s1c1\\\"></a>\\n### Define the Process\\n\\nIf your business has any regulation obligations it is highly likely that a prescriptive risk assessment process is part of that regulators white papers or guidance material.\\n\\nIf you do not have a regulatory authority to report to, consider the contents of this post and start developing a process during your first risk assessment, keeping up incremental improvements as you practice.\\n\\nSome key considerations for a process is to focus on determining service and features to be assessed, any compliance documentation, and if the service is in-scope. \\n\\nTry to remember that doing a risk assessment should also determine the suitability of the service to the business. Don't be afraid to point out reasons why the service may not be suitable, in fact build the risk assessment process expressly with this in mind to encourage critical thinking and eliminating bias and conflict of interest.\\n\\n<a name=\\\"s1c2\\\"></a>\\n### Risk Appetite\\n\\nThis is a disputed topic, some might talk to you about how risk is 'accepted' or registered and planned work allocated. These are concepts for risk management rather than risk assessment.\\n\\nIn context of assessing risk, the appetite plays a roll in the scrutiny applied to the inherent risk. This is a topic to be explored later, but for now it is important to explore the risk appetite in your organisation by speaking to the CIO, CISA, CFO, and really any interested stakeholder. \\nDepending on who you're speaking to, try to find out the tolerance acceptance, how much financial loss or brand and reputation damage would be considered an incident, and beyond that how much impact would raise that to higher levels of concern. \\n\\n<a name=\\\"s1c3\\\"></a>\\n### Risk Modeling\\n\\nA risk model should represent a distinct and unique perspective for your business. Anything generic or borrowed will be a critical flaw in your risk assessment attempt.\\n\\nWith consideration to the investigations into risk appetite, formalise the risk profile into a Risk Matrix complete with justification and definition to the different levels of likelihood and impact. Once these risk tables are defined, these should be presented to stakeholders and accepted.\\n\\n<a name=\\\"s1c4\\\"></a>\\n#### Risk Matrix\\n\\nThere are many sources available to you for inspiration, from study materials for CISSP, to NIST, and the various frameworks out there. The basics of a risk matrix are impacts and likelihood, where each has their own level of calculation such as certainty for likelihood, and the intersection of the two  is a measurement from various levels ranging from Low to High.\\n\\n<a name=\\\"s1c5\\\"></a>\\n#### Likelihood\\n\\nConsider the risk, if it is expected to occur in most circumstances or events, frequency in the wild, and probability your business is a target.\\n\\nUse intelligence feeds that indicate targeted industries being exploited, viewed over a long period of time to see trends, and whether public exploits are known.\\n\\n<a name=\\\"s1c6\\\"></a>\\n#### Impact\\n\\nThis ties directly into the Risk Appetite addressed earlier, more than that you should consider the severity of the impact based on the areas of impact given the nature of the risk. Some risk will incur a financial impact, others are stakeholder confidence, brand or reputation damage, but in the extreme you could face sanctions, legal action, or loss of life.\\n\\nSetting a clearly defined impact rating with clear and well thought out descriptions will help you to produce valuable risk assessments going forward.\\n\\n<a name=\\\"s1c7\\\"></a>\\n### Data Classification\\n\\nIt is highly likely that all assessments conducted will include some sort of data transmission or storage. Understanding what the data sensitivity is through a data dictionary and analysis that produces a clear classification model will be instrumental for your risk assessments.\\n\\nOne piece of advice from experience, try to first figure out how you would classify data yourself when presented with a new development and analysis task. If you can't marry some new data tot he classification model you developed, how do you expect your colleagues too going forward?\\n\\n<a name=\\\"s2\\\"></a>\\n## Report Preparations\\n\\nIn this section we explore the tricks that may not at first seem risk or even information security related, but will be invaluable to you when you hit that blocker down the track of how you should approach a service to discover it's vulnerabilities and risk.\\n\\nThese tricks should invoke critical thinking and scrutiny in key areas of risk, and the more you prepare content for these areas the easier it will be for you to more identify inherent risk in your assessments.\\n\\n<a name=\\\"s2c1\\\"></a>\\n### Compliance\\n\\nThe best short-cut we have in assessing risk, but try to remain skeptical as you'll quickly learn that vendor compliance can be a lot of smoke and mirrors.\\n\\n<a name=\\\"s2c2\\\"></a>\\n### Threat Modeling\\n\\n\\n\\n<a name=\\\"s2c3\\\"></a>\\n### Pricing Model\\n\\n\\n\\n<a name=\\\"s2c4\\\"></a>\\n### Capabilities and Limitations\\n\\n\\n\\n\\n<a name=\\\"s2c5\\\"></a>\\n### Monitoring\\n\\n\\n\\n\\n<a name=\\\"s2c6\\\"></a>\\n### Access Control\\n\\n\\n\\n\\n<a name=\\\"s3\\\"></a>\\n## Assessment Analysis\\n\\n\\n\\n\\n<a name=\\\"s3c1\\\"></a>\\n### Risk Domains\\n\\n\\n\\n<a name=\\\"s3c2\\\"></a>\\n### Workbook\\n\\n\\n\\n<a name=\\\"s3c3\\\"></a>\\n### Inherent Risk\\n\\n\\n\\n<a name=\\\"s3c4\\\"></a>\\n### Threat Actors and Motivation\\n\\n\\n\\n<a name=\\\"s3c5\\\"></a>\\n### Controls\\n\\n\\n\\n<a name=\\\"s3c6\\\"></a>\\n### Residual Risk\\n\\n\\n\\n<a name=\\\"s4\\\"></a>\\n## Reporting\\n\\n\\n\\n<a name=\\\"s5\\\"></a>\\n## Conclusion\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Stop wasting effort on risk assessments that raise more questions than they address, get it right the first time.</p>\n<ol>\n<li><a href=\"#s1\"> One-time preparation. </a>\n<ul>\n<li><a href=\"#s1c1\"> Define the Process </a></li>\n<li><a href=\"#s1c2\"> Risk Appetite </a></li>\n<li><a href=\"#s1c3\"> Risk Modeling </a>\n<ul>\n<li><a href=\"#s1c4\"> Risk Matrix </a></li>\n<li><a href=\"#s1c5\"> Likelihood </a></li>\n<li><a href=\"#s1c6\"> Impact </a></li>\n</ul>\n</li>\n<li><a href=\"#s1c7\"> Data Classification </a></li>\n</ul>\n</li>\n<li><a href=\"#s2\"> Report Preparations </a>\n<ul>\n<li><a href=\"#s2c1\"> Compliance </a></li>\n<li><a href=\"#s2c2\"> Threat Modeling </a></li>\n<li><a href=\"#s2c3\"> Pricing Model </a></li>\n<li><a href=\"#s2c4\"> Capabilities and Limitations </a></li>\n<li><a href=\"#s2c4\"> Monitoring </a></li>\n<li><a href=\"#s2c5\"> Access Control </a></li>\n</ul>\n</li>\n<li><a href=\"#s3\"> Assessment Analysis </a>\n<ul>\n<li><a href=\"#s3c1\"> Risk Domains </a></li>\n<li><a href=\"#s3c2\"> Workbook </a></li>\n<li><a href=\"#s3c3\"> Inherent Risk </a></li>\n<li><a href=\"#s3c4\"> Threat Actors and Motivation </a></li>\n<li><a href=\"#s3c5\"> Controls </a></li>\n<li><a href=\"#s3c6\"> Residual Risk </a></li>\n</ul>\n</li>\n<li><a href=\"#s4\"> Reporting </a></li>\n<li><a href=\"#s5\"> Conclusion </a></li>\n</ol>\n<p><a name=\"s1\"></a></p>\n<h2 id=\"onetimepreparation\">One-time Preparation</h2>\n<p>The items in this section will be re-usable for each report and save a lot of time. Hopefully the business already provides resources that can help you rapidly develop these resources, otherwise you are in for a few long interviews with your colleagues to learn, and likely let them define for you, each of the following.</p>\n<p><a name=\"s1c1\"></a></p>\n<h3 id=\"definetheprocess\">Define the Process</h3>\n<p>If your business has any regulation obligations it is highly likely that a prescriptive risk assessment process is part of that regulators white papers or guidance material.</p>\n<p>If you do not have a regulatory authority to report to, consider the contents of this post and start developing a process during your first risk assessment, keeping up incremental improvements as you practice.</p>\n<p>Some key considerations for a process is to focus on determining service and features to be assessed, any compliance documentation, and if the service is in-scope.</p>\n<p>Try to remember that doing a risk assessment should also determine the suitability of the service to the business. Don't be afraid to point out reasons why the service may not be suitable, in fact build the risk assessment process expressly with this in mind to encourage critical thinking and eliminating bias and conflict of interest.</p>\n<p><a name=\"s1c2\"></a></p>\n<h3 id=\"riskappetite\">Risk Appetite</h3>\n<p>This is a disputed topic, some might talk to you about how risk is 'accepted' or registered and planned work allocated. These are concepts for risk management rather than risk assessment.</p>\n<p>In context of assessing risk, the appetite plays a roll in the scrutiny applied to the inherent risk. This is a topic to be explored later, but for now it is important to explore the risk appetite in your organisation by speaking to the CIO, CISA, CFO, and really any interested stakeholder.<br>\nDepending on who you're speaking to, try to find out the tolerance acceptance, how much financial loss or brand and reputation damage would be considered an incident, and beyond that how much impact would raise that to higher levels of concern.</p>\n<p><a name=\"s1c3\"></a></p>\n<h3 id=\"riskmodeling\">Risk Modeling</h3>\n<p>A risk model should represent a distinct and unique perspective for your business. Anything generic or borrowed will be a critical flaw in your risk assessment attempt.</p>\n<p>With consideration to the investigations into risk appetite, formalise the risk profile into a Risk Matrix complete with justification and definition to the different levels of likelihood and impact. Once these risk tables are defined, these should be presented to stakeholders and accepted.</p>\n<p><a name=\"s1c4\"></a></p>\n<h4 id=\"riskmatrix\">Risk Matrix</h4>\n<p>There are many sources available to you for inspiration, from study materials for CISSP, to NIST, and the various frameworks out there. The basics of a risk matrix are impacts and likelihood, where each has their own level of calculation such as certainty for likelihood, and the intersection of the two  is a measurement from various levels ranging from Low to High.</p>\n<p><a name=\"s1c5\"></a></p>\n<h4 id=\"likelihood\">Likelihood</h4>\n<p>Consider the risk, if it is expected to occur in most circumstances or events, frequency in the wild, and probability your business is a target.</p>\n<p>Use intelligence feeds that indicate targeted industries being exploited, viewed over a long period of time to see trends, and whether public exploits are known.</p>\n<p><a name=\"s1c6\"></a></p>\n<h4 id=\"impact\">Impact</h4>\n<p>This ties directly into the Risk Appetite addressed earlier, more than that you should consider the severity of the impact based on the areas of impact given the nature of the risk. Some risk will incur a financial impact, others are stakeholder confidence, brand or reputation damage, but in the extreme you could face sanctions, legal action, or loss of life.</p>\n<p>Setting a clearly defined impact rating with clear and well thought out descriptions will help you to produce valuable risk assessments going forward.</p>\n<p><a name=\"s1c7\"></a></p>\n<h3 id=\"dataclassification\">Data Classification</h3>\n<p>It is highly likely that all assessments conducted will include some sort of data transmission or storage. Understanding what the data sensitivity is through a data dictionary and analysis that produces a clear classification model will be instrumental for your risk assessments.</p>\n<p>One piece of advice from experience, try to first figure out how you would classify data yourself when presented with a new development and analysis task. If you can't marry some new data tot he classification model you developed, how do you expect your colleagues too going forward?</p>\n<p><a name=\"s2\"></a></p>\n<h2 id=\"reportpreparations\">Report Preparations</h2>\n<p>In this section we explore the tricks that may not at first seem risk or even information security related, but will be invaluable to you when you hit that blocker down the track of how you should approach a service to discover it's vulnerabilities and risk.</p>\n<p>These tricks should invoke critical thinking and scrutiny in key areas of risk, and the more you prepare content for these areas the easier it will be for you to more identify inherent risk in your assessments.</p>\n<p><a name=\"s2c1\"></a></p>\n<h3 id=\"compliance\">Compliance</h3>\n<p>The best short-cut we have in assessing risk, but try to remain skeptical as you'll quickly learn that vendor compliance can be a lot of smoke and mirrors.</p>\n<p><a name=\"s2c2\"></a></p>\n<h3 id=\"threatmodeling\">Threat Modeling</h3>\n<p><a name=\"s2c3\"></a></p>\n<h3 id=\"pricingmodel\">Pricing Model</h3>\n<p><a name=\"s2c4\"></a></p>\n<h3 id=\"capabilitiesandlimitations\">Capabilities and Limitations</h3>\n<p><a name=\"s2c5\"></a></p>\n<h3 id=\"monitoring\">Monitoring</h3>\n<p><a name=\"s2c6\"></a></p>\n<h3 id=\"accesscontrol\">Access Control</h3>\n<p><a name=\"s3\"></a></p>\n<h2 id=\"assessmentanalysis\">Assessment Analysis</h2>\n<p><a name=\"s3c1\"></a></p>\n<h3 id=\"riskdomains\">Risk Domains</h3>\n<p><a name=\"s3c2\"></a></p>\n<h3 id=\"workbook\">Workbook</h3>\n<p><a name=\"s3c3\"></a></p>\n<h3 id=\"inherentrisk\">Inherent Risk</h3>\n<p><a name=\"s3c4\"></a></p>\n<h3 id=\"threatactorsandmotivation\">Threat Actors and Motivation</h3>\n<p><a name=\"s3c5\"></a></p>\n<h3 id=\"controls\">Controls</h3>\n<p><a name=\"s3c6\"></a></p>\n<h3 id=\"residualrisk\">Residual Risk</h3>\n<p><a name=\"s4\"></a></p>\n<h2 id=\"reporting\">Reporting</h2>\n<p><a name=\"s5\"></a></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<!--kg-card-end: markdown-->","comment_id":"5c5983ec0b905706a2ab0b35","plaintext":"Stop wasting effort on risk assessments that raise more questions than they\naddress, get it right the first time.\n\n 1.  One-time preparation.  *  Define the Process \n     *  Risk Appetite \n     *  Risk Modeling  *  Risk Matrix \n        *  Likelihood \n        *  Impact \n       \n       \n     *  Data Classification \n    \n    \n 2.  Report Preparations  *  Compliance \n     *  Threat Modeling \n     *  Pricing Model \n     *  Capabilities and Limitations \n     *  Monitoring \n     *  Access Control \n    \n    \n 3.  Assessment Analysis  *  Risk Domains \n     *  Workbook \n     *  Inherent Risk \n     *  Threat Actors and Motivation \n     *  Controls \n     *  Residual Risk \n    \n    \n 4.  Reporting \n 5.  Conclusion \n\n\n\nOne-time Preparation\nThe items in this section will be re-usable for each report and save a lot of\ntime. Hopefully the business already provides resources that can help you\nrapidly develop these resources, otherwise you are in for a few long interviews\nwith your colleagues to learn, and likely let them define for you, each of the\nfollowing.\n\n\n\nDefine the Process\nIf your business has any regulation obligations it is highly likely that a\nprescriptive risk assessment process is part of that regulators white papers or\nguidance material.\n\nIf you do not have a regulatory authority to report to, consider the contents of\nthis post and start developing a process during your first risk assessment,\nkeeping up incremental improvements as you practice.\n\nSome key considerations for a process is to focus on determining service and\nfeatures to be assessed, any compliance documentation, and if the service is\nin-scope.\n\nTry to remember that doing a risk assessment should also determine the\nsuitability of the service to the business. Don't be afraid to point out reasons\nwhy the service may not be suitable, in fact build the risk assessment process\nexpressly with this in mind to encourage critical thinking and eliminating bias\nand conflict of interest.\n\n\n\nRisk Appetite\nThis is a disputed topic, some might talk to you about how risk is 'accepted' or\nregistered and planned work allocated. These are concepts for risk management\nrather than risk assessment.\n\nIn context of assessing risk, the appetite plays a roll in the scrutiny applied\nto the inherent risk. This is a topic to be explored later, but for now it is\nimportant to explore the risk appetite in your organisation by speaking to the\nCIO, CISA, CFO, and really any interested stakeholder.\nDepending on who you're speaking to, try to find out the tolerance acceptance,\nhow much financial loss or brand and reputation damage would be considered an\nincident, and beyond that how much impact would raise that to higher levels of\nconcern.\n\n\n\nRisk Modeling\nA risk model should represent a distinct and unique perspective for your\nbusiness. Anything generic or borrowed will be a critical flaw in your risk\nassessment attempt.\n\nWith consideration to the investigations into risk appetite, formalise the risk\nprofile into a Risk Matrix complete with justification and definition to the\ndifferent levels of likelihood and impact. Once these risk tables are defined,\nthese should be presented to stakeholders and accepted.\n\n\n\nRisk Matrix\nThere are many sources available to you for inspiration, from study materials\nfor CISSP, to NIST, and the various frameworks out there. The basics of a risk\nmatrix are impacts and likelihood, where each has their own level of calculation\nsuch as certainty for likelihood, and the intersection of the two is a\nmeasurement from various levels ranging from Low to High.\n\n\n\nLikelihood\nConsider the risk, if it is expected to occur in most circumstances or events,\nfrequency in the wild, and probability your business is a target.\n\nUse intelligence feeds that indicate targeted industries being exploited, viewed\nover a long period of time to see trends, and whether public exploits are known.\n\n\n\nImpact\nThis ties directly into the Risk Appetite addressed earlier, more than that you\nshould consider the severity of the impact based on the areas of impact given\nthe nature of the risk. Some risk will incur a financial impact, others are\nstakeholder confidence, brand or reputation damage, but in the extreme you could\nface sanctions, legal action, or loss of life.\n\nSetting a clearly defined impact rating with clear and well thought out\ndescriptions will help you to produce valuable risk assessments going forward.\n\n\n\nData Classification\nIt is highly likely that all assessments conducted will include some sort of\ndata transmission or storage. Understanding what the data sensitivity is through\na data dictionary and analysis that produces a clear classification model will\nbe instrumental for your risk assessments.\n\nOne piece of advice from experience, try to first figure out how you would\nclassify data yourself when presented with a new development and analysis task.\nIf you can't marry some new data tot he classification model you developed, how\ndo you expect your colleagues too going forward?\n\n\n\nReport Preparations\nIn this section we explore the tricks that may not at first seem risk or even\ninformation security related, but will be invaluable to you when you hit that\nblocker down the track of how you should approach a service to discover it's\nvulnerabilities and risk.\n\nThese tricks should invoke critical thinking and scrutiny in key areas of risk,\nand the more you prepare content for these areas the easier it will be for you\nto more identify inherent risk in your assessments.\n\n\n\nCompliance\nThe best short-cut we have in assessing risk, but try to remain skeptical as\nyou'll quickly learn that vendor compliance can be a lot of smoke and mirrors.\n\n\n\nThreat Modeling\n\n\nPricing Model\n\n\nCapabilities and Limitations\n\n\nMonitoring\n\n\nAccess Control\n\n\nAssessment Analysis\n\n\nRisk Domains\n\n\nWorkbook\n\n\nInherent Risk\n\n\nThreat Actors and Motivation\n\n\nControls\n\n\nResidual Risk\n\n\nReporting\n\n\nConclusion","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2019-02-05 12:39:08","created_by":"1","updated_at":"2021-03-31 13:56:25","updated_by":"1","published_at":"2019-02-05 13:29:47","published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc5","uuid":"b204461b-db8b-4d6d-8722-ea5cfe180d91","title":"You're probably a Blue Team, not a Red Team","slug":"youre-probably-a-blue-team-not-a-red-team","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"],[\"u\"],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"For those unfamiliar with Cybersecurity team colors, there are \"],[0,[0],1,\"Blue Teams\"],[0,[],0,\" which are the defenders, and there are \"],[0,[0],1,\"Red Teams\"],[0,[],0,\" who are simulating attackers with the goal to prove a circumvention of these defences. \"]]],[1,\"p\",[[0,[],0,\"There are a few other colors too, the only other well known one is _Purple Team_, cleverly named as a mix of Blue and Red with the maybe not so expected goal of these Blue and Red Teams working together to teach one another of the tools, tactics, techniques, and procedures each use. This sharing helps each advance to achieve greater success in their individual goals.\"]]],[1,\"h2\",[[0,[],0,\"Who are the Red Team?\"]]],[1,\"p\",[[0,[],0,\"With all the hype and popularisation of \"],[0,[0],1,\"hacking\"],[0,[],0,\" so many certified \\\"ethical white hat\\\" hackers often refer to themselves as a \"],[0,[0],1,\"Red Team\"],[0,[],0,\" capability in an organisation, and in rare occasions this is actually true, but most of them are \"],[0,[0],1,\"Blue Team\"],[0,[],0,\" trying to be trendy and riding the hype train.\"]]],[1,\"p\",[[0,[],0,\"Red Teams are often employed to fill a penetration tester, threat hunter, and vulnerability management role. Generally none of these are unique Red Team tasks, but all are common tasks for a Blue Team. Blue Teams always defend their organisation which many see this as applying preventative controls to stop a threat but the bulk, if not all of a Blue Teams effort, goes into monitoring and remediation - or what information security professionals define as detective and corrective controls. The popularisation of ethical hacking has made a lot of advancements in the reconnaissance tools space, which is directly a detective effort for a Blue Team that Red Teams also must develop before they can find assets and exploit them.\"]]],[1,\"h2\",[[0,[],0,\"Who are the Blue Team?\"]]],[1,\"p\",[[0,[],0,\"The most effective Blue Team have a head start, well they should, as they begin with full knowledge of what needs to be defended, often known as an asset inventory, bill of materials, or just the billing from vendors. Either way, running recon tools is not a unique Red Team task, the Red Team can completely and entirely skip recon altogether by acquiring the targets asset inventory and go straight to exploit development which defines the Red Team.\"]]],[1,\"p\",[[0,[],0,\"Anyone in a Blue Team who has seen reports from the top penetration testing firms, know that maybe a couple of them deliver reports that could be useful. Most are just recon and findings, for a report to act in a Red Team capacity they would be required to develop their own exploits. Otherwise they are not offering any skills that a Blue Teams doesn't already have. \"]]],[1,\"p\",[[0,[],0,\"In a report there is a lot of other content, apart from providing exploits, all of the other content is suitable for the broader organisation. What the Blue Team requires is the exploit, everything else they can do themselves.\"]]],[1,\"h2\",[[0,[],0,\"What exactly is an exploit?\"]]],[1,\"p\",[[0,[],0,\"An \"],[0,[0],1,\"exploit\"],[0,[],0,\" is the procedure, often in the form of code, that is used to validate a \"],[0,[0],1,\"vulnerability\"],[0,[],0,\". Before you have a working exploit, a \"],[0,[0],1,\"vulnerability\"],[0,[],0,\" is actually called a \"],[0,[0],1,\"finding.\"]]],[1,\"p\",[[0,[0],1,\"Just focus on that a second,\"],[0,[],0,\" it is only considered a \"],[0,[0],1,\"vulnerability\"],[0,[],0,\" \"],[0,[1],1,\"after the exploit is used to validate a finding\"],[0,[],0,\". This is why the professional penetration testing reports only give you a \"],[0,[1],1,\"findings report\"],[0,[],0,\" for most white box or grey box penetration tests, but if  you are engaging them to perform a black box it is ingenious for the report to contain only findings, it is expected that these black box reports only provide you vulnerabilities and therefore inherently every item will include a working exploit. White box tests must define a scope, including assets and exclusion lists - whereas a black box is usually void of any scope or advance target knowledge being primarily scenario based or emulating a specific threat actor which makes it difficult to justify the value of the report with mere unvalidated findings which might be acceptable for a white box report that is used as evidence for compliance audits and heavily scrutinises the scoped targets.\"]]],[1,\"p\",[[0,[],0,\"If you skip the validation step, i.e. have no corresponding exploit to prove a findings is actually a real vulnerability, and you then ingest these findings into your vulnerability management process - you are adding a lot of noise and often very few (if any) vulnerabilities to the vulnerability management workload. So you will start to commit time and resources on fixing unverified findings, as though they were actually vulnerabilities, which can be more damaging to the business than a vulnerability in the first place.\"]]],[1,\"h2\",[[0,[],0,\"What is the problem with Findings?\"]]],[1,\"p\",[[0,[],0,\"A Finding cannot be given a Risk score, a vulnerability should be assigned a CVSS.\"]]],[1,\"p\",[[0,[],0,\"Most mature businesses would want to align their Risk Management program of work to the Risk ratings they assign based on the CVSS of each vulnerability identified and tracked in the vulnerability management strategy. Risk ratings are not the same as a CVSS, they are very different, but this is a topic for a whole other post.\"]]],[1,\"p\",[[0,[],0,\"Anyone intimately familiar with the CVSS know the components that attribute to the scoring. If you're unfamiliar just know that a few attributes can be scored for a Finding, some that may apply are confidence or complexity, but other attributes related to the business asset like criticality, likelihood of an occurrence in this environment, the confidentiality or availability or integrity modifiers, and of course the impact if the threat is realised - are almost all impossible to given a value for a Finding. \"]]],[1,\"p\",[[0,[],0,\"Equally, a vendor or security researcher may attempt to score the CVSS attributes but are equally unaware of your environment or assets. So beware CVSS, and you should always rate vulnerabilities yourself.\"]]],[1,\"p\",[[0,[],0,\"There is a fundamental misunderstanding here with regards to how exploits interact with the concepts of Findings versus Vulnerabilities, but is not the fault of the organisations Risk or Vulnerability management programs. These strategies are likely mature, frequently scrutinised, and well understood. Unfortunately these programs have one critical flaw that makes them ineffective, and that flaw is an inherent trust in the vendors that inform these programs. These vendors assure completeness and accurate data and in practice barely understand what completeness represents and therefore delivery faulty data and compromise the programs fundamentally. \"]]],[1,\"h2\",[[0,[],0,\"How to identify a real exploit in reports?\"]]],[1,\"p\",[[0,[],0,\"In your reports an occurrence with the name \"],[0,[0],1,\"finding\"],[0,[],0,\" or \"],[0,[0],1,\"vulnerability\"],[0,[],0,\" is usually (hopefully) followed by a procedure for you to reproduce it yourself (if not, find a better vendor).\"]]],[1,\"p\",[[0,[],0,\"There are two types of reproduction procedures;\"]]],[1,\"p\",[[0,[2],1,\"Reproduces a finding\"],[0,[],0,\": executing the procedure that will force an event or run a tool that informs you that something risky was identified. These are sometimes measurements against a standard, benchmark, policy, or compliance obligation. Others report a possible CVE, which only tells you someone has at least once in the past proved a vulnerability exists that looks like your finding. But the reporter did not validate that vulnerability in your environment, and that reporter is not related to your vendor in any way, so reporter who found the CVE can't possibly know anything about this finding in your report or inform you if have a vulnerability or not.\"]]],[1,\"p\",[[0,[2],1,\"Reproduces a vulnerability\"],[0,[],0,\": Usually contains both a procedure and a payload, that upon execution and resulting detonation will produce confidential or material data. These may also result in a compromise instead, elevated access, lateral movement, or uncovering some other vector to perform the next attack that leads to a compromise.\"]]],[1,\"p\",[[0,[],0,\"In the example of a Finding there is no exploit, therefore for Findings you must still validate before you can include it into your vulnerability management program. This practice of finding validation reduces the vulnerabilities you need to report to auditors, and has a huge reduction of eventual work for other business unit that comes out of typical vulnerability management programs.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[2],1,\"As a security or risk professional\"],[0,[],0,\"; unless you have a working exploit for a vulnerability, it not actually a vulnerability yet and the finding may be completely benign in your environment. Before polluting your risk and vulnerability management with unvalidated Findings, ensure you have working exploits and verify the vulnerability is real.\"]]],[1,\"p\",[[0,[2],0,\"As self-proclaimed \"],[0,[0],1,\"Red Team \"],[0,[],1,\"professional\"],[0,[],0,\"; if you are reporting only findings you are actually a \"],[0,[0],1,\"Blue Team\"],[0,[],0,\" professional. Real honest \"],[0,[0],1,\"Red Team\"],[0,[],0,\" professionals develop exploits, never detonate them without express permission from an authorised owner, and ethically discloses them. Blue team will recon their networks and assets to identify threats and findings, they check for miss configured services, threat hunt, and scan public places for exposures, this practice does not make you a red team professional.\"]]],[1,\"p\",[[0,[2],1,\"As a penetration tester\"],[0,[],0,\"; Your reports are only providing findings until you actually prove your work by disclosing a reproduction procedure for the exploits that validate vulnerabilities really exist. Stop calling these findings vulnerabilities, it is a disservice and shameful. And no, a CVE is not an exploit either, you actually need to perform a procedure that gains you further access, or breaches the confidentiality, availability, and integrity of the target.\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","html":"<p>For those unfamiliar with Cybersecurity team colors, there are <em>Blue Teams</em> which are the defenders, and there are <em>Red Teams</em> who are simulating attackers with the goal to prove a circumvention of these defences. </p><p>There are a few other colors too, the only other well known one is _Purple Team_, cleverly named as a mix of Blue and Red with the maybe not so expected goal of these Blue and Red Teams working together to teach one another of the tools, tactics, techniques, and procedures each use. This sharing helps each advance to achieve greater success in their individual goals.</p><h2 id=\"who-are-the-red-team\">Who are the Red Team?</h2><p>With all the hype and popularisation of <em>hacking</em> so many certified \"ethical white hat\" hackers often refer to themselves as a <em>Red Team</em> capability in an organisation, and in rare occasions this is actually true, but most of them are <em>Blue Team</em> trying to be trendy and riding the hype train.</p><p>Red Teams are often employed to fill a penetration tester, threat hunter, and vulnerability management role. Generally none of these are unique Red Team tasks, but all are common tasks for a Blue Team. Blue Teams always defend their organisation which many see this as applying preventative controls to stop a threat but the bulk, if not all of a Blue Teams effort, goes into monitoring and remediation - or what information security professionals define as detective and corrective controls. The popularisation of ethical hacking has made a lot of advancements in the reconnaissance tools space, which is directly a detective effort for a Blue Team that Red Teams also must develop before they can find assets and exploit them.</p><h2 id=\"who-are-the-blue-team\">Who are the Blue Team?</h2><p>The most effective Blue Team have a head start, well they should, as they begin with full knowledge of what needs to be defended, often known as an asset inventory, bill of materials, or just the billing from vendors. Either way, running recon tools is not a unique Red Team task, the Red Team can completely and entirely skip recon altogether by acquiring the targets asset inventory and go straight to exploit development which defines the Red Team.</p><p>Anyone in a Blue Team who has seen reports from the top penetration testing firms, know that maybe a couple of them deliver reports that could be useful. Most are just recon and findings, for a report to act in a Red Team capacity they would be required to develop their own exploits. Otherwise they are not offering any skills that a Blue Teams doesn't already have. </p><p>In a report there is a lot of other content, apart from providing exploits, all of the other content is suitable for the broader organisation. What the Blue Team requires is the exploit, everything else they can do themselves.</p><h2 id=\"what-exactly-is-an-exploit\">What exactly is an exploit?</h2><p>An <em>exploit</em> is the procedure, often in the form of code, that is used to validate a <em>vulnerability</em>. Before you have a working exploit, a <em>vulnerability</em> is actually called a <em>finding.</em></p><p><em>Just focus on that a second,</em> it is only considered a <em>vulnerability</em> <u>after the exploit is used to validate a finding</u>. This is why the professional penetration testing reports only give you a <u>findings report</u> for most white box or grey box penetration tests, but if  you are engaging them to perform a black box it is ingenious for the report to contain only findings, it is expected that these black box reports only provide you vulnerabilities and therefore inherently every item will include a working exploit. White box tests must define a scope, including assets and exclusion lists - whereas a black box is usually void of any scope or advance target knowledge being primarily scenario based or emulating a specific threat actor which makes it difficult to justify the value of the report with mere unvalidated findings which might be acceptable for a white box report that is used as evidence for compliance audits and heavily scrutinises the scoped targets.</p><p>If you skip the validation step, i.e. have no corresponding exploit to prove a findings is actually a real vulnerability, and you then ingest these findings into your vulnerability management process - you are adding a lot of noise and often very few (if any) vulnerabilities to the vulnerability management workload. So you will start to commit time and resources on fixing unverified findings, as though they were actually vulnerabilities, which can be more damaging to the business than a vulnerability in the first place.</p><h2 id=\"what-is-the-problem-with-findings\">What is the problem with Findings?</h2><p>A Finding cannot be given a Risk score, a vulnerability should be assigned a CVSS.</p><p>Most mature businesses would want to align their Risk Management program of work to the Risk ratings they assign based on the CVSS of each vulnerability identified and tracked in the vulnerability management strategy. Risk ratings are not the same as a CVSS, they are very different, but this is a topic for a whole other post.</p><p>Anyone intimately familiar with the CVSS know the components that attribute to the scoring. If you're unfamiliar just know that a few attributes can be scored for a Finding, some that may apply are confidence or complexity, but other attributes related to the business asset like criticality, likelihood of an occurrence in this environment, the confidentiality or availability or integrity modifiers, and of course the impact if the threat is realised - are almost all impossible to given a value for a Finding. </p><p>Equally, a vendor or security researcher may attempt to score the CVSS attributes but are equally unaware of your environment or assets. So beware CVSS, and you should always rate vulnerabilities yourself.</p><p>There is a fundamental misunderstanding here with regards to how exploits interact with the concepts of Findings versus Vulnerabilities, but is not the fault of the organisations Risk or Vulnerability management programs. These strategies are likely mature, frequently scrutinised, and well understood. Unfortunately these programs have one critical flaw that makes them ineffective, and that flaw is an inherent trust in the vendors that inform these programs. These vendors assure completeness and accurate data and in practice barely understand what completeness represents and therefore delivery faulty data and compromise the programs fundamentally. </p><h2 id=\"how-to-identify-a-real-exploit-in-reports\">How to identify a real exploit in reports?</h2><p>In your reports an occurrence with the name <em>finding</em> or <em>vulnerability</em> is usually (hopefully) followed by a procedure for you to reproduce it yourself (if not, find a better vendor).</p><p>There are two types of reproduction procedures;</p><p><strong>Reproduces a finding</strong>: executing the procedure that will force an event or run a tool that informs you that something risky was identified. These are sometimes measurements against a standard, benchmark, policy, or compliance obligation. Others report a possible CVE, which only tells you someone has at least once in the past proved a vulnerability exists that looks like your finding. But the reporter did not validate that vulnerability in your environment, and that reporter is not related to your vendor in any way, so reporter who found the CVE can't possibly know anything about this finding in your report or inform you if have a vulnerability or not.</p><p><strong>Reproduces a vulnerability</strong>: Usually contains both a procedure and a payload, that upon execution and resulting detonation will produce confidential or material data. These may also result in a compromise instead, elevated access, lateral movement, or uncovering some other vector to perform the next attack that leads to a compromise.</p><p>In the example of a Finding there is no exploit, therefore for Findings you must still validate before you can include it into your vulnerability management program. This practice of finding validation reduces the vulnerabilities you need to report to auditors, and has a huge reduction of eventual work for other business unit that comes out of typical vulnerability management programs.</p><h2 id=\"conclusion\">Conclusion</h2><p><strong>As a security or risk professional</strong>; unless you have a working exploit for a vulnerability, it not actually a vulnerability yet and the finding may be completely benign in your environment. Before polluting your risk and vulnerability management with unvalidated Findings, ensure you have working exploits and verify the vulnerability is real.</p><p><strong>As self-proclaimed <em>Red Team </em>professional</strong>; if you are reporting only findings you are actually a <em>Blue Team</em> professional. Real honest <em>Red Team</em> professionals develop exploits, never detonate them without express permission from an authorised owner, and ethically discloses them. Blue team will recon their networks and assets to identify threats and findings, they check for miss configured services, threat hunt, and scan public places for exposures, this practice does not make you a red team professional.</p><p><strong>As a penetration tester</strong>; Your reports are only providing findings until you actually prove your work by disclosing a reproduction procedure for the exploits that validate vulnerabilities really exist. Stop calling these findings vulnerabilities, it is a disservice and shameful. And no, a CVE is not an exploit either, you actually need to perform a procedure that gains you further access, or breaches the confidentiality, availability, and integrity of the target.</p>","comment_id":"5ee6e13305bdaa04b9c9348b","plaintext":"For those unfamiliar with Cybersecurity team colors, there are Blue Teams which\nare the defenders, and there are Red Teams who are simulating attackers with the\ngoal to prove a circumvention of these defences. \n\nThere are a few other colors too, the only other well known one is _Purple\nTeam_, cleverly named as a mix of Blue and Red with the maybe not so expected\ngoal of these Blue and Red Teams working together to teach one another of the\ntools, tactics, techniques, and procedures each use. This sharing helps each\nadvance to achieve greater success in their individual goals.\n\nWho are the Red Team?\nWith all the hype and popularisation of hacking so many certified \"ethical white\nhat\" hackers often refer to themselves as a Red Team capability in an\norganisation, and in rare occasions this is actually true, but most of them are \nBlue Team trying to be trendy and riding the hype train.\n\nRed Teams are often employed to fill a penetration tester, threat hunter, and\nvulnerability management role. Generally none of these are unique Red Team\ntasks, but all are common tasks for a Blue Team. Blue Teams always defend their\norganisation which many see this as applying preventative controls to stop a\nthreat but the bulk, if not all of a Blue Teams effort, goes into monitoring and\nremediation - or what information security professionals define as detective and\ncorrective controls. The popularisation of ethical hacking has made a lot of\nadvancements in the reconnaissance tools space, which is directly a detective\neffort for a Blue Team that Red Teams also must develop before they can find\nassets and exploit them.\n\nWho are the Blue Team?\nThe most effective Blue Team have a head start, well they should, as they begin\nwith full knowledge of what needs to be defended, often known as an asset\ninventory, bill of materials, or just the billing from vendors. Either way,\nrunning recon tools is not a unique Red Team task, the Red Team can completely\nand entirely skip recon altogether by acquiring the targets asset inventory and\ngo straight to exploit development which defines the Red Team.\n\nAnyone in a Blue Team who has seen reports from the top penetration testing\nfirms, know that maybe a couple of them deliver reports that could be useful.\nMost are just recon and findings, for a report to act in a Red Team capacity\nthey would be required to develop their own exploits. Otherwise they are not\noffering any skills that a Blue Teams doesn't already have. \n\nIn a report there is a lot of other content, apart from providing exploits, all\nof the other content is suitable for the broader organisation. What the Blue\nTeam requires is the exploit, everything else they can do themselves.\n\nWhat exactly is an exploit?\nAn exploit is the procedure, often in the form of code, that is used to validate\na vulnerability. Before you have a working exploit, a vulnerability is actually\ncalled a finding.\n\nJust focus on that a second, it is only considered a vulnerability after the\nexploit is used to validate a finding. This is why the professional penetration\ntesting reports only give you a findings report for most white box or grey box\npenetration tests, but if  you are engaging them to perform a black box it is\ningenious for the report to contain only findings, it is expected that these\nblack box reports only provide you vulnerabilities and therefore inherently\nevery item will include a working exploit. White box tests must define a scope,\nincluding assets and exclusion lists - whereas a black box is usually void of\nany scope or advance target knowledge being primarily scenario based or\nemulating a specific threat actor which makes it difficult to justify the value\nof the report with mere unvalidated findings which might be acceptable for a\nwhite box report that is used as evidence for compliance audits and heavily\nscrutinises the scoped targets.\n\nIf you skip the validation step, i.e. have no corresponding exploit to prove a\nfindings is actually a real vulnerability, and you then ingest these findings\ninto your vulnerability management process - you are adding a lot of noise and\noften very few (if any) vulnerabilities to the vulnerability management\nworkload. So you will start to commit time and resources on fixing unverified\nfindings, as though they were actually vulnerabilities, which can be more\ndamaging to the business than a vulnerability in the first place.\n\nWhat is the problem with Findings?\nA Finding cannot be given a Risk score, a vulnerability should be assigned a\nCVSS.\n\nMost mature businesses would want to align their Risk Management program of work\nto the Risk ratings they assign based on the CVSS of each vulnerability\nidentified and tracked in the vulnerability management strategy. Risk ratings\nare not the same as a CVSS, they are very different, but this is a topic for a\nwhole other post.\n\nAnyone intimately familiar with the CVSS know the components that attribute to\nthe scoring. If you're unfamiliar just know that a few attributes can be scored\nfor a Finding, some that may apply are confidence or complexity, but other\nattributes related to the business asset like criticality, likelihood of an\noccurrence in this environment, the confidentiality or availability or integrity\nmodifiers, and of course the impact if the threat is realised - are almost all\nimpossible to given a value for a Finding. \n\nEqually, a vendor or security researcher may attempt to score the CVSS\nattributes but are equally unaware of your environment or assets. So beware\nCVSS, and you should always rate vulnerabilities yourself.\n\nThere is a fundamental misunderstanding here with regards to how exploits\ninteract with the concepts of Findings versus Vulnerabilities, but is not the\nfault of the organisations Risk or Vulnerability management programs. These\nstrategies are likely mature, frequently scrutinised, and well understood.\nUnfortunately these programs have one critical flaw that makes them ineffective,\nand that flaw is an inherent trust in the vendors that inform these programs.\nThese vendors assure completeness and accurate data and in practice barely\nunderstand what completeness represents and therefore delivery faulty data and\ncompromise the programs fundamentally. \n\nHow to identify a real exploit in reports?\nIn your reports an occurrence with the name finding or vulnerability is usually\n(hopefully) followed by a procedure for you to reproduce it yourself (if not,\nfind a better vendor).\n\nThere are two types of reproduction procedures;\n\nReproduces a finding: executing the procedure that will force an event or run a\ntool that informs you that something risky was identified. These are sometimes\nmeasurements against a standard, benchmark, policy, or compliance obligation.\nOthers report a possible CVE, which only tells you someone has at least once in\nthe past proved a vulnerability exists that looks like your finding. But the\nreporter did not validate that vulnerability in your environment, and that\nreporter is not related to your vendor in any way, so reporter who found the CVE\ncan't possibly know anything about this finding in your report or inform you if\nhave a vulnerability or not.\n\nReproduces a vulnerability: Usually contains both a procedure and a payload,\nthat upon execution and resulting detonation will produce confidential or\nmaterial data. These may also result in a compromise instead, elevated access,\nlateral movement, or uncovering some other vector to perform the next attack\nthat leads to a compromise.\n\nIn the example of a Finding there is no exploit, therefore for Findings you must\nstill validate before you can include it into your vulnerability management\nprogram. This practice of finding validation reduces the vulnerabilities you\nneed to report to auditors, and has a huge reduction of eventual work for other\nbusiness unit that comes out of typical vulnerability management programs.\n\nConclusion\nAs a security or risk professional; unless you have a working exploit for a\nvulnerability, it not actually a vulnerability yet and the finding may be\ncompletely benign in your environment. Before polluting your risk and\nvulnerability management with unvalidated Findings, ensure you have working\nexploits and verify the vulnerability is real.\n\nAs self-proclaimed Red Team professional; if you are reporting only findings you\nare actually a Blue Team professional. Real honest Red Team professionals\ndevelop exploits, never detonate them without express permission from an\nauthorised owner, and ethically discloses them. Blue team will recon their\nnetworks and assets to identify threats and findings, they check for miss\nconfigured services, threat hunt, and scan public places for exposures, this\npractice does not make you a red team professional.\n\nAs a penetration tester; Your reports are only providing findings until you\nactually prove your work by disclosing a reproduction procedure for the exploits\nthat validate vulnerabilities really exist. Stop calling these findings\nvulnerabilities, it is a disservice and shameful. And no, a CVE is not an\nexploit either, you actually need to perform a procedure that gains you further\naccess, or breaches the confidentiality, availability, and integrity of the\ntarget.","feature_image":"__GHOST_URL__/content/images/2020/06/red-team-vs-blue-team.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-06-15 02:47:15","created_by":"1","updated_at":"2021-03-31 13:59:13","updated_by":"1","published_at":"2020-06-13 13:40:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc6","uuid":"003b0388-c511-4ee1-85c6-7e84caa86493","title":"Private AWS S3 - How hard could that be?","slug":"private-aws-s3-how-hard-could-that-be","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"```\\n{\\n   \\\"Statement\\\": [\\n     {\\n       \\\"NotPrincipal\\\": {\\n         \\\"AWS\\\": \\\"1234567890\\\"\\n       },\\n       \\\"Action\\\": \\\"*\\\",\\n       \\\"Effect\\\": \\\"Deny\\\",\\n       \\\"Resource\\\": [\\n         \\\"arn:aws:s3:::targetbucket\\\",\\n         \\\"arn:aws:s3:::targetbucket/*\\\"\\n       ]\\n     }\\n   ]\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```\\n{\\n   \\\"Version\\\": \\\"2012-10-17\\\",\\n   \\\"Statement\\\": [\\n     {\\n       \\\"Principal\\\": \\\"*\\\",\\n       \\\"Action\\\": \\\"*\\\",\\n       \\\"Effect\\\": \\\"Deny\\\",\\n       \\\"Resource\\\": [\\n         \\\"arn:aws:s3:::targetbucket\\\",\\n         \\\"arn:aws:s3:::targetbucket/*\\\"\\n       ],\\n       \\\"Condition\\\": {\\n         \\\"StringNotEquals\\\": {\\n           \\\"aws:sourceVpce\\\": \\\"vpce-xxxx\\\"\\n         }\\n       }\\n     }\\n   ]\\n}\\n```\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"__GHOST_URL__/everything-in-aws-is-an-api-is-it-secure/\"]],[\"u\"],[\"strong\"],[\"code\"],[\"a\",[\"href\",\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\"]],[\"a\",[\"href\",\"https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html\"]],[\"a\",[\"href\",\"http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-prefix-lists.html\"]],[\"a\",[\"href\",\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\"]],[\"a\",[\"href\",\"https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Before I begin I’d first like to clarify a few things:\"]]],[3,\"ul\",[[[0,[],0,\"I have used AWS for almost a decade, since 2011, yes before the Sydney region release (good times)\"]],[[0,[],0,\"I will continue to use AWS; personally and professionally\"]],[[0,[],0,\"Yes, S3 and AWS as a whole will continue to be recommended by me; knowing is not hating.\"]],[[0,[],0,\"I value truth, secrecy is harmful\"]]]],[1,\"p\",[[0,[],0,\"Critical thinking is virtuous. Without it, we have no intelligence or innovation.\"],[1,[],0,0],[0,[],0,\"Being critical is a positive, not a negative - the negative attitude in technical conversation would be showing dogmatic or ignorant opinions.\"]]],[1,\"h2\",[[0,[],0,\"You think you know the S3 history, right?\"]]],[1,\"p\",[[0,[],0,\"You're probably wrong about S3 past and current insecurities.\"]]],[1,\"p\",[[0,[],0,\"For a long time it wasn't ever possible to be private, yet S3 has always been secured from the public by default.\"],[1,[],0,1],[0,[],0,\"Data leaks were always the result of users \"],[0,[0],1,\"removing authentication\"],[0,[],0,\", making all of the objects stored in the S3 bucket accessible by anyone.\"]]],[1,\"p\",[[0,[],0,\"This was initially an ACL issue, and Amazon addressed this by introducing S3 resource policies. Since their release, these policies have been continuously updated, providing new and improved options to help secure the data they govern.\"],[1,[],0,2],[0,[],0,\"More recent changes in the Console UI now tells users, in the most obvious way, that you have made your data Public. With various scanning tools like AWS Config that scan your accounts to identify risky configurations, users can now identify leaky buckets at the time they are created, perhaps unintentionally made public.\"],[1,[],0,3],[0,[],0,\"These additional services and improvements by AWS are not them fixing anything wrong with S3, it is quite bluntly AWS screaming out to customers \\\"Hey, you have made this public yo! Fix it?\\\"\"]]],[1,\"blockquote\",[[0,[],0,\"S3 was always subject to authentication by default.\"]]],[1,\"p\",[[0,[],0,\"We all know that everything in AWS is an API right? Read \"],[0,[1],1,\"this post\"],[0,[],0,\" to better understand why this is an important security consideration.\"]]],[1,\"p\",[[0,[],0,\"The Query Request API is public, all AWS service endpoints are Public.\"],[1,[],0,4],[0,[],0,\"Meaning the entire \"],[0,[2],1,\"management plane\"],[0,[],0,\" is \"],[0,[0],1,\"Public\"],[0,[],0,\" facing inherently - by design - but all requests are subject to authentication checks. Some other services, like Elasticsearch and Memcache for Elasticache to name a couple, have their \"],[0,[2],1,\"data plane\"],[0,[],0,\" \"],[0,[0],1,\"Public\"],[0,[],0,\" also, but not S3.\"]]],[1,\"h2\",[[0,[],0,\"Can AWS S3 be made Private?\"]]],[1,\"p\",[[0,[],0,\"First what does it mean to be \"],[0,[0],1,\"private\"],[0,[],0,\"?\"]]],[3,\"ul\",[[[0,[3],1,\"Confidential\"],[0,[],0,\" data we transmit cannot be intercepted and read without authorisation\"]],[[0,[3],1,\"Internal Address\"],[0,[],0,\" unless customer configuration specifies otherwise, the IP resolved for the endpoint is internal to AWS when the caller is inside the same AWS account, transmissions do not leave our network\"]],[[0,[3],1,\"Default Protections\"],[0,[],0,\" unauthenticated external actors cannot identify it's existence, i.e. Public. So no public endpoint urls appear in public indexes\"]]]],[1,\"p\",[[0,[],0,\"So, can AWS meet these? The answer is simply; \"],[0,[0],1,\"not by default\"],[1,[],0,5],[0,[],0,\"Is it easy, straight forward to configure AWS to be private? No, there are too many resource level configurations with different names depending on the service context, and more confusing is the various client configurations needed for specific scenarios that are actually some of the most common use cases. That is not even the most frustrating thing about private endpoints in AWS, even if you know how to configure the AWS resource \"],[0,[0],1,\"and your client\"],[0,[],0,\", \"],[0,[2],1,\"the Public route is still the default and it remains active\"],[0,[],0,\"! \"],[1,[],0,6],[0,[],0,\"Did I say that was the last problem? no, no ,no, we still haven't discussed troubleshooting all of that mess above have we? If the service is EC2, i.e. it allows you to run \"],[0,[4],1,\"traceroute\"],[0,[],0,\" or similar you're going to have no trouble. But if the caller service is highly abstracted like Fargate, you might try collecting VPC Flow data or use the new VPC Interface for network packets (both cost you extra). If you are advanced enough to analyse this data, you might be able to observe if the private route is being utilised.\"]]],[1,\"p\",[[0,[],0,\"So what are all of the private options?\"]]],[3,\"ul\",[[[0,[3],1,\"AWS PrivateLink\"],[0,[],0,\": creates an Elastic Network Interface (ENI)\"]],[[0,[3],1,\"Interface VPC Endpoint\"],[0,[],0,\": provides connection to AWS PrivateLink\"]],[[0,[3],1,\"Gateway VPC Endpoint\"],[0,[],0,\": configures VPC route table to supported AWS Resources\"]],[[0,[3],1,\"AWS Direct Connect\"],[0,[],0,\": physical network backbone connectivity from a non-AWS data centre to an availability zone or region\"]],[[0,[3],1,\"AWS Site-to-Site VPN\"],[0,[],0,\": an encrypted tunnel over a the public internet that terminates in your VPC and can be configured to resolve VPC private routes from the origin end of the tunnel\"]]]],[1,\"p\",[[0,[],0,\"Since 2015, S3 has used the Gateway VPC Endpoint method, which in turn can be utilised with VPN and DirectConnect. You can use the \"],[0,[4],1,\"describe-vpc-endpoint-services\"],[0,[],0,\" command to get a list of other available services.\"]]],[1,\"p\",[[0,[],0,\"More often than not, private routing is configured but is never utilised in practice. It just sits there looking good for your audit while public routes are still being used.\"]]],[1,\"p\",[[0,[],0,\"See the confusing \"],[0,[5],1,\"troubleshooting guide\"],[0,[],0,\" published by AWS called \"],[0,[3],1,\"Why can’t I connect to an S3 bucket using a gateway VPC endpoint?\"],[0,[],0,\" Yes it states \\\"Outbound rules with Destination as the \"],[0,[2],1,\"public IPs\"],[0,[],0,\" used by Amazon S3\\\" for your NACL.\"]]],[1,\"p\",[[0,[],0,\"Even AWS can't effectively help customers make Gateway Endpoint private routes work, their instructions tell customers to route traffic publicly. They instruct you to open your firewall rules to the public IP address to get it (Gateway Endpoint private route) to work, effectively by-passing the Gateway Endpoint. When you follow AWS guidance and fix the routing issue, you did not get the private route you wanted, your Gateway Endpoint remains idle, unused, redundant.\"]]],[1,\"h2\",[[0,[],0,\"Configuring a Gateway VPC Endpoint\"]]],[1,\"p\",[[0,[],0,\"There are of course prerequisites and constraints, which I'll raise as they relate to specific steps.\"]]],[1,\"p\",[[0,[],0,\"Using the Console, common (but not recommended) method to use AWS. I encourage the use of Infrastructure as Code tools such as CloudFormation, AWS CDK, and Terraform by Hashicorp.\"]]],[1,\"p\",[[0,[],0,\"1. From the VPC Dashboard, select Endpoints, and Create Endpoint. You'll have to chose the VPC and S3 service\"]]],[1,\"blockquote\",[[0,[],0,\"The endpoint needs to be created in the same region as the S3 bucket\"]]],[1,\"p\",[[0,[],0,\"2. Specify the \"],[0,[0],1,\"optional\"],[0,[],0,\" Custom access policy to define your own using the following JSON as a base, adjust to your needs\"]]],[10,0],[1,\"blockquote\",[[0,[3,3],1,\"Important:\"],[0,[],1,\" \"],[0,[],0,\"The default access policy allows all users, all services, all AWS accounts\"]]],[1,\"p\",[[0,[],0,\"3. Gateway VPC Endpoints configure your route tables, and as such can provide routes to S3 limited to certain subnets. If you're doing this you probably only want to choose a subnet that doesn't have a NAT Gateway. Save that and take note of the \"],[0,[4],1,\"vpce-xxxx\"],[0,[],0,\" identifier.\"]]],[1,\"p\",[[0,[],0,\"4. This next step is necessary to ensure the default public route to the S3 bucket is not utilised, even when you think you are using the private route.\"]]],[1,\"blockquote\",[[0,[3],1,\"Caution: \"],[0,[],0,\"Enforcing this will likely break things, particularly things you think are using the private route will stop working too\"]]],[1,\"p\",[[0,[],0,\"Now go to S3, choose the target bucket, choose Bucket Policy, use the following JSON as a base, adjust as needed.\"]]],[10,1],[1,\"p\",[[0,[],0,\"5. Configure the clients. For demonstration purposes I chose the lowest common service EC2, which other services sit on top of.\"]]],[1,\"blockquote\",[[0,[],0,\"Only IPv4 is supported when using a Gateway VPC Endpoint\"]]],[1,\"p\",[[0,[],0,\"Check the following;\"]]],[3,\"ul\",[[[0,[],0,\"EC2 instance is in the correct subnet (inherently the right VPC and region)\"]],[[0,[],0,\"Configure the default AWS region for your tools or SDK. If you don't use profiles you can set the \"],[0,[4],1,\"--region\"],[0,[],0,\" option in each command.\"]],[[0,[],0,\"Verify IPv6 is not used \"],[0,[4],1,\"aws configure get default.s3.use_dualstack_endpoint\"],[0,[],0,\" should be \"],[0,[4],1,\"false\"],[0,[],0,\" (also check profiles if they are used). When dualstack is false communication stays on IPv4\"]],[[0,[],0,\"If you're using your own DNS server, be sure DNS requests to AWS services resolve to \"],[0,[6],1,\"AWS-maintained IP addresses\"],[0,[],0,\".\"]],[[0,[],0,\"Check outbound rule of Security Groups allowing traffic from the ID of the prefix list \"],[0,[7],1,\"associated with the gateway VPC endpoint\"],[0,[],0,\".\"]]]],[1,\"blockquote\",[[0,[],0,\"Important: DNS resolution must be enabled in your VPC\"]]],[1,\"p\",[[0,[],0,\"Now you have a private route from EC2 to your S3 bucket. Easy right?\"]]],[1,\"h2\",[[0,[],0,\"Apply better security options\"]]],[1,\"p\",[[0,[],0,\"Focusing on the data transmission topic of this article, you might want to also consider a few additional configuration changes to improve the public S3 endpoint - private communication can also benefit if your threat model includes any assumed breach vectors, or your risk posture demands defence in depth.\"]]],[3,\"ol\",[[[0,[3],1,\"Force HMAC-SHA256\"],[0,[],0,\": \"],[0,[8],1,\"Specifying the Signature Version\"],[0,[],0,\" in Request Authentication using \"],[0,[4],1,\"aws configure set default.s3.signature_version s3v4\"],[0,[],0,\" ensures Signature Version 2 is never used, which implements the deprecated SHA1 hashing algorithm. \"]],[[0,[3],1,\"Force Forward Secrecy\"],[0,[],0,\": Many SDKs can't do this including the python SDK (botocore and Boto3), which the AWS Command Line tools use, and is built into all of AWS compute services based on Amazon Linux 1/2 AMIs (so almost all of them).\"],[1,[],0,7],[0,[],0,\"To get around this, I have developed an optional extension to various libraries which introduces a feature flag to support Forward Secrecy. I submitted this change to AWS via PR but it is yet to be merged. Hopefully AWS developers see this as an important feature and accept this optional feature for all to use.\"],[1,[],0,8],[0,[],0,\"If however you are using a tool or SDK that supports custom ciphers, you might try specifying the following Forward Secrecy cipher suite; \"],[0,[4],1,\"kEECDH:kEDH:!aNULL:!eNULL:!DES:!3DES:!RC4\"]]]],[1,\"p\",[[0,[],0,\"While AWS \"],[0,[0],1,\"offers\"],[0,[],0,\" Forward Secrecy, it is near impossible to enforce in most places. I only know \"],[0,[9],1,\"how to do it\"],[0,[],0,\" when manually interacting with the Query Request API using a HTTP client, and not any SDK or command line tool.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>Before I begin I’d first like to clarify a few things:</p><ul><li>I have used AWS for almost a decade, since 2011, yes before the Sydney region release (good times)</li><li>I will continue to use AWS; personally and professionally</li><li>Yes, S3 and AWS as a whole will continue to be recommended by me; knowing is not hating.</li><li>I value truth, secrecy is harmful</li></ul><p>Critical thinking is virtuous. Without it, we have no intelligence or innovation.<br>Being critical is a positive, not a negative - the negative attitude in technical conversation would be showing dogmatic or ignorant opinions.</p><h2 id=\"you-think-you-know-the-s3-history-right\">You think you know the S3 history, right?</h2><p>You're probably wrong about S3 past and current insecurities.</p><p>For a long time it wasn't ever possible to be private, yet S3 has always been secured from the public by default.<br>Data leaks were always the result of users <em>removing authentication</em>, making all of the objects stored in the S3 bucket accessible by anyone.</p><p>This was initially an ACL issue, and Amazon addressed this by introducing S3 resource policies. Since their release, these policies have been continuously updated, providing new and improved options to help secure the data they govern.<br>More recent changes in the Console UI now tells users, in the most obvious way, that you have made your data Public. With various scanning tools like AWS Config that scan your accounts to identify risky configurations, users can now identify leaky buckets at the time they are created, perhaps unintentionally made public.<br>These additional services and improvements by AWS are not them fixing anything wrong with S3, it is quite bluntly AWS screaming out to customers \"Hey, you have made this public yo! Fix it?\"</p><blockquote>S3 was always subject to authentication by default.</blockquote><p>We all know that everything in AWS is an API right? Read <a href=\"__GHOST_URL__/everything-in-aws-is-an-api-is-it-secure/\">this post</a> to better understand why this is an important security consideration.</p><p>The Query Request API is public, all AWS service endpoints are Public.<br>Meaning the entire <u>management plane</u> is <em>Public</em> facing inherently - by design - but all requests are subject to authentication checks. Some other services, like Elasticsearch and Memcache for Elasticache to name a couple, have their <u>data plane</u> <em>Public</em> also, but not S3.</p><h2 id=\"can-aws-s3-be-made-private\">Can AWS S3 be made Private?</h2><p>First what does it mean to be <em>private</em>?</p><ul><li><strong>Confidential</strong> data we transmit cannot be intercepted and read without authorisation</li><li><strong>Internal Address</strong> unless customer configuration specifies otherwise, the IP resolved for the endpoint is internal to AWS when the caller is inside the same AWS account, transmissions do not leave our network</li><li><strong>Default Protections</strong> unauthenticated external actors cannot identify it's existence, i.e. Public. So no public endpoint urls appear in public indexes</li></ul><p>So, can AWS meet these? The answer is simply; <em>not by default</em><br>Is it easy, straight forward to configure AWS to be private? No, there are too many resource level configurations with different names depending on the service context, and more confusing is the various client configurations needed for specific scenarios that are actually some of the most common use cases. That is not even the most frustrating thing about private endpoints in AWS, even if you know how to configure the AWS resource <em>and your client</em>, <u>the Public route is still the default and it remains active</u>! <br>Did I say that was the last problem? no, no ,no, we still haven't discussed troubleshooting all of that mess above have we? If the service is EC2, i.e. it allows you to run <code>traceroute</code> or similar you're going to have no trouble. But if the caller service is highly abstracted like Fargate, you might try collecting VPC Flow data or use the new VPC Interface for network packets (both cost you extra). If you are advanced enough to analyse this data, you might be able to observe if the private route is being utilised.</p><p>So what are all of the private options?</p><ul><li><strong>AWS PrivateLink</strong>: creates an Elastic Network Interface (ENI)</li><li><strong>Interface VPC Endpoint</strong>: provides connection to AWS PrivateLink</li><li><strong>Gateway VPC Endpoint</strong>: configures VPC route table to supported AWS Resources</li><li><strong>AWS Direct Connect</strong>: physical network backbone connectivity from a non-AWS data centre to an availability zone or region</li><li><strong>AWS Site-to-Site VPN</strong>: an encrypted tunnel over a the public internet that terminates in your VPC and can be configured to resolve VPC private routes from the origin end of the tunnel</li></ul><p>Since 2015, S3 has used the Gateway VPC Endpoint method, which in turn can be utilised with VPN and DirectConnect. You can use the <code>describe-vpc-endpoint-services</code> command to get a list of other available services.</p><p>More often than not, private routing is configured but is never utilised in practice. It just sits there looking good for your audit while public routes are still being used.</p><p>See the confusing <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\">troubleshooting guide</a> published by AWS called <strong>Why can’t I connect to an S3 bucket using a gateway VPC endpoint?</strong> Yes it states \"Outbound rules with Destination as the <u>public IPs</u> used by Amazon S3\" for your NACL.</p><p>Even AWS can't effectively help customers make Gateway Endpoint private routes work, their instructions tell customers to route traffic publicly. They instruct you to open your firewall rules to the public IP address to get it (Gateway Endpoint private route) to work, effectively by-passing the Gateway Endpoint. When you follow AWS guidance and fix the routing issue, you did not get the private route you wanted, your Gateway Endpoint remains idle, unused, redundant.</p><h2 id=\"configuring-a-gateway-vpc-endpoint\">Configuring a Gateway VPC Endpoint</h2><p>There are of course prerequisites and constraints, which I'll raise as they relate to specific steps.</p><p>Using the Console, common (but not recommended) method to use AWS. I encourage the use of Infrastructure as Code tools such as CloudFormation, AWS CDK, and Terraform by Hashicorp.</p><p>1. From the VPC Dashboard, select Endpoints, and Create Endpoint. You'll have to chose the VPC and S3 service</p><blockquote>The endpoint needs to be created in the same region as the S3 bucket</blockquote><p>2. Specify the <em>optional</em> Custom access policy to define your own using the following JSON as a base, adjust to your needs</p><!--kg-card-begin: markdown--><pre><code>{\n   &quot;Statement&quot;: [\n     {\n       &quot;NotPrincipal&quot;: {\n         &quot;AWS&quot;: &quot;1234567890&quot;\n       },\n       &quot;Action&quot;: &quot;*&quot;,\n       &quot;Effect&quot;: &quot;Deny&quot;,\n       &quot;Resource&quot;: [\n         &quot;arn:aws:s3:::targetbucket&quot;,\n         &quot;arn:aws:s3:::targetbucket/*&quot;\n       ]\n     }\n   ]\n}\n</code></pre>\n<!--kg-card-end: markdown--><blockquote><strong><strong>Important:</strong> </strong>The default access policy allows all users, all services, all AWS accounts</blockquote><p>3. Gateway VPC Endpoints configure your route tables, and as such can provide routes to S3 limited to certain subnets. If you're doing this you probably only want to choose a subnet that doesn't have a NAT Gateway. Save that and take note of the <code>vpce-xxxx</code> identifier.</p><p>4. This next step is necessary to ensure the default public route to the S3 bucket is not utilised, even when you think you are using the private route.</p><blockquote><strong>Caution: </strong>Enforcing this will likely break things, particularly things you think are using the private route will stop working too</blockquote><p>Now go to S3, choose the target bucket, choose Bucket Policy, use the following JSON as a base, adjust as needed.</p><!--kg-card-begin: markdown--><pre><code>{\n   &quot;Version&quot;: &quot;2012-10-17&quot;,\n   &quot;Statement&quot;: [\n     {\n       &quot;Principal&quot;: &quot;*&quot;,\n       &quot;Action&quot;: &quot;*&quot;,\n       &quot;Effect&quot;: &quot;Deny&quot;,\n       &quot;Resource&quot;: [\n         &quot;arn:aws:s3:::targetbucket&quot;,\n         &quot;arn:aws:s3:::targetbucket/*&quot;\n       ],\n       &quot;Condition&quot;: {\n         &quot;StringNotEquals&quot;: {\n           &quot;aws:sourceVpce&quot;: &quot;vpce-xxxx&quot;\n         }\n       }\n     }\n   ]\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>5. Configure the clients. For demonstration purposes I chose the lowest common service EC2, which other services sit on top of.</p><blockquote>Only IPv4 is supported when using a Gateway VPC Endpoint</blockquote><p>Check the following;</p><ul><li>EC2 instance is in the correct subnet (inherently the right VPC and region)</li><li>Configure the default AWS region for your tools or SDK. If you don't use profiles you can set the <code>--region</code> option in each command.</li><li>Verify IPv6 is not used <code>aws configure get default.s3.use_dualstack_endpoint</code> should be <code>false</code> (also check profiles if they are used). When dualstack is false communication stays on IPv4</li><li>If you're using your own DNS server, be sure DNS requests to AWS services resolve to <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html\">AWS-maintained IP addresses</a>.</li><li>Check outbound rule of Security Groups allowing traffic from the ID of the prefix list <a href=\"http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-prefix-lists.html\">associated with the gateway VPC endpoint</a>.</li></ul><blockquote>Important: DNS resolution must be enabled in your VPC</blockquote><p>Now you have a private route from EC2 to your S3 bucket. Easy right?</p><h2 id=\"apply-better-security-options\">Apply better security options</h2><p>Focusing on the data transmission topic of this article, you might want to also consider a few additional configuration changes to improve the public S3 endpoint - private communication can also benefit if your threat model includes any assumed breach vectors, or your risk posture demands defence in depth.</p><ol><li><strong>Force HMAC-SHA256</strong>: <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\">Specifying the Signature Version</a> in Request Authentication using <code>aws configure set default.s3.signature_version s3v4</code> ensures Signature Version 2 is never used, which implements the deprecated SHA1 hashing algorithm. </li><li><strong>Force Forward Secrecy</strong>: Many SDKs can't do this including the python SDK (botocore and Boto3), which the AWS Command Line tools use, and is built into all of AWS compute services based on Amazon Linux 1/2 AMIs (so almost all of them).<br>To get around this, I have developed an optional extension to various libraries which introduces a feature flag to support Forward Secrecy. I submitted this change to AWS via PR but it is yet to be merged. Hopefully AWS developers see this as an important feature and accept this optional feature for all to use.<br>If however you are using a tool or SDK that supports custom ciphers, you might try specifying the following Forward Secrecy cipher suite; <code>kEECDH:kEDH:!aNULL:!eNULL:!DES:!3DES:!RC4</code></li></ol><p>While AWS <em>offers</em> Forward Secrecy, it is near impossible to enforce in most places. I only know <a href=\"https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a\">how to do it</a> when manually interacting with the Query Request API using a HTTP client, and not any SDK or command line tool.</p>","comment_id":"5ee8414f05bdaa04b9c9363e","plaintext":"Before I begin I’d first like to clarify a few things:\n\n * I have used AWS for almost a decade, since 2011, yes before the Sydney region\n   release (good times)\n * I will continue to use AWS; personally and professionally\n * Yes, S3 and AWS as a whole will continue to be recommended by me; knowing is\n   not hating.\n * I value truth, secrecy is harmful\n\nCritical thinking is virtuous. Without it, we have no intelligence or\ninnovation.\nBeing critical is a positive, not a negative - the negative attitude in\ntechnical conversation would be showing dogmatic or ignorant opinions.\n\nYou think you know the S3 history, right?\nYou're probably wrong about S3 past and current insecurities.\n\nFor a long time it wasn't ever possible to be private, yet S3 has always been\nsecured from the public by default.\nData leaks were always the result of users removing authentication, making all\nof the objects stored in the S3 bucket accessible by anyone.\n\nThis was initially an ACL issue, and Amazon addressed this by introducing S3\nresource policies. Since their release, these policies have been continuously\nupdated, providing new and improved options to help secure the data they govern.\nMore recent changes in the Console UI now tells users, in the most obvious way,\nthat you have made your data Public. With various scanning tools like AWS Config\nthat scan your accounts to identify risky configurations, users can now identify\nleaky buckets at the time they are created, perhaps unintentionally made public.\nThese additional services and improvements by AWS are not them fixing anything\nwrong with S3, it is quite bluntly AWS screaming out to customers \"Hey, you have\nmade this public yo! Fix it?\"\n\n> S3 was always subject to authentication by default.\nWe all know that everything in AWS is an API right? Read this post\n[/everything-in-aws-is-an-api-is-it-secure/] to better understand why this is an\nimportant security consideration.\n\nThe Query Request API is public, all AWS service endpoints are Public.\nMeaning the entire management plane is Public facing inherently - by design -\nbut all requests are subject to authentication checks. Some other services, like\nElasticsearch and Memcache for Elasticache to name a couple, have their data\nplane Public also, but not S3.\n\nCan AWS S3 be made Private?\nFirst what does it mean to be private?\n\n * Confidential data we transmit cannot be intercepted and read without\n   authorisation\n * Internal Address unless customer configuration specifies otherwise, the IP\n   resolved for the endpoint is internal to AWS when the caller is inside the\n   same AWS account, transmissions do not leave our network\n * Default Protections unauthenticated external actors cannot identify it's\n   existence, i.e. Public. So no public endpoint urls appear in public indexes\n\nSo, can AWS meet these? The answer is simply; not by default\nIs it easy, straight forward to configure AWS to be private? No, there are too\nmany resource level configurations with different names depending on the service\ncontext, and more confusing is the various client configurations needed for\nspecific scenarios that are actually some of the most common use cases. That is\nnot even the most frustrating thing about private endpoints in AWS, even if you\nknow how to configure the AWS resource and your client, the Public route is\nstill the default and it remains active! \nDid I say that was the last problem? no, no ,no, we still haven't discussed\ntroubleshooting all of that mess above have we? If the service is EC2, i.e. it\nallows you to run traceroute or similar you're going to have no trouble. But if\nthe caller service is highly abstracted like Fargate, you might try collecting\nVPC Flow data or use the new VPC Interface for network packets (both cost you\nextra). If you are advanced enough to analyse this data, you might be able to\nobserve if the private route is being utilised.\n\nSo what are all of the private options?\n\n * AWS PrivateLink: creates an Elastic Network Interface (ENI)\n * Interface VPC Endpoint: provides connection to AWS PrivateLink\n * Gateway VPC Endpoint: configures VPC route table to supported AWS Resources\n * AWS Direct Connect: physical network backbone connectivity from a non-AWS\n   data centre to an availability zone or region\n * AWS Site-to-Site VPN: an encrypted tunnel over a the public internet that\n   terminates in your VPC and can be configured to resolve VPC private routes\n   from the origin end of the tunnel\n\nSince 2015, S3 has used the Gateway VPC Endpoint method, which in turn can be\nutilised with VPN and DirectConnect. You can use the \ndescribe-vpc-endpoint-services command to get a list of other available\nservices.\n\nMore often than not, private routing is configured but is never utilised in\npractice. It just sits there looking good for your audit while public routes are\nstill being used.\n\nSee the confusing troubleshooting guide\n[https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/] \npublished by AWS called Why can’t I connect to an S3 bucket using a gateway VPC\nendpoint? Yes it states \"Outbound rules with Destination as the public IPs used\nby Amazon S3\" for your NACL.\n\nEven AWS can't effectively help customers make Gateway Endpoint private routes\nwork, their instructions tell customers to route traffic publicly. They instruct\nyou to open your firewall rules to the public IP address to get it (Gateway\nEndpoint private route) to work, effectively by-passing the Gateway Endpoint.\nWhen you follow AWS guidance and fix the routing issue, you did not get the\nprivate route you wanted, your Gateway Endpoint remains idle, unused, redundant.\n\nConfiguring a Gateway VPC Endpoint\nThere are of course prerequisites and constraints, which I'll raise as they\nrelate to specific steps.\n\nUsing the Console, common (but not recommended) method to use AWS. I encourage\nthe use of Infrastructure as Code tools such as CloudFormation, AWS CDK, and\nTerraform by Hashicorp.\n\n1. From the VPC Dashboard, select Endpoints, and Create Endpoint. You'll have to\nchose the VPC and S3 service\n\n> The endpoint needs to be created in the same region as the S3 bucket\n2. Specify the optional Custom access policy to define your own using the\nfollowing JSON as a base, adjust to your needs\n\n{\n   \"Statement\": [\n     {\n       \"NotPrincipal\": {\n         \"AWS\": \"1234567890\"\n       },\n       \"Action\": \"*\",\n       \"Effect\": \"Deny\",\n       \"Resource\": [\n         \"arn:aws:s3:::targetbucket\",\n         \"arn:aws:s3:::targetbucket/*\"\n       ]\n     }\n   ]\n}\n\n\n> Important: The default access policy allows all users, all services, all AWS\naccounts\n3. Gateway VPC Endpoints configure your route tables, and as such can provide\nroutes to S3 limited to certain subnets. If you're doing this you probably only\nwant to choose a subnet that doesn't have a NAT Gateway. Save that and take note\nof the vpce-xxxx identifier.\n\n4. This next step is necessary to ensure the default public route to the S3\nbucket is not utilised, even when you think you are using the private route.\n\n> Caution: Enforcing this will likely break things, particularly things you think\nare using the private route will stop working too\nNow go to S3, choose the target bucket, choose Bucket Policy, use the following\nJSON as a base, adjust as needed.\n\n{\n   \"Version\": \"2012-10-17\",\n   \"Statement\": [\n     {\n       \"Principal\": \"*\",\n       \"Action\": \"*\",\n       \"Effect\": \"Deny\",\n       \"Resource\": [\n         \"arn:aws:s3:::targetbucket\",\n         \"arn:aws:s3:::targetbucket/*\"\n       ],\n       \"Condition\": {\n         \"StringNotEquals\": {\n           \"aws:sourceVpce\": \"vpce-xxxx\"\n         }\n       }\n     }\n   ]\n}\n\n\n5. Configure the clients. For demonstration purposes I chose the lowest common\nservice EC2, which other services sit on top of.\n\n> Only IPv4 is supported when using a Gateway VPC Endpoint\nCheck the following;\n\n * EC2 instance is in the correct subnet (inherently the right VPC and region)\n * Configure the default AWS region for your tools or SDK. If you don't use\n   profiles you can set the --region option in each command.\n * Verify IPv6 is not used aws configure get default.s3.use_dualstack_endpoint \n   should be false (also check profiles if they are used). When dualstack is\n   false communication stays on IPv4\n * If you're using your own DNS server, be sure DNS requests to AWS services\n   resolve to AWS-maintained IP addresses\n   [https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html].\n * Check outbound rule of Security Groups allowing traffic from the ID of the\n   prefix list associated with the gateway VPC endpoint\n   [http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-prefix-lists.html]\n   .\n\n> Important: DNS resolution must be enabled in your VPC\nNow you have a private route from EC2 to your S3 bucket. Easy right?\n\nApply better security options\nFocusing on the data transmission topic of this article, you might want to also\nconsider a few additional configuration changes to improve the public S3\nendpoint - private communication can also benefit if your threat model includes\nany assumed breach vectors, or your risk posture demands defence in depth.\n\n 1. Force HMAC-SHA256: Specifying the Signature Version\n    [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version] \n    in Request Authentication using aws configure set\n    default.s3.signature_version s3v4 ensures Signature Version 2 is never used,\n    which implements the deprecated SHA1 hashing algorithm. \n 2. Force Forward Secrecy: Many SDKs can't do this including the python SDK\n    (botocore and Boto3), which the AWS Command Line tools use, and is built\n    into all of AWS compute services based on Amazon Linux 1/2 AMIs (so almost\n    all of them).\n    To get around this, I have developed an optional extension to various\n    libraries which introduces a feature flag to support Forward Secrecy. I\n    submitted this change to AWS via PR but it is yet to be merged. Hopefully\n    AWS developers see this as an important feature and accept this optional\n    feature for all to use.\n    If however you are using a tool or SDK that supports custom ciphers, you\n    might try specifying the following Forward Secrecy cipher suite; \n    kEECDH:kEDH:!aNULL:!eNULL:!DES:!3DES:!RC4\n\nWhile AWS offers Forward Secrecy, it is near impossible to enforce in most\nplaces. I only know how to do it\n[https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a] when\nmanually interacting with the Query Request API using a HTTP client, and not any\nSDK or command line tool.","feature_image":"__GHOST_URL__/content/images/2020/06/leaky-bucket.jpg","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-06-16 03:49:35","created_by":"1","updated_at":"2021-03-31 13:58:54","updated_by":"1","published_at":"2020-06-17 09:26:00","published_by":"1","custom_excerpt":"Applying private routing to AWS management APIs is hard.\nAWS S3 has had some poor press coverage, but to Amazon's credit it has always been subject to authentication by default and not leaked any customer data.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":"__GHOST_URL__/private-aws-s3-how-hard-could-that-be/"},{"id":"6064789220c4c500017fbcc7","uuid":"fd8442f5-bad7-436a-9c08-9fc7fcd90c4a","title":"Everything in AWS is an API, is it secure?","slug":"everything-in-aws-is-an-api-is-it-secure","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"| AWS Auth name      | Hashing Signature  | Status             |\\n| ------------------ | :----------------: | :----------------: |\\n| Sigv2              | HMAC-SHA1          | Deprecated 2012    |\\n| Sigv4              | HMAC-SHA256        | First flaw was CVE-2018-10844 effecting AWS |\"}],[\"code\",{\"code\":\"testssl --quiet --mode parallel --standard --protocols -c --header --vulnerable --sneaky --phone-out --ids-friendly --nodns min --warnings off --hints --wide --grease --pfs --show-each --server-defaults --server-preference --client-simulation --color 0 https://calculator.s3.amazonaws.com/index.html\"}],[\"code\",{\"code\":\" Start 2020-08-29 22:20:32        -->> 52.216.186.211:443 (calculator.s3.amazonaws.com) <<--\\n\\n rDNS (52.216.186.211):  (instructed to minimize DNS queries)\\n Service detected:       HTTP\\n\\n\\n Testing protocols via sockets except NPN+ALPN \\n\\n SSLv2      not offered (OK)\\n SSLv3      not offered (OK)\\n TLS 1      offered (deprecated)\\n TLS 1.1    offered (deprecated)\\n TLS 1.2    offered (OK)\\n TLS 1.3    not offered and downgraded to a weaker protocol\\n NPN/SPDY   not offered\\n ALPN/HTTP2 not offered\\n\\n Testing for server implementation bugs \\n\\n No bugs found.\\n\\n Testing cipher categories \\n\\n NULL ciphers (no encryption)                  not offered (OK)\\n Anonymous NULL Ciphers (no authentication)    not offered (OK)\\n Export ciphers (w/o ADH+NULL)                 not offered (OK)\\n LOW: 64 Bit + DES, RC[2,4] (w/o export)       not offered (OK)\\n Triple DES Ciphers / IDEA                     offered\\n Obsolete: SEED + 128+256 Bit CBC cipher       offered\\n Strong encryption (AEAD ciphers)              offered (OK)\\n\\n\\n Testing robust (perfect) forward secrecy, (P)FS -- omitting Null Authentication/Encryption, 3DES, RC4 \\n\\n PFS is offered (OK), ciphers follow (client/browser support is important here) \\n\\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\\n-----------------------------------------------------------------------------------------------------------------------------\\n x1302   TLS_AES_256_GCM_SHA384            any        AESGCM      256      TLS_AES_256_GCM_SHA384                             not a/v\\n x1303   TLS_CHACHA20_POLY1305_SHA256      any        ChaCha20    256      TLS_CHACHA20_POLY1305_SHA256                       not a/v\\n xcc14   ECDHE-ECDSA-CHACHA20-POLY1305-OLD ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256_OLD  not a/v\\n xcc13   ECDHE-RSA-CHACHA20-POLY1305-OLD   ECDH       ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD    not a/v\\n xcc15   DHE-RSA-CHACHA20-POLY1305-OLD     DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD      not a/v\\n xc030   ECDHE-RSA-AES256-GCM-SHA384       ECDH 256   AESGCM      256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384              available\\n xc02c   ECDHE-ECDSA-AES256-GCM-SHA384     ECDH       AESGCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384            not a/v\\n xc028   ECDHE-RSA-AES256-SHA384           ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384              available\\n xc024   ECDHE-ECDSA-AES256-SHA384         ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384            not a/v\\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\\n xa3     DHE-DSS-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_DSS_WITH_AES_256_GCM_SHA384                not a/v\\n x9f     DHE-RSA-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_RSA_WITH_AES_256_GCM_SHA384                not a/v\\n xcca9   ECDHE-ECDSA-CHACHA20-POLY1305     ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256      not a/v\\n xcca8   ECDHE-RSA-CHACHA20-POLY1305       ECDH 256   ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256        available\\n xccaa   DHE-RSA-CHACHA20-POLY1305         DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256          not a/v\\n xc0af   ECDHE-ECDSA-AES256-CCM8           ECDH       AESCCM8     256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8                 not a/v\\n xc0ad   ECDHE-ECDSA-AES256-CCM            ECDH       AESCCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM                   not a/v\\n xc0a3   DHE-RSA-AES256-CCM8               DH         AESCCM8     256      TLS_DHE_RSA_WITH_AES_256_CCM_8                     not a/v\\n xc09f   DHE-RSA-AES256-CCM                DH         AESCCM      256      TLS_DHE_RSA_WITH_AES_256_CCM                       not a/v\\n x6b     DHE-RSA-AES256-SHA256             DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA256                not a/v\\n x6a     DHE-DSS-AES256-SHA256             DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA256                not a/v\\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\\n xc077   ECDHE-RSA-CAMELLIA256-SHA384      ECDH       Camellia    256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_CBC_SHA384         not a/v\\n xc073   ECDHE-ECDSA-CAMELLIA256-SHA384    ECDH       Camellia    256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_CBC_SHA384       not a/v\\n xc4     DHE-RSA-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA256           not a/v\\n xc3     DHE-DSS-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA256           not a/v\\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\\n xc043   DHE-DSS-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_DSS_WITH_ARIA_256_CBC_SHA384               not a/v\\n xc045   DHE-RSA-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_RSA_WITH_ARIA_256_CBC_SHA384               not a/v\\n xc049   ECDHE-ECDSA-ARIA256-CBC-SHA384    ECDH       ARIA        256      TLS_ECDHE_ECDSA_WITH_ARIA_256_CBC_SHA384           not a/v\\n xc04d   ECDHE-RSA-ARIA256-CBC-SHA384      ECDH       ARIA        256      TLS_ECDHE_RSA_WITH_ARIA_256_CBC_SHA384             not a/v\\n xc053   DHE-RSA-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_RSA_WITH_ARIA_256_GCM_SHA384               not a/v\\n xc057   DHE-DSS-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_DSS_WITH_ARIA_256_GCM_SHA384               not a/v\\n xc05d   ECDHE-ECDSA-ARIA256-GCM-SHA384    ECDH       ARIAGCM     256      TLS_ECDHE_ECDSA_WITH_ARIA_256_GCM_SHA384           not a/v\\n xc061   ECDHE-ARIA256-GCM-SHA384          ECDH       ARIAGCM     256      TLS_ECDHE_RSA_WITH_ARIA_256_GCM_SHA384             not a/v\\n xc07d   -                                 DH         CamelliaGCM 256      TLS_DHE_RSA_WITH_CAMELLIA_256_GCM_SHA384           not a/v\\n xc081   -                                 DH         CamelliaGCM 256      TLS_DHE_DSS_WITH_CAMELLIA_256_GCM_SHA384           not a/v\\n xc087   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_GCM_SHA384       not a/v\\n xc08b   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_GCM_SHA384         not a/v\\n x1301   TLS_AES_128_GCM_SHA256            any        AESGCM      128      TLS_AES_128_GCM_SHA256                             not a/v\\n x1304   TLS_AES_128_CCM_SHA256            any        AESCCM      128      TLS_AES_128_CCM_SHA256                             not a/v\\n x1305   TLS_AES_128_CCM_8_SHA256          any        AESCCM8     128      TLS_AES_128_CCM_8_SHA256                           not a/v\\n xc02f   ECDHE-RSA-AES128-GCM-SHA256       ECDH 256   AESGCM      128      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256              available\\n xc02b   ECDHE-ECDSA-AES128-GCM-SHA256     ECDH       AESGCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256            not a/v\\n xc027   ECDHE-RSA-AES128-SHA256           ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256              available\\n xc023   ECDHE-ECDSA-AES128-SHA256         ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256            not a/v\\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\\n xa2     DHE-DSS-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_DSS_WITH_AES_128_GCM_SHA256                not a/v\\n x9e     DHE-RSA-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_RSA_WITH_AES_128_GCM_SHA256                not a/v\\n xc0ae   ECDHE-ECDSA-AES128-CCM8           ECDH       AESCCM8     128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8                 not a/v\\n xc0ac   ECDHE-ECDSA-AES128-CCM            ECDH       AESCCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM                   not a/v\\n xc0a2   DHE-RSA-AES128-CCM8               DH         AESCCM8     128      TLS_DHE_RSA_WITH_AES_128_CCM_8                     not a/v\\n xc09e   DHE-RSA-AES128-CCM                DH         AESCCM      128      TLS_DHE_RSA_WITH_AES_128_CCM                       not a/v\\n x67     DHE-RSA-AES128-SHA256             DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA256                not a/v\\n x40     DHE-DSS-AES128-SHA256             DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA256                not a/v\\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\\n xc076   ECDHE-RSA-CAMELLIA128-SHA256      ECDH       Camellia    128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_CBC_SHA256         not a/v\\n xc072   ECDHE-ECDSA-CAMELLIA128-SHA256    ECDH       Camellia    128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_CBC_SHA256       not a/v\\n xbe     DHE-RSA-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA256           not a/v\\n xbd     DHE-DSS-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA256           not a/v\\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\\n xc042   DHE-DSS-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_DSS_WITH_ARIA_128_CBC_SHA256               not a/v\\n xc044   DHE-RSA-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_RSA_WITH_ARIA_128_CBC_SHA256               not a/v\\n xc048   ECDHE-ECDSA-ARIA128-CBC-SHA256    ECDH       ARIA        128      TLS_ECDHE_ECDSA_WITH_ARIA_128_CBC_SHA256           not a/v\\n xc04c   ECDHE-RSA-ARIA128-CBC-SHA256      ECDH       ARIA        128      TLS_ECDHE_RSA_WITH_ARIA_128_CBC_SHA256             not a/v\\n xc052   DHE-RSA-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_RSA_WITH_ARIA_128_GCM_SHA256               not a/v\\n xc056   DHE-DSS-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_DSS_WITH_ARIA_128_GCM_SHA256               not a/v\\n xc05c   ECDHE-ECDSA-ARIA128-GCM-SHA256    ECDH       ARIAGCM     128      TLS_ECDHE_ECDSA_WITH_ARIA_128_GCM_SHA256           not a/v\\n xc060   ECDHE-ARIA128-GCM-SHA256          ECDH       ARIAGCM     128      TLS_ECDHE_RSA_WITH_ARIA_128_GCM_SHA256             not a/v\\n xc07c   -                                 DH         CamelliaGCM 128      TLS_DHE_RSA_WITH_CAMELLIA_128_GCM_SHA256           not a/v\\n xc080   -                                 DH         CamelliaGCM 128      TLS_DHE_DSS_WITH_CAMELLIA_128_GCM_SHA256           not a/v\\n xc086   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_GCM_SHA256       not a/v\\n xc08a   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_GCM_SHA256         not a/v\\n\\n Elliptic curves offered:     prime256v1 secp384r1 \\n\\n\\n Testing server preferences \\n\\n Has server cipher order?     yes (OK)\\n Negotiated protocol          TLSv1.2\\n Negotiated cipher            ECDHE-RSA-AES128-GCM-SHA256, 256 bit ECDH (P-256)\\n Cipher order\\n    TLSv1:     ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \\n    TLSv1.1:   ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \\n    TLSv1.2:   ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-CHACHA20-POLY1305 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES256-SHA ECDHE-RSA-AES256-SHA384 AES128-GCM-SHA256\\n               AES256-GCM-SHA384 AES128-SHA AES128-SHA256 AES256-SHA AES256-SHA256 DES-CBC3-SHA \\n\\n\\n Testing server defaults (Server Hello) \\n\\n TLS extensions (standard)    \\\"server name/#0\\\" \\\"EC point formats/#11\\\" \\\"renegotiation info/#65281\\\"\\n Session Ticket RFC 5077 hint no -- no lifetime advertised\\n SSL Session ID support       yes\\n Session Resumption           Tickets no, ID: no\\n TLS clock skew               Random values, no fingerprinting possible \\n Signature Algorithm          SHA256 with RSA\\n Server key size              RSA 2048 bits\\n Server key usage             Digital Signature, Key Encipherment\\n Server extended key usage    TLS Web Server Authentication, TLS Web Client Authentication\\n Serial / Fingerprints        082DF68EE9C69315BEBF72079B3810FD / SHA1 3FE05B486E3F0987130BA1D4EA0F299539A58243\\n                              SHA256 272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674\\n Common Name (CN)             *.s3.amazonaws.com \\n subjectAltName (SAN)         *.s3.amazonaws.com s3.amazonaws.com \\n Issuer                       DigiCert Baltimore CA-2 G2 (DigiCert Inc from US)\\n Trust (hostname)             Ok via SAN wildcard (same w/o SNI)\\n Chain of trust               Ok   \\n EV cert (experimental)       no \\n ETS/\\\"eTLS\\\", visibility info  not present\\n Certificate Validity (UTC)   194 >= 60 days (2019-11-09 11:00 --> 2021-03-12 23:00)\\n # of certificates provided   2\\n In pwnedkeys.com DB          not in database\\n Certificate Revocation List  http://crl3.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\\n                              http://crl4.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\\n OCSP URI                     http://ocsp.digicert.com, not revoked\\n OCSP stapling                not offered\\n OCSP must staple extension   --\\n DNS CAA RR (experimental)    (instructed to minimize DNS queries)\\n Certificate Transparency     yes (certificate extension)\\n\\n\\n Testing HTTP header response @ \\\"/index.html\\\" \\n\\n HTTP Status Code             200 OK\\n HTTP clock skew              0 sec from localtime\\n Strict Transport Security    not offered\\n Public Key Pinning           --\\n Server banner                AmazonS3\\n Application banner           --\\n Cookie(s)                    (none issued at \\\"/index.html\\\")\\n Security headers             Cache-Control no-cache no-store must-revalidate\\n Reverse Proxy banner         --\\n\\n\\n Testing vulnerabilities \\n\\n Heartbleed (CVE-2014-0160)                not vulnerable (OK), no heartbeat extension\\n CCS (CVE-2014-0224)                       not vulnerable (OK)\\n Ticketbleed (CVE-2016-9244), experiment.  not vulnerable (OK), no session ticket extension\\n ROBOT                                     not vulnerable (OK)\\n Secure Renegotiation (RFC 5746)           supported (OK)\\n Secure Client-Initiated Renegotiation     likely not vulnerable (OK), timed out\\n CRIME, TLS (CVE-2012-4929)                not vulnerable (OK)\\n BREACH (CVE-2013-3587)                    no HTTP compression (OK)  - only supplied \\\"/index.html\\\" tested\\n POODLE, SSL (CVE-2014-3566)               not vulnerable (OK), no SSLv3 support\\n TLS_FALLBACK_SCSV (RFC 7507)              Check failed, unexpected result , run testssl -Z --debug=1 and look at /tmp/testssl.TpvX5W/*tls_fallback_scsv.txt\\n SWEET32 (CVE-2016-2183, CVE-2016-6329)    VULNERABLE, uses 64 bit block ciphers\\n FREAK (CVE-2015-0204)                     not vulnerable (OK)\\n DROWN (CVE-2016-0800, CVE-2016-0703)      not vulnerable on this host and port (OK)\\n                                           make sure you don't use this certificate elsewhere with SSLv2 enabled services\\n                                           https://censys.io/ipv4?q=272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674 could help you to find out\\n LOGJAM (CVE-2015-4000), experimental      not vulnerable (OK): no DH EXPORT ciphers, no DH key detected with <= TLS 1.2\\n BEAST (CVE-2011-3389)                     \\n TLS1:\\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\\n-----------------------------------------------------------------------------------------------------------------------------\\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\\n xc022   SRP-DSS-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_DSS_WITH_AES_256_CBC_SHA               not a/v\\n xc021   SRP-RSA-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_RSA_WITH_AES_256_CBC_SHA               not a/v\\n xc020   SRP-AES-256-CBC-SHA               SRP        AES         256      TLS_SRP_SHA_WITH_AES_256_CBC_SHA                   not a/v\\n x91     DHE-PSK-AES256-CBC-SHA            DHEPSK     AES         256      TLS_DHE_PSK_WITH_AES_256_CBC_SHA                   not a/v\\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\\n x37     DH-RSA-AES256-SHA                 DH/RSA     AES         256      TLS_DH_RSA_WITH_AES_256_CBC_SHA                    not a/v\\n x36     DH-DSS-AES256-SHA                 DH/DSS     AES         256      TLS_DH_DSS_WITH_AES_256_CBC_SHA                    not a/v\\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\\n x86     DH-RSA-CAMELLIA256-SHA            DH/RSA     Camellia    256      TLS_DH_RSA_WITH_CAMELLIA_256_CBC_SHA               not a/v\\n x85     DH-DSS-CAMELLIA256-SHA            DH/DSS     Camellia    256      TLS_DH_DSS_WITH_CAMELLIA_256_CBC_SHA               not a/v\\n xc019   AECDH-AES256-SHA                  ECDH       AES         256      TLS_ECDH_anon_WITH_AES_256_CBC_SHA                 not a/v\\n x3a     ADH-AES256-SHA                    DH         AES         256      TLS_DH_anon_WITH_AES_256_CBC_SHA                   not a/v\\n x89     ADH-CAMELLIA256-SHA               DH         Camellia    256      TLS_DH_anon_WITH_CAMELLIA_256_CBC_SHA              not a/v\\n xc00f   ECDH-RSA-AES256-SHA               ECDH/RSA   AES         256      TLS_ECDH_RSA_WITH_AES_256_CBC_SHA                  not a/v\\n xc005   ECDH-ECDSA-AES256-SHA             ECDH/ECDSA AES         256      TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA                not a/v\\n x35     AES256-SHA                        RSA        AES         256      TLS_RSA_WITH_AES_256_CBC_SHA                       available\\n xc036   ECDHE-PSK-AES256-CBC-SHA          ECDHEPSK   AES         256      TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA                 not a/v\\n x84     CAMELLIA256-SHA                   RSA        Camellia    256      TLS_RSA_WITH_CAMELLIA_256_CBC_SHA                  not a/v\\n x95     RSA-PSK-AES256-CBC-SHA            RSAPSK     AES         256      TLS_RSA_PSK_WITH_AES_256_CBC_SHA                   not a/v\\n x8d     PSK-AES256-CBC-SHA                PSK        AES         256      TLS_PSK_WITH_AES_256_CBC_SHA                       not a/v\\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\\n xc01f   SRP-DSS-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_DSS_WITH_AES_128_CBC_SHA               not a/v\\n xc01e   SRP-RSA-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_RSA_WITH_AES_128_CBC_SHA               not a/v\\n xc01d   SRP-AES-128-CBC-SHA               SRP        AES         128      TLS_SRP_SHA_WITH_AES_128_CBC_SHA                   not a/v\\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\\n x31     DH-RSA-AES128-SHA                 DH/RSA     AES         128      TLS_DH_RSA_WITH_AES_128_CBC_SHA                    not a/v\\n x30     DH-DSS-AES128-SHA                 DH/DSS     AES         128      TLS_DH_DSS_WITH_AES_128_CBC_SHA                    not a/v\\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\\n x98     DH-RSA-SEED-SHA                   DH/RSA     SEED        128      TLS_DH_RSA_WITH_SEED_CBC_SHA                       not a/v\\n x97     DH-DSS-SEED-SHA                   DH/DSS     SEED        128      TLS_DH_DSS_WITH_SEED_CBC_SHA                       not a/v\\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\\n x43     DH-RSA-CAMELLIA128-SHA            DH/RSA     Camellia    128      TLS_DH_RSA_WITH_CAMELLIA_128_CBC_SHA               not a/v\\n x42     DH-DSS-CAMELLIA128-SHA            DH/DSS     Camellia    128      TLS_DH_DSS_WITH_CAMELLIA_128_CBC_SHA               not a/v\\n xc018   AECDH-AES128-SHA                  ECDH       AES         128      TLS_ECDH_anon_WITH_AES_128_CBC_SHA                 not a/v\\n x34     ADH-AES128-SHA                    DH         AES         128      TLS_DH_anon_WITH_AES_128_CBC_SHA                   not a/v\\n x9b     ADH-SEED-SHA                      DH         SEED        128      TLS_DH_anon_WITH_SEED_CBC_SHA                      not a/v\\n x46     ADH-CAMELLIA128-SHA               DH         Camellia    128      TLS_DH_anon_WITH_CAMELLIA_128_CBC_SHA              not a/v\\n xc00e   ECDH-RSA-AES128-SHA               ECDH/RSA   AES         128      TLS_ECDH_RSA_WITH_AES_128_CBC_SHA                  not a/v\\n xc004   ECDH-ECDSA-AES128-SHA             ECDH/ECDSA AES         128      TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA                not a/v\\n x2f     AES128-SHA                        RSA        AES         128      TLS_RSA_WITH_AES_128_CBC_SHA                       available\\n xc035   ECDHE-PSK-AES128-CBC-SHA          ECDHEPSK   AES         128      TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA                 not a/v\\n x90     DHE-PSK-AES128-CBC-SHA            DHEPSK     AES         128      TLS_DHE_PSK_WITH_AES_128_CBC_SHA                   not a/v\\n x96     SEED-SHA                          RSA        SEED        128      TLS_RSA_WITH_SEED_CBC_SHA                          not a/v\\n x41     CAMELLIA128-SHA                   RSA        Camellia    128      TLS_RSA_WITH_CAMELLIA_128_CBC_SHA                  not a/v\\n x07     IDEA-CBC-SHA                      RSA        IDEA        128      TLS_RSA_WITH_IDEA_CBC_SHA                          not a/v\\n x94     RSA-PSK-AES128-CBC-SHA            RSAPSK     AES         128      TLS_RSA_PSK_WITH_AES_128_CBC_SHA                   not a/v\\n x8c     PSK-AES128-CBC-SHA                PSK        AES         128      TLS_PSK_WITH_AES_128_CBC_SHA                       not a/v\\n x21     KRB5-IDEA-CBC-SHA                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_SHA                         not a/v\\n x25     KRB5-IDEA-CBC-MD5                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_MD5                         not a/v\\n xc012   ECDHE-RSA-DES-CBC3-SHA            ECDH       3DES        168      TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA                not a/v\\n xc008   ECDHE-ECDSA-DES-CBC3-SHA          ECDH       3DES        168      TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA              not a/v\\n xc01c   SRP-DSS-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_DSS_WITH_3DES_EDE_CBC_SHA              not a/v\\n xc01b   SRP-RSA-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_RSA_WITH_3DES_EDE_CBC_SHA              not a/v\\n xc01a   SRP-3DES-EDE-CBC-SHA              SRP        3DES        168      TLS_SRP_SHA_WITH_3DES_EDE_CBC_SHA                  not a/v\\n x16     EDH-RSA-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA                  not a/v\\n x13     EDH-DSS-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA                  not a/v\\n x10     DH-RSA-DES-CBC3-SHA               DH/RSA     3DES        168      TLS_DH_RSA_WITH_3DES_EDE_CBC_SHA                   not a/v\\n x0d     DH-DSS-DES-CBC3-SHA               DH/DSS     3DES        168      TLS_DH_DSS_WITH_3DES_EDE_CBC_SHA                   not a/v\\n xc017   AECDH-DES-CBC3-SHA                ECDH       3DES        168      TLS_ECDH_anon_WITH_3DES_EDE_CBC_SHA                not a/v\\n x1b     ADH-DES-CBC3-SHA                  DH         3DES        168      TLS_DH_anon_WITH_3DES_EDE_CBC_SHA                  not a/v\\n xc00d   ECDH-RSA-DES-CBC3-SHA             ECDH/RSA   3DES        168      TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA                 not a/v\\n xc003   ECDH-ECDSA-DES-CBC3-SHA           ECDH/ECDSA 3DES        168      TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA               not a/v\\n x0a     DES-CBC3-SHA                      RSA        3DES        168      TLS_RSA_WITH_3DES_EDE_CBC_SHA                      available\\n x93     RSA-PSK-3DES-EDE-CBC-SHA          RSAPSK     3DES        168      TLS_RSA_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\\n x8b     PSK-3DES-EDE-CBC-SHA              PSK        3DES        168      TLS_PSK_WITH_3DES_EDE_CBC_SHA                      not a/v\\n x1f     KRB5-DES-CBC3-SHA                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_SHA                     not a/v\\n x23     KRB5-DES-CBC3-MD5                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_MD5                     not a/v\\n xc034   ECDHE-PSK-3DES-EDE-CBC-SHA        ECDHEPSK   3DES        168      TLS_ECDHE_PSK_WITH_3DES_EDE_CBC_SHA                not a/v\\n x8f     DHE-PSK-3DES-EDE-CBC-SHA          DHEPSK     3DES        168      TLS_DHE_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\\n xfeff   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\\n xffe0   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\\n x63     EXP1024-DHE-DSS-DES-CBC-SHA       DH(1024)   DES         56,exp   TLS_DHE_DSS_EXPORT1024_WITH_DES_CBC_SHA            not a/v\\n x15     EDH-RSA-DES-CBC-SHA               DH         DES         56       TLS_DHE_RSA_WITH_DES_CBC_SHA                       not a/v\\n x12     EDH-DSS-DES-CBC-SHA               DH         DES         56       TLS_DHE_DSS_WITH_DES_CBC_SHA                       not a/v\\n x0f     DH-RSA-DES-CBC-SHA                DH/RSA     DES         56       TLS_DH_RSA_WITH_DES_CBC_SHA                        not a/v\\n x0c     DH-DSS-DES-CBC-SHA                DH/DSS     DES         56       TLS_DH_DSS_WITH_DES_CBC_SHA                        not a/v\\n x1a     ADH-DES-CBC-SHA                   DH         DES         56       TLS_DH_anon_WITH_DES_CBC_SHA                       not a/v\\n x62     EXP1024-DES-CBC-SHA               RSA(1024)  DES         56,exp   TLS_RSA_EXPORT1024_WITH_DES_CBC_SHA                not a/v\\n x09     DES-CBC-SHA                       RSA        DES         56       TLS_RSA_WITH_DES_CBC_SHA                           not a/v\\n x1e     KRB5-DES-CBC-SHA                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_SHA                          not a/v\\n x22     KRB5-DES-CBC-MD5                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_MD5                          not a/v\\n xfefe   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\\n xffe1   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\\n x14     EXP-EDH-RSA-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA              not a/v\\n x11     EXP-EDH-DSS-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA              not a/v\\n x19     EXP-ADH-DES-CBC-SHA               DH(512)    DES         40,exp   TLS_DH_anon_EXPORT_WITH_DES40_CBC_SHA              not a/v\\n x08     EXP-DES-CBC-SHA                   RSA(512)   DES         40,exp   TLS_RSA_EXPORT_WITH_DES40_CBC_SHA                  not a/v\\n x06     EXP-RC2-CBC-MD5                   RSA(512)   RC2         40,exp   TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5                 not a/v\\n x27     EXP-KRB5-RC2-CBC-SHA              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_SHA                not a/v\\n x26     EXP-KRB5-DES-CBC-SHA              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_SHA                not a/v\\n x2a     EXP-KRB5-RC2-CBC-MD5              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_MD5                not a/v\\n x29     EXP-KRB5-DES-CBC-MD5              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_MD5                not a/v\\n x0b     EXP-DH-DSS-DES-CBC-SHA            DH/DSS     DES         40,exp   TLS_DH_DSS_EXPORT_WITH_DES40_CBC_SHA               not a/v\\n x0e     EXP-DH-RSA-DES-CBC-SHA            DH/RSA     DES         40,exp   TLS_DH_RSA_EXPORT_WITH_DES40_CBC_SHA               not a/v\\n\\n VULNERABLE -- but also supports higher protocols (possible mitigation)  TLSv1.1 TLSv1.2\\n\\n LUCKY13 (CVE-2013-0169), experimental     potentially VULNERABLE, uses cipher block chaining (CBC) ciphers with TLS. Check patches\\n RC4 (CVE-2013-2566, CVE-2015-2808)        no RC4 ciphers detected (OK)\\n\\n\\n Running client simulations (HTTP) via sockets \\n\\n Browser                      Protocol  Cipher Suite Name (OpenSSL)       Forward Secrecy\\n------------------------------------------------------------------------------------------------\\n Android 4.4.2                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 5.0.0                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 6.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 7.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 8.1 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 9.0 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Android 10.0 (native)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Chrome 74 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Chrome 79 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Firefox 66 (Win 8.1/10)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Firefox 71 (Win 10)          TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n IE 6 XP                      No connection\\n IE 8 Win 7                   TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n IE 8 XP                      TLSv1.0   DES-CBC3-SHA                      No FS\\n IE 11 Win 7                  TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n IE 11 Win 8.1                TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n IE 11 Win Phone 8.1          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n IE 11 Win 10                 TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Edge 15 Win 10               TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Edge 17 (Win 10)             TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Opera 66 (Win 10)            TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Safari 9 iOS 9               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Safari 9 OS X 10.11          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Safari 10 OS X 10.12         TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Safari 12.1 (iOS 12.2)       TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Safari 13.0 (macOS 10.14.6)  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Apple ATS 9 iOS 9            TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Java 6u45                    TLSv1.0   AES128-SHA                        No FS\\n Java 7u25                    TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Java 8u161                   TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n Java 11.0.2 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Java 12.0.1 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n OpenSSL 1.0.2e               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\\n OpenSSL 1.1.0l (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n OpenSSL 1.1.1d (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n Thunderbird (68.3)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\\n\\n Done 2020-08-29 22:26:19 [ 349s] -->> 52.216.186.211:443 (calculator.s3.amazonaws.com) <<--\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://techcommunity.microsoft.com/t5/windows-it-pro-blog/sha-1-windows-content-to-be-retired-august-3-2020/ba-p/1544373\"]],[\"a\",[\"href\",\"https://gfw.report/blog/gfw_esni_blocking/en/\"]],[\"a\",[\"href\",\"https://www.microsoft.com/security/blog/2020/08/20/taking-transport-layer-security-tls-to-the-next-level-with-tls-1-3/\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/microsoft-365/compliance/tls-1.0-and-1.1-deprecation-for-office-365?view=o365-worldwide\"]],[\"a\",[\"href\",\"https://docs.microsoft.com/en-us/windows-server/administration/linux-package-repository-for-microsoft-software\"]],[\"a\",[\"href\",\"https://docs.aws.amazon.com/general/latest/gr/rande.html\"]],[\"u\"],[\"a\",[\"href\",\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Query-Requests.html\"]],[\"code\"],[\"em\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Length_extension_attack\",\"rel\",\"noreferrer\"]],[\"a\",[\"href\",\"__GHOST_URL__/tls-secures-my-data-right/\"]],[\"a\",[\"href\",\"https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a\"]],[\"a\",[\"href\",\"https://www.ssllabs.com/ssltest/analyze.html?d=calculator.s3.amazonaws.com\"]]],\"sections\":[[1,\"p\",[[0,[0],0,\"EDIT: 2020-08-01 \"],[0,[1],1,\"SHA-1 Windows content to be retired\"],[0,[],1,\" in 2 days. Amazon is in an ever decreasing group of cryptography ignorant providers.\"]]],[1,\"p\",[[0,[0],1,\"EDIT: 2020-08-07\"],[0,[],0,\" China believes TLS 1.3 privacy (hidden identity, not to be confused with confidentiality of data) is so effective they are going to block all TLS 1.3 traffic. \"],[0,[2],1,\"Researchers have a work around\"],[0,[],0,\" but China will surely adapt. \"]]],[1,\"p\",[[0,[0],1,\"EDIT: 2020-08-20\"],[0,[],0,\" Microsoft \"],[0,[3],1,\"enables TLS 1.3 by default\"],[0,[],0,\" in latest Windows 10 builds\"]]],[1,\"p\",[[0,[0],1,\"EDIT: 2020-08-29\"],[0,[],0,\" On \"],[0,[4],1,\"October 15 Microsoft will no longer support\"],[0,[],0,\" TLS 1.0/1.1 for any Office365 product. Also \"],[0,[5],1,\"all linux software in the Microsoft repos\"],[0,[],0,\" will disable these insecure protocols.\"]]],[1,\"p\",[[0,[],0,\"We all know that everything in AWS is an API right?\"],[1,[],0,0],[0,[],0,\"Some people I speak to, as a consultant, hear this common cliche but not many consider the implications of this infamous characteristic of AWS.\"]]],[1,\"p\",[[0,[],0,\"All of the services in AWS internally communicate \"],[0,[6],1,\"over the same APIs\"],[0,[],0,\" that customers also use, and non-customers alike, as all access by default is \"],[0,[7],1,\"public\"],[0,[],0,\".\"],[1,[],0,1],[0,[],0,\"This public API has a name; \"],[0,[8],1,\"Query Request HTTP API\"],[0,[],0,\", it's not common knowledge and is the lowest common methods for how all things happen in AWS.\"]]],[1,\"h2\",[[0,[],0,\"Where is the Query Request API used?\"]]],[1,\"p\",[[0,[],0,\"Everywhere.\"]]],[1,\"p\",[[0,[],0,\"You simply cannot use AWS without interacting with the Query Request API, and AWS itself cannot operate without it;\"]]],[3,\"ul\",[[[0,[],0,\"Services that intercommunicate. e.g.\"],[1,[],0,2],[0,[],0,\"Lambda > CloudWatch\"],[1,[],0,3],[0,[],0,\"IAM > CloudTrail\"],[1,[],0,4],[0,[],0,\"CloudFormation deploying things\"],[1,[],0,5],[0,[],0,\"everything uses it\"]],[[0,[],0,\"Official SDKs; like botocore for python, all SDKs use it\"]],[[0,[],0,\"Terraform by HashiCorp used boto3 but pivoted to the Query Request API\"]]]],[1,\"p\",[[0,[],0,\"Everything uses the Query Request API!\"]]],[1,\"h2\",[[0,[],0,\"How does the Query Request API work?\"]]],[1,\"p\",[[0,[],0,\"AWS implement the RESTful API model on top of HTTP, with both TLS and plain HTTP. endpoints available.\"],[1,[],0,6],[0,[],0,\"Though it is almost impossible to be strictly RESTful and remain functionally usable, AWS provide an exceptionally close alignment to RESTful designs.\"]]],[1,\"p\",[[0,[],0,\"There are multiple ways to craft a HTTP request to interact with the Query Request API, any HTTP client, even the web browser itself, is capable.\"]]],[1,\"h2\",[[0,[],0,\"Authentication\"]]],[1,\"p\",[[0,[],0,\"The common supported authentication method of the Query Request API is called Signature Version 4. My investigations into EC2 and Lambda has uncovered that AWS still uses (and therefore supports) Signature Version 2.\"]]],[1,\"p\",[[0,[],0,\"Both Sigv2 and Sigv4 implement hash-based message authentication code (HMAC), which relies on an out-of-band pre-shared secret with the client.\"],[1,[],0,7],[0,[],0,\"This secret also enables the server to derive a client identity.\"]]],[1,\"p\",[[0,[],0,\"The AWS implementation of HMAC uses their \"],[0,[9],1,\"Secret Key\"],[0,[],0,\" as the secret, which corresponds to their \"],[0,[9],1,\"Key Id\"],[0,[],0,\" as the identity.\"],[1,[],0,8],[0,[],0,\"Clients perform a special procedure called \"],[0,[10],1,\"request signing\"],[0,[],0,\", essentially giving you a data integrity guarantee no different from what TLS offers.\"],[1,[],0,9],[0,[],0,\"Signing is complex, so many developers prefer to use an SDK instead of understanding and performing the signing procedure themselves.\"]]],[1,\"h2\",[[0,[],0,\"Are the SDKs secure?\"]]],[1,\"p\",[[0,[],0,\"Not exactly.\"],[1,[],0,10],[0,[],0,\"Using SDKs often make it difficult, even impossible, to customise your interactions with the Query Request API and optimise for security.\"]]],[1,\"p\",[[0,[],0,\"SDKs by-design will abstract complexities away from the developers.\"],[1,[],0,11],[0,[],0,\"Framework primarily prioritise developer experience, reduced lines of code, and low barrier of learning to get started. \"],[1,[],0,12],[0,[],0,\"Developers who never move down the stack below the SDK need to understand their security configuration, and equally the frameworks don't consider security configurations (such as protocol and ciphers) important to developer.\"],[1,[],0,13],[0,[],0,\"This means the SDK often optimise functionality for the widest user environment, support backwards-compatibility for old technology, and are slow to expose new security features to users if they are not requesting them.\"]]],[1,\"p\",[[0,[],0,\"I have identified various SDKs security defaults to be insufficient in some client environments. A good observation is they typically don't allow insecure downgrades in protocol or weak cipher negotiation (which are typically controlled by priority order server-side) but common to all SDKs I investigated, none allowed the client to specify the desired ciphers (aligned to the customers requirements) that inform the server negotiation procedure.\"]]],[1,\"h2\",[[0,[],0,\"Is the AWS Secret Key secure?\"]]],[1,\"p\",[[0,[],0,\"To obtain a pre-shared secret, AWS customers can generate \"],[0,[9],1,\"Key Id\"],[0,[],0,\" and it's \"],[0,[9],1,\"Secret Key\"],[0,[],0,\" from an IAM User, or obtain a token issued via STS. Most AWS Resources also provide you a built-in key/secret pair representing the identity of the AWS Resource belonging to you rather than one of your user identities.\"]]],[1,\"p\",[[0,[],0,\"In terms of security of the \"],[0,[9],1,\"Secret Key\"],[0,[],0,\" itself, it would normally be important for an attacker to try figuring out what makes up the value of the \"],[0,[9],1,\"Secret Key\"],[0,[],0,\", to learn how to produce a \"],[0,[9],1,\"Secret Key\"],[0,[],0,\" in a predictable reproducible way, to learn what it itself contains within.\"],[1,[],0,14],[0,[],0,\"But for HMAC which is completely random, there is only a perceived value in figuring this out.\"],[1,[],0,15],[0,[],0,\"The true value in the HMAC secret is that you perform actions as the identity it belongs to - skipping any authentications checks.\"],[1,[],0,16],[0,[],0,\"By having the \"],[0,[9],1,\"Secret Key\"],[0,[],0,\" AWS assumes you are both authenticated and authorised to perform all action associated with that identity - limited only by IAM policy and various resource policies that may be in place.\"]]],[1,\"p\",[[0,[],0,\"So yes, Secret Key's are secure. Keep them safe, they are necessary, powerful, and radioactive.\"]]],[1,\"p\",[[0,[],0,\"It is surprising Signature Version 2 is still supported because it implements HMAC-SHA1, which was deprecated in 2013 after being broken and proven to be insecure the year prior.\"]]],[1,\"h2\",[[0,[],0,\"But isn't HMAC still considered secure?\"]]],[1,\"p\",[[0,[],0,\"Yes Sigv4 HMAC is still considered safe today, but Sigv2 uses SHA1.\"]]],[1,\"p\",[[0,[],0,\"The first research demonstrating flaws with SHA1 appeared in 2005, various researchers improved it over the past decade. The advancement announced in 2012 and published in 2013 prompted the industry to officially announce it deprecation.\"]]],[1,\"p\",[[0,[],0,\"Essentially HMAC uses a signature algorithm no different from TLS, AWS signature algorithm choices are;\"]]],[10,0],[1,\"p\",[[0,[],0,\"Basically the reason for SHA1 deprecation was due to the hash uses Merkle–Damgård padding, which is fundamentally vulnerable to a \"],[0,[11],1,\"length extension\"],[0,[],0,\" attack and cannot be fixed. Just consider the computing power in 2005 when SHA1 was published as broken, when the proofs emerged in 2012 is still an extremely long time ago for technology.\"],[1,[],0,17],[0,[],0,\"We are now in 2020.\"]]],[1,\"p\",[[0,[],0,\"HMAC-SHA256 has been subjected to various attacks, so it is an insufficient signature algorithm choice for AWS to continue supporting. It is not immune to hash collisions either, so this alone should be reason enough to deprecate it.\"]]],[1,\"p\",[[0,[],0,\"This is the time where we need to simply point out that as an industry, it is well known that the SHA-based signatures are problematic at best, and generally considered harmful.\"],[1,[],0,18],[0,[],0,\"I wonder why AWS has chosen to ignore this?\"]]],[1,\"p\",[[0,[],0,\"As a recommendation for AWS, the AES-GCM signature algorithm is already prolific, it would be a good choice.\"],[1,[],0,19],[0,[],0,\"It is an widely accepted and scrutinised choice, with many sources available for implementation. A certain video conferencing product who Amazon recently announced partnership with, has relatively much fewer resources, has announced a 90-day plan to implement this as 1 small feature from a long list of security improvements.\"]]],[1,\"h2\",[[0,[],0,\"The Query Request API is secure right?\"]]],[1,\"p\",[[0,[],0,\"To answer that let's define \"],[0,[10],1,\"secure\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Most security professionals will agree on at least 3 characteristics, but i'll provide another 3 that can be more important\"]]],[3,\"ul\",[[[0,[0],1,\"Confidential\"],[0,[],0,\" in most cases yes, but not always. \"],[1,[],0,20],[0,[],0,\"Read \"],[0,[12],1,\"about TLS\"],[0,[],0,\" confidentiality myths, basically \"],[0,[13],1,\"enforcing ciphers that apply Forward Secrecy\"],[0,[],0,\" is the best we have for Query Request API confidentiality, and that only applies where confidentiality can be enforced by TLS of course\"]],[[0,[0],1,\"Integrity\"],[0,[],0,\" yes, in all cases by virtue of both TLS1.3 and HMAC-SHA256\"]],[[0,[0],1,\"Available\"],[0,[],0,\" for any service to have at least 99.99% SLA, the API must also\"]],[[0,[0],1,\"Non-repudiation\"],[0,[],0,\" yes, HMAC-SHA256 provides identity and TCP/HTTP includes teh additional metadata\"]],[[0,[0],1,\"Known vulnerabilities\"],[0,[],0,\" exist however, so this is undesired.\"]],[[0,[0],1,\"Customer Secure Configurations\"],[0,[],0,\" are possible if you use the Query Request API directly rather than the SDKs.\"]]]],[1,\"p\",[[0,[],0,\"AWS do offer us many security features - few are default secure.\"]]],[1,\"h2\",[[0,[],0,\"Testing\"]]],[1,\"p\",[[0,[],0,\"Not too technical? That's fine, \"],[0,[14],1,\"Qualys has a pretty decent test online\"],[0,[],0,\" here that gives similar results.\"]]],[1,\"p\",[[0,[],0,\"Let's gather some data using https://testssl.sh \"]]],[10,1],[1,\"p\",[[0,[],0,\"produces;\"]]],[10,2],[1,\"p\",[[0,[],0,\"Which shows us;\"]]],[3,\"ul\",[[[0,[],0,\"TLS1.0 and 1.1 are still supported, with a few noticeable ciphers we can exploit for over a decade\"]],[[0,[],0,\"Vulnerable to SWEET32 (CVE-2016-2183, CVE-2016-6329) which has some trivial exploits online for the script kiddies to run without learning much more than how to open a terminal window..\"]],[[0,[],0,\"no OSCP at all, not stabled or the CA enforcement of the must staple flag in the cert.\"]],[[0,[],0,\"Valid for 60 days, meaning we have a very wide chance to wait for IP churn to provide a window to do a DNS version of domain take over.\"]],[[0,[],0,\"Session Side-Jacking seems like a promising attack vector too, but requires further investigation than we have done here\"]],[[0,[],0,\"best for last, those CBC ciphers are a great target to test some very well known padding related attack vectors considering the internet has pretty much gotten rid of CBC ciphers between 2013-2015.\"]]]],[1,\"p\",[[0,[],0,\"If i can spend a whole 15mins and next to zero technical effort to learn this, and I am not even slightly considered a motivated attacker, or an attacker of any description. What do you think can happen if there is 1 human out there on planet earth targeting your data? Amazon's security won't stop them (clearly), and you have no way to configure Amazon to protect yourself. I guess we can hope that the name Amazon is a deterrent and you're safe in the expanding sea of customers, but the news cycles about S3 breaches tells a very different story about the deterrent quality of an North American corporate to the rest of the world.. You decide, it's your threat model, your risk posture - not mine.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"While AWS offer many security features, they allow insecure and deprecated protocol downgrades to TLS 1.0, and support deprecated signature algorithms for their API authentication.\"]]],[1,\"p\",[[0,[],0,\"These backwards-compatibility decisions and continued use of deprecated security features are open to attackers who will utilise anything available, they don't stick to the best offered security options.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p><strong>EDIT: 2020-08-01 <a href=\"https://techcommunity.microsoft.com/t5/windows-it-pro-blog/sha-1-windows-content-to-be-retired-august-3-2020/ba-p/1544373\">SHA-1 Windows content to be retired</a> in 2 days. Amazon is in an ever decreasing group of cryptography ignorant providers.</strong></p><p><strong>EDIT: 2020-08-07</strong> China believes TLS 1.3 privacy (hidden identity, not to be confused with confidentiality of data) is so effective they are going to block all TLS 1.3 traffic. <a href=\"https://gfw.report/blog/gfw_esni_blocking/en/\">Researchers have a work around</a> but China will surely adapt. </p><p><strong>EDIT: 2020-08-20</strong> Microsoft <a href=\"https://www.microsoft.com/security/blog/2020/08/20/taking-transport-layer-security-tls-to-the-next-level-with-tls-1-3/\">enables TLS 1.3 by default</a> in latest Windows 10 builds</p><p><strong>EDIT: 2020-08-29</strong> On <a href=\"https://docs.microsoft.com/en-us/microsoft-365/compliance/tls-1.0-and-1.1-deprecation-for-office-365?view=o365-worldwide\">October 15 Microsoft will no longer support</a> TLS 1.0/1.1 for any Office365 product. Also <a href=\"https://docs.microsoft.com/en-us/windows-server/administration/linux-package-repository-for-microsoft-software\">all linux software in the Microsoft repos</a> will disable these insecure protocols.</p><p>We all know that everything in AWS is an API right?<br>Some people I speak to, as a consultant, hear this common cliche but not many consider the implications of this infamous characteristic of AWS.</p><p>All of the services in AWS internally communicate <a href=\"https://docs.aws.amazon.com/general/latest/gr/rande.html\">over the same APIs</a> that customers also use, and non-customers alike, as all access by default is <u>public</u>.<br>This public API has a name; <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Query-Requests.html\">Query Request HTTP API</a>, it's not common knowledge and is the lowest common methods for how all things happen in AWS.</p><h2 id=\"where-is-the-query-request-api-used\">Where is the Query Request API used?</h2><p>Everywhere.</p><p>You simply cannot use AWS without interacting with the Query Request API, and AWS itself cannot operate without it;</p><ul><li>Services that intercommunicate. e.g.<br>Lambda &gt; CloudWatch<br>IAM &gt; CloudTrail<br>CloudFormation deploying things<br>everything uses it</li><li>Official SDKs; like botocore for python, all SDKs use it</li><li>Terraform by HashiCorp used boto3 but pivoted to the Query Request API</li></ul><p>Everything uses the Query Request API!</p><h2 id=\"how-does-the-query-request-api-work\">How does the Query Request API work?</h2><p>AWS implement the RESTful API model on top of HTTP, with both TLS and plain HTTP. endpoints available.<br>Though it is almost impossible to be strictly RESTful and remain functionally usable, AWS provide an exceptionally close alignment to RESTful designs.</p><p>There are multiple ways to craft a HTTP request to interact with the Query Request API, any HTTP client, even the web browser itself, is capable.</p><h2 id=\"authentication\">Authentication</h2><p>The common supported authentication method of the Query Request API is called Signature Version 4. My investigations into EC2 and Lambda has uncovered that AWS still uses (and therefore supports) Signature Version 2.</p><p>Both Sigv2 and Sigv4 implement hash-based message authentication code (HMAC), which relies on an out-of-band pre-shared secret with the client.<br>This secret also enables the server to derive a client identity.</p><p>The AWS implementation of HMAC uses their <code>Secret Key</code> as the secret, which corresponds to their <code>Key Id</code> as the identity.<br>Clients perform a special procedure called <em>request signing</em>, essentially giving you a data integrity guarantee no different from what TLS offers.<br>Signing is complex, so many developers prefer to use an SDK instead of understanding and performing the signing procedure themselves.</p><h2 id=\"are-the-sdks-secure\">Are the SDKs secure?</h2><p>Not exactly.<br>Using SDKs often make it difficult, even impossible, to customise your interactions with the Query Request API and optimise for security.</p><p>SDKs by-design will abstract complexities away from the developers.<br>Framework primarily prioritise developer experience, reduced lines of code, and low barrier of learning to get started. <br>Developers who never move down the stack below the SDK need to understand their security configuration, and equally the frameworks don't consider security configurations (such as protocol and ciphers) important to developer.<br>This means the SDK often optimise functionality for the widest user environment, support backwards-compatibility for old technology, and are slow to expose new security features to users if they are not requesting them.</p><p>I have identified various SDKs security defaults to be insufficient in some client environments. A good observation is they typically don't allow insecure downgrades in protocol or weak cipher negotiation (which are typically controlled by priority order server-side) but common to all SDKs I investigated, none allowed the client to specify the desired ciphers (aligned to the customers requirements) that inform the server negotiation procedure.</p><h2 id=\"is-the-aws-secret-key-secure\">Is the AWS Secret Key secure?</h2><p>To obtain a pre-shared secret, AWS customers can generate <code>Key Id</code> and it's <code>Secret Key</code> from an IAM User, or obtain a token issued via STS. Most AWS Resources also provide you a built-in key/secret pair representing the identity of the AWS Resource belonging to you rather than one of your user identities.</p><p>In terms of security of the <code>Secret Key</code> itself, it would normally be important for an attacker to try figuring out what makes up the value of the <code>Secret Key</code>, to learn how to produce a <code>Secret Key</code> in a predictable reproducible way, to learn what it itself contains within.<br>But for HMAC which is completely random, there is only a perceived value in figuring this out.<br>The true value in the HMAC secret is that you perform actions as the identity it belongs to - skipping any authentications checks.<br>By having the <code>Secret Key</code> AWS assumes you are both authenticated and authorised to perform all action associated with that identity - limited only by IAM policy and various resource policies that may be in place.</p><p>So yes, Secret Key's are secure. Keep them safe, they are necessary, powerful, and radioactive.</p><p>It is surprising Signature Version 2 is still supported because it implements HMAC-SHA1, which was deprecated in 2013 after being broken and proven to be insecure the year prior.</p><h2 id=\"but-isn-t-hmac-still-considered-secure\">But isn't HMAC still considered secure?</h2><p>Yes Sigv4 HMAC is still considered safe today, but Sigv2 uses SHA1.</p><p>The first research demonstrating flaws with SHA1 appeared in 2005, various researchers improved it over the past decade. The advancement announced in 2012 and published in 2013 prompted the industry to officially announce it deprecation.</p><p>Essentially HMAC uses a signature algorithm no different from TLS, AWS signature algorithm choices are;</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>AWS Auth name</th>\n<th style=\"text-align:center\">Hashing Signature</th>\n<th style=\"text-align:center\">Status</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sigv2</td>\n<td style=\"text-align:center\">HMAC-SHA1</td>\n<td style=\"text-align:center\">Deprecated 2012</td>\n</tr>\n<tr>\n<td>Sigv4</td>\n<td style=\"text-align:center\">HMAC-SHA256</td>\n<td style=\"text-align:center\">First flaw was CVE-2018-10844 effecting AWS</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Basically the reason for SHA1 deprecation was due to the hash uses Merkle–Damgård padding, which is fundamentally vulnerable to a <a href=\"https://en.wikipedia.org/wiki/Length_extension_attack\" rel=\"noreferrer\">length extension</a> attack and cannot be fixed. Just consider the computing power in 2005 when SHA1 was published as broken, when the proofs emerged in 2012 is still an extremely long time ago for technology.<br>We are now in 2020.</p><p>HMAC-SHA256 has been subjected to various attacks, so it is an insufficient signature algorithm choice for AWS to continue supporting. It is not immune to hash collisions either, so this alone should be reason enough to deprecate it.</p><p>This is the time where we need to simply point out that as an industry, it is well known that the SHA-based signatures are problematic at best, and generally considered harmful.<br>I wonder why AWS has chosen to ignore this?</p><p>As a recommendation for AWS, the AES-GCM signature algorithm is already prolific, it would be a good choice.<br>It is an widely accepted and scrutinised choice, with many sources available for implementation. A certain video conferencing product who Amazon recently announced partnership with, has relatively much fewer resources, has announced a 90-day plan to implement this as 1 small feature from a long list of security improvements.</p><h2 id=\"the-query-request-api-is-secure-right\">The Query Request API is secure right?</h2><p>To answer that let's define <em>secure</em>.</p><p>Most security professionals will agree on at least 3 characteristics, but i'll provide another 3 that can be more important</p><ul><li><strong>Confidential</strong> in most cases yes, but not always. <br>Read <a href=\"__GHOST_URL__/tls-secures-my-data-right/\">about TLS</a> confidentiality myths, basically <a href=\"https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a\">enforcing ciphers that apply Forward Secrecy</a> is the best we have for Query Request API confidentiality, and that only applies where confidentiality can be enforced by TLS of course</li><li><strong>Integrity</strong> yes, in all cases by virtue of both TLS1.3 and HMAC-SHA256</li><li><strong>Available</strong> for any service to have at least 99.99% SLA, the API must also</li><li><strong>Non-repudiation</strong> yes, HMAC-SHA256 provides identity and TCP/HTTP includes teh additional metadata</li><li><strong>Known vulnerabilities</strong> exist however, so this is undesired.</li><li><strong>Customer Secure Configurations</strong> are possible if you use the Query Request API directly rather than the SDKs.</li></ul><p>AWS do offer us many security features - few are default secure.</p><h2 id=\"testing\">Testing</h2><p>Not too technical? That's fine, <a href=\"https://www.ssllabs.com/ssltest/analyze.html?d=calculator.s3.amazonaws.com\">Qualys has a pretty decent test online</a> here that gives similar results.</p><p>Let's gather some data using https://testssl.sh </p><pre><code>testssl --quiet --mode parallel --standard --protocols -c --header --vulnerable --sneaky --phone-out --ids-friendly --nodns min --warnings off --hints --wide --grease --pfs --show-each --server-defaults --server-preference --client-simulation --color 0 https://calculator.s3.amazonaws.com/index.html</code></pre><p>produces;</p><pre><code> Start 2020-08-29 22:20:32        --&gt;&gt; 52.216.186.211:443 (calculator.s3.amazonaws.com) &lt;&lt;--\n\n rDNS (52.216.186.211):  (instructed to minimize DNS queries)\n Service detected:       HTTP\n\n\n Testing protocols via sockets except NPN+ALPN \n\n SSLv2      not offered (OK)\n SSLv3      not offered (OK)\n TLS 1      offered (deprecated)\n TLS 1.1    offered (deprecated)\n TLS 1.2    offered (OK)\n TLS 1.3    not offered and downgraded to a weaker protocol\n NPN/SPDY   not offered\n ALPN/HTTP2 not offered\n\n Testing for server implementation bugs \n\n No bugs found.\n\n Testing cipher categories \n\n NULL ciphers (no encryption)                  not offered (OK)\n Anonymous NULL Ciphers (no authentication)    not offered (OK)\n Export ciphers (w/o ADH+NULL)                 not offered (OK)\n LOW: 64 Bit + DES, RC[2,4] (w/o export)       not offered (OK)\n Triple DES Ciphers / IDEA                     offered\n Obsolete: SEED + 128+256 Bit CBC cipher       offered\n Strong encryption (AEAD ciphers)              offered (OK)\n\n\n Testing robust (perfect) forward secrecy, (P)FS -- omitting Null Authentication/Encryption, 3DES, RC4 \n\n PFS is offered (OK), ciphers follow (client/browser support is important here) \n\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\n-----------------------------------------------------------------------------------------------------------------------------\n x1302   TLS_AES_256_GCM_SHA384            any        AESGCM      256      TLS_AES_256_GCM_SHA384                             not a/v\n x1303   TLS_CHACHA20_POLY1305_SHA256      any        ChaCha20    256      TLS_CHACHA20_POLY1305_SHA256                       not a/v\n xcc14   ECDHE-ECDSA-CHACHA20-POLY1305-OLD ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256_OLD  not a/v\n xcc13   ECDHE-RSA-CHACHA20-POLY1305-OLD   ECDH       ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD    not a/v\n xcc15   DHE-RSA-CHACHA20-POLY1305-OLD     DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD      not a/v\n xc030   ECDHE-RSA-AES256-GCM-SHA384       ECDH 256   AESGCM      256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384              available\n xc02c   ECDHE-ECDSA-AES256-GCM-SHA384     ECDH       AESGCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384            not a/v\n xc028   ECDHE-RSA-AES256-SHA384           ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384              available\n xc024   ECDHE-ECDSA-AES256-SHA384         ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384            not a/v\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\n xa3     DHE-DSS-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_DSS_WITH_AES_256_GCM_SHA384                not a/v\n x9f     DHE-RSA-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_RSA_WITH_AES_256_GCM_SHA384                not a/v\n xcca9   ECDHE-ECDSA-CHACHA20-POLY1305     ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256      not a/v\n xcca8   ECDHE-RSA-CHACHA20-POLY1305       ECDH 256   ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256        available\n xccaa   DHE-RSA-CHACHA20-POLY1305         DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256          not a/v\n xc0af   ECDHE-ECDSA-AES256-CCM8           ECDH       AESCCM8     256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8                 not a/v\n xc0ad   ECDHE-ECDSA-AES256-CCM            ECDH       AESCCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM                   not a/v\n xc0a3   DHE-RSA-AES256-CCM8               DH         AESCCM8     256      TLS_DHE_RSA_WITH_AES_256_CCM_8                     not a/v\n xc09f   DHE-RSA-AES256-CCM                DH         AESCCM      256      TLS_DHE_RSA_WITH_AES_256_CCM                       not a/v\n x6b     DHE-RSA-AES256-SHA256             DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA256                not a/v\n x6a     DHE-DSS-AES256-SHA256             DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA256                not a/v\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\n xc077   ECDHE-RSA-CAMELLIA256-SHA384      ECDH       Camellia    256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_CBC_SHA384         not a/v\n xc073   ECDHE-ECDSA-CAMELLIA256-SHA384    ECDH       Camellia    256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_CBC_SHA384       not a/v\n xc4     DHE-RSA-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA256           not a/v\n xc3     DHE-DSS-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA256           not a/v\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\n xc043   DHE-DSS-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_DSS_WITH_ARIA_256_CBC_SHA384               not a/v\n xc045   DHE-RSA-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_RSA_WITH_ARIA_256_CBC_SHA384               not a/v\n xc049   ECDHE-ECDSA-ARIA256-CBC-SHA384    ECDH       ARIA        256      TLS_ECDHE_ECDSA_WITH_ARIA_256_CBC_SHA384           not a/v\n xc04d   ECDHE-RSA-ARIA256-CBC-SHA384      ECDH       ARIA        256      TLS_ECDHE_RSA_WITH_ARIA_256_CBC_SHA384             not a/v\n xc053   DHE-RSA-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_RSA_WITH_ARIA_256_GCM_SHA384               not a/v\n xc057   DHE-DSS-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_DSS_WITH_ARIA_256_GCM_SHA384               not a/v\n xc05d   ECDHE-ECDSA-ARIA256-GCM-SHA384    ECDH       ARIAGCM     256      TLS_ECDHE_ECDSA_WITH_ARIA_256_GCM_SHA384           not a/v\n xc061   ECDHE-ARIA256-GCM-SHA384          ECDH       ARIAGCM     256      TLS_ECDHE_RSA_WITH_ARIA_256_GCM_SHA384             not a/v\n xc07d   -                                 DH         CamelliaGCM 256      TLS_DHE_RSA_WITH_CAMELLIA_256_GCM_SHA384           not a/v\n xc081   -                                 DH         CamelliaGCM 256      TLS_DHE_DSS_WITH_CAMELLIA_256_GCM_SHA384           not a/v\n xc087   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_GCM_SHA384       not a/v\n xc08b   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_GCM_SHA384         not a/v\n x1301   TLS_AES_128_GCM_SHA256            any        AESGCM      128      TLS_AES_128_GCM_SHA256                             not a/v\n x1304   TLS_AES_128_CCM_SHA256            any        AESCCM      128      TLS_AES_128_CCM_SHA256                             not a/v\n x1305   TLS_AES_128_CCM_8_SHA256          any        AESCCM8     128      TLS_AES_128_CCM_8_SHA256                           not a/v\n xc02f   ECDHE-RSA-AES128-GCM-SHA256       ECDH 256   AESGCM      128      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256              available\n xc02b   ECDHE-ECDSA-AES128-GCM-SHA256     ECDH       AESGCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256            not a/v\n xc027   ECDHE-RSA-AES128-SHA256           ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256              available\n xc023   ECDHE-ECDSA-AES128-SHA256         ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256            not a/v\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\n xa2     DHE-DSS-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_DSS_WITH_AES_128_GCM_SHA256                not a/v\n x9e     DHE-RSA-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_RSA_WITH_AES_128_GCM_SHA256                not a/v\n xc0ae   ECDHE-ECDSA-AES128-CCM8           ECDH       AESCCM8     128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8                 not a/v\n xc0ac   ECDHE-ECDSA-AES128-CCM            ECDH       AESCCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM                   not a/v\n xc0a2   DHE-RSA-AES128-CCM8               DH         AESCCM8     128      TLS_DHE_RSA_WITH_AES_128_CCM_8                     not a/v\n xc09e   DHE-RSA-AES128-CCM                DH         AESCCM      128      TLS_DHE_RSA_WITH_AES_128_CCM                       not a/v\n x67     DHE-RSA-AES128-SHA256             DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA256                not a/v\n x40     DHE-DSS-AES128-SHA256             DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA256                not a/v\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\n xc076   ECDHE-RSA-CAMELLIA128-SHA256      ECDH       Camellia    128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_CBC_SHA256         not a/v\n xc072   ECDHE-ECDSA-CAMELLIA128-SHA256    ECDH       Camellia    128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_CBC_SHA256       not a/v\n xbe     DHE-RSA-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA256           not a/v\n xbd     DHE-DSS-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA256           not a/v\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\n xc042   DHE-DSS-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_DSS_WITH_ARIA_128_CBC_SHA256               not a/v\n xc044   DHE-RSA-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_RSA_WITH_ARIA_128_CBC_SHA256               not a/v\n xc048   ECDHE-ECDSA-ARIA128-CBC-SHA256    ECDH       ARIA        128      TLS_ECDHE_ECDSA_WITH_ARIA_128_CBC_SHA256           not a/v\n xc04c   ECDHE-RSA-ARIA128-CBC-SHA256      ECDH       ARIA        128      TLS_ECDHE_RSA_WITH_ARIA_128_CBC_SHA256             not a/v\n xc052   DHE-RSA-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_RSA_WITH_ARIA_128_GCM_SHA256               not a/v\n xc056   DHE-DSS-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_DSS_WITH_ARIA_128_GCM_SHA256               not a/v\n xc05c   ECDHE-ECDSA-ARIA128-GCM-SHA256    ECDH       ARIAGCM     128      TLS_ECDHE_ECDSA_WITH_ARIA_128_GCM_SHA256           not a/v\n xc060   ECDHE-ARIA128-GCM-SHA256          ECDH       ARIAGCM     128      TLS_ECDHE_RSA_WITH_ARIA_128_GCM_SHA256             not a/v\n xc07c   -                                 DH         CamelliaGCM 128      TLS_DHE_RSA_WITH_CAMELLIA_128_GCM_SHA256           not a/v\n xc080   -                                 DH         CamelliaGCM 128      TLS_DHE_DSS_WITH_CAMELLIA_128_GCM_SHA256           not a/v\n xc086   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_GCM_SHA256       not a/v\n xc08a   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_GCM_SHA256         not a/v\n\n Elliptic curves offered:     prime256v1 secp384r1 \n\n\n Testing server preferences \n\n Has server cipher order?     yes (OK)\n Negotiated protocol          TLSv1.2\n Negotiated cipher            ECDHE-RSA-AES128-GCM-SHA256, 256 bit ECDH (P-256)\n Cipher order\n    TLSv1:     ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \n    TLSv1.1:   ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \n    TLSv1.2:   ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-CHACHA20-POLY1305 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES256-SHA ECDHE-RSA-AES256-SHA384 AES128-GCM-SHA256\n               AES256-GCM-SHA384 AES128-SHA AES128-SHA256 AES256-SHA AES256-SHA256 DES-CBC3-SHA \n\n\n Testing server defaults (Server Hello) \n\n TLS extensions (standard)    \"server name/#0\" \"EC point formats/#11\" \"renegotiation info/#65281\"\n Session Ticket RFC 5077 hint no -- no lifetime advertised\n SSL Session ID support       yes\n Session Resumption           Tickets no, ID: no\n TLS clock skew               Random values, no fingerprinting possible \n Signature Algorithm          SHA256 with RSA\n Server key size              RSA 2048 bits\n Server key usage             Digital Signature, Key Encipherment\n Server extended key usage    TLS Web Server Authentication, TLS Web Client Authentication\n Serial / Fingerprints        082DF68EE9C69315BEBF72079B3810FD / SHA1 3FE05B486E3F0987130BA1D4EA0F299539A58243\n                              SHA256 272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674\n Common Name (CN)             *.s3.amazonaws.com \n subjectAltName (SAN)         *.s3.amazonaws.com s3.amazonaws.com \n Issuer                       DigiCert Baltimore CA-2 G2 (DigiCert Inc from US)\n Trust (hostname)             Ok via SAN wildcard (same w/o SNI)\n Chain of trust               Ok   \n EV cert (experimental)       no \n ETS/\"eTLS\", visibility info  not present\n Certificate Validity (UTC)   194 &gt;= 60 days (2019-11-09 11:00 --&gt; 2021-03-12 23:00)\n # of certificates provided   2\n In pwnedkeys.com DB          not in database\n Certificate Revocation List  http://crl3.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\n                              http://crl4.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\n OCSP URI                     http://ocsp.digicert.com, not revoked\n OCSP stapling                not offered\n OCSP must staple extension   --\n DNS CAA RR (experimental)    (instructed to minimize DNS queries)\n Certificate Transparency     yes (certificate extension)\n\n\n Testing HTTP header response @ \"/index.html\" \n\n HTTP Status Code             200 OK\n HTTP clock skew              0 sec from localtime\n Strict Transport Security    not offered\n Public Key Pinning           --\n Server banner                AmazonS3\n Application banner           --\n Cookie(s)                    (none issued at \"/index.html\")\n Security headers             Cache-Control no-cache no-store must-revalidate\n Reverse Proxy banner         --\n\n\n Testing vulnerabilities \n\n Heartbleed (CVE-2014-0160)                not vulnerable (OK), no heartbeat extension\n CCS (CVE-2014-0224)                       not vulnerable (OK)\n Ticketbleed (CVE-2016-9244), experiment.  not vulnerable (OK), no session ticket extension\n ROBOT                                     not vulnerable (OK)\n Secure Renegotiation (RFC 5746)           supported (OK)\n Secure Client-Initiated Renegotiation     likely not vulnerable (OK), timed out\n CRIME, TLS (CVE-2012-4929)                not vulnerable (OK)\n BREACH (CVE-2013-3587)                    no HTTP compression (OK)  - only supplied \"/index.html\" tested\n POODLE, SSL (CVE-2014-3566)               not vulnerable (OK), no SSLv3 support\n TLS_FALLBACK_SCSV (RFC 7507)              Check failed, unexpected result , run testssl -Z --debug=1 and look at /tmp/testssl.TpvX5W/*tls_fallback_scsv.txt\n SWEET32 (CVE-2016-2183, CVE-2016-6329)    VULNERABLE, uses 64 bit block ciphers\n FREAK (CVE-2015-0204)                     not vulnerable (OK)\n DROWN (CVE-2016-0800, CVE-2016-0703)      not vulnerable on this host and port (OK)\n                                           make sure you don't use this certificate elsewhere with SSLv2 enabled services\n                                           https://censys.io/ipv4?q=272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674 could help you to find out\n LOGJAM (CVE-2015-4000), experimental      not vulnerable (OK): no DH EXPORT ciphers, no DH key detected with &lt;= TLS 1.2\n BEAST (CVE-2011-3389)                     \n TLS1:\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\n-----------------------------------------------------------------------------------------------------------------------------\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\n xc022   SRP-DSS-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_DSS_WITH_AES_256_CBC_SHA               not a/v\n xc021   SRP-RSA-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_RSA_WITH_AES_256_CBC_SHA               not a/v\n xc020   SRP-AES-256-CBC-SHA               SRP        AES         256      TLS_SRP_SHA_WITH_AES_256_CBC_SHA                   not a/v\n x91     DHE-PSK-AES256-CBC-SHA            DHEPSK     AES         256      TLS_DHE_PSK_WITH_AES_256_CBC_SHA                   not a/v\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\n x37     DH-RSA-AES256-SHA                 DH/RSA     AES         256      TLS_DH_RSA_WITH_AES_256_CBC_SHA                    not a/v\n x36     DH-DSS-AES256-SHA                 DH/DSS     AES         256      TLS_DH_DSS_WITH_AES_256_CBC_SHA                    not a/v\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x86     DH-RSA-CAMELLIA256-SHA            DH/RSA     Camellia    256      TLS_DH_RSA_WITH_CAMELLIA_256_CBC_SHA               not a/v\n x85     DH-DSS-CAMELLIA256-SHA            DH/DSS     Camellia    256      TLS_DH_DSS_WITH_CAMELLIA_256_CBC_SHA               not a/v\n xc019   AECDH-AES256-SHA                  ECDH       AES         256      TLS_ECDH_anon_WITH_AES_256_CBC_SHA                 not a/v\n x3a     ADH-AES256-SHA                    DH         AES         256      TLS_DH_anon_WITH_AES_256_CBC_SHA                   not a/v\n x89     ADH-CAMELLIA256-SHA               DH         Camellia    256      TLS_DH_anon_WITH_CAMELLIA_256_CBC_SHA              not a/v\n xc00f   ECDH-RSA-AES256-SHA               ECDH/RSA   AES         256      TLS_ECDH_RSA_WITH_AES_256_CBC_SHA                  not a/v\n xc005   ECDH-ECDSA-AES256-SHA             ECDH/ECDSA AES         256      TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA                not a/v\n x35     AES256-SHA                        RSA        AES         256      TLS_RSA_WITH_AES_256_CBC_SHA                       available\n xc036   ECDHE-PSK-AES256-CBC-SHA          ECDHEPSK   AES         256      TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA                 not a/v\n x84     CAMELLIA256-SHA                   RSA        Camellia    256      TLS_RSA_WITH_CAMELLIA_256_CBC_SHA                  not a/v\n x95     RSA-PSK-AES256-CBC-SHA            RSAPSK     AES         256      TLS_RSA_PSK_WITH_AES_256_CBC_SHA                   not a/v\n x8d     PSK-AES256-CBC-SHA                PSK        AES         256      TLS_PSK_WITH_AES_256_CBC_SHA                       not a/v\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\n xc01f   SRP-DSS-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_DSS_WITH_AES_128_CBC_SHA               not a/v\n xc01e   SRP-RSA-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_RSA_WITH_AES_128_CBC_SHA               not a/v\n xc01d   SRP-AES-128-CBC-SHA               SRP        AES         128      TLS_SRP_SHA_WITH_AES_128_CBC_SHA                   not a/v\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\n x31     DH-RSA-AES128-SHA                 DH/RSA     AES         128      TLS_DH_RSA_WITH_AES_128_CBC_SHA                    not a/v\n x30     DH-DSS-AES128-SHA                 DH/DSS     AES         128      TLS_DH_DSS_WITH_AES_128_CBC_SHA                    not a/v\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\n x98     DH-RSA-SEED-SHA                   DH/RSA     SEED        128      TLS_DH_RSA_WITH_SEED_CBC_SHA                       not a/v\n x97     DH-DSS-SEED-SHA                   DH/DSS     SEED        128      TLS_DH_DSS_WITH_SEED_CBC_SHA                       not a/v\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x43     DH-RSA-CAMELLIA128-SHA            DH/RSA     Camellia    128      TLS_DH_RSA_WITH_CAMELLIA_128_CBC_SHA               not a/v\n x42     DH-DSS-CAMELLIA128-SHA            DH/DSS     Camellia    128      TLS_DH_DSS_WITH_CAMELLIA_128_CBC_SHA               not a/v\n xc018   AECDH-AES128-SHA                  ECDH       AES         128      TLS_ECDH_anon_WITH_AES_128_CBC_SHA                 not a/v\n x34     ADH-AES128-SHA                    DH         AES         128      TLS_DH_anon_WITH_AES_128_CBC_SHA                   not a/v\n x9b     ADH-SEED-SHA                      DH         SEED        128      TLS_DH_anon_WITH_SEED_CBC_SHA                      not a/v\n x46     ADH-CAMELLIA128-SHA               DH         Camellia    128      TLS_DH_anon_WITH_CAMELLIA_128_CBC_SHA              not a/v\n xc00e   ECDH-RSA-AES128-SHA               ECDH/RSA   AES         128      TLS_ECDH_RSA_WITH_AES_128_CBC_SHA                  not a/v\n xc004   ECDH-ECDSA-AES128-SHA             ECDH/ECDSA AES         128      TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA                not a/v\n x2f     AES128-SHA                        RSA        AES         128      TLS_RSA_WITH_AES_128_CBC_SHA                       available\n xc035   ECDHE-PSK-AES128-CBC-SHA          ECDHEPSK   AES         128      TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA                 not a/v\n x90     DHE-PSK-AES128-CBC-SHA            DHEPSK     AES         128      TLS_DHE_PSK_WITH_AES_128_CBC_SHA                   not a/v\n x96     SEED-SHA                          RSA        SEED        128      TLS_RSA_WITH_SEED_CBC_SHA                          not a/v\n x41     CAMELLIA128-SHA                   RSA        Camellia    128      TLS_RSA_WITH_CAMELLIA_128_CBC_SHA                  not a/v\n x07     IDEA-CBC-SHA                      RSA        IDEA        128      TLS_RSA_WITH_IDEA_CBC_SHA                          not a/v\n x94     RSA-PSK-AES128-CBC-SHA            RSAPSK     AES         128      TLS_RSA_PSK_WITH_AES_128_CBC_SHA                   not a/v\n x8c     PSK-AES128-CBC-SHA                PSK        AES         128      TLS_PSK_WITH_AES_128_CBC_SHA                       not a/v\n x21     KRB5-IDEA-CBC-SHA                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_SHA                         not a/v\n x25     KRB5-IDEA-CBC-MD5                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_MD5                         not a/v\n xc012   ECDHE-RSA-DES-CBC3-SHA            ECDH       3DES        168      TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA                not a/v\n xc008   ECDHE-ECDSA-DES-CBC3-SHA          ECDH       3DES        168      TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01c   SRP-DSS-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_DSS_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01b   SRP-RSA-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_RSA_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01a   SRP-3DES-EDE-CBC-SHA              SRP        3DES        168      TLS_SRP_SHA_WITH_3DES_EDE_CBC_SHA                  not a/v\n x16     EDH-RSA-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA                  not a/v\n x13     EDH-DSS-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA                  not a/v\n x10     DH-RSA-DES-CBC3-SHA               DH/RSA     3DES        168      TLS_DH_RSA_WITH_3DES_EDE_CBC_SHA                   not a/v\n x0d     DH-DSS-DES-CBC3-SHA               DH/DSS     3DES        168      TLS_DH_DSS_WITH_3DES_EDE_CBC_SHA                   not a/v\n xc017   AECDH-DES-CBC3-SHA                ECDH       3DES        168      TLS_ECDH_anon_WITH_3DES_EDE_CBC_SHA                not a/v\n x1b     ADH-DES-CBC3-SHA                  DH         3DES        168      TLS_DH_anon_WITH_3DES_EDE_CBC_SHA                  not a/v\n xc00d   ECDH-RSA-DES-CBC3-SHA             ECDH/RSA   3DES        168      TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA                 not a/v\n xc003   ECDH-ECDSA-DES-CBC3-SHA           ECDH/ECDSA 3DES        168      TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA               not a/v\n x0a     DES-CBC3-SHA                      RSA        3DES        168      TLS_RSA_WITH_3DES_EDE_CBC_SHA                      available\n x93     RSA-PSK-3DES-EDE-CBC-SHA          RSAPSK     3DES        168      TLS_RSA_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\n x8b     PSK-3DES-EDE-CBC-SHA              PSK        3DES        168      TLS_PSK_WITH_3DES_EDE_CBC_SHA                      not a/v\n x1f     KRB5-DES-CBC3-SHA                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_SHA                     not a/v\n x23     KRB5-DES-CBC3-MD5                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_MD5                     not a/v\n xc034   ECDHE-PSK-3DES-EDE-CBC-SHA        ECDHEPSK   3DES        168      TLS_ECDHE_PSK_WITH_3DES_EDE_CBC_SHA                not a/v\n x8f     DHE-PSK-3DES-EDE-CBC-SHA          DHEPSK     3DES        168      TLS_DHE_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\n xfeff   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\n xffe0   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\n x63     EXP1024-DHE-DSS-DES-CBC-SHA       DH(1024)   DES         56,exp   TLS_DHE_DSS_EXPORT1024_WITH_DES_CBC_SHA            not a/v\n x15     EDH-RSA-DES-CBC-SHA               DH         DES         56       TLS_DHE_RSA_WITH_DES_CBC_SHA                       not a/v\n x12     EDH-DSS-DES-CBC-SHA               DH         DES         56       TLS_DHE_DSS_WITH_DES_CBC_SHA                       not a/v\n x0f     DH-RSA-DES-CBC-SHA                DH/RSA     DES         56       TLS_DH_RSA_WITH_DES_CBC_SHA                        not a/v\n x0c     DH-DSS-DES-CBC-SHA                DH/DSS     DES         56       TLS_DH_DSS_WITH_DES_CBC_SHA                        not a/v\n x1a     ADH-DES-CBC-SHA                   DH         DES         56       TLS_DH_anon_WITH_DES_CBC_SHA                       not a/v\n x62     EXP1024-DES-CBC-SHA               RSA(1024)  DES         56,exp   TLS_RSA_EXPORT1024_WITH_DES_CBC_SHA                not a/v\n x09     DES-CBC-SHA                       RSA        DES         56       TLS_RSA_WITH_DES_CBC_SHA                           not a/v\n x1e     KRB5-DES-CBC-SHA                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_SHA                          not a/v\n x22     KRB5-DES-CBC-MD5                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_MD5                          not a/v\n xfefe   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\n xffe1   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\n x14     EXP-EDH-RSA-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x11     EXP-EDH-DSS-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x19     EXP-ADH-DES-CBC-SHA               DH(512)    DES         40,exp   TLS_DH_anon_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x08     EXP-DES-CBC-SHA                   RSA(512)   DES         40,exp   TLS_RSA_EXPORT_WITH_DES40_CBC_SHA                  not a/v\n x06     EXP-RC2-CBC-MD5                   RSA(512)   RC2         40,exp   TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5                 not a/v\n x27     EXP-KRB5-RC2-CBC-SHA              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_SHA                not a/v\n x26     EXP-KRB5-DES-CBC-SHA              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_SHA                not a/v\n x2a     EXP-KRB5-RC2-CBC-MD5              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_MD5                not a/v\n x29     EXP-KRB5-DES-CBC-MD5              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_MD5                not a/v\n x0b     EXP-DH-DSS-DES-CBC-SHA            DH/DSS     DES         40,exp   TLS_DH_DSS_EXPORT_WITH_DES40_CBC_SHA               not a/v\n x0e     EXP-DH-RSA-DES-CBC-SHA            DH/RSA     DES         40,exp   TLS_DH_RSA_EXPORT_WITH_DES40_CBC_SHA               not a/v\n\n VULNERABLE -- but also supports higher protocols (possible mitigation)  TLSv1.1 TLSv1.2\n\n LUCKY13 (CVE-2013-0169), experimental     potentially VULNERABLE, uses cipher block chaining (CBC) ciphers with TLS. Check patches\n RC4 (CVE-2013-2566, CVE-2015-2808)        no RC4 ciphers detected (OK)\n\n\n Running client simulations (HTTP) via sockets \n\n Browser                      Protocol  Cipher Suite Name (OpenSSL)       Forward Secrecy\n------------------------------------------------------------------------------------------------\n Android 4.4.2                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 5.0.0                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 6.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 7.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 8.1 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 9.0 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 10.0 (native)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Chrome 74 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Chrome 79 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Firefox 66 (Win 8.1/10)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Firefox 71 (Win 10)          TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n IE 6 XP                      No connection\n IE 8 Win 7                   TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 8 XP                      TLSv1.0   DES-CBC3-SHA                      No FS\n IE 11 Win 7                  TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win 8.1                TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win Phone 8.1          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win 10                 TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Edge 15 Win 10               TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Edge 17 (Win 10)             TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Opera 66 (Win 10)            TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Safari 9 iOS 9               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 9 OS X 10.11          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 10 OS X 10.12         TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 12.1 (iOS 12.2)       TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Safari 13.0 (macOS 10.14.6)  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Apple ATS 9 iOS 9            TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 6u45                    TLSv1.0   AES128-SHA                        No FS\n Java 7u25                    TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 8u161                   TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 11.0.2 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Java 12.0.1 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n OpenSSL 1.0.2e               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n OpenSSL 1.1.0l (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n OpenSSL 1.1.1d (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Thunderbird (68.3)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n\n Done 2020-08-29 22:26:19 [ 349s] --&gt;&gt; 52.216.186.211:443 (calculator.s3.amazonaws.com) &lt;&lt;--</code></pre><p>Which shows us;</p><ul><li>TLS1.0 and 1.1 are still supported, with a few noticeable ciphers we can exploit for over a decade</li><li>Vulnerable to SWEET32 (CVE-2016-2183, CVE-2016-6329) which has some trivial exploits online for the script kiddies to run without learning much more than how to open a terminal window..</li><li>no OSCP at all, not stabled or the CA enforcement of the must staple flag in the cert.</li><li>Valid for 60 days, meaning we have a very wide chance to wait for IP churn to provide a window to do a DNS version of domain take over.</li><li>Session Side-Jacking seems like a promising attack vector too, but requires further investigation than we have done here</li><li>best for last, those CBC ciphers are a great target to test some very well known padding related attack vectors considering the internet has pretty much gotten rid of CBC ciphers between 2013-2015.</li></ul><p>If i can spend a whole 15mins and next to zero technical effort to learn this, and I am not even slightly considered a motivated attacker, or an attacker of any description. What do you think can happen if there is 1 human out there on planet earth targeting your data? Amazon's security won't stop them (clearly), and you have no way to configure Amazon to protect yourself. I guess we can hope that the name Amazon is a deterrent and you're safe in the expanding sea of customers, but the news cycles about S3 breaches tells a very different story about the deterrent quality of an North American corporate to the rest of the world.. You decide, it's your threat model, your risk posture - not mine.</p><h2 id=\"conclusion\">Conclusion</h2><p>While AWS offer many security features, they allow insecure and deprecated protocol downgrades to TLS 1.0, and support deprecated signature algorithms for their API authentication.</p><p>These backwards-compatibility decisions and continued use of deprecated security features are open to attackers who will utilise anything available, they don't stick to the best offered security options.</p>","comment_id":"5ee8499f05bdaa04b9c93707","plaintext":"EDIT: 2020-08-01 SHA-1 Windows content to be retired\n[https://techcommunity.microsoft.com/t5/windows-it-pro-blog/sha-1-windows-content-to-be-retired-august-3-2020/ba-p/1544373] \nin 2 days. Amazon is in an ever decreasing group of cryptography ignorant\nproviders.\n\nEDIT: 2020-08-07 China believes TLS 1.3 privacy (hidden identity, not to be\nconfused with confidentiality of data) is so effective they are going to block\nall TLS 1.3 traffic. Researchers have a work around\n[https://gfw.report/blog/gfw_esni_blocking/en/] but China will surely adapt. \n\nEDIT: 2020-08-20 Microsoft enables TLS 1.3 by default\n[https://www.microsoft.com/security/blog/2020/08/20/taking-transport-layer-security-tls-to-the-next-level-with-tls-1-3/] \nin latest Windows 10 builds\n\nEDIT: 2020-08-29 On October 15 Microsoft will no longer support\n[https://docs.microsoft.com/en-us/microsoft-365/compliance/tls-1.0-and-1.1-deprecation-for-office-365?view=o365-worldwide] \nTLS 1.0/1.1 for any Office365 product. Also all linux software in the Microsoft\nrepos\n[https://docs.microsoft.com/en-us/windows-server/administration/linux-package-repository-for-microsoft-software] \nwill disable these insecure protocols.\n\nWe all know that everything in AWS is an API right?\nSome people I speak to, as a consultant, hear this common cliche but not many\nconsider the implications of this infamous characteristic of AWS.\n\nAll of the services in AWS internally communicate over the same APIs\n[https://docs.aws.amazon.com/general/latest/gr/rande.html] that customers also\nuse, and non-customers alike, as all access by default is public.\nThis public API has a name; Query Request HTTP API\n[https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Query-Requests.html],\nit's not common knowledge and is the lowest common methods for how all things\nhappen in AWS.\n\nWhere is the Query Request API used?\nEverywhere.\n\nYou simply cannot use AWS without interacting with the Query Request API, and\nAWS itself cannot operate without it;\n\n * Services that intercommunicate. e.g.\n   Lambda > CloudWatch\n   IAM > CloudTrail\n   CloudFormation deploying things\n   everything uses it\n * Official SDKs; like botocore for python, all SDKs use it\n * Terraform by HashiCorp used boto3 but pivoted to the Query Request API\n\nEverything uses the Query Request API!\n\nHow does the Query Request API work?\nAWS implement the RESTful API model on top of HTTP, with both TLS and plain\nHTTP. endpoints available.\nThough it is almost impossible to be strictly RESTful and remain functionally\nusable, AWS provide an exceptionally close alignment to RESTful designs.\n\nThere are multiple ways to craft a HTTP request to interact with the Query\nRequest API, any HTTP client, even the web browser itself, is capable.\n\nAuthentication\nThe common supported authentication method of the Query Request API is called\nSignature Version 4. My investigations into EC2 and Lambda has uncovered that\nAWS still uses (and therefore supports) Signature Version 2.\n\nBoth Sigv2 and Sigv4 implement hash-based message authentication code (HMAC),\nwhich relies on an out-of-band pre-shared secret with the client.\nThis secret also enables the server to derive a client identity.\n\nThe AWS implementation of HMAC uses their Secret Key as the secret, which\ncorresponds to their Key Id as the identity.\nClients perform a special procedure called request signing, essentially giving\nyou a data integrity guarantee no different from what TLS offers.\nSigning is complex, so many developers prefer to use an SDK instead of\nunderstanding and performing the signing procedure themselves.\n\nAre the SDKs secure?\nNot exactly.\nUsing SDKs often make it difficult, even impossible, to customise your\ninteractions with the Query Request API and optimise for security.\n\nSDKs by-design will abstract complexities away from the developers.\nFramework primarily prioritise developer experience, reduced lines of code, and\nlow barrier of learning to get started. \nDevelopers who never move down the stack below the SDK need to understand their\nsecurity configuration, and equally the frameworks don't consider security\nconfigurations (such as protocol and ciphers) important to developer.\nThis means the SDK often optimise functionality for the widest user environment,\nsupport backwards-compatibility for old technology, and are slow to expose new\nsecurity features to users if they are not requesting them.\n\nI have identified various SDKs security defaults to be insufficient in some\nclient environments. A good observation is they typically don't allow insecure\ndowngrades in protocol or weak cipher negotiation (which are typically\ncontrolled by priority order server-side) but common to all SDKs I investigated,\nnone allowed the client to specify the desired ciphers (aligned to the customers\nrequirements) that inform the server negotiation procedure.\n\nIs the AWS Secret Key secure?\nTo obtain a pre-shared secret, AWS customers can generate Key Id and it's Secret\nKey from an IAM User, or obtain a token issued via STS. Most AWS Resources also\nprovide you a built-in key/secret pair representing the identity of the AWS\nResource belonging to you rather than one of your user identities.\n\nIn terms of security of the Secret Key itself, it would normally be important\nfor an attacker to try figuring out what makes up the value of the Secret Key,\nto learn how to produce a Secret Key in a predictable reproducible way, to learn\nwhat it itself contains within.\nBut for HMAC which is completely random, there is only a perceived value in\nfiguring this out.\nThe true value in the HMAC secret is that you perform actions as the identity it\nbelongs to - skipping any authentications checks.\nBy having the Secret Key AWS assumes you are both authenticated and authorised\nto perform all action associated with that identity - limited only by IAM policy\nand various resource policies that may be in place.\n\nSo yes, Secret Key's are secure. Keep them safe, they are necessary, powerful,\nand radioactive.\n\nIt is surprising Signature Version 2 is still supported because it implements\nHMAC-SHA1, which was deprecated in 2013 after being broken and proven to be\ninsecure the year prior.\n\nBut isn't HMAC still considered secure?\nYes Sigv4 HMAC is still considered safe today, but Sigv2 uses SHA1.\n\nThe first research demonstrating flaws with SHA1 appeared in 2005, various\nresearchers improved it over the past decade. The advancement announced in 2012\nand published in 2013 prompted the industry to officially announce it\ndeprecation.\n\nEssentially HMAC uses a signature algorithm no different from TLS, AWS signature\nalgorithm choices are;\n\nAWS Auth nameHashing SignatureStatusSigv2HMAC-SHA1Deprecated 2012Sigv4\nHMAC-SHA256First flaw was CVE-2018-10844 effecting AWSBasically the reason for\nSHA1 deprecation was due to the hash uses Merkle–Damgård padding, which is\nfundamentally vulnerable to a length extension\n[https://en.wikipedia.org/wiki/Length_extension_attack] attack and cannot be\nfixed. Just consider the computing power in 2005 when SHA1 was published as\nbroken, when the proofs emerged in 2012 is still an extremely long time ago for\ntechnology.\nWe are now in 2020.\n\nHMAC-SHA256 has been subjected to various attacks, so it is an insufficient\nsignature algorithm choice for AWS to continue supporting. It is not immune to\nhash collisions either, so this alone should be reason enough to deprecate it.\n\nThis is the time where we need to simply point out that as an industry, it is\nwell known that the SHA-based signatures are problematic at best, and generally\nconsidered harmful.\nI wonder why AWS has chosen to ignore this?\n\nAs a recommendation for AWS, the AES-GCM signature algorithm is already\nprolific, it would be a good choice.\nIt is an widely accepted and scrutinised choice, with many sources available for\nimplementation. A certain video conferencing product who Amazon recently\nannounced partnership with, has relatively much fewer resources, has announced a\n90-day plan to implement this as 1 small feature from a long list of security\nimprovements.\n\nThe Query Request API is secure right?\nTo answer that let's define secure.\n\nMost security professionals will agree on at least 3 characteristics, but i'll\nprovide another 3 that can be more important\n\n * Confidential in most cases yes, but not always. \n   Read about TLS [__GHOST_URL__/tls-secures-my-data-right/] confidentiality myths, basically \n   enforcing ciphers that apply Forward Secrecy\n   [https://gist.github.com/chrisdlangton/25c3f9356ff7ebfb324fb6acc5665b4a] is\n   the best we have for Query Request API confidentiality, and that only applies\n   where confidentiality can be enforced by TLS of course\n * Integrity yes, in all cases by virtue of both TLS1.3 and HMAC-SHA256\n * Available for any service to have at least 99.99% SLA, the API must also\n * Non-repudiation yes, HMAC-SHA256 provides identity and TCP/HTTP includes teh\n   additional metadata\n * Known vulnerabilities exist however, so this is undesired.\n * Customer Secure Configurations are possible if you use the Query Request API\n   directly rather than the SDKs.\n\nAWS do offer us many security features - few are default secure.\n\nTesting\nNot too technical? That's fine, Qualys has a pretty decent test online\n[https://www.ssllabs.com/ssltest/analyze.html?d=calculator.s3.amazonaws.com] \nhere that gives similar results.\n\nLet's gather some data using https://testssl.sh \n\ntestssl --quiet --mode parallel --standard --protocols -c --header --vulnerable --sneaky --phone-out --ids-friendly --nodns min --warnings off --hints --wide --grease --pfs --show-each --server-defaults --server-preference --client-simulation --color 0 https://calculator.s3.amazonaws.com/index.html\n\nproduces;\n\n Start 2020-08-29 22:20:32        -->> 52.216.186.211:443 (calculator.s3.amazonaws.com) <<--\n\n rDNS (52.216.186.211):  (instructed to minimize DNS queries)\n Service detected:       HTTP\n\n\n Testing protocols via sockets except NPN+ALPN \n\n SSLv2      not offered (OK)\n SSLv3      not offered (OK)\n TLS 1      offered (deprecated)\n TLS 1.1    offered (deprecated)\n TLS 1.2    offered (OK)\n TLS 1.3    not offered and downgraded to a weaker protocol\n NPN/SPDY   not offered\n ALPN/HTTP2 not offered\n\n Testing for server implementation bugs \n\n No bugs found.\n\n Testing cipher categories \n\n NULL ciphers (no encryption)                  not offered (OK)\n Anonymous NULL Ciphers (no authentication)    not offered (OK)\n Export ciphers (w/o ADH+NULL)                 not offered (OK)\n LOW: 64 Bit + DES, RC[2,4] (w/o export)       not offered (OK)\n Triple DES Ciphers / IDEA                     offered\n Obsolete: SEED + 128+256 Bit CBC cipher       offered\n Strong encryption (AEAD ciphers)              offered (OK)\n\n\n Testing robust (perfect) forward secrecy, (P)FS -- omitting Null Authentication/Encryption, 3DES, RC4 \n\n PFS is offered (OK), ciphers follow (client/browser support is important here) \n\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\n-----------------------------------------------------------------------------------------------------------------------------\n x1302   TLS_AES_256_GCM_SHA384            any        AESGCM      256      TLS_AES_256_GCM_SHA384                             not a/v\n x1303   TLS_CHACHA20_POLY1305_SHA256      any        ChaCha20    256      TLS_CHACHA20_POLY1305_SHA256                       not a/v\n xcc14   ECDHE-ECDSA-CHACHA20-POLY1305-OLD ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256_OLD  not a/v\n xcc13   ECDHE-RSA-CHACHA20-POLY1305-OLD   ECDH       ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD    not a/v\n xcc15   DHE-RSA-CHACHA20-POLY1305-OLD     DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256_OLD      not a/v\n xc030   ECDHE-RSA-AES256-GCM-SHA384       ECDH 256   AESGCM      256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384              available\n xc02c   ECDHE-ECDSA-AES256-GCM-SHA384     ECDH       AESGCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384            not a/v\n xc028   ECDHE-RSA-AES256-SHA384           ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384              available\n xc024   ECDHE-ECDSA-AES256-SHA384         ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384            not a/v\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\n xa3     DHE-DSS-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_DSS_WITH_AES_256_GCM_SHA384                not a/v\n x9f     DHE-RSA-AES256-GCM-SHA384         DH         AESGCM      256      TLS_DHE_RSA_WITH_AES_256_GCM_SHA384                not a/v\n xcca9   ECDHE-ECDSA-CHACHA20-POLY1305     ECDH       ChaCha20    256      TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256      not a/v\n xcca8   ECDHE-RSA-CHACHA20-POLY1305       ECDH 256   ChaCha20    256      TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256        available\n xccaa   DHE-RSA-CHACHA20-POLY1305         DH         ChaCha20    256      TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256          not a/v\n xc0af   ECDHE-ECDSA-AES256-CCM8           ECDH       AESCCM8     256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8                 not a/v\n xc0ad   ECDHE-ECDSA-AES256-CCM            ECDH       AESCCM      256      TLS_ECDHE_ECDSA_WITH_AES_256_CCM                   not a/v\n xc0a3   DHE-RSA-AES256-CCM8               DH         AESCCM8     256      TLS_DHE_RSA_WITH_AES_256_CCM_8                     not a/v\n xc09f   DHE-RSA-AES256-CCM                DH         AESCCM      256      TLS_DHE_RSA_WITH_AES_256_CCM                       not a/v\n x6b     DHE-RSA-AES256-SHA256             DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA256                not a/v\n x6a     DHE-DSS-AES256-SHA256             DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA256                not a/v\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\n xc077   ECDHE-RSA-CAMELLIA256-SHA384      ECDH       Camellia    256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_CBC_SHA384         not a/v\n xc073   ECDHE-ECDSA-CAMELLIA256-SHA384    ECDH       Camellia    256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_CBC_SHA384       not a/v\n xc4     DHE-RSA-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA256           not a/v\n xc3     DHE-DSS-CAMELLIA256-SHA256        DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA256           not a/v\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\n xc043   DHE-DSS-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_DSS_WITH_ARIA_256_CBC_SHA384               not a/v\n xc045   DHE-RSA-ARIA256-CBC-SHA384        DH         ARIA        256      TLS_DHE_RSA_WITH_ARIA_256_CBC_SHA384               not a/v\n xc049   ECDHE-ECDSA-ARIA256-CBC-SHA384    ECDH       ARIA        256      TLS_ECDHE_ECDSA_WITH_ARIA_256_CBC_SHA384           not a/v\n xc04d   ECDHE-RSA-ARIA256-CBC-SHA384      ECDH       ARIA        256      TLS_ECDHE_RSA_WITH_ARIA_256_CBC_SHA384             not a/v\n xc053   DHE-RSA-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_RSA_WITH_ARIA_256_GCM_SHA384               not a/v\n xc057   DHE-DSS-ARIA256-GCM-SHA384        DH         ARIAGCM     256      TLS_DHE_DSS_WITH_ARIA_256_GCM_SHA384               not a/v\n xc05d   ECDHE-ECDSA-ARIA256-GCM-SHA384    ECDH       ARIAGCM     256      TLS_ECDHE_ECDSA_WITH_ARIA_256_GCM_SHA384           not a/v\n xc061   ECDHE-ARIA256-GCM-SHA384          ECDH       ARIAGCM     256      TLS_ECDHE_RSA_WITH_ARIA_256_GCM_SHA384             not a/v\n xc07d   -                                 DH         CamelliaGCM 256      TLS_DHE_RSA_WITH_CAMELLIA_256_GCM_SHA384           not a/v\n xc081   -                                 DH         CamelliaGCM 256      TLS_DHE_DSS_WITH_CAMELLIA_256_GCM_SHA384           not a/v\n xc087   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_ECDSA_WITH_CAMELLIA_256_GCM_SHA384       not a/v\n xc08b   -                                 ECDH       CamelliaGCM 256      TLS_ECDHE_RSA_WITH_CAMELLIA_256_GCM_SHA384         not a/v\n x1301   TLS_AES_128_GCM_SHA256            any        AESGCM      128      TLS_AES_128_GCM_SHA256                             not a/v\n x1304   TLS_AES_128_CCM_SHA256            any        AESCCM      128      TLS_AES_128_CCM_SHA256                             not a/v\n x1305   TLS_AES_128_CCM_8_SHA256          any        AESCCM8     128      TLS_AES_128_CCM_8_SHA256                           not a/v\n xc02f   ECDHE-RSA-AES128-GCM-SHA256       ECDH 256   AESGCM      128      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256              available\n xc02b   ECDHE-ECDSA-AES128-GCM-SHA256     ECDH       AESGCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256            not a/v\n xc027   ECDHE-RSA-AES128-SHA256           ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256              available\n xc023   ECDHE-ECDSA-AES128-SHA256         ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256            not a/v\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\n xa2     DHE-DSS-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_DSS_WITH_AES_128_GCM_SHA256                not a/v\n x9e     DHE-RSA-AES128-GCM-SHA256         DH         AESGCM      128      TLS_DHE_RSA_WITH_AES_128_GCM_SHA256                not a/v\n xc0ae   ECDHE-ECDSA-AES128-CCM8           ECDH       AESCCM8     128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8                 not a/v\n xc0ac   ECDHE-ECDSA-AES128-CCM            ECDH       AESCCM      128      TLS_ECDHE_ECDSA_WITH_AES_128_CCM                   not a/v\n xc0a2   DHE-RSA-AES128-CCM8               DH         AESCCM8     128      TLS_DHE_RSA_WITH_AES_128_CCM_8                     not a/v\n xc09e   DHE-RSA-AES128-CCM                DH         AESCCM      128      TLS_DHE_RSA_WITH_AES_128_CCM                       not a/v\n x67     DHE-RSA-AES128-SHA256             DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA256                not a/v\n x40     DHE-DSS-AES128-SHA256             DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA256                not a/v\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\n xc076   ECDHE-RSA-CAMELLIA128-SHA256      ECDH       Camellia    128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_CBC_SHA256         not a/v\n xc072   ECDHE-ECDSA-CAMELLIA128-SHA256    ECDH       Camellia    128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_CBC_SHA256       not a/v\n xbe     DHE-RSA-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA256           not a/v\n xbd     DHE-DSS-CAMELLIA128-SHA256        DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA256           not a/v\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\n xc042   DHE-DSS-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_DSS_WITH_ARIA_128_CBC_SHA256               not a/v\n xc044   DHE-RSA-ARIA128-CBC-SHA256        DH         ARIA        128      TLS_DHE_RSA_WITH_ARIA_128_CBC_SHA256               not a/v\n xc048   ECDHE-ECDSA-ARIA128-CBC-SHA256    ECDH       ARIA        128      TLS_ECDHE_ECDSA_WITH_ARIA_128_CBC_SHA256           not a/v\n xc04c   ECDHE-RSA-ARIA128-CBC-SHA256      ECDH       ARIA        128      TLS_ECDHE_RSA_WITH_ARIA_128_CBC_SHA256             not a/v\n xc052   DHE-RSA-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_RSA_WITH_ARIA_128_GCM_SHA256               not a/v\n xc056   DHE-DSS-ARIA128-GCM-SHA256        DH         ARIAGCM     128      TLS_DHE_DSS_WITH_ARIA_128_GCM_SHA256               not a/v\n xc05c   ECDHE-ECDSA-ARIA128-GCM-SHA256    ECDH       ARIAGCM     128      TLS_ECDHE_ECDSA_WITH_ARIA_128_GCM_SHA256           not a/v\n xc060   ECDHE-ARIA128-GCM-SHA256          ECDH       ARIAGCM     128      TLS_ECDHE_RSA_WITH_ARIA_128_GCM_SHA256             not a/v\n xc07c   -                                 DH         CamelliaGCM 128      TLS_DHE_RSA_WITH_CAMELLIA_128_GCM_SHA256           not a/v\n xc080   -                                 DH         CamelliaGCM 128      TLS_DHE_DSS_WITH_CAMELLIA_128_GCM_SHA256           not a/v\n xc086   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_ECDSA_WITH_CAMELLIA_128_GCM_SHA256       not a/v\n xc08a   -                                 ECDH       CamelliaGCM 128      TLS_ECDHE_RSA_WITH_CAMELLIA_128_GCM_SHA256         not a/v\n\n Elliptic curves offered:     prime256v1 secp384r1 \n\n\n Testing server preferences \n\n Has server cipher order?     yes (OK)\n Negotiated protocol          TLSv1.2\n Negotiated cipher            ECDHE-RSA-AES128-GCM-SHA256, 256 bit ECDH (P-256)\n Cipher order\n    TLSv1:     ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \n    TLSv1.1:   ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA AES128-SHA AES256-SHA DES-CBC3-SHA \n    TLSv1.2:   ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-CHACHA20-POLY1305 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES256-SHA ECDHE-RSA-AES256-SHA384 AES128-GCM-SHA256\n               AES256-GCM-SHA384 AES128-SHA AES128-SHA256 AES256-SHA AES256-SHA256 DES-CBC3-SHA \n\n\n Testing server defaults (Server Hello) \n\n TLS extensions (standard)    \"server name/#0\" \"EC point formats/#11\" \"renegotiation info/#65281\"\n Session Ticket RFC 5077 hint no -- no lifetime advertised\n SSL Session ID support       yes\n Session Resumption           Tickets no, ID: no\n TLS clock skew               Random values, no fingerprinting possible \n Signature Algorithm          SHA256 with RSA\n Server key size              RSA 2048 bits\n Server key usage             Digital Signature, Key Encipherment\n Server extended key usage    TLS Web Server Authentication, TLS Web Client Authentication\n Serial / Fingerprints        082DF68EE9C69315BEBF72079B3810FD / SHA1 3FE05B486E3F0987130BA1D4EA0F299539A58243\n                              SHA256 272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674\n Common Name (CN)             *.s3.amazonaws.com \n subjectAltName (SAN)         *.s3.amazonaws.com s3.amazonaws.com \n Issuer                       DigiCert Baltimore CA-2 G2 (DigiCert Inc from US)\n Trust (hostname)             Ok via SAN wildcard (same w/o SNI)\n Chain of trust               Ok   \n EV cert (experimental)       no \n ETS/\"eTLS\", visibility info  not present\n Certificate Validity (UTC)   194 >= 60 days (2019-11-09 11:00 --> 2021-03-12 23:00)\n # of certificates provided   2\n In pwnedkeys.com DB          not in database\n Certificate Revocation List  http://crl3.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\n                              http://crl4.digicert.com/DigiCertBaltimoreCA-2G2.crl, not revoked\n OCSP URI                     http://ocsp.digicert.com, not revoked\n OCSP stapling                not offered\n OCSP must staple extension   --\n DNS CAA RR (experimental)    (instructed to minimize DNS queries)\n Certificate Transparency     yes (certificate extension)\n\n\n Testing HTTP header response @ \"/index.html\" \n\n HTTP Status Code             200 OK\n HTTP clock skew              0 sec from localtime\n Strict Transport Security    not offered\n Public Key Pinning           --\n Server banner                AmazonS3\n Application banner           --\n Cookie(s)                    (none issued at \"/index.html\")\n Security headers             Cache-Control no-cache no-store must-revalidate\n Reverse Proxy banner         --\n\n\n Testing vulnerabilities \n\n Heartbleed (CVE-2014-0160)                not vulnerable (OK), no heartbeat extension\n CCS (CVE-2014-0224)                       not vulnerable (OK)\n Ticketbleed (CVE-2016-9244), experiment.  not vulnerable (OK), no session ticket extension\n ROBOT                                     not vulnerable (OK)\n Secure Renegotiation (RFC 5746)           supported (OK)\n Secure Client-Initiated Renegotiation     likely not vulnerable (OK), timed out\n CRIME, TLS (CVE-2012-4929)                not vulnerable (OK)\n BREACH (CVE-2013-3587)                    no HTTP compression (OK)  - only supplied \"/index.html\" tested\n POODLE, SSL (CVE-2014-3566)               not vulnerable (OK), no SSLv3 support\n TLS_FALLBACK_SCSV (RFC 7507)              Check failed, unexpected result , run testssl -Z --debug=1 and look at /tmp/testssl.TpvX5W/*tls_fallback_scsv.txt\n SWEET32 (CVE-2016-2183, CVE-2016-6329)    VULNERABLE, uses 64 bit block ciphers\n FREAK (CVE-2015-0204)                     not vulnerable (OK)\n DROWN (CVE-2016-0800, CVE-2016-0703)      not vulnerable on this host and port (OK)\n                                           make sure you don't use this certificate elsewhere with SSLv2 enabled services\n                                           https://censys.io/ipv4?q=272FC283BF3EDC52F6F3387A9C5247A20C5D7176FE81EC3EABA4B3A8E57F8674 could help you to find out\n LOGJAM (CVE-2015-4000), experimental      not vulnerable (OK): no DH EXPORT ciphers, no DH key detected with <= TLS 1.2\n BEAST (CVE-2011-3389)                     \n TLS1:\nHexcode  Cipher Suite Name (OpenSSL)       KeyExch.   Encryption  Bits     Cipher Suite Name (IANA/RFC)\n-----------------------------------------------------------------------------------------------------------------------------\n xc014   ECDHE-RSA-AES256-SHA              ECDH 256   AES         256      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA                 available\n xc00a   ECDHE-ECDSA-AES256-SHA            ECDH       AES         256      TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA               not a/v\n xc022   SRP-DSS-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_DSS_WITH_AES_256_CBC_SHA               not a/v\n xc021   SRP-RSA-AES-256-CBC-SHA           SRP        AES         256      TLS_SRP_SHA_RSA_WITH_AES_256_CBC_SHA               not a/v\n xc020   SRP-AES-256-CBC-SHA               SRP        AES         256      TLS_SRP_SHA_WITH_AES_256_CBC_SHA                   not a/v\n x91     DHE-PSK-AES256-CBC-SHA            DHEPSK     AES         256      TLS_DHE_PSK_WITH_AES_256_CBC_SHA                   not a/v\n x39     DHE-RSA-AES256-SHA                DH         AES         256      TLS_DHE_RSA_WITH_AES_256_CBC_SHA                   not a/v\n x38     DHE-DSS-AES256-SHA                DH         AES         256      TLS_DHE_DSS_WITH_AES_256_CBC_SHA                   not a/v\n x37     DH-RSA-AES256-SHA                 DH/RSA     AES         256      TLS_DH_RSA_WITH_AES_256_CBC_SHA                    not a/v\n x36     DH-DSS-AES256-SHA                 DH/DSS     AES         256      TLS_DH_DSS_WITH_AES_256_CBC_SHA                    not a/v\n x88     DHE-RSA-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x87     DHE-DSS-CAMELLIA256-SHA           DH         Camellia    256      TLS_DHE_DSS_WITH_CAMELLIA_256_CBC_SHA              not a/v\n x86     DH-RSA-CAMELLIA256-SHA            DH/RSA     Camellia    256      TLS_DH_RSA_WITH_CAMELLIA_256_CBC_SHA               not a/v\n x85     DH-DSS-CAMELLIA256-SHA            DH/DSS     Camellia    256      TLS_DH_DSS_WITH_CAMELLIA_256_CBC_SHA               not a/v\n xc019   AECDH-AES256-SHA                  ECDH       AES         256      TLS_ECDH_anon_WITH_AES_256_CBC_SHA                 not a/v\n x3a     ADH-AES256-SHA                    DH         AES         256      TLS_DH_anon_WITH_AES_256_CBC_SHA                   not a/v\n x89     ADH-CAMELLIA256-SHA               DH         Camellia    256      TLS_DH_anon_WITH_CAMELLIA_256_CBC_SHA              not a/v\n xc00f   ECDH-RSA-AES256-SHA               ECDH/RSA   AES         256      TLS_ECDH_RSA_WITH_AES_256_CBC_SHA                  not a/v\n xc005   ECDH-ECDSA-AES256-SHA             ECDH/ECDSA AES         256      TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA                not a/v\n x35     AES256-SHA                        RSA        AES         256      TLS_RSA_WITH_AES_256_CBC_SHA                       available\n xc036   ECDHE-PSK-AES256-CBC-SHA          ECDHEPSK   AES         256      TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA                 not a/v\n x84     CAMELLIA256-SHA                   RSA        Camellia    256      TLS_RSA_WITH_CAMELLIA_256_CBC_SHA                  not a/v\n x95     RSA-PSK-AES256-CBC-SHA            RSAPSK     AES         256      TLS_RSA_PSK_WITH_AES_256_CBC_SHA                   not a/v\n x8d     PSK-AES256-CBC-SHA                PSK        AES         256      TLS_PSK_WITH_AES_256_CBC_SHA                       not a/v\n xc013   ECDHE-RSA-AES128-SHA              ECDH 256   AES         128      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA                 available\n xc009   ECDHE-ECDSA-AES128-SHA            ECDH       AES         128      TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA               not a/v\n xc01f   SRP-DSS-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_DSS_WITH_AES_128_CBC_SHA               not a/v\n xc01e   SRP-RSA-AES-128-CBC-SHA           SRP        AES         128      TLS_SRP_SHA_RSA_WITH_AES_128_CBC_SHA               not a/v\n xc01d   SRP-AES-128-CBC-SHA               SRP        AES         128      TLS_SRP_SHA_WITH_AES_128_CBC_SHA                   not a/v\n x33     DHE-RSA-AES128-SHA                DH         AES         128      TLS_DHE_RSA_WITH_AES_128_CBC_SHA                   not a/v\n x32     DHE-DSS-AES128-SHA                DH         AES         128      TLS_DHE_DSS_WITH_AES_128_CBC_SHA                   not a/v\n x31     DH-RSA-AES128-SHA                 DH/RSA     AES         128      TLS_DH_RSA_WITH_AES_128_CBC_SHA                    not a/v\n x30     DH-DSS-AES128-SHA                 DH/DSS     AES         128      TLS_DH_DSS_WITH_AES_128_CBC_SHA                    not a/v\n x9a     DHE-RSA-SEED-SHA                  DH         SEED        128      TLS_DHE_RSA_WITH_SEED_CBC_SHA                      not a/v\n x99     DHE-DSS-SEED-SHA                  DH         SEED        128      TLS_DHE_DSS_WITH_SEED_CBC_SHA                      not a/v\n x98     DH-RSA-SEED-SHA                   DH/RSA     SEED        128      TLS_DH_RSA_WITH_SEED_CBC_SHA                       not a/v\n x97     DH-DSS-SEED-SHA                   DH/DSS     SEED        128      TLS_DH_DSS_WITH_SEED_CBC_SHA                       not a/v\n x45     DHE-RSA-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x44     DHE-DSS-CAMELLIA128-SHA           DH         Camellia    128      TLS_DHE_DSS_WITH_CAMELLIA_128_CBC_SHA              not a/v\n x43     DH-RSA-CAMELLIA128-SHA            DH/RSA     Camellia    128      TLS_DH_RSA_WITH_CAMELLIA_128_CBC_SHA               not a/v\n x42     DH-DSS-CAMELLIA128-SHA            DH/DSS     Camellia    128      TLS_DH_DSS_WITH_CAMELLIA_128_CBC_SHA               not a/v\n xc018   AECDH-AES128-SHA                  ECDH       AES         128      TLS_ECDH_anon_WITH_AES_128_CBC_SHA                 not a/v\n x34     ADH-AES128-SHA                    DH         AES         128      TLS_DH_anon_WITH_AES_128_CBC_SHA                   not a/v\n x9b     ADH-SEED-SHA                      DH         SEED        128      TLS_DH_anon_WITH_SEED_CBC_SHA                      not a/v\n x46     ADH-CAMELLIA128-SHA               DH         Camellia    128      TLS_DH_anon_WITH_CAMELLIA_128_CBC_SHA              not a/v\n xc00e   ECDH-RSA-AES128-SHA               ECDH/RSA   AES         128      TLS_ECDH_RSA_WITH_AES_128_CBC_SHA                  not a/v\n xc004   ECDH-ECDSA-AES128-SHA             ECDH/ECDSA AES         128      TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA                not a/v\n x2f     AES128-SHA                        RSA        AES         128      TLS_RSA_WITH_AES_128_CBC_SHA                       available\n xc035   ECDHE-PSK-AES128-CBC-SHA          ECDHEPSK   AES         128      TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA                 not a/v\n x90     DHE-PSK-AES128-CBC-SHA            DHEPSK     AES         128      TLS_DHE_PSK_WITH_AES_128_CBC_SHA                   not a/v\n x96     SEED-SHA                          RSA        SEED        128      TLS_RSA_WITH_SEED_CBC_SHA                          not a/v\n x41     CAMELLIA128-SHA                   RSA        Camellia    128      TLS_RSA_WITH_CAMELLIA_128_CBC_SHA                  not a/v\n x07     IDEA-CBC-SHA                      RSA        IDEA        128      TLS_RSA_WITH_IDEA_CBC_SHA                          not a/v\n x94     RSA-PSK-AES128-CBC-SHA            RSAPSK     AES         128      TLS_RSA_PSK_WITH_AES_128_CBC_SHA                   not a/v\n x8c     PSK-AES128-CBC-SHA                PSK        AES         128      TLS_PSK_WITH_AES_128_CBC_SHA                       not a/v\n x21     KRB5-IDEA-CBC-SHA                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_SHA                         not a/v\n x25     KRB5-IDEA-CBC-MD5                 KRB5       IDEA        128      TLS_KRB5_WITH_IDEA_CBC_MD5                         not a/v\n xc012   ECDHE-RSA-DES-CBC3-SHA            ECDH       3DES        168      TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA                not a/v\n xc008   ECDHE-ECDSA-DES-CBC3-SHA          ECDH       3DES        168      TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01c   SRP-DSS-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_DSS_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01b   SRP-RSA-3DES-EDE-CBC-SHA          SRP        3DES        168      TLS_SRP_SHA_RSA_WITH_3DES_EDE_CBC_SHA              not a/v\n xc01a   SRP-3DES-EDE-CBC-SHA              SRP        3DES        168      TLS_SRP_SHA_WITH_3DES_EDE_CBC_SHA                  not a/v\n x16     EDH-RSA-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA                  not a/v\n x13     EDH-DSS-DES-CBC3-SHA              DH         3DES        168      TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA                  not a/v\n x10     DH-RSA-DES-CBC3-SHA               DH/RSA     3DES        168      TLS_DH_RSA_WITH_3DES_EDE_CBC_SHA                   not a/v\n x0d     DH-DSS-DES-CBC3-SHA               DH/DSS     3DES        168      TLS_DH_DSS_WITH_3DES_EDE_CBC_SHA                   not a/v\n xc017   AECDH-DES-CBC3-SHA                ECDH       3DES        168      TLS_ECDH_anon_WITH_3DES_EDE_CBC_SHA                not a/v\n x1b     ADH-DES-CBC3-SHA                  DH         3DES        168      TLS_DH_anon_WITH_3DES_EDE_CBC_SHA                  not a/v\n xc00d   ECDH-RSA-DES-CBC3-SHA             ECDH/RSA   3DES        168      TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA                 not a/v\n xc003   ECDH-ECDSA-DES-CBC3-SHA           ECDH/ECDSA 3DES        168      TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA               not a/v\n x0a     DES-CBC3-SHA                      RSA        3DES        168      TLS_RSA_WITH_3DES_EDE_CBC_SHA                      available\n x93     RSA-PSK-3DES-EDE-CBC-SHA          RSAPSK     3DES        168      TLS_RSA_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\n x8b     PSK-3DES-EDE-CBC-SHA              PSK        3DES        168      TLS_PSK_WITH_3DES_EDE_CBC_SHA                      not a/v\n x1f     KRB5-DES-CBC3-SHA                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_SHA                     not a/v\n x23     KRB5-DES-CBC3-MD5                 KRB5       3DES        168      TLS_KRB5_WITH_3DES_EDE_CBC_MD5                     not a/v\n xc034   ECDHE-PSK-3DES-EDE-CBC-SHA        ECDHEPSK   3DES        168      TLS_ECDHE_PSK_WITH_3DES_EDE_CBC_SHA                not a/v\n x8f     DHE-PSK-3DES-EDE-CBC-SHA          DHEPSK     3DES        168      TLS_DHE_PSK_WITH_3DES_EDE_CBC_SHA                  not a/v\n xfeff   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\n xffe0   -                                 RSA        3DES        168      SSL_RSA_FIPS_WITH_3DES_EDE_CBC_SHA                 not a/v\n x63     EXP1024-DHE-DSS-DES-CBC-SHA       DH(1024)   DES         56,exp   TLS_DHE_DSS_EXPORT1024_WITH_DES_CBC_SHA            not a/v\n x15     EDH-RSA-DES-CBC-SHA               DH         DES         56       TLS_DHE_RSA_WITH_DES_CBC_SHA                       not a/v\n x12     EDH-DSS-DES-CBC-SHA               DH         DES         56       TLS_DHE_DSS_WITH_DES_CBC_SHA                       not a/v\n x0f     DH-RSA-DES-CBC-SHA                DH/RSA     DES         56       TLS_DH_RSA_WITH_DES_CBC_SHA                        not a/v\n x0c     DH-DSS-DES-CBC-SHA                DH/DSS     DES         56       TLS_DH_DSS_WITH_DES_CBC_SHA                        not a/v\n x1a     ADH-DES-CBC-SHA                   DH         DES         56       TLS_DH_anon_WITH_DES_CBC_SHA                       not a/v\n x62     EXP1024-DES-CBC-SHA               RSA(1024)  DES         56,exp   TLS_RSA_EXPORT1024_WITH_DES_CBC_SHA                not a/v\n x09     DES-CBC-SHA                       RSA        DES         56       TLS_RSA_WITH_DES_CBC_SHA                           not a/v\n x1e     KRB5-DES-CBC-SHA                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_SHA                          not a/v\n x22     KRB5-DES-CBC-MD5                  KRB5       DES         56       TLS_KRB5_WITH_DES_CBC_MD5                          not a/v\n xfefe   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\n xffe1   -                                 RSA        DES         56       SSL_RSA_FIPS_WITH_DES_CBC_SHA                      not a/v\n x14     EXP-EDH-RSA-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x11     EXP-EDH-DSS-DES-CBC-SHA           DH(512)    DES         40,exp   TLS_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x19     EXP-ADH-DES-CBC-SHA               DH(512)    DES         40,exp   TLS_DH_anon_EXPORT_WITH_DES40_CBC_SHA              not a/v\n x08     EXP-DES-CBC-SHA                   RSA(512)   DES         40,exp   TLS_RSA_EXPORT_WITH_DES40_CBC_SHA                  not a/v\n x06     EXP-RC2-CBC-MD5                   RSA(512)   RC2         40,exp   TLS_RSA_EXPORT_WITH_RC2_CBC_40_MD5                 not a/v\n x27     EXP-KRB5-RC2-CBC-SHA              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_SHA                not a/v\n x26     EXP-KRB5-DES-CBC-SHA              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_SHA                not a/v\n x2a     EXP-KRB5-RC2-CBC-MD5              KRB5       RC2         40,exp   TLS_KRB5_EXPORT_WITH_RC2_CBC_40_MD5                not a/v\n x29     EXP-KRB5-DES-CBC-MD5              KRB5       DES         40,exp   TLS_KRB5_EXPORT_WITH_DES_CBC_40_MD5                not a/v\n x0b     EXP-DH-DSS-DES-CBC-SHA            DH/DSS     DES         40,exp   TLS_DH_DSS_EXPORT_WITH_DES40_CBC_SHA               not a/v\n x0e     EXP-DH-RSA-DES-CBC-SHA            DH/RSA     DES         40,exp   TLS_DH_RSA_EXPORT_WITH_DES40_CBC_SHA               not a/v\n\n VULNERABLE -- but also supports higher protocols (possible mitigation)  TLSv1.1 TLSv1.2\n\n LUCKY13 (CVE-2013-0169), experimental     potentially VULNERABLE, uses cipher block chaining (CBC) ciphers with TLS. Check patches\n RC4 (CVE-2013-2566, CVE-2015-2808)        no RC4 ciphers detected (OK)\n\n\n Running client simulations (HTTP) via sockets \n\n Browser                      Protocol  Cipher Suite Name (OpenSSL)       Forward Secrecy\n------------------------------------------------------------------------------------------------\n Android 4.4.2                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 5.0.0                TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 6.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 7.0                  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 8.1 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 9.0 (native)         TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Android 10.0 (native)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Chrome 74 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Chrome 79 (Win 10)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Firefox 66 (Win 8.1/10)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Firefox 71 (Win 10)          TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n IE 6 XP                      No connection\n IE 8 Win 7                   TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 8 XP                      TLSv1.0   DES-CBC3-SHA                      No FS\n IE 11 Win 7                  TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win 8.1                TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win Phone 8.1          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n IE 11 Win 10                 TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Edge 15 Win 10               TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Edge 17 (Win 10)             TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Opera 66 (Win 10)            TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Safari 9 iOS 9               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 9 OS X 10.11          TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 10 OS X 10.12         TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Safari 12.1 (iOS 12.2)       TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Safari 13.0 (macOS 10.14.6)  TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Apple ATS 9 iOS 9            TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 6u45                    TLSv1.0   AES128-SHA                        No FS\n Java 7u25                    TLSv1.0   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 8u161                   TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n Java 11.0.2 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Java 12.0.1 (OpenJDK)        TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n OpenSSL 1.0.2e               TLSv1.2   ECDHE-RSA-AES128-SHA              256 bit ECDH (P-256)\n OpenSSL 1.1.0l (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n OpenSSL 1.1.1d (Debian)      TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n Thunderbird (68.3)           TLSv1.2   ECDHE-RSA-AES128-GCM-SHA256       256 bit ECDH (P-256)\n\n Done 2020-08-29 22:26:19 [ 349s] -->> 52.216.186.211:443 (calculator.s3.amazonaws.com) <<--\n\nWhich shows us;\n\n * TLS1.0 and 1.1 are still supported, with a few noticeable ciphers we can\n   exploit for over a decade\n * Vulnerable to SWEET32 (CVE-2016-2183, CVE-2016-6329) which has some trivial\n   exploits online for the script kiddies to run without learning much more than\n   how to open a terminal window..\n * no OSCP at all, not stabled or the CA enforcement of the must staple flag in\n   the cert.\n * Valid for 60 days, meaning we have a very wide chance to wait for IP churn to\n   provide a window to do a DNS version of domain take over.\n * Session Side-Jacking seems like a promising attack vector too, but requires\n   further investigation than we have done here\n * best for last, those CBC ciphers are a great target to test some very well\n   known padding related attack vectors considering the internet has pretty much\n   gotten rid of CBC ciphers between 2013-2015.\n\nIf i can spend a whole 15mins and next to zero technical effort to learn this,\nand I am not even slightly considered a motivated attacker, or an attacker of\nany description. What do you think can happen if there is 1 human out there on\nplanet earth targeting your data? Amazon's security won't stop them (clearly),\nand you have no way to configure Amazon to protect yourself. I guess we can hope\nthat the name Amazon is a deterrent and you're safe in the expanding sea of\ncustomers, but the news cycles about S3 breaches tells a very different story\nabout the deterrent quality of an North American corporate to the rest of the\nworld.. You decide, it's your threat model, your risk posture - not mine.\n\nConclusion\nWhile AWS offer many security features, they allow insecure and deprecated\nprotocol downgrades to TLS 1.0, and support deprecated signature algorithms for\ntheir API authentication.\n\nThese backwards-compatibility decisions and continued use of deprecated security\nfeatures are open to attackers who will utilise anything available, they don't\nstick to the best offered security options.","feature_image":"__GHOST_URL__/content/images/2020/06/aws.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-06-16 04:25:03","created_by":"1","updated_at":"2021-03-31 13:59:20","updated_by":"1","published_at":"2020-06-07 10:30:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc8","uuid":"48a8acc7-12d1-4656-be3d-baf0f09a6c72","title":"TLS secures my data, right?","slug":"tls-secures-my-data-right","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"em\"],[\"u\"],[\"strong\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Load_balancing_(computing)\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Proxy_server\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Computer_security\"]],[\"a\",[\"href\",\"http://tools.ietf.org/html/rfc6347\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"You have to understand that TLS \"],[0,[0],1,\"by-design\"],[0,[],0,\" is intended to have all data read by \"],[0,[1],1,\"anyone\"],[0,[],0,\" without any authorisation checks.\"]]],[1,\"p\",[[0,[],0,\"Don't just take my word for it. In the words of ‌‌Nate Lawson;\"],[1,[],0,0],[0,[],0,\"(cryptographer and software engineer who has contributed to the protocols since SSL3.0)\"]]],[1,\"blockquote\",[[0,[],0,\"Data within the session should not be recoverable by anyone except the endpoints\"]]],[1,\"p\",[[0,[],0,\"It is that simple.\"],[1,[],0,1],[0,[],0,\"‌‌Whether you have good or bad intentions, all you need is to be either the requester or the 'receiver'. TLS is designed to let you see the 'protected' data. With zero authentication at the protocol level there are no real way to control the identity of a requester or a receiver, effectively this means in plain speak that if you have the encrypted data you can read it if you are able to perform the procedure a requester or a receiver would.\"]]],[1,\"blockquote\",[[0,[2],1,\"A quick acknowledgement\"],[0,[],0,\": identity and access controls do exist outside the TLS protocol; HMAC is the leading implementation in modern APIs. Moving on.\"]]],[1,\"p\",[[0,[],0,\"The TLS protocol allows client and server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. All are characteristic of integrity, confidentiality can never be effectively achieved, it is a myth of TLS.\"]]],[1,\"blockquote\",[[0,[],0,\"Confidentiality is a TLS myth\"]]],[1,\"p\",[[0,[],0,\"Let's explore some TLS basic concepts - demystifying TLS (buzzword!).\"]]],[1,\"h2\",[[0,[],0,\"Point-to-point\"]]],[1,\"p\",[[0,[],0,\"TLS goal is to protect data from eaves-droppers reading the data between point A and point B. This characteristic of TLS is commonly called point-to-point encryption.\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"Low traffic websites\"]],[[0,[],0,\"Client initiated data; i.e. a Dropbox, file upload or download\"]],[[0,[],0,\"Media streaming (not live); podcast, music, video\"]]]],[1,\"h2\",[[0,[],0,\"e2ee\"]]],[1,\"p\",[[0,[],0,\"If we are using TLS from A to B, TLS from B to C, and controlling all A, B and C points - this is called end-to-end encryption (e2ee). This is common where load-balancing or proxy servers are used in the most secure environments that require e2ee.‌‌However modern privacy enthusiasts argue that traditional e2ee is insecure because the origin and destination do not utilise the same certificate to achieve encryption in TLS. Therefore TLS had to introduce Forward secrecy (a.k.a PFS for Perfect Forward secrecy).\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"High traffic websites (load balancing)\"]],[[0,[],0,\"The \"],[0,[3],1,\"load balancing\"],[0,[],0,\" software vendors\"]],[[0,[],0,\"Enterprise networks (proxy)\"]],[[0,[],0,\"The \"],[0,[4],1,\"proxy\"],[0,[],0,\" software vendors\"]],[[0,[],0,\"Many \"],[0,[5],1,\"security software\"],[0,[],0,\" vendors\"]],[[0,[],0,\"Cloud (SaaS / PaaS / IaaS) providers management plane\"]]]],[1,\"h2\",[[0,[],0,\"Forward secrecy\"]]],[1,\"p\",[[0,[],0,\"Using Perfect Forward Secrecy (PFS) reduces the impact of the compromise of a TLS session.\"],[1,[],0,2],[0,[],0,\"Forward secrecy provides cryptographic verifiable assurances of e2ee, or a more simplistic version of that is PFS is an agreement between sender and receiver, regardless of any servers in the middle.\"]]],[1,\"p\",[[0,[],0,\"However many privacy advocates argue that the middle server is still a privacy violation.‌‌ This privacy purist view is ignorance as it disregards facts about the ownership of the technology. Specifically both the middle server and endpoint are the same owner, and in many use cases it is the very the same hardware - so this argument is useless until it is focused solely on a peer-to-peer messaging platform where the peers expect all data (including metadata) is e2ee without anything in the middle. These purists are totally crazy, this opinion is ignorant of the basic concept that \"],[0,[1],1,\"it is the internet\"],[0,[],0,\" made up of all sorts of hardware that forwards your data from peer to peer. It is not a cross-over cat5 cable!\"]]],[1,\"blockquote\",[[0,[],0,\"Try to remember, peer-to-peer is still the internet made up of hardware devices to transmit your data\"]]],[1,\"p\",[[0,[],0,\"We still call Forward Secrecy an e2ee implementation, if information security were to change the meaning of something so fundamental to the way the internet works, would be beyond irrational.\"]]],[1,\"p\",[[0,[],0,\"For these privacy purists narrow view of e2ee only being used to describe peer-to-peer, the information security professionals have designated something called \"],[0,[2],1,\"Zero Knowledge\"],[0,[],0,\". \"]]],[1,\"p\",[[0,[],0,\"It's worth pointing out that educated information security professionals prefer the term \"],[0,[2],1,\"Forward anonymity\"],[0,[],0,\" over the analogous Forward secrecy, because there is no real \"],[0,[0],1,\"added\"],[0,[],0,\" secrecy (confidentiality) characteristics over it's TLS-based predecessors. TLS will only ever provide a guarantee of integrity with any cryptographic solution, so Forward anonymity adds the concept to TLS that your identity can now be preserved in TLS.\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"Everywhere TLS is used, without it you are vulnerable to compromise via trivial attacks\"]],[[0,[],0,\"Australian ISM Security Control 1453\"]],[[0,[],0,\"Future standards in draft now (NIST SP 800-77, PCI DSS v4, CSA CCM v4)\"]]]],[1,\"h2\",[[0,[],0,\"Zero Knowledge\"]]],[1,\"p\",[[0,[],0,\"A form of e2ee mostly desired for peer-to-peer communications.\"]]],[1,\"p\",[[0,[],0,\"Zero Knowledge e2ee can provide both confidentiality and integrity cryptographic verifiable assurances.\"]]],[1,\"blockquote\",[[0,[],0,\"TLS does not provide zero knowledge e2ee\"]]],[1,\"p\",[[0,[],0,\"There is no specific ciphers used to distinctly achieve Zero Knowledge.\"],[1,[],0,3],[0,[],0,\"Equally there is no part of TLS or any other standardised protocol specification that currently defines what zero knowledge e2ee should technically implement.\"]]],[1,\"p\",[[0,[],0,\"To achieve Zero Knowledge today, one must convince the privacy purists that their bespoke implementation is a Zero Knowledge scenario, in a beautiful example of irony the \"],[0,[1],1,\"privacy purists must trust\"],[0,[],0,\" the offer is Zero Knowledge through the scrutiny of white papers and external observations. Sometimes a claimed Zero Knowledge implementation provides full source code, but more often (e.g. Signal) the actual source code users are running is never provided, but a protocol that implements the design goals of a white paper is made available - and we must trust that the real source code that users run is the same - no one but the vendors themselves really know.\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"Use instead of TLS where peer-to-peer e2ee is desired\"]]]],[1,\"h2\",[[0,[],0,\"Datagram Transport Layer Security (DTLS)\"]]],[1,\"p\",[[0,[],0,\"DTLS is a protocol based on TLS concepts that is capable of securing the datagram transport.\"]]],[1,\"p\",[[0,[],0,\"Basically \"],[0,[6],1,\"DTLS\"],[0,[],0,\" is to construct TLS over datagram (UDP, DCCP) and solves two problems TLS does not. DTLS adds packet lost tolerance and reordering packets, through assigning sequence number within the handshake packet re-transmission and replay detection is possible which were not available characteristics of TLS.\"]]],[1,\"p\",[[0,[],0,\"In terms of the cipher suites that are available only RC4 is not permitted, so DTLS can generally be considered the same as TLS in discussions related to cryptographic assurances.\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"Delay-sensitive (and hence use datagram transport) applications; media stream buffering, VPN, Interactive SSH\"]],[[0,[],0,\"Applications that tend to run out of file descriptors or socket buffers; data clusters, container clusters\"]]]],[1,\"h2\",[[0,[],0,\"Mutual TLS\"]]],[1,\"p\",[[0,[],0,\"TLS guarantees the identity of the server to the client and provides a two-way encrypted channel between the server and client. Mutual TLS (mTLS) allows an application on the \"],[0,[0,0],2,\"server\"],[0,[],0,\" to confirm the identity of the \"],[0,[0,0],1,\"client\"],[0,[],1,\".\"]]],[1,\"p\",[[0,[],0,\"We've established that TLS assumes anyone who sends and receives are authorised to read, therefore MTLS being based on TLS fundamentally it also assumes this characteristic, so using MTLS we can learn both sides of the communication \"],[0,[0],1,\"identity - But\"],[0,[],0,\" MTLS will not assure or promise either side any authentication assurances so the identity remains unauthenticated (no matter what the internet tells you).\"]]],[1,\"blockquote\",[[0,[],0,\"MTLS remains unauthenticated during communication, only the identity is offered and an out-of-band authentication is assumed\"]]],[1,\"p\",[[0,[],0,\"If you take a casual look at any MTLS you will find it relies on either Basic Authentication (considered deprecated for over a decade) or something more modern like HMAC. So MTLS alone is unauthenticated but almost always ass a layer of authentication over the top of MTLS.\"]]],[1,\"p\",[[0,[],0,\"Typical scenarios;\"]]],[3,\"ul\",[[[0,[],0,\"In co-called Zero Trust networks, where an identity of all servers in the network is expected so a least privileged approach can be applied to network communications\"]]]],[1,\"h2\",[[0,[],0,\"TLS Dependencies\"]]],[1,\"p\",[[0,[],0,\"There are a number of assumptions in TLS that lead to many myths.\"]]],[1,\"p\",[[0,[],0,\"Primarily TCP is intentionally based on TCP which introduces too many optional implementation interpretations like ciphers suite choice, which are problematic due to both the signature hashes used and ways cipher negotiation can compromise security.\"],[1,[],0,4],[0,[],0,\"There are several other things either left open to interpretation in TLS, or have made too many assumptions such as version negotiation for backwards compatibility, data compression, and root-CA trust. There are so many possible security issues with TLS that are only possible because TCP does not define compression well, or it doesn't account for side-channel attacks, ignores that we can deriving private keys from public keys due to hash collisions or data leaks in poor signature algorithm implementations.\"]]],[1,\"p\",[[0,[],0,\"These are just a few security topics of conversation that make easy compromise or security implications, and TLS protocol offers no answers to assure security professionals.\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Basically TLS is \"],[0,[1],1,\"not a security-first protocol\"],[0,[],0,\", downgrade attacks alone tell us that TLS compromises security in favour of adoption by-design. Heartbleed also showed us that TCP itself can cause TLS issues that are impossible if TLS were based on UDP or something uniquely defined for TLS on top of IP (like ICMP) which is actually security first.\"]]],[1,\"p\",[[0,[],0,\"So for security reasons TLS alone is never sufficient in any of it's variant forms, you should chose additional security standards like authentication to implement on top of your TLS.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>You have to understand that TLS <em>by-design</em> is intended to have all data read by <u>anyone</u> without any authorisation checks.</p><p>Don't just take my word for it. In the words of ‌‌Nate Lawson;<br>(cryptographer and software engineer who has contributed to the protocols since SSL3.0)</p><blockquote>Data within the session should not be recoverable by anyone except the endpoints</blockquote><p>It is that simple.<br>‌‌Whether you have good or bad intentions, all you need is to be either the requester or the 'receiver'. TLS is designed to let you see the 'protected' data. With zero authentication at the protocol level there are no real way to control the identity of a requester or a receiver, effectively this means in plain speak that if you have the encrypted data you can read it if you are able to perform the procedure a requester or a receiver would.</p><blockquote><strong>A quick acknowledgement</strong>: identity and access controls do exist outside the TLS protocol; HMAC is the leading implementation in modern APIs. Moving on.</blockquote><p>The TLS protocol allows client and server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. All are characteristic of integrity, confidentiality can never be effectively achieved, it is a myth of TLS.</p><blockquote>Confidentiality is a TLS myth</blockquote><p>Let's explore some TLS basic concepts - demystifying TLS (buzzword!).</p><h2 id=\"point-to-point\">Point-to-point</h2><p>TLS goal is to protect data from eaves-droppers reading the data between point A and point B. This characteristic of TLS is commonly called point-to-point encryption.</p><p>Typical scenarios;</p><ul><li>Low traffic websites</li><li>Client initiated data; i.e. a Dropbox, file upload or download</li><li>Media streaming (not live); podcast, music, video</li></ul><h2 id=\"e2ee\">e2ee</h2><p>If we are using TLS from A to B, TLS from B to C, and controlling all A, B and C points - this is called end-to-end encryption (e2ee). This is common where load-balancing or proxy servers are used in the most secure environments that require e2ee.‌‌However modern privacy enthusiasts argue that traditional e2ee is insecure because the origin and destination do not utilise the same certificate to achieve encryption in TLS. Therefore TLS had to introduce Forward secrecy (a.k.a PFS for Perfect Forward secrecy).</p><p>Typical scenarios;</p><ul><li>High traffic websites (load balancing)</li><li>The <a href=\"https://en.wikipedia.org/wiki/Load_balancing_(computing)\">load balancing</a> software vendors</li><li>Enterprise networks (proxy)</li><li>The <a href=\"https://en.wikipedia.org/wiki/Proxy_server\">proxy</a> software vendors</li><li>Many <a href=\"https://en.wikipedia.org/wiki/Computer_security\">security software</a> vendors</li><li>Cloud (SaaS / PaaS / IaaS) providers management plane</li></ul><h2 id=\"forward-secrecy\">Forward secrecy</h2><p>Using Perfect Forward Secrecy (PFS) reduces the impact of the compromise of a TLS session.<br>Forward secrecy provides cryptographic verifiable assurances of e2ee, or a more simplistic version of that is PFS is an agreement between sender and receiver, regardless of any servers in the middle.</p><p>However many privacy advocates argue that the middle server is still a privacy violation.‌‌ This privacy purist view is ignorance as it disregards facts about the ownership of the technology. Specifically both the middle server and endpoint are the same owner, and in many use cases it is the very the same hardware - so this argument is useless until it is focused solely on a peer-to-peer messaging platform where the peers expect all data (including metadata) is e2ee without anything in the middle. These purists are totally crazy, this opinion is ignorant of the basic concept that <u>it is the internet</u> made up of all sorts of hardware that forwards your data from peer to peer. It is not a cross-over cat5 cable!</p><blockquote>Try to remember, peer-to-peer is still the internet made up of hardware devices to transmit your data</blockquote><p>We still call Forward Secrecy an e2ee implementation, if information security were to change the meaning of something so fundamental to the way the internet works, would be beyond irrational.</p><p>For these privacy purists narrow view of e2ee only being used to describe peer-to-peer, the information security professionals have designated something called <strong>Zero Knowledge</strong>. </p><p>It's worth pointing out that educated information security professionals prefer the term <strong>Forward anonymity</strong> over the analogous Forward secrecy, because there is no real <em>added</em> secrecy (confidentiality) characteristics over it's TLS-based predecessors. TLS will only ever provide a guarantee of integrity with any cryptographic solution, so Forward anonymity adds the concept to TLS that your identity can now be preserved in TLS.</p><p>Typical scenarios;</p><ul><li>Everywhere TLS is used, without it you are vulnerable to compromise via trivial attacks</li><li>Australian ISM Security Control 1453</li><li>Future standards in draft now (NIST SP 800-77, PCI DSS v4, CSA CCM v4)</li></ul><h2 id=\"zero-knowledge\">Zero Knowledge</h2><p>A form of e2ee mostly desired for peer-to-peer communications.</p><p>Zero Knowledge e2ee can provide both confidentiality and integrity cryptographic verifiable assurances.</p><blockquote>TLS does not provide zero knowledge e2ee</blockquote><p>There is no specific ciphers used to distinctly achieve Zero Knowledge.<br>Equally there is no part of TLS or any other standardised protocol specification that currently defines what zero knowledge e2ee should technically implement.</p><p>To achieve Zero Knowledge today, one must convince the privacy purists that their bespoke implementation is a Zero Knowledge scenario, in a beautiful example of irony the <u>privacy purists must trust</u> the offer is Zero Knowledge through the scrutiny of white papers and external observations. Sometimes a claimed Zero Knowledge implementation provides full source code, but more often (e.g. Signal) the actual source code users are running is never provided, but a protocol that implements the design goals of a white paper is made available - and we must trust that the real source code that users run is the same - no one but the vendors themselves really know.</p><p>Typical scenarios;</p><ul><li>Use instead of TLS where peer-to-peer e2ee is desired</li></ul><h2 id=\"datagram-transport-layer-security-dtls-\">Datagram Transport Layer Security (DTLS)</h2><p>DTLS is a protocol based on TLS concepts that is capable of securing the datagram transport.</p><p>Basically <a href=\"http://tools.ietf.org/html/rfc6347\">DTLS</a> is to construct TLS over datagram (UDP, DCCP) and solves two problems TLS does not. DTLS adds packet lost tolerance and reordering packets, through assigning sequence number within the handshake packet re-transmission and replay detection is possible which were not available characteristics of TLS.</p><p>In terms of the cipher suites that are available only RC4 is not permitted, so DTLS can generally be considered the same as TLS in discussions related to cryptographic assurances.</p><p>Typical scenarios;</p><ul><li>Delay-sensitive (and hence use datagram transport) applications; media stream buffering, VPN, Interactive SSH</li><li>Applications that tend to run out of file descriptors or socket buffers; data clusters, container clusters</li></ul><h2 id=\"mutual-tls\">Mutual TLS</h2><p>TLS guarantees the identity of the server to the client and provides a two-way encrypted channel between the server and client. Mutual TLS (mTLS) allows an application on the <em><em>server</em></em> to confirm the identity of the <em><em>client</em>.</em></p><p>We've established that TLS assumes anyone who sends and receives are authorised to read, therefore MTLS being based on TLS fundamentally it also assumes this characteristic, so using MTLS we can learn both sides of the communication <em>identity - But</em> MTLS will not assure or promise either side any authentication assurances so the identity remains unauthenticated (no matter what the internet tells you).</p><blockquote>MTLS remains unauthenticated during communication, only the identity is offered and an out-of-band authentication is assumed</blockquote><p>If you take a casual look at any MTLS you will find it relies on either Basic Authentication (considered deprecated for over a decade) or something more modern like HMAC. So MTLS alone is unauthenticated but almost always ass a layer of authentication over the top of MTLS.</p><p>Typical scenarios;</p><ul><li>In co-called Zero Trust networks, where an identity of all servers in the network is expected so a least privileged approach can be applied to network communications</li></ul><h2 id=\"tls-dependencies\">TLS Dependencies</h2><p>There are a number of assumptions in TLS that lead to many myths.</p><p>Primarily TCP is intentionally based on TCP which introduces too many optional implementation interpretations like ciphers suite choice, which are problematic due to both the signature hashes used and ways cipher negotiation can compromise security.<br>There are several other things either left open to interpretation in TLS, or have made too many assumptions such as version negotiation for backwards compatibility, data compression, and root-CA trust. There are so many possible security issues with TLS that are only possible because TCP does not define compression well, or it doesn't account for side-channel attacks, ignores that we can deriving private keys from public keys due to hash collisions or data leaks in poor signature algorithm implementations.</p><p>These are just a few security topics of conversation that make easy compromise or security implications, and TLS protocol offers no answers to assure security professionals.</p><h2 id=\"conclusion\">Conclusion</h2><p>Basically TLS is <u>not a security-first protocol</u>, downgrade attacks alone tell us that TLS compromises security in favour of adoption by-design. Heartbleed also showed us that TCP itself can cause TLS issues that are impossible if TLS were based on UDP or something uniquely defined for TLS on top of IP (like ICMP) which is actually security first.</p><p>So for security reasons TLS alone is never sufficient in any of it's variant forms, you should chose additional security standards like authentication to implement on top of your TLS.</p>","comment_id":"5ee85fad05bdaa04b9c93901","plaintext":"You have to understand that TLS by-design is intended to have all data read by \nanyone without any authorisation checks.\n\nDon't just take my word for it. In the words of ‌‌Nate Lawson;\n(cryptographer and software engineer who has contributed to the protocols since\nSSL3.0)\n\n> Data within the session should not be recoverable by anyone except the endpoints\nIt is that simple.\n‌‌Whether you have good or bad intentions, all you need is to be either the\nrequester or the 'receiver'. TLS is designed to let you see the 'protected'\ndata. With zero authentication at the protocol level there are no real way to\ncontrol the identity of a requester or a receiver, effectively this means in\nplain speak that if you have the encrypted data you can read it if you are able\nto perform the procedure a requester or a receiver would.\n\n> A quick acknowledgement: identity and access controls do exist outside the TLS\nprotocol; HMAC is the leading implementation in modern APIs. Moving on.\nThe TLS protocol allows client and server applications to communicate in a way\nthat is designed to prevent eavesdropping, tampering, or message forgery. All\nare characteristic of integrity, confidentiality can never be effectively\nachieved, it is a myth of TLS.\n\n> Confidentiality is a TLS myth\nLet's explore some TLS basic concepts - demystifying TLS (buzzword!).\n\nPoint-to-point\nTLS goal is to protect data from eaves-droppers reading the data between point A\nand point B. This characteristic of TLS is commonly called point-to-point\nencryption.\n\nTypical scenarios;\n\n * Low traffic websites\n * Client initiated data; i.e. a Dropbox, file upload or download\n * Media streaming (not live); podcast, music, video\n\ne2ee\nIf we are using TLS from A to B, TLS from B to C, and controlling all A, B and C\npoints - this is called end-to-end encryption (e2ee). This is common where\nload-balancing or proxy servers are used in the most secure environments that\nrequire e2ee.‌‌However modern privacy enthusiasts argue that traditional e2ee is\ninsecure because the origin and destination do not utilise the same certificate\nto achieve encryption in TLS. Therefore TLS had to introduce Forward secrecy\n(a.k.a PFS for Perfect Forward secrecy).\n\nTypical scenarios;\n\n * High traffic websites (load balancing)\n * The load balancing [https://en.wikipedia.org/wiki/Load_balancing_(computing)] \n   software vendors\n * Enterprise networks (proxy)\n * The proxy [https://en.wikipedia.org/wiki/Proxy_server] software vendors\n * Many security software [https://en.wikipedia.org/wiki/Computer_security] \n   vendors\n * Cloud (SaaS / PaaS / IaaS) providers management plane\n\nForward secrecy\nUsing Perfect Forward Secrecy (PFS) reduces the impact of the compromise of a\nTLS session.\nForward secrecy provides cryptographic verifiable assurances of e2ee, or a more\nsimplistic version of that is PFS is an agreement between sender and receiver,\nregardless of any servers in the middle.\n\nHowever many privacy advocates argue that the middle server is still a privacy\nviolation.‌‌ This privacy purist view is ignorance as it disregards facts about\nthe ownership of the technology. Specifically both the middle server and\nendpoint are the same owner, and in many use cases it is the very the same\nhardware - so this argument is useless until it is focused solely on a\npeer-to-peer messaging platform where the peers expect all data (including\nmetadata) is e2ee without anything in the middle. These purists are totally\ncrazy, this opinion is ignorant of the basic concept that it is the internet \nmade up of all sorts of hardware that forwards your data from peer to peer. It\nis not a cross-over cat5 cable!\n\n> Try to remember, peer-to-peer is still the internet made up of hardware devices\nto transmit your data\nWe still call Forward Secrecy an e2ee implementation, if information security\nwere to change the meaning of something so fundamental to the way the internet\nworks, would be beyond irrational.\n\nFor these privacy purists narrow view of e2ee only being used to describe\npeer-to-peer, the information security professionals have designated something\ncalled Zero Knowledge. \n\nIt's worth pointing out that educated information security professionals prefer\nthe term Forward anonymity over the analogous Forward secrecy, because there is\nno real added secrecy (confidentiality) characteristics over it's TLS-based\npredecessors. TLS will only ever provide a guarantee of integrity with any\ncryptographic solution, so Forward anonymity adds the concept to TLS that your\nidentity can now be preserved in TLS.\n\nTypical scenarios;\n\n * Everywhere TLS is used, without it you are vulnerable to compromise via\n   trivial attacks\n * Australian ISM Security Control 1453\n * Future standards in draft now (NIST SP 800-77, PCI DSS v4, CSA CCM v4)\n\nZero Knowledge\nA form of e2ee mostly desired for peer-to-peer communications.\n\nZero Knowledge e2ee can provide both confidentiality and integrity cryptographic\nverifiable assurances.\n\n> TLS does not provide zero knowledge e2ee\nThere is no specific ciphers used to distinctly achieve Zero Knowledge.\nEqually there is no part of TLS or any other standardised protocol specification\nthat currently defines what zero knowledge e2ee should technically implement.\n\nTo achieve Zero Knowledge today, one must convince the privacy purists that\ntheir bespoke implementation is a Zero Knowledge scenario, in a beautiful\nexample of irony the privacy purists must trust the offer is Zero Knowledge\nthrough the scrutiny of white papers and external observations. Sometimes a\nclaimed Zero Knowledge implementation provides full source code, but more often\n(e.g. Signal) the actual source code users are running is never provided, but a\nprotocol that implements the design goals of a white paper is made available -\nand we must trust that the real source code that users run is the same - no one\nbut the vendors themselves really know.\n\nTypical scenarios;\n\n * Use instead of TLS where peer-to-peer e2ee is desired\n\nDatagram Transport Layer Security (DTLS)\nDTLS is a protocol based on TLS concepts that is capable of securing the\ndatagram transport.\n\nBasically DTLS [http://tools.ietf.org/html/rfc6347] is to construct TLS over\ndatagram (UDP, DCCP) and solves two problems TLS does not. DTLS adds packet lost\ntolerance and reordering packets, through assigning sequence number within the\nhandshake packet re-transmission and replay detection is possible which were not\navailable characteristics of TLS.\n\nIn terms of the cipher suites that are available only RC4 is not permitted, so\nDTLS can generally be considered the same as TLS in discussions related to\ncryptographic assurances.\n\nTypical scenarios;\n\n * Delay-sensitive (and hence use datagram transport) applications; media stream\n   buffering, VPN, Interactive SSH\n * Applications that tend to run out of file descriptors or socket buffers; data\n   clusters, container clusters\n\nMutual TLS\nTLS guarantees the identity of the server to the client and provides a two-way\nencrypted channel between the server and client. Mutual TLS (mTLS) allows an\napplication on the server to confirm the identity of the client.\n\nWe've established that TLS assumes anyone who sends and receives are authorised\nto read, therefore MTLS being based on TLS fundamentally it also assumes this\ncharacteristic, so using MTLS we can learn both sides of the communication \nidentity - But MTLS will not assure or promise either side any authentication\nassurances so the identity remains unauthenticated (no matter what the internet\ntells you).\n\n> MTLS remains unauthenticated during communication, only the identity is offered\nand an out-of-band authentication is assumed\nIf you take a casual look at any MTLS you will find it relies on either Basic\nAuthentication (considered deprecated for over a decade) or something more\nmodern like HMAC. So MTLS alone is unauthenticated but almost always ass a layer\nof authentication over the top of MTLS.\n\nTypical scenarios;\n\n * In co-called Zero Trust networks, where an identity of all servers in the\n   network is expected so a least privileged approach can be applied to network\n   communications\n\nTLS Dependencies\nThere are a number of assumptions in TLS that lead to many myths.\n\nPrimarily TCP is intentionally based on TCP which introduces too many optional\nimplementation interpretations like ciphers suite choice, which are problematic\ndue to both the signature hashes used and ways cipher negotiation can compromise\nsecurity.\nThere are several other things either left open to interpretation in TLS, or\nhave made too many assumptions such as version negotiation for backwards\ncompatibility, data compression, and root-CA trust. There are so many possible\nsecurity issues with TLS that are only possible because TCP does not define\ncompression well, or it doesn't account for side-channel attacks, ignores that\nwe can deriving private keys from public keys due to hash collisions or data\nleaks in poor signature algorithm implementations.\n\nThese are just a few security topics of conversation that make easy compromise\nor security implications, and TLS protocol offers no answers to assure security\nprofessionals.\n\nConclusion\nBasically TLS is not a security-first protocol, downgrade attacks alone tell us\nthat TLS compromises security in favour of adoption by-design. Heartbleed also\nshowed us that TCP itself can cause TLS issues that are impossible if TLS were\nbased on UDP or something uniquely defined for TLS on top of IP (like ICMP)\nwhich is actually security first.\n\nSo for security reasons TLS alone is never sufficient in any of it's variant\nforms, you should chose additional security standards like authentication to\nimplement on top of your TLS.","feature_image":"__GHOST_URL__/content/images/2020/06/hearthbleed.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-06-16 05:59:09","created_by":"1","updated_at":"2021-03-31 13:59:50","updated_by":"1","published_at":"2020-05-31 00:00:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcc9","uuid":"695c3c43-6618-4df3-95df-695a1347d744","title":"Python pip requirements.txt lock file","slug":"python-pip-requirements-txt-lock-file","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"#!/usr/bin/env bash\\n\\nCWD=$(pwd)\\nTMP_DIR=$1\\n\\nif [[ $EUID -eq 0 ]]; then\\n   echo -e \\\"${RED}x${NC} This script must not be run as root\\\" \\n   exit 1\\nfi\\nif [ -z $(which python3) ]; then\\n  echo \\\"python3 not found\\\"\\n  exit 1\\nfi\\nif [ -z $(which pip) ]; then\\n  echo \\\"python3 pip not found\\\"\\n  exit 1\\nfi\\nif [[ ! -f requirements.txt ]]; then\\n  echo \\\"requirements.txt not found\\\"\\n  exit 1\\nfi\\n\\nif [[ -z \\\"${TMP_DIR}\\\" ]]; then\\n  TMP_DIR=/tmp/piplock.$(date +'%s%N')\\nfi\\nif [[ ! -z \\\"$(which deactivate)\\\" ]]; then\\n  deactivate\\nfi\\n\\nmkdir -p ${TMP_DIR}\\ncd ${TMP_DIR}\\npython3 -m pip install -U pip\\npython3 -m pip install -U virtualenv\\npython3 -m venv .venv\\nsource .venv/bin/activate\\npip install -q -U --no-cache-dir --isolated --no-warn-conflict -r ${CWD}/requirements.txt\\ncheck=$(pip check --no-cache-dir --isolated)\\ncheck_exit=$?\\nif [[ $check_exit -ne 0 ]]; then\\n  echo ${check}\\n  exit 1\\nfi\\nLOCK=\\\"$(pip freeze)\\\"\\ndeactivate\\ncd ${CWD}\\nrm -rf ${TMP_DIR}\\necho ${LOCK}\",\"language\":\"bash\"}],[\"code\",{\"code\":\"wget -q https://gist.githubusercontent.com/chrisdlangton/905a14e7a42118e0d6b5a45c8ce1d3a0/raw/f0916b0b99187a9e839146fbd4e3d5bc26e5d97a/piplock.sh -O piplock.sh\\nchmod a+x piplock.sh\\nmv piplock.sh /usr/local/bin/piplock\",\"language\":\"bash\"}],[\"code\",{\"code\":\"mkdir -p test-piplock\\ncd test-piplock\\necho -e \\\"retry\\\\nrequests\\\" > requirements.txt\",\"language\":\"bash\"}],[\"code\",{\"code\":\"certifi==2020.6.20\\nchardet==3.0.4\\ndecorator==4.4.2\\nidna==2.10\\npy==1.9.0\\nrequests==2.24.0\\nretry==0.9.2\\nurllib3==1.25.10\\n\",\"language\":\"\"}]],\"markups\":[[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://github.com/pypa/pip/issues?q=is%3Aopen+is%3Aissue+label%3A%22C%3A+new+resolver%22\"]]],\"sections\":[[1,\"h3\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"Opinions vary on how one should make use of lock files, depending on whether the project is the main application, or the project is actually a library that is meant to be consumed by an application or another library.\"]]],[1,\"p\",[[0,[],0,\"Lock files are unquestionably useful if you \"],[0,[0],1,\"build\"],[0,[],0,\" any application. However, if you publish a library or CLI to a packing publisher like Pypi, lock files are \"],[0,[0],1,\"never\"],[0,[],0,\" published. Meaning your users and you might use different versions of dependencies, and that is fine if you internally still use lock files.\"]]],[1,\"p\",[[0,[],0,\"Personally, I see library projects as no different to any other project, and all projects that build no matter where they get deployed should include a lock file for the sake of  reproducible builds and team member collaboration.\"]]],[1,\"p\",[[0,[],0,\"If you are familiar with other languages that use lock file such as Node.js, you might have asked at least once asked if Python can provide the same functionality.\"],[1,[],0,0],[0,[],0,\"The short answer is python has no concept of lock files, equally it can be argued python has no package dependency files at all and that's why there are many options outside the core python team like \"],[0,[1],1,\"setup.py\"],[0,[],0,\", \"],[0,[1],1,\"Pipfile\"],[0,[],0,\", and the most common \"],[0,[1],1,\"requirements.txt\"],[0,[],0,\" as a pattern for Pip.\"]]],[1,\"p\",[[0,[],0,\"A long answer to the lock file question that you might have learned is that you must painstakingly write out the packages by hand and your packages whole dependency tree, and theirs, and so on. Locking the versions using \"],[0,[1],1,\"==\"],[0,[],0,\" of all of these.\"],[1,[],0,1],[0,[],0,\"You might also be familiar with \"],[0,[1],1,\"pip freeze\"],[0,[],0,\" to try make that easier, but everyone has encountered the  complications of skipping the hand crafted requirements after running \"],[0,[1],1,\"pip freeze\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"The Future\"]]],[1,\"p\",[[0,[],0,\"Pip are hard at work creating a new dependency resolver and will soon provide a python native solution to this age old problem. They are working with the various existing implementations (that use the current pip solution anyway) which has been represented in the solution iterations I've looked at. Pip is looking a lot like Pipenv.\"]]],[1,\"p\",[[0,[],0,\"If you have a free minute, run \"],[0,[1],1,\"pip check\"],[0,[],0,\" on some of your more complex projects and \"],[0,[2],1,\"provide the Pip team\"],[0,[],0,\" with your results for research. And consider a generous donation to support their efforts for the whole community and proprietary companies that rely on their work they give away for free.\"]]],[1,\"h3\",[[0,[],0,\"Solution\"]]],[1,\"p\",[[0,[],0,\"While Pip are hard at developing their solution, I have written a very simple script that automates that painstaking process of locking versions of a whole project dependency tree.\"]]],[1,\"p\",[[0,[],0,\"Show me the code!\"]]],[10,0],[1,\"p\",[[0,[],0,\"Example - locking \"],[0,[1],1,\"requests\"],[0,[],0,\" dependency.\"]]],[1,\"p\",[[0,[],0,\"It is as easy, get the script;\"]]],[10,1],[1,\"p\",[[0,[],0,\"Add some simple requirements;\"]]],[10,2],[1,\"p\",[[0,[],0,\"Running \"],[0,[1],1,\"piplock\"],[0,[],0,\" produces;\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"This flow allows you to maintain reproducible builds and consistent dependencies for development workflow, and at the same time enables developers to catch any potentially breaking changes for your library consumers—and all of this, while also keeping all of the developers on your team happy.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<h3 id=\"overview\">Overview</h3><p>Opinions vary on how one should make use of lock files, depending on whether the project is the main application, or the project is actually a library that is meant to be consumed by an application or another library.</p><p>Lock files are unquestionably useful if you <em>build</em> any application. However, if you publish a library or CLI to a packing publisher like Pypi, lock files are <em>never</em> published. Meaning your users and you might use different versions of dependencies, and that is fine if you internally still use lock files.</p><p>Personally, I see library projects as no different to any other project, and all projects that build no matter where they get deployed should include a lock file for the sake of  reproducible builds and team member collaboration.</p><p>If you are familiar with other languages that use lock file such as Node.js, you might have asked at least once asked if Python can provide the same functionality.<br>The short answer is python has no concept of lock files, equally it can be argued python has no package dependency files at all and that's why there are many options outside the core python team like <code>setup.py</code>, <code>Pipfile</code>, and the most common <code>requirements.txt</code> as a pattern for Pip.</p><p>A long answer to the lock file question that you might have learned is that you must painstakingly write out the packages by hand and your packages whole dependency tree, and theirs, and so on. Locking the versions using <code>==</code> of all of these.<br>You might also be familiar with <code>pip freeze</code> to try make that easier, but everyone has encountered the  complications of skipping the hand crafted requirements after running <code>pip freeze</code>.</p><h3 id=\"the-future\">The Future</h3><p>Pip are hard at work creating a new dependency resolver and will soon provide a python native solution to this age old problem. They are working with the various existing implementations (that use the current pip solution anyway) which has been represented in the solution iterations I've looked at. Pip is looking a lot like Pipenv.</p><p>If you have a free minute, run <code>pip check</code> on some of your more complex projects and <a href=\"https://github.com/pypa/pip/issues?q=is%3Aopen+is%3Aissue+label%3A%22C%3A+new+resolver%22\">provide the Pip team</a> with your results for research. And consider a generous donation to support their efforts for the whole community and proprietary companies that rely on their work they give away for free.</p><h3 id=\"solution\">Solution</h3><p>While Pip are hard at developing their solution, I have written a very simple script that automates that painstaking process of locking versions of a whole project dependency tree.</p><p>Show me the code!</p><pre><code class=\"language-bash\">#!/usr/bin/env bash\n\nCWD=$(pwd)\nTMP_DIR=$1\n\nif [[ $EUID -eq 0 ]]; then\n   echo -e \"${RED}x${NC} This script must not be run as root\" \n   exit 1\nfi\nif [ -z $(which python3) ]; then\n  echo \"python3 not found\"\n  exit 1\nfi\nif [ -z $(which pip) ]; then\n  echo \"python3 pip not found\"\n  exit 1\nfi\nif [[ ! -f requirements.txt ]]; then\n  echo \"requirements.txt not found\"\n  exit 1\nfi\n\nif [[ -z \"${TMP_DIR}\" ]]; then\n  TMP_DIR=/tmp/piplock.$(date +'%s%N')\nfi\nif [[ ! -z \"$(which deactivate)\" ]]; then\n  deactivate\nfi\n\nmkdir -p ${TMP_DIR}\ncd ${TMP_DIR}\npython3 -m pip install -U pip\npython3 -m pip install -U virtualenv\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -q -U --no-cache-dir --isolated --no-warn-conflict -r ${CWD}/requirements.txt\ncheck=$(pip check --no-cache-dir --isolated)\ncheck_exit=$?\nif [[ $check_exit -ne 0 ]]; then\n  echo ${check}\n  exit 1\nfi\nLOCK=\"$(pip freeze)\"\ndeactivate\ncd ${CWD}\nrm -rf ${TMP_DIR}\necho ${LOCK}</code></pre><p>Example - locking <code>requests</code> dependency.</p><p>It is as easy, get the script;</p><pre><code class=\"language-bash\">wget -q https://gist.githubusercontent.com/chrisdlangton/905a14e7a42118e0d6b5a45c8ce1d3a0/raw/f0916b0b99187a9e839146fbd4e3d5bc26e5d97a/piplock.sh -O piplock.sh\nchmod a+x piplock.sh\nmv piplock.sh /usr/local/bin/piplock</code></pre><p>Add some simple requirements;</p><pre><code class=\"language-bash\">mkdir -p test-piplock\ncd test-piplock\necho -e \"retry\\nrequests\" &gt; requirements.txt</code></pre><p>Running <code>piplock</code> produces;</p><pre><code>certifi==2020.6.20\nchardet==3.0.4\ndecorator==4.4.2\nidna==2.10\npy==1.9.0\nrequests==2.24.0\nretry==0.9.2\nurllib3==1.25.10\n</code></pre><h3 id=\"conclusion\">Conclusion</h3><p>This flow allows you to maintain reproducible builds and consistent dependencies for development workflow, and at the same time enables developers to catch any potentially breaking changes for your library consumers—and all of this, while also keeping all of the developers on your team happy.</p>","comment_id":"5f1bae75621b4004d8308ebb","plaintext":"Overview\nOpinions vary on how one should make use of lock files, depending on whether the\nproject is the main application, or the project is actually a library that is\nmeant to be consumed by an application or another library.\n\nLock files are unquestionably useful if you build any application. However, if\nyou publish a library or CLI to a packing publisher like Pypi, lock files are \nnever published. Meaning your users and you might use different versions of\ndependencies, and that is fine if you internally still use lock files.\n\nPersonally, I see library projects as no different to any other project, and all\nprojects that build no matter where they get deployed should include a lock file\nfor the sake of  reproducible builds and team member collaboration.\n\nIf you are familiar with other languages that use lock file such as Node.js, you\nmight have asked at least once asked if Python can provide the same\nfunctionality.\nThe short answer is python has no concept of lock files, equally it can be\nargued python has no package dependency files at all and that's why there are\nmany options outside the core python team like setup.py, Pipfile, and the most\ncommon requirements.txt as a pattern for Pip.\n\nA long answer to the lock file question that you might have learned is that you\nmust painstakingly write out the packages by hand and your packages whole\ndependency tree, and theirs, and so on. Locking the versions using == of all of\nthese.\nYou might also be familiar with pip freeze to try make that easier, but everyone\nhas encountered the  complications of skipping the hand crafted requirements\nafter running pip freeze.\n\nThe Future\nPip are hard at work creating a new dependency resolver and will soon provide a\npython native solution to this age old problem. They are working with the\nvarious existing implementations (that use the current pip solution anyway)\nwhich has been represented in the solution iterations I've looked at. Pip is\nlooking a lot like Pipenv.\n\nIf you have a free minute, run pip check on some of your more complex projects\nand provide the Pip team\n[https://github.com/pypa/pip/issues?q=is%3Aopen+is%3Aissue+label%3A%22C%3A+new+resolver%22] \nwith your results for research. And consider a generous donation to support\ntheir efforts for the whole community and proprietary companies that rely on\ntheir work they give away for free.\n\nSolution\nWhile Pip are hard at developing their solution, I have written a very simple\nscript that automates that painstaking process of locking versions of a whole\nproject dependency tree.\n\nShow me the code!\n\n#!/usr/bin/env bash\n\nCWD=$(pwd)\nTMP_DIR=$1\n\nif [[ $EUID -eq 0 ]]; then\n   echo -e \"${RED}x${NC} This script must not be run as root\" \n   exit 1\nfi\nif [ -z $(which python3) ]; then\n  echo \"python3 not found\"\n  exit 1\nfi\nif [ -z $(which pip) ]; then\n  echo \"python3 pip not found\"\n  exit 1\nfi\nif [[ ! -f requirements.txt ]]; then\n  echo \"requirements.txt not found\"\n  exit 1\nfi\n\nif [[ -z \"${TMP_DIR}\" ]]; then\n  TMP_DIR=/tmp/piplock.$(date +'%s%N')\nfi\nif [[ ! -z \"$(which deactivate)\" ]]; then\n  deactivate\nfi\n\nmkdir -p ${TMP_DIR}\ncd ${TMP_DIR}\npython3 -m pip install -U pip\npython3 -m pip install -U virtualenv\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -q -U --no-cache-dir --isolated --no-warn-conflict -r ${CWD}/requirements.txt\ncheck=$(pip check --no-cache-dir --isolated)\ncheck_exit=$?\nif [[ $check_exit -ne 0 ]]; then\n  echo ${check}\n  exit 1\nfi\nLOCK=\"$(pip freeze)\"\ndeactivate\ncd ${CWD}\nrm -rf ${TMP_DIR}\necho ${LOCK}\n\nExample - locking requests dependency.\n\nIt is as easy, get the script;\n\nwget -q https://gist.githubusercontent.com/chrisdlangton/905a14e7a42118e0d6b5a45c8ce1d3a0/raw/f0916b0b99187a9e839146fbd4e3d5bc26e5d97a/piplock.sh -O piplock.sh\nchmod a+x piplock.sh\nmv piplock.sh /usr/local/bin/piplock\n\nAdd some simple requirements;\n\nmkdir -p test-piplock\ncd test-piplock\necho -e \"retry\\nrequests\" > requirements.txt\n\nRunning piplock produces;\n\ncertifi==2020.6.20\nchardet==3.0.4\ndecorator==4.4.2\nidna==2.10\npy==1.9.0\nrequests==2.24.0\nretry==0.9.2\nurllib3==1.25.10\n\n\nConclusion\nThis flow allows you to maintain reproducible builds and consistent dependencies\nfor development workflow, and at the same time enables developers to catch any\npotentially breaking changes for your library consumers—and all of this, while\nalso keeping all of the developers on your team happy.","feature_image":"__GHOST_URL__/content/images/2020/07/package-loc.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-07-25 04:00:53","created_by":"1","updated_at":"2021-03-31 13:59:05","updated_by":"1","published_at":"2020-07-25 05:18:08","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbcca","uuid":"9ac329c8-1ebb-41fc-8364-0bccdefacd07","title":"You don't know OWASP","slug":"you-dont-know-owasp","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://cloudsecurityalliance.org/research/contribute/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/Category:OWASP_CSA_Project\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-zap/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-amass/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-dependency-check/\"]],[\"a\",[\"href\",\"https://github.com/DevSlop\"]],[\"a\",[\"href\",\"https://twitter.com/shehackspurple\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-top-ten/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-api-security/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-mobile-top-10/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-docker-top-10/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-serverless-top-10/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/Category:OWASP_Source_Code_Flaws_Top_10_Project\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/Category:OWASP_Cloud_%E2%80%90_10_Project\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cloud-native-application-security-top-10/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-internet-of-things-top-10/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-machine-learning-security-top-10/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-top-10-privacy-risks/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-top-10-card-game/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cornucopia/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-snakes-and-ladders/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-secure-coding-dojo/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-pygoat/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-webgoat/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-node.js-goat/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-serverless-goat/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/OWASP_iGoat_Tool_Project\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/OWASP_Hacking_Lab\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-juice-shop/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/OWASP_Data_Exchange_Format_Project\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-application-security-verification-standard/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-big-data/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-software-component-verification-standard/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-container-security-verification-standard/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-risk-assessment-framework/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-vulnerability-management-guide/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cyber-controls-matrix/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cyber-defense-matrix/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-blockchain-security-framework/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-game-security-framework/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-mobile-security-testing-guide/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-go-secure-coding-practices-guide/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-samm/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-security-knowledge-framework/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-security-shepherd/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-web-security-testing-guide/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-anti-ransomware-guide/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-application-security-curriculum/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cloud-security-mentor/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-samuraiwtf/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-secureflag-community-edition/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-timegap-theory/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cheat-sheets/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-secure-headers/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-modsecurity-core-rule-set/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-access-log-parser/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-antisamy/\"]],[\"a\",[\"href\",\"https://owasp.org/www-community/OWASP_Validation_Regex_Repository\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/Category:OWASP_Best_Practices:_Use_of_Web_Application_Firewalls\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-appsec-pipeline/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-pytm/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-sso/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-android-security-inspector-toolkit/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-code-review-guide/\"]],[\"a\",[\"href\",\"https://github.com/OWASP/sonarqube\"]],[\"a\",[\"href\",\"https://owasp.org/www-community/Source_Code_Analysis_Tools\"]],[\"a\",[\"href\",\"https://owasp.org/www-community/Free_for_Open_Source_Application_Security_Tools\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-proactive-controls/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-attack-surface-detector/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-benchmark/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/OWASP_ASVS_Assessment_tool\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-securityrat/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-patton/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-threat-dragon/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-automated-threats-to-web-applications/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-cyber-security-enterprise-operations-architecture/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-web-mapper/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-dependency-track/\"]],[\"a\",[\"href\",\"https://wiki.owasp.org/index.php/OWASP_Periodic_Table_of_Vulnerabilities#tab=Periodic_Table_of_Vulnerabilities\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-o-saft/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-code-pulse/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-dpd/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-honeypot/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-securetea/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-information-security-metrics-bank/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-enterprise-security-api/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-core-business-application-security/\"]],[\"a\",[\"href\",\"https://www.zaproxy.org/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-defectdojo/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-owtf/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-securecodebox/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-vulnerable-web-applications-directory/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-maryam/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-bug-logging-tool/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-d4n155/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-damn-vulnerable-crypto-wallet/\"]],[\"a\",[\"href\",\"https://owasp.org/www-project-best-practices-in-vulnerability-disclosure-and-bug-bounty-programs/\"]],[\"strong\"],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"About a year ago I was just getting involved with the \"],[0,[0],1,\"CSA Working Group\"],[0,[],0,\" for CCM version 4 when I came across the \"],[0,[1],1,\"OWASP CSA Project\"],[0,[],0,\" and went on a little adventure to learn more about OWASP. Many readers probably think they're aware of OWASP, as I once thought I did, but you likely have no idea.\"]]],[1,\"p\",[[0,[],0,\"So you've heard about the OWASP Top 10 and maybe some tools like \"],[0,[2],1,\"ZAP\"],[0,[],0,\", \"],[0,[3],1,\"Amass\"],[0,[],0,\", \"],[0,[4],1,\"Dependency-Check\"],[0,[],0,\", or \"],[0,[5],1,\"DevSlop\"],[0,[],0,\" made famous by Tanya Janca (@\"],[0,[6],1,\"shehackspurple\"],[0,[],0,\"). But how many OWASP Top 10 projects do you know about (hint: there are 10+), and these projects are just a few of hundreds.\"]]],[1,\"p\",[[0,[],0,\"If  you've ever visited owasp.org I feel your pain on how hard it is to navigate and find anything.. So I've done that hard work and the following is a Top 10 (that's a joke) list of what I found interesting or useful;\"]]],[1,\"h2\",[[0,[],0,\"OWASP Top 10 Projects\"]]],[3,\"ul\",[[[0,[7],1,\"Flagship Top 10\"]],[[0,[8],1,\"API Security Top 10\"]],[[0,[9],1,\"Mobile Top 10\"]],[[0,[10],1,\"Docker Top 10\"]],[[0,[11],1,\"Serverless Top 10\"]],[[0,[12],1,\"Source Code Flaws Top 10 Project\"]],[[0,[13],1,\"Cloud Top 10 Project\"]],[[0,[14],1,\"Cloud-Native Application Security Top 10\"]],[[0,[15],1,\"Internet of Things top 10\"]],[[0,[16],1,\"Machine Learning Security Top 10\"]],[[0,[7],1,\"Top Ten Web Application Security Risks\"]],[[0,[17],1,\"Top 10 Privacy Risks\"]]]],[1,\"h2\",[[0,[],0,\"Games\"]]],[3,\"ul\",[[[0,[18],1,\"Top 10 Card Game\"]],[[0,[19],1,\"Cornucopia\"]],[[0,[20],1,\"Snakes And Ladders\"]]]],[1,\"h2\",[[0,[],0,\"Labs\"]]],[3,\"ul\",[[[0,[21],1,\"Secure Coding Dojo\"]],[[0,[22],1,\"Pygoat\"]],[[0,[23],1,\"WebGoat\"]],[[0,[24],1,\"Node.js Goat\"]],[[0,[25],1,\"Serverless Goat\"]],[[0,[26],1,\"iGoat\"]],[[0,[27],1,\"Hacking Lab\"]],[[0,[28],1,\"Juice Shop\"]]]],[1,\"h2\",[[0,[],0,\"Standards, Frameworks, Training, and Guidance\"]]],[3,\"ul\",[[[0,[29],1,\"Data Exchange Format Project\"]],[[0,[30],1,\"Application Security Verification Standard\"]],[[0,[31],1,\"Big Data Security Verification Standard\"]],[[0,[32],1,\"Software Component Verification Standard\"]],[[0,[33],1,\"Container Security Verification Standard\"]],[[0,[34],1,\"Risk Assessment Framework\"]],[[0,[35],1,\"Vulnerability Management Guide\"]],[[0,[36],1,\"Cyber Controls Matrix (OCCM)\"]],[[0,[37],1,\"Cyber Defense Matrix\"]],[[0,[38],1,\"Blockchain Security Framework\"]],[[0,[39],1,\"Game Security Framework\"]],[[0,[40],1,\"Mobile Security Testing Guide\"]],[[0,[41],1,\"Go Secure Coding Practices Guide\"]],[[0,[42],1,\"SAMM\"],[0,[],0,\" Software Assurance Maturity Model\"]],[[0,[43],1,\"SKF\"],[0,[],0,\" Security Knowledge Framework\"]],[[0,[44],1,\"Security Shepherd\"]],[[0,[45],1,\"Web Security Testing Guide\"]],[[0,[8],1,\"API Security Project\"]],[[0,[46],1,\"Anti-Ransomware Guide\"]],[[0,[47],1,\"Application Security Curriculum\"]],[[0,[48],1,\"Cloud Security Mentor\"]],[[0,[49],1,\"SamuraiWTF\"],[0,[],0,\" training and testing\"]],[[0,[50],1,\"SecureFlag\"],[0,[],0,\" capture the flag\"]],[[0,[51],1,\"TimeGap Theory\"],[0,[],0,\" capture the flag\"]]]],[1,\"h2\",[[0,[],0,\"Developers\"]]],[3,\"ul\",[[[0,[52],1,\"Cheat Sheet Series\"]],[[0,[53],1,\"Secure Headers Project\"]],[[0,[54],1,\"ModSecurity Core Rule Set\"]],[[0,[55],1,\"Access Log Parser\"]],[[0,[56],1,\"AntiSamy\"],[0,[],0,\" HTML/CSS compliance\"]],[[0,[57],1,\"Validation Regex Repository\"]],[[0,[58],1,\"Best Practices of WAF (Web Application Firewalls)\"]],[[0,[59],1,\"Appsec Pipeline\"]],[[0,[60],1,\"pytm\"],[0,[],0,\" threat modelling\"]],[[0,[],0,\"self-hosted \"],[0,[61],1,\"Single Sign-On\"]],[[0,[62],1,\"Android Security Inspector Toolkit\"]],[[0,[63],1,\"Code Review Guide\"]],[[0,[64],1,\"Sonarqube\"]],[[0,[65],1,\"Source Code Analysis Tools\"]],[[0,[66],1,\"Free for Open Source Application Security Tools\"]]]],[1,\"h2\",[[0,[],0,\"Defensive Security (Blue Team)\"]]],[3,\"ul\",[[[0,[67],1,\"Proactive Controls\"]],[[0,[68],1,\"Attack Surface Detector\"]],[[0,[69],1,\"OWASP Benchmark\"]],[[0,[70],1,\"ASVS Assessment tool\"]],[[0,[71],1,\"SecurityRAT\"],[0,[],0,\" Security Requirement Automation Tool\"]],[[0,[72],1,\"Patton\"],[0,[],0,\" vulnerability knowledge base\"]],[[0,[73],1,\"Threat Dragon\"]],[[0,[74],1,\"Automated Threats to Web Applications\"]],[[0,[75],1,\"Cyber Security Enterprise Operations Architecture\"]],[[0,[76],1,\"Web Mapper\"]],[[0,[4],1,\"Dependency-Check\"]],[[0,[77],1,\"Dependency-Track\"]],[[0,[78],1,\"Periodic Table of Vulnerabilities\"]],[[0,[79],1,\"O-Saft\"],[0,[],0,\" (TLS/SSL)\"]],[[0,[80],1,\"Code Pulse\"]],[[0,[81],1,\"DPD (DDOS Prevention using DPI)\"]],[[0,[82],1,\"Honeypot\"]],[[0,[83],1,\"SecureTea Project\"],[0,[],0,\" IDS/IPS\"]],[[0,[84],1,\"Information Security Metrics Bank\"]],[[0,[85],1,\"Enterprise Security API (ESAPI)\"]],[[0,[86],1,\"Core Business Application Security\"]]]],[1,\"h2\",[[0,[],0,\"Offensive Security (Red Team)\"]]],[3,\"ul\",[[[0,[3],1,\"Amass\"]],[[0,[87],1,\"Zed Attack Proxy\"],[0,[],0,\" (ZAP)\"]],[[0,[88],1,\"Defectdojo\"]],[[0,[28],1,\"Juice Shop\"]],[[0,[89],1,\"OWTF\"],[0,[],0,\" PenTest Exploit Framework\"]],[[0,[90],1,\"secureCodeBox\"]],[[0,[91],1,\"VWAD\"],[0,[],0,\" (Vulnerable Web Applications Directory)\"]],[[0,[],0,\"Desktop Goat (archived)\"]],[[0,[92],1,\"Maryam\"],[0,[],0,\" Open-Source intelligence (OSINT)\"]],[[0,[93],1,\"Bug Logging Tool\"]],[[0,[94],1,\"Tool Project D4N155\"],[0,[],0,\" (wordlists and reports)\"]],[[0,[95],1,\"Damn Vulnerable Crypto Wallet\"]],[[0,[96],1,\"Best Practices In Vulnerability Disclosure And Bug Bounty Programs\"]]]],[1,\"h2\",[[0,[],0,\"Practical use of OWASP\"]]],[1,\"p\",[[0,[],0,\"As a developer of 20 years, a consultant, and startup founder - the following examples are some practical uses of OWASP from my own experiences that hopefully help you too.\"]]],[1,\"h3\",[[0,[],0,\"Consulting\"]]],[1,\"p\",[[0,[],0,\"As a consultant I am often engaged in conversation with customers in high level positions, either as a deliberate part of the delivery which could be an advisory capacity, or just casually by some accident of being in the right place or meeting. There are also the many great people i get to work alongside that often take the opportunity to pick the brain of the consultant - which being human can often be quite the challenging predicament if you don't remain humble or are not completely honest. Essentially, consults need to be informed at the least, and the really trusted consultants tend to also have great sources to freely provide customers their own path to come to the same conclusions you provide as advice.\"]]],[1,\"p\",[[0,[],0,\"This preparedness is nothing more than the same kind of learning everyone experiences in school, the only difference is that you must adapt a wider breadth of sources to take from each where they fit the unique customer's situation.\"]]],[1,\"p\",[[0,[],0,\"Sources that have become immensely useful are;\"]]],[3,\"ol\",[[[0,[],0,\"Proactive Controls; when I needed to define some preventative measures for a new capability being proposed, these controls are mostly useful on their own merits but are a great tool to get the creative thinking started if you are stuck.\"]],[[0,[],0,\"OWASP Benchmark; for the predictable question we are always asked about \\\"hardening\\\". It's fine to be opinionated and have your own bespoke solutions that work, but it helps to lean on standards and highly scrutinised benchmarks like this and the ones from CIS so any friction can be addressed by a community of experiences, not personal opinions of the individual that hold little weight when made alone to a room of people who work together.\"]],[[0,[],0,\"List of Open Source Application Security Tools; basically the dream source for any qualitative analysis exercise in appsec. Are the client or team member proposing a proprietary tool without a community and they charge for supporting the simplest of use cases because they don't offer \"],[0,[97],1,\"usable\"],[0,[],0,\" documentation? Find a more feature rich and battle tested option here, you'd be sure these are heavily scrutinised \"],[0,[98],1,\"in public\"],[0,[],0,\" so any flaws are known up front instead of a 3 month project spike that ends in using one of these  tools anyway.\"]]]],[1,\"h3\",[[0,[],0,\"Project sanity checks\"]]],[1,\"p\",[[0,[],0,\"Before handing over a proposal or solution, take a quick look at the top 10s to make sure you're not going to miss something that would embarrass yourself or your employer. In fact, check back in on these top 10s and the standards listed above as the project kicks off and before a showcase just to make sure you're armed with a good roadmap and concise project plan that you understand well and can speak about articulately when those security related questions are asked, they always are right?\"]]],[1,\"h3\",[[0,[],0,\"Pull Request / Code reviews\"]]],[1,\"p\",[[0,[],0,\"Secure code reviews, or just a typical pull request code review, often means you are making suggested changes. The \"],[0,[63],1,\"Code Review Guide\"],[0,[],0,\" and following items are some great ways to approach this task;\"]]],[3,\"ol\",[[[0,[],0,\"Secure Headers Project; I can't stress this one enough. In the hundreds of projects i've code reviewed for customers, a memorable few came close to having no HTTP related bugs. The remaining were all easily compromised, by common trivial techniques that are preventable with some simple headers.\"]],[[0,[],0,\"Cheat Sheet Series, particularly V3 Session Management from the ASVS Index as this is another one of those defects that is responsible for common trivial vulnerabilities like session-riding that allows attackers to perform actions using legitimate sessions that were previously established and remain trusted indefinitely or for a significantly long time.\"],[1,[],0,0],[0,[],0,\"These cheat sheets have \"],[0,[97],1,\"real code\"],[0,[],0,\" examples - seriously a superpower when providing constructive criticism to improve code. We all know that suggesting changes is a loaded gun that often fires off into heated debate and often hurt feelings. Using real code samples from these cheat sheets has not only defused these situations more time than i can count, it has also made the developer feel empowered to figure things out for themselves with only the slightest nudge from my efforts. I like to think these interactions have led to some of these developers getting interested in security rather than hating or avoiding it.\"]],[[0,[],0,\"Logging and validation seem to always make an appearance in the Tops 10 lists, so make sure you check out the standards and guidance above. So many secrets are exposed through poor logging practices and if validation is used at all it is best to make sure you're not opening your app to vulnerabilities when you think you're doing the right thing by all validating inputs like you were taught.\"]]]],[1,\"blockquote\",[[0,[],0,\"I could go on\"]]],[1,\"p\",[[0,[],0,\"So there you have it, all the good parts of OWASP demystified.\"]]],[1,\"p\",[[0,[],0,\"Enjoy, spread the good word.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>About a year ago I was just getting involved with the <a href=\"https://cloudsecurityalliance.org/research/contribute/\">CSA Working Group</a> for CCM version 4 when I came across the <a href=\"https://wiki.owasp.org/index.php/Category:OWASP_CSA_Project\">OWASP CSA Project</a> and went on a little adventure to learn more about OWASP. Many readers probably think they're aware of OWASP, as I once thought I did, but you likely have no idea.</p><p>So you've heard about the OWASP Top 10 and maybe some tools like <a href=\"https://owasp.org/www-project-zap/\">ZAP</a>, <a href=\"https://owasp.org/www-project-amass/\">Amass</a>, <a href=\"https://owasp.org/www-project-dependency-check/\">Dependency-Check</a>, or <a href=\"https://github.com/DevSlop\">DevSlop</a> made famous by Tanya Janca (@<a href=\"https://twitter.com/shehackspurple\">shehackspurple</a>). But how many OWASP Top 10 projects do you know about (hint: there are 10+), and these projects are just a few of hundreds.</p><p>If  you've ever visited owasp.org I feel your pain on how hard it is to navigate and find anything.. So I've done that hard work and the following is a Top 10 (that's a joke) list of what I found interesting or useful;</p><h2 id=\"owasp-top-10-projects\">OWASP Top 10 Projects</h2><ul><li><a href=\"https://owasp.org/www-project-top-ten/\">Flagship Top 10</a></li><li><a href=\"https://owasp.org/www-project-api-security/\">API Security Top 10</a></li><li><a href=\"https://owasp.org/www-project-mobile-top-10/\">Mobile Top 10</a></li><li><a href=\"https://owasp.org/www-project-docker-top-10/\">Docker Top 10</a></li><li><a href=\"https://owasp.org/www-project-serverless-top-10/\">Serverless Top 10</a></li><li><a href=\"https://wiki.owasp.org/index.php/Category:OWASP_Source_Code_Flaws_Top_10_Project\">Source Code Flaws Top 10 Project</a></li><li><a href=\"https://wiki.owasp.org/index.php/Category:OWASP_Cloud_%E2%80%90_10_Project\">Cloud Top 10 Project</a></li><li><a href=\"https://owasp.org/www-project-cloud-native-application-security-top-10/\">Cloud-Native Application Security Top 10</a></li><li><a href=\"https://owasp.org/www-project-internet-of-things-top-10/\">Internet of Things top 10</a></li><li><a href=\"https://owasp.org/www-project-machine-learning-security-top-10/\">Machine Learning Security Top 10</a></li><li><a href=\"https://owasp.org/www-project-top-ten/\">Top Ten Web Application Security Risks</a></li><li><a href=\"https://owasp.org/www-project-top-10-privacy-risks/\">Top 10 Privacy Risks</a></li></ul><h2 id=\"games\">Games</h2><ul><li><a href=\"https://owasp.org/www-project-top-10-card-game/\">Top 10 Card Game</a></li><li><a href=\"https://owasp.org/www-project-cornucopia/\">Cornucopia</a></li><li><a href=\"https://owasp.org/www-project-snakes-and-ladders/\">Snakes And Ladders</a></li></ul><h2 id=\"labs\">Labs</h2><ul><li><a href=\"https://owasp.org/www-project-secure-coding-dojo/\">Secure Coding Dojo</a></li><li><a href=\"https://owasp.org/www-project-pygoat/\">Pygoat</a></li><li><a href=\"https://owasp.org/www-project-webgoat/\">WebGoat</a></li><li><a href=\"https://owasp.org/www-project-node.js-goat/\">Node.js Goat</a></li><li><a href=\"https://owasp.org/www-project-serverless-goat/\">Serverless Goat</a></li><li><a href=\"https://wiki.owasp.org/index.php/OWASP_iGoat_Tool_Project\">iGoat</a></li><li><a href=\"https://wiki.owasp.org/index.php/OWASP_Hacking_Lab\">Hacking Lab</a></li><li><a href=\"https://owasp.org/www-project-juice-shop/\">Juice Shop</a></li></ul><h2 id=\"standards-frameworks-training-and-guidance\">Standards, Frameworks, Training, and Guidance</h2><ul><li><a href=\"https://wiki.owasp.org/index.php/OWASP_Data_Exchange_Format_Project\">Data Exchange Format Project</a></li><li><a href=\"https://owasp.org/www-project-application-security-verification-standard/\">Application Security Verification Standard</a></li><li><a href=\"https://owasp.org/www-project-big-data/\">Big Data Security Verification Standard</a></li><li><a href=\"https://owasp.org/www-project-software-component-verification-standard/\">Software Component Verification Standard</a></li><li><a href=\"https://owasp.org/www-project-container-security-verification-standard/\">Container Security Verification Standard</a></li><li><a href=\"https://owasp.org/www-project-risk-assessment-framework/\">Risk Assessment Framework</a></li><li><a href=\"https://owasp.org/www-project-vulnerability-management-guide/\">Vulnerability Management Guide</a></li><li><a href=\"https://owasp.org/www-project-cyber-controls-matrix/\">Cyber Controls Matrix (OCCM)</a></li><li><a href=\"https://owasp.org/www-project-cyber-defense-matrix/\">Cyber Defense Matrix</a></li><li><a href=\"https://owasp.org/www-project-blockchain-security-framework/\">Blockchain Security Framework</a></li><li><a href=\"https://owasp.org/www-project-game-security-framework/\">Game Security Framework</a></li><li><a href=\"https://owasp.org/www-project-mobile-security-testing-guide/\">Mobile Security Testing Guide</a></li><li><a href=\"https://owasp.org/www-project-go-secure-coding-practices-guide/\">Go Secure Coding Practices Guide</a></li><li><a href=\"https://owasp.org/www-project-samm/\">SAMM</a> Software Assurance Maturity Model</li><li><a href=\"https://owasp.org/www-project-security-knowledge-framework/\">SKF</a> Security Knowledge Framework</li><li><a href=\"https://owasp.org/www-project-security-shepherd/\">Security Shepherd</a></li><li><a href=\"https://owasp.org/www-project-web-security-testing-guide/\">Web Security Testing Guide</a></li><li><a href=\"https://owasp.org/www-project-api-security/\">API Security Project</a></li><li><a href=\"https://owasp.org/www-project-anti-ransomware-guide/\">Anti-Ransomware Guide</a></li><li><a href=\"https://owasp.org/www-project-application-security-curriculum/\">Application Security Curriculum</a></li><li><a href=\"https://owasp.org/www-project-cloud-security-mentor/\">Cloud Security Mentor</a></li><li><a href=\"https://owasp.org/www-project-samuraiwtf/\">SamuraiWTF</a> training and testing</li><li><a href=\"https://owasp.org/www-project-secureflag-community-edition/\">SecureFlag</a> capture the flag</li><li><a href=\"https://owasp.org/www-project-timegap-theory/\">TimeGap Theory</a> capture the flag</li></ul><h2 id=\"developers\">Developers</h2><ul><li><a href=\"https://owasp.org/www-project-cheat-sheets/\">Cheat Sheet Series</a></li><li><a href=\"https://owasp.org/www-project-secure-headers/\">Secure Headers Project</a></li><li><a href=\"https://owasp.org/www-project-modsecurity-core-rule-set/\">ModSecurity Core Rule Set</a></li><li><a href=\"https://owasp.org/www-project-access-log-parser/\">Access Log Parser</a></li><li><a href=\"https://owasp.org/www-project-antisamy/\">AntiSamy</a> HTML/CSS compliance</li><li><a href=\"https://owasp.org/www-community/OWASP_Validation_Regex_Repository\">Validation Regex Repository</a></li><li><a href=\"https://wiki.owasp.org/index.php/Category:OWASP_Best_Practices:_Use_of_Web_Application_Firewalls\">Best Practices of WAF (Web Application Firewalls)</a></li><li><a href=\"https://owasp.org/www-project-appsec-pipeline/\">Appsec Pipeline</a></li><li><a href=\"https://owasp.org/www-project-pytm/\">pytm</a> threat modelling</li><li>self-hosted <a href=\"https://owasp.org/www-project-sso/\">Single Sign-On</a></li><li><a href=\"https://owasp.org/www-project-android-security-inspector-toolkit/\">Android Security Inspector Toolkit</a></li><li><a href=\"https://owasp.org/www-project-code-review-guide/\">Code Review Guide</a></li><li><a href=\"https://github.com/OWASP/sonarqube\">Sonarqube</a></li><li><a href=\"https://owasp.org/www-community/Source_Code_Analysis_Tools\">Source Code Analysis Tools</a></li><li><a href=\"https://owasp.org/www-community/Free_for_Open_Source_Application_Security_Tools\">Free for Open Source Application Security Tools</a></li></ul><h2 id=\"defensive-security-blue-team-\">Defensive Security (Blue Team)</h2><ul><li><a href=\"https://owasp.org/www-project-proactive-controls/\">Proactive Controls</a></li><li><a href=\"https://owasp.org/www-project-attack-surface-detector/\">Attack Surface Detector</a></li><li><a href=\"https://owasp.org/www-project-benchmark/\">OWASP Benchmark</a></li><li><a href=\"https://wiki.owasp.org/index.php/OWASP_ASVS_Assessment_tool\">ASVS Assessment tool</a></li><li><a href=\"https://owasp.org/www-project-securityrat/\">SecurityRAT</a> Security Requirement Automation Tool</li><li><a href=\"https://owasp.org/www-project-patton/\">Patton</a> vulnerability knowledge base</li><li><a href=\"https://owasp.org/www-project-threat-dragon/\">Threat Dragon</a></li><li><a href=\"https://owasp.org/www-project-automated-threats-to-web-applications/\">Automated Threats to Web Applications</a></li><li><a href=\"https://owasp.org/www-project-cyber-security-enterprise-operations-architecture/\">Cyber Security Enterprise Operations Architecture</a></li><li><a href=\"https://owasp.org/www-project-web-mapper/\">Web Mapper</a></li><li><a href=\"https://owasp.org/www-project-dependency-check/\">Dependency-Check</a></li><li><a href=\"https://owasp.org/www-project-dependency-track/\">Dependency-Track</a></li><li><a href=\"https://wiki.owasp.org/index.php/OWASP_Periodic_Table_of_Vulnerabilities#tab=Periodic_Table_of_Vulnerabilities\">Periodic Table of Vulnerabilities</a></li><li><a href=\"https://owasp.org/www-project-o-saft/\">O-Saft</a> (TLS/SSL)</li><li><a href=\"https://owasp.org/www-project-code-pulse/\">Code Pulse</a></li><li><a href=\"https://owasp.org/www-project-dpd/\">DPD (DDOS Prevention using DPI)</a></li><li><a href=\"https://owasp.org/www-project-honeypot/\">Honeypot</a></li><li><a href=\"https://owasp.org/www-project-securetea/\">SecureTea Project</a> IDS/IPS</li><li><a href=\"https://owasp.org/www-project-information-security-metrics-bank/\">Information Security Metrics Bank</a></li><li><a href=\"https://owasp.org/www-project-enterprise-security-api/\">Enterprise Security API (ESAPI)</a></li><li><a href=\"https://owasp.org/www-project-core-business-application-security/\">Core Business Application Security</a></li></ul><h2 id=\"offensive-security-red-team-\">Offensive Security (Red Team)</h2><ul><li><a href=\"https://owasp.org/www-project-amass/\">Amass</a></li><li><a href=\"https://www.zaproxy.org/\">Zed Attack Proxy</a> (ZAP)</li><li><a href=\"https://owasp.org/www-project-defectdojo/\">Defectdojo</a></li><li><a href=\"https://owasp.org/www-project-juice-shop/\">Juice Shop</a></li><li><a href=\"https://owasp.org/www-project-owtf/\">OWTF</a> PenTest Exploit Framework</li><li><a href=\"https://owasp.org/www-project-securecodebox/\">secureCodeBox</a></li><li><a href=\"https://owasp.org/www-project-vulnerable-web-applications-directory/\">VWAD</a> (Vulnerable Web Applications Directory)</li><li>Desktop Goat (archived)</li><li><a href=\"https://owasp.org/www-project-maryam/\">Maryam</a> Open-Source intelligence (OSINT)</li><li><a href=\"https://owasp.org/www-project-bug-logging-tool/\">Bug Logging Tool</a></li><li><a href=\"https://owasp.org/www-project-d4n155/\">Tool Project D4N155</a> (wordlists and reports)</li><li><a href=\"https://owasp.org/www-project-damn-vulnerable-crypto-wallet/\">Damn Vulnerable Crypto Wallet</a></li><li><a href=\"https://owasp.org/www-project-best-practices-in-vulnerability-disclosure-and-bug-bounty-programs/\">Best Practices In Vulnerability Disclosure And Bug Bounty Programs</a></li></ul><h2 id=\"practical-use-of-owasp\">Practical use of OWASP</h2><p>As a developer of 20 years, a consultant, and startup founder - the following examples are some practical uses of OWASP from my own experiences that hopefully help you too.</p><h3 id=\"consulting\">Consulting</h3><p>As a consultant I am often engaged in conversation with customers in high level positions, either as a deliberate part of the delivery which could be an advisory capacity, or just casually by some accident of being in the right place or meeting. There are also the many great people i get to work alongside that often take the opportunity to pick the brain of the consultant - which being human can often be quite the challenging predicament if you don't remain humble or are not completely honest. Essentially, consults need to be informed at the least, and the really trusted consultants tend to also have great sources to freely provide customers their own path to come to the same conclusions you provide as advice.</p><p>This preparedness is nothing more than the same kind of learning everyone experiences in school, the only difference is that you must adapt a wider breadth of sources to take from each where they fit the unique customer's situation.</p><p>Sources that have become immensely useful are;</p><ol><li>Proactive Controls; when I needed to define some preventative measures for a new capability being proposed, these controls are mostly useful on their own merits but are a great tool to get the creative thinking started if you are stuck.</li><li>OWASP Benchmark; for the predictable question we are always asked about \"hardening\". It's fine to be opinionated and have your own bespoke solutions that work, but it helps to lean on standards and highly scrutinised benchmarks like this and the ones from CIS so any friction can be addressed by a community of experiences, not personal opinions of the individual that hold little weight when made alone to a room of people who work together.</li><li>List of Open Source Application Security Tools; basically the dream source for any qualitative analysis exercise in appsec. Are the client or team member proposing a proprietary tool without a community and they charge for supporting the simplest of use cases because they don't offer <strong>usable</strong> documentation? Find a more feature rich and battle tested option here, you'd be sure these are heavily scrutinised <em>in public</em> so any flaws are known up front instead of a 3 month project spike that ends in using one of these  tools anyway.</li></ol><h3 id=\"project-sanity-checks\">Project sanity checks</h3><p>Before handing over a proposal or solution, take a quick look at the top 10s to make sure you're not going to miss something that would embarrass yourself or your employer. In fact, check back in on these top 10s and the standards listed above as the project kicks off and before a showcase just to make sure you're armed with a good roadmap and concise project plan that you understand well and can speak about articulately when those security related questions are asked, they always are right?</p><h3 id=\"pull-request-code-reviews\">Pull Request / Code reviews</h3><p>Secure code reviews, or just a typical pull request code review, often means you are making suggested changes. The <a href=\"https://owasp.org/www-project-code-review-guide/\">Code Review Guide</a> and following items are some great ways to approach this task;</p><ol><li>Secure Headers Project; I can't stress this one enough. In the hundreds of projects i've code reviewed for customers, a memorable few came close to having no HTTP related bugs. The remaining were all easily compromised, by common trivial techniques that are preventable with some simple headers.</li><li>Cheat Sheet Series, particularly V3 Session Management from the ASVS Index as this is another one of those defects that is responsible for common trivial vulnerabilities like session-riding that allows attackers to perform actions using legitimate sessions that were previously established and remain trusted indefinitely or for a significantly long time.<br>These cheat sheets have <strong>real code</strong> examples - seriously a superpower when providing constructive criticism to improve code. We all know that suggesting changes is a loaded gun that often fires off into heated debate and often hurt feelings. Using real code samples from these cheat sheets has not only defused these situations more time than i can count, it has also made the developer feel empowered to figure things out for themselves with only the slightest nudge from my efforts. I like to think these interactions have led to some of these developers getting interested in security rather than hating or avoiding it.</li><li>Logging and validation seem to always make an appearance in the Tops 10 lists, so make sure you check out the standards and guidance above. So many secrets are exposed through poor logging practices and if validation is used at all it is best to make sure you're not opening your app to vulnerabilities when you think you're doing the right thing by all validating inputs like you were taught.</li></ol><blockquote>I could go on</blockquote><p>So there you have it, all the good parts of OWASP demystified.</p><p>Enjoy, spread the good word.</p>","comment_id":"5f24f5da621b4004d8308f83","plaintext":"About a year ago I was just getting involved with the CSA Working Group\n[https://cloudsecurityalliance.org/research/contribute/] for CCM version 4 when\nI came across the OWASP CSA Project\n[https://wiki.owasp.org/index.php/Category:OWASP_CSA_Project] and went on a\nlittle adventure to learn more about OWASP. Many readers probably think they're\naware of OWASP, as I once thought I did, but you likely have no idea.\n\nSo you've heard about the OWASP Top 10 and maybe some tools like ZAP\n[https://owasp.org/www-project-zap/], Amass\n[https://owasp.org/www-project-amass/], Dependency-Check\n[https://owasp.org/www-project-dependency-check/], or DevSlop\n[https://github.com/DevSlop] made famous by Tanya Janca (@shehackspurple\n[https://twitter.com/shehackspurple]). But how many OWASP Top 10 projects do you\nknow about (hint: there are 10+), and these projects are just a few of hundreds.\n\nIf  you've ever visited owasp.org I feel your pain on how hard it is to navigate\nand find anything.. So I've done that hard work and the following is a Top 10\n(that's a joke) list of what I found interesting or useful;\n\nOWASP Top 10 Projects\n * Flagship Top 10 [https://owasp.org/www-project-top-ten/]\n * API Security Top 10 [https://owasp.org/www-project-api-security/]\n * Mobile Top 10 [https://owasp.org/www-project-mobile-top-10/]\n * Docker Top 10 [https://owasp.org/www-project-docker-top-10/]\n * Serverless Top 10 [https://owasp.org/www-project-serverless-top-10/]\n * Source Code Flaws Top 10 Project\n   [https://wiki.owasp.org/index.php/Category:OWASP_Source_Code_Flaws_Top_10_Project]\n * Cloud Top 10 Project\n   [https://wiki.owasp.org/index.php/Category:OWASP_Cloud_%E2%80%90_10_Project]\n * Cloud-Native Application Security Top 10\n   [https://owasp.org/www-project-cloud-native-application-security-top-10/]\n * Internet of Things top 10\n   [https://owasp.org/www-project-internet-of-things-top-10/]\n * Machine Learning Security Top 10\n   [https://owasp.org/www-project-machine-learning-security-top-10/]\n * Top Ten Web Application Security Risks\n   [https://owasp.org/www-project-top-ten/]\n * Top 10 Privacy Risks [https://owasp.org/www-project-top-10-privacy-risks/]\n\nGames\n * Top 10 Card Game [https://owasp.org/www-project-top-10-card-game/]\n * Cornucopia [https://owasp.org/www-project-cornucopia/]\n * Snakes And Ladders [https://owasp.org/www-project-snakes-and-ladders/]\n\nLabs\n * Secure Coding Dojo [https://owasp.org/www-project-secure-coding-dojo/]\n * Pygoat [https://owasp.org/www-project-pygoat/]\n * WebGoat [https://owasp.org/www-project-webgoat/]\n * Node.js Goat [https://owasp.org/www-project-node.js-goat/]\n * Serverless Goat [https://owasp.org/www-project-serverless-goat/]\n * iGoat [https://wiki.owasp.org/index.php/OWASP_iGoat_Tool_Project]\n * Hacking Lab [https://wiki.owasp.org/index.php/OWASP_Hacking_Lab]\n * Juice Shop [https://owasp.org/www-project-juice-shop/]\n\nStandards, Frameworks, Training, and Guidance\n * Data Exchange Format Project\n   [https://wiki.owasp.org/index.php/OWASP_Data_Exchange_Format_Project]\n * Application Security Verification Standard\n   [https://owasp.org/www-project-application-security-verification-standard/]\n * Big Data Security Verification Standard\n   [https://owasp.org/www-project-big-data/]\n * Software Component Verification Standard\n   [https://owasp.org/www-project-software-component-verification-standard/]\n * Container Security Verification Standard\n   [https://owasp.org/www-project-container-security-verification-standard/]\n * Risk Assessment Framework\n   [https://owasp.org/www-project-risk-assessment-framework/]\n * Vulnerability Management Guide\n   [https://owasp.org/www-project-vulnerability-management-guide/]\n * Cyber Controls Matrix (OCCM)\n   [https://owasp.org/www-project-cyber-controls-matrix/]\n * Cyber Defense Matrix [https://owasp.org/www-project-cyber-defense-matrix/]\n * Blockchain Security Framework\n   [https://owasp.org/www-project-blockchain-security-framework/]\n * Game Security Framework\n   [https://owasp.org/www-project-game-security-framework/]\n * Mobile Security Testing Guide\n   [https://owasp.org/www-project-mobile-security-testing-guide/]\n * Go Secure Coding Practices Guide\n   [https://owasp.org/www-project-go-secure-coding-practices-guide/]\n * SAMM [https://owasp.org/www-project-samm/] Software Assurance Maturity Model\n * SKF [https://owasp.org/www-project-security-knowledge-framework/] Security\n   Knowledge Framework\n * Security Shepherd [https://owasp.org/www-project-security-shepherd/]\n * Web Security Testing Guide\n   [https://owasp.org/www-project-web-security-testing-guide/]\n * API Security Project [https://owasp.org/www-project-api-security/]\n * Anti-Ransomware Guide [https://owasp.org/www-project-anti-ransomware-guide/]\n * Application Security Curriculum\n   [https://owasp.org/www-project-application-security-curriculum/]\n * Cloud Security Mentor [https://owasp.org/www-project-cloud-security-mentor/]\n * SamuraiWTF [https://owasp.org/www-project-samuraiwtf/] training and testing\n * SecureFlag [https://owasp.org/www-project-secureflag-community-edition/] \n   capture the flag\n * TimeGap Theory [https://owasp.org/www-project-timegap-theory/] capture the\n   flag\n\nDevelopers\n * Cheat Sheet Series [https://owasp.org/www-project-cheat-sheets/]\n * Secure Headers Project [https://owasp.org/www-project-secure-headers/]\n * ModSecurity Core Rule Set\n   [https://owasp.org/www-project-modsecurity-core-rule-set/]\n * Access Log Parser [https://owasp.org/www-project-access-log-parser/]\n * AntiSamy [https://owasp.org/www-project-antisamy/] HTML/CSS compliance\n * Validation Regex Repository\n   [https://owasp.org/www-community/OWASP_Validation_Regex_Repository]\n * Best Practices of WAF (Web Application Firewalls)\n   [https://wiki.owasp.org/index.php/Category:OWASP_Best_Practices:_Use_of_Web_Application_Firewalls]\n * Appsec Pipeline [https://owasp.org/www-project-appsec-pipeline/]\n * pytm [https://owasp.org/www-project-pytm/] threat modelling\n * self-hosted Single Sign-On [https://owasp.org/www-project-sso/]\n * Android Security Inspector Toolkit\n   [https://owasp.org/www-project-android-security-inspector-toolkit/]\n * Code Review Guide [https://owasp.org/www-project-code-review-guide/]\n * Sonarqube [https://github.com/OWASP/sonarqube]\n * Source Code Analysis Tools\n   [https://owasp.org/www-community/Source_Code_Analysis_Tools]\n * Free for Open Source Application Security Tools\n   [https://owasp.org/www-community/Free_for_Open_Source_Application_Security_Tools]\n\nDefensive Security (Blue Team)\n * Proactive Controls [https://owasp.org/www-project-proactive-controls/]\n * Attack Surface Detector\n   [https://owasp.org/www-project-attack-surface-detector/]\n * OWASP Benchmark [https://owasp.org/www-project-benchmark/]\n * ASVS Assessment tool\n   [https://wiki.owasp.org/index.php/OWASP_ASVS_Assessment_tool]\n * SecurityRAT [https://owasp.org/www-project-securityrat/] Security Requirement\n   Automation Tool\n * Patton [https://owasp.org/www-project-patton/] vulnerability knowledge base\n * Threat Dragon [https://owasp.org/www-project-threat-dragon/]\n * Automated Threats to Web Applications\n   [https://owasp.org/www-project-automated-threats-to-web-applications/]\n * Cyber Security Enterprise Operations Architecture\n   [https://owasp.org/www-project-cyber-security-enterprise-operations-architecture/]\n * Web Mapper [https://owasp.org/www-project-web-mapper/]\n * Dependency-Check [https://owasp.org/www-project-dependency-check/]\n * Dependency-Track [https://owasp.org/www-project-dependency-track/]\n * Periodic Table of Vulnerabilities\n   [https://wiki.owasp.org/index.php/OWASP_Periodic_Table_of_Vulnerabilities#tab=Periodic_Table_of_Vulnerabilities]\n * O-Saft [https://owasp.org/www-project-o-saft/] (TLS/SSL)\n * Code Pulse [https://owasp.org/www-project-code-pulse/]\n * DPD (DDOS Prevention using DPI) [https://owasp.org/www-project-dpd/]\n * Honeypot [https://owasp.org/www-project-honeypot/]\n * SecureTea Project [https://owasp.org/www-project-securetea/] IDS/IPS\n * Information Security Metrics Bank\n   [https://owasp.org/www-project-information-security-metrics-bank/]\n * Enterprise Security API (ESAPI)\n   [https://owasp.org/www-project-enterprise-security-api/]\n * Core Business Application Security\n   [https://owasp.org/www-project-core-business-application-security/]\n\nOffensive Security (Red Team)\n * Amass [https://owasp.org/www-project-amass/]\n * Zed Attack Proxy [https://www.zaproxy.org/] (ZAP)\n * Defectdojo [https://owasp.org/www-project-defectdojo/]\n * Juice Shop [https://owasp.org/www-project-juice-shop/]\n * OWTF [https://owasp.org/www-project-owtf/] PenTest Exploit Framework\n * secureCodeBox [https://owasp.org/www-project-securecodebox/]\n * VWAD [https://owasp.org/www-project-vulnerable-web-applications-directory/] \n   (Vulnerable Web Applications Directory)\n * Desktop Goat (archived)\n * Maryam [https://owasp.org/www-project-maryam/] Open-Source intelligence\n   (OSINT)\n * Bug Logging Tool [https://owasp.org/www-project-bug-logging-tool/]\n * Tool Project D4N155 [https://owasp.org/www-project-d4n155/] (wordlists and\n   reports)\n * Damn Vulnerable Crypto Wallet\n   [https://owasp.org/www-project-damn-vulnerable-crypto-wallet/]\n * Best Practices In Vulnerability Disclosure And Bug Bounty Programs\n   [https://owasp.org/www-project-best-practices-in-vulnerability-disclosure-and-bug-bounty-programs/]\n\nPractical use of OWASP\nAs a developer of 20 years, a consultant, and startup founder - the following\nexamples are some practical uses of OWASP from my own experiences that hopefully\nhelp you too.\n\nConsulting\nAs a consultant I am often engaged in conversation with customers in high level\npositions, either as a deliberate part of the delivery which could be an\nadvisory capacity, or just casually by some accident of being in the right place\nor meeting. There are also the many great people i get to work alongside that\noften take the opportunity to pick the brain of the consultant - which being\nhuman can often be quite the challenging predicament if you don't remain humble\nor are not completely honest. Essentially, consults need to be informed at the\nleast, and the really trusted consultants tend to also have great sources to\nfreely provide customers their own path to come to the same conclusions you\nprovide as advice.\n\nThis preparedness is nothing more than the same kind of learning everyone\nexperiences in school, the only difference is that you must adapt a wider\nbreadth of sources to take from each where they fit the unique customer's\nsituation.\n\nSources that have become immensely useful are;\n\n 1. Proactive Controls; when I needed to define some preventative measures for a\n    new capability being proposed, these controls are mostly useful on their own\n    merits but are a great tool to get the creative thinking started if you are\n    stuck.\n 2. OWASP Benchmark; for the predictable question we are always asked about\n    \"hardening\". It's fine to be opinionated and have your own bespoke solutions\n    that work, but it helps to lean on standards and highly scrutinised\n    benchmarks like this and the ones from CIS so any friction can be addressed\n    by a community of experiences, not personal opinions of the individual that\n    hold little weight when made alone to a room of people who work together.\n 3. List of Open Source Application Security Tools; basically the dream source\n    for any qualitative analysis exercise in appsec. Are the client or team\n    member proposing a proprietary tool without a community and they charge for\n    supporting the simplest of use cases because they don't offer usable \n    documentation? Find a more feature rich and battle tested option here, you'd\n    be sure these are heavily scrutinised in public so any flaws are known up\n    front instead of a 3 month project spike that ends in using one of these\n     tools anyway.\n\nProject sanity checks\nBefore handing over a proposal or solution, take a quick look at the top 10s to\nmake sure you're not going to miss something that would embarrass yourself or\nyour employer. In fact, check back in on these top 10s and the standards listed\nabove as the project kicks off and before a showcase just to make sure you're\narmed with a good roadmap and concise project plan that you understand well and\ncan speak about articulately when those security related questions are asked,\nthey always are right?\n\nPull Request / Code reviews\nSecure code reviews, or just a typical pull request code review, often means you\nare making suggested changes. The Code Review Guide\n[https://owasp.org/www-project-code-review-guide/] and following items are some\ngreat ways to approach this task;\n\n 1. Secure Headers Project; I can't stress this one enough. In the hundreds of\n    projects i've code reviewed for customers, a memorable few came close to\n    having no HTTP related bugs. The remaining were all easily compromised, by\n    common trivial techniques that are preventable with some simple headers.\n 2. Cheat Sheet Series, particularly V3 Session Management from the ASVS Index\n    as this is another one of those defects that is responsible for common\n    trivial vulnerabilities like session-riding that allows attackers to perform\n    actions using legitimate sessions that were previously established and\n    remain trusted indefinitely or for a significantly long time.\n    These cheat sheets have real code examples - seriously a superpower when\n    providing constructive criticism to improve code. We all know that\n    suggesting changes is a loaded gun that often fires off into heated debate\n    and often hurt feelings. Using real code samples from these cheat sheets has\n    not only defused these situations more time than i can count, it has also\n    made the developer feel empowered to figure things out for themselves with\n    only the slightest nudge from my efforts. I like to think these interactions\n    have led to some of these developers getting interested in security rather\n    than hating or avoiding it.\n 3. Logging and validation seem to always make an appearance in the Tops 10\n    lists, so make sure you check out the standards and guidance above. So many\n    secrets are exposed through poor logging practices and if validation is used\n    at all it is best to make sure you're not opening your app to\n    vulnerabilities when you think you're doing the right thing by all\n    validating inputs like you were taught.\n\n> I could go on\nSo there you have it, all the good parts of OWASP demystified.\n\nEnjoy, spread the good word.","feature_image":"__GHOST_URL__/content/images/2020/08/owasp-logo.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-08-01 04:55:54","created_by":"1","updated_at":"2021-03-31 13:58:47","updated_by":"1","published_at":"2020-08-01 02:43:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbccb","uuid":"943812a4-6b46-4752-a869-3973761a4705","title":"Zero-trust doesn't exist but that's OK","slug":"zero-trust-can-never-exist","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"em\"],[\"strong\"],[\"a\",[\"href\",\"https://tools.ietf.org/html/rfc2246#section-7.4.6\"]],[\"a\",[\"href\",\"https://tools.ietf.org/html/rfc5246#section-7.4.6\"]],[\"u\"]],\"sections\":[[1,\"h1\",[[0,[],0,\"Where zero-trust might exist\"]]],[3,\"ol\",[[[0,[],0,\"Scenarios that have \"],[0,[0],1,\"no data\"]],[[0,[],0,\"Scenarios with data that are never connected to power\"]]]],[1,\"p\",[[0,[],0,\"Why? \"]]],[1,\"p\",[[0,[],0,\"Because the moment data is accessible by a human, or a system with potential human access - you inherently \"],[0,[1],1,\"trust\"],[0,[],0,\" that human.\"]]],[1,\"h1\",[[0,[],0,\"What about identity?\"]]],[1,\"p\",[[0,[],0,\"Digital identity is the only identity. Digital identities can be bio-metric or software-born authentication processes. Where bio-metrics are \"],[0,[0],1,\"something you are\"],[0,[],0,\", there are also software-born processes based on the concept of \"],[0,[0],1,\"something you know\"],[0,[],0,\" or \"],[0,[0],1,\"something you have\"],[0,[],0,\" challenges.\"]]],[1,\"p\",[[0,[],0,\"As we just explained, bio-metrics are authentication and not an identity. The resultant identity is a digital representation of a bio-metric authentication process that is usually unique to an individual person with some exceptions with twins and situations where parents and children are similar enough for some facial recognition not be able to differentiate them as seen with the iPhone.\"]]],[1,\"p\",[[0,[],0,\"Another reason bio-metric authentication can never be assurance that the person behind a digital identity is the authorised person is because it cannot eliminate coercion or physical force by other unauthorised people.\"]]],[1,\"p\",[[0,[],0,\"Regardless of which person operates the authentication process, no digital identity offers assurance that they are a true representation of a unique person.\"]]],[1,\"p\",[[0,[],0,\"In all zero-trust scenarios there is a digital identity, thus we cannot be assured the identity is the unique person we authorise.\"]]],[1,\"p\",[[0,[],0,\"Essentially, bio-metric and digital identity are synonymous for software processes, like zero-trust.\"]]],[1,\"h1\",[[0,[],0,\"Who are authorised people?\"]]],[1,\"p\",[[0,[],0,\"When discussing identity or zero-trust, we always eventually need to use digital identity of an authorised person. What makes the authorised identity \\\"trusted\\\" at all times? That is literally the definition of \\\"trust\\\" so there can never be a zero-trust based on digital identity scenarios.\"]]],[1,\"p\",[[0,[],0,\"So how than can zero-trust provide any assurances that only authorised people and no other unauthorised person will? \"],[0,[1],1,\"Zero-trust must trust the authorised human\"],[0,[],0,\" in an ironic twist of inherent conflicting design implementations; zero-trust uses identities and therefore assumes you have at some point previously authorised the person using that identity - trusting your one-time authentication for all future zero-trust actions using htat identity.\"]]],[1,\"h1\",[[0,[],0,\"Can anything identity-based assure protection form  unauthorised humans?\"]]],[1,\"p\",[[0,[],0,\"Short answer, no.\"]]],[1,\"p\",[[0,[],0,\"Why? Let's look at the 2 most common identity-based systems; M/TLS and auto-segmentation.\"]]],[1,\"h2\",[[0,[],0,\"Mutual TLS\"]]],[1,\"p\",[[0,[],0,\"There is no RFC for Mutual TLS.\"]]],[1,\"p\",[[0,[],0,\"I have had people tell me that rfc8705 defines M/TLS, but each person who said this must have simply done an internet search and saw \"],[0,[0],1,\"Mutual-TLS\"],[0,[],0,\" in its very long title;\"]]],[1,\"blockquote\",[[0,[],0,\"rfc8705: OAuth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access Tokens\"]]],[1,\"p\",[[0,[0],1,\"But not actually read anything beyond the title..\"]]],[1,\"p\",[[0,[],0,\"To quote the description of the rfc; \"],[0,[1],1,\"OAuth clients are provided a mechanism for authentication to the authorization server using mutual TLS\"],[0,[],0,\". This is plain and clear this OAuth 2.0 rfc simply happens to implement M/TLS for its Access Tokens because they are Certificate-Bound and they needed support for both self-signed and PKI.\"]]],[1,\"p\",[[0,[],0,\"Notably, rfc8705 is a \"],[0,[1],1,\"proposed standard\"],[0,[],0,\" and Mutual TLS has been around \"],[0,[0],1,\"a lot\"],[0,[],0,\" longer than February 2020 when rfc8705 was proposed.\"],[1,[],0,0],[0,[],0,\"The actual Mutual TLS rfc was none other than the rfc2246 for \"],[0,[2],1,\"TLS 1.0 January 1999\"],[0,[],0,\", and slightly improved in rfc5246 \"],[0,[3],1,\"August 2008 for TLS 1.2\"],[0,[],0,\" when the condition for Diffie-Hellman of TLS 1.0 no longer made sense in TLS 1.2 due to the supported cipher suites. \"],[1,[],0,1],[0,[],0,\"As you see, Mutual TLS has existed for more than 2 decades even if the name itself only started being used by zero-trust marketing slogans more recently.\"]]],[1,\"p\",[[0,[],0,\"The reason Mutual TLS is often associated to zero-trust is simply because both the client and server identities are certificate based, the transport layer handshake requires an authentication based on the validity of these certificates. In normal TLS the client will establish a connection only after it validates the server provided certificate which assures the client that the server identity is true based on a signing process done by the Certificate Authority (CA), ergo there was a one-time authentication the server performed to obtain the certificate. To establish a M/TLS connection the normal TLS connection is first established, then a mutual process is then required by the server to validate the client certificate in the same way the client validated the server. Ergo, each peer validates the identity mutually to then start transmitting data using TLS.\"]]],[1,\"p\",[[0,[],0,\"Key nuance to fully understand here;\"]]],[3,\"ul\",[[[0,[],0,\"The server assumes the client has at some point previously performed a one-time authentication to obtain the certificate\"]],[[0,[],0,\"The server does not know if the original client has the certificate with the current client\"]],[[0,[],0,\"The client assumes the server has at some point previously performed a one-time authentication to obtain the certificate\"]],[[0,[],0,\"The client does not know if the original server has the certificate with the current server, unless the SNI extension is added and utilised correctly\"]],[[0,[],0,\"These mutually validated certificates provide authentication in the TLS handshake, they are unaware of the actually data being transmitted after M/TLS is established\"]]]],[1,\"p\",[[0,[],0,\"During M/TLS data transmission, validated certificates provide a digital identity that may be used for authorisation purposes only. M/TLS does not confirm any authentication has occurred at the time of transport, the only authentication that occurred was one-time for the server when obtaining a server certificate and one-time for the client when obtaining the client certificate.\"]]],[1,\"p\",[[0,[],0,\"For authentication to occur during data transmission you can implement OAuth, HMAC, or some other additional process that provides authentication.\"],[1,[],0,2],[0,[],0,\"Without authentication per data transport, you \\\"trust\\\" it was done by this client at some prior point because you are not performing any verification of your own apart from this client having possession of the certificate. With SNI used the client is at-least verifying the server identity is associated with the domain name they expected but there is equally no challenge therefore without any challenges on this data transport context there is no authentication.\"]]],[1,\"p\",[[0,[],0,\"The clear take away here is authentication requires a challenge, and one-time prior authentication and a resultant identity is significantly different to authentication performed in the context of each data access or transport - due to \"],[0,[0],1,\"inherent trust without verification\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Network Microsegmentation\"]]],[1,\"p\",[[0,[],0,\"There are 4 types of identities used in network segmentation implementations;\"]]],[1,\"p\",[[0,[1],1,\"1. machine-based\"],[1,[],0,3],[0,[],0,\"Similar to software licenses often being bound to unique hardware identifiers, identities used in network segmentation are often representing the lowest level operating system when there are virtual hosts or containers being used.\"],[1,[],0,4],[0,[],0,\"Common use case for this is network segmentation to assure data confidentiality in a multi-tenant environment where dedicated network infrastructure is infeasible or uneconomic.\"]]],[1,\"p\",[[0,[1],1,\"2. host-based\"],[1,[],0,5],[0,[],0,\"When there are virtual network interfaces, visualised operating system guests, or containers being used the identity is typically generated by the host to uniquely differentiate the guests.\"],[1,[],0,6],[0,[],0,\"In most cases, but not all, the host shares the unique identifier with guests.\"],[1,[],0,7],[0,[],0,\"Even when a guest is self-aware the guest's unique identifier may not be unique on the network due to conflicts that may arise when other hosts generate matching guest identifiers. This is common in container environments like Kubernetes because the guest identifiers cannot be unique as they are not reproducible using the hardware based identifiers as the seed to the resultant cryptographicly verifiable hash. For Kubernetes the cluster generates the guest identifier using a pseudo-random seed regardless of the host.\"],[1,[],0,8],[0,[],0,\"This means the identifier is unique only to the cluster, not unique to the network (which often only consists of the Kubernetes cluster, but not limited to)\"]]],[1,\"p\",[[0,[1],1,\"3 application based\"],[0,[],0,\" (microsegmentation)\"],[1,[],0,9],[0,[],0,\"Very similar to the above scenario, there is a private certificate authority or public key infrastructure (PKI) that issue certificates to clients given the client provided unique identifier. Clients generally request their unique identifier from the host but in some environments like virtual machine guests and Kubernetes. But they often move to a pseudo-random value instead if they encounter any conflicts, sometimes even starting with this pseudo-random value if the administrator has encountered conflicts in their personal past experiences.\"],[1,[],0,10],[0,[],0,\"This is going to generate an identity for your application, but like the host-based approach it generally cannot be assumed it is a identity that remains unique to the network. It is possible, but we cannot assume it is fact.\"]]],[1,\"p\",[[0,[1],1,\"4. account based\"],[1,[],0,11],[0,[],0,\"The least common method in context to network segmentation, but it is becoming more common in cloud native architectures.\"],[1,[],0,12],[0,[],0,\"The way this works is where a cloud service provider can assure each account is sufficiently segregated and you register multiple accounts and while you control each account you actually treat them as strong segregation control in workload architectures.\"],[1,[],0,13],[0,[],0,\"Commonly you have an account separate from workloads for audit logs, identity, and other highly sensitive purposes. You may also use multiple accounts to segregate each SDLC environment, i.e. production and any other non-production named account that will never need to be connected to production. \"],[0,[1],1,\"Warning\"],[0,[],0,\": these are semantically names only, do not think for a moment that your developer environment is exempt from the same security controls as production - \"],[0,[4],1,\"they are both an account in a public cloud\"],[0,[],0,\" and you simply decided to semantically name them differently but that name does not change the risk in any way.\"]]],[1,\"p\",[[0,[],0,\"In the context of zero-trust and it's common referred to by network segmentation implementations, all 4 cases;\"]]],[1,\"p\",[[0,[],0,\"- uses identities for peers, but only some implementations will be sufficiently unique\"],[1,[],0,14],[0,[],0,\"- none are representation of an individual person\"],[1,[],0,15],[0,[],0,\"- none implement an authentication challenge per data access or transport\"],[1,[],0,16],[0,[],0,\"- all authentication is done one-time in the past\"],[1,[],0,17],[0,[],0,\"- the peers are always assumed/trusted to be previously authenticated\"],[1,[],0,18],[0,[],0,\"- identity is trusted to be that of the identity it represents, and not shared/copied/exposed\"]]],[1,\"h1\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Zero-trust security means that no one is trusted by default from inside or outside the network, and verification is required from everyone trying to gain access to resources on the network. Each scenario is identity-based, and with that there is inherent trust and zero-challenges with assumed verification outside the data exchange.\"]]],[1,\"p\",[[0,[],0,\"We began this post stating zero-trust cannot exist, but the reality is much worse. All current implementations using only the identity do not even limit data access or decrease trust over any other system that uses actual authentication processes per data access and transmission.\"]]],[1,\"p\",[[0,[1],1,\"But that's OK\"]]],[1,\"p\",[[0,[],0,\"Because we have a massive number of great authentication options, some that are session-based (app logins/time-based tokens), some that rely on pre-shared out-of-band secrets (OAuth/HMAC) that must be used per data access, and some that actually occur anytime data is accessed and expire the moment that access has ceased (database connections). The leading method for zero-trust (excluding micro-segmentation and M/TLS) is Multi-factor authentication (MFA) for the user performing actions (and we know the trust this requires from above), void of a person operator where we are in context of machine-to-machine there cannot be a MFA process, there must be some other machine friendly authentication method based on a secret \"],[0,[1],1,\"which requires trust\"],[0,[],0,\" in how that machine obtains and stores the secret.\"]]],[1,\"p\",[[0,[],0,\"Our existing methods of authentication are already better security options for data access than zero-trust, network segmentation, and Mutual TLS can ever provide. Each has it's own use case, and offer a defence-in-depth layer critically important to use with authentication for data access. But do not be fooled that zero-trust exists or that anything identity-based can actually provide safe data access, safer than public, but less safe than real authentication provides.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<h1 id=\"where-zero-trust-might-exist\">Where zero-trust might exist</h1><ol><li>Scenarios that have <em>no data</em></li><li>Scenarios with data that are never connected to power</li></ol><p>Why? </p><p>Because the moment data is accessible by a human, or a system with potential human access - you inherently <strong>trust</strong> that human.</p><h1 id=\"what-about-identity\">What about identity?</h1><p>Digital identity is the only identity. Digital identities can be bio-metric or software-born authentication processes. Where bio-metrics are <em>something you are</em>, there are also software-born processes based on the concept of <em>something you know</em> or <em>something you have</em> challenges.</p><p>As we just explained, bio-metrics are authentication and not an identity. The resultant identity is a digital representation of a bio-metric authentication process that is usually unique to an individual person with some exceptions with twins and situations where parents and children are similar enough for some facial recognition not be able to differentiate them as seen with the iPhone.</p><p>Another reason bio-metric authentication can never be assurance that the person behind a digital identity is the authorised person is because it cannot eliminate coercion or physical force by other unauthorised people.</p><p>Regardless of which person operates the authentication process, no digital identity offers assurance that they are a true representation of a unique person.</p><p>In all zero-trust scenarios there is a digital identity, thus we cannot be assured the identity is the unique person we authorise.</p><p>Essentially, bio-metric and digital identity are synonymous for software processes, like zero-trust.</p><h1 id=\"who-are-authorised-people\">Who are authorised people?</h1><p>When discussing identity or zero-trust, we always eventually need to use digital identity of an authorised person. What makes the authorised identity \"trusted\" at all times? That is literally the definition of \"trust\" so there can never be a zero-trust based on digital identity scenarios.</p><p>So how than can zero-trust provide any assurances that only authorised people and no other unauthorised person will? <strong>Zero-trust must trust the authorised human</strong> in an ironic twist of inherent conflicting design implementations; zero-trust uses identities and therefore assumes you have at some point previously authorised the person using that identity - trusting your one-time authentication for all future zero-trust actions using htat identity.</p><h1 id=\"can-anything-identity-based-assure-protection-form-unauthorised-humans\">Can anything identity-based assure protection form  unauthorised humans?</h1><p>Short answer, no.</p><p>Why? Let's look at the 2 most common identity-based systems; M/TLS and auto-segmentation.</p><h2 id=\"mutual-tls\">Mutual TLS</h2><p>There is no RFC for Mutual TLS.</p><p>I have had people tell me that rfc8705 defines M/TLS, but each person who said this must have simply done an internet search and saw <em>Mutual-TLS</em> in its very long title;</p><blockquote>rfc8705: OAuth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access Tokens</blockquote><p><em>But not actually read anything beyond the title..</em></p><p>To quote the description of the rfc; <strong>OAuth clients are provided a mechanism for authentication to the authorization server using mutual TLS</strong>. This is plain and clear this OAuth 2.0 rfc simply happens to implement M/TLS for its Access Tokens because they are Certificate-Bound and they needed support for both self-signed and PKI.</p><p>Notably, rfc8705 is a <strong>proposed standard</strong> and Mutual TLS has been around <em>a lot</em> longer than February 2020 when rfc8705 was proposed.<br>The actual Mutual TLS rfc was none other than the rfc2246 for <a href=\"https://tools.ietf.org/html/rfc2246#section-7.4.6\">TLS 1.0 January 1999</a>, and slightly improved in rfc5246 <a href=\"https://tools.ietf.org/html/rfc5246#section-7.4.6\">August 2008 for TLS 1.2</a> when the condition for Diffie-Hellman of TLS 1.0 no longer made sense in TLS 1.2 due to the supported cipher suites. <br>As you see, Mutual TLS has existed for more than 2 decades even if the name itself only started being used by zero-trust marketing slogans more recently.</p><p>The reason Mutual TLS is often associated to zero-trust is simply because both the client and server identities are certificate based, the transport layer handshake requires an authentication based on the validity of these certificates. In normal TLS the client will establish a connection only after it validates the server provided certificate which assures the client that the server identity is true based on a signing process done by the Certificate Authority (CA), ergo there was a one-time authentication the server performed to obtain the certificate. To establish a M/TLS connection the normal TLS connection is first established, then a mutual process is then required by the server to validate the client certificate in the same way the client validated the server. Ergo, each peer validates the identity mutually to then start transmitting data using TLS.</p><p>Key nuance to fully understand here;</p><ul><li>The server assumes the client has at some point previously performed a one-time authentication to obtain the certificate</li><li>The server does not know if the original client has the certificate with the current client</li><li>The client assumes the server has at some point previously performed a one-time authentication to obtain the certificate</li><li>The client does not know if the original server has the certificate with the current server, unless the SNI extension is added and utilised correctly</li><li>These mutually validated certificates provide authentication in the TLS handshake, they are unaware of the actually data being transmitted after M/TLS is established</li></ul><p>During M/TLS data transmission, validated certificates provide a digital identity that may be used for authorisation purposes only. M/TLS does not confirm any authentication has occurred at the time of transport, the only authentication that occurred was one-time for the server when obtaining a server certificate and one-time for the client when obtaining the client certificate.</p><p>For authentication to occur during data transmission you can implement OAuth, HMAC, or some other additional process that provides authentication.<br>Without authentication per data transport, you \"trust\" it was done by this client at some prior point because you are not performing any verification of your own apart from this client having possession of the certificate. With SNI used the client is at-least verifying the server identity is associated with the domain name they expected but there is equally no challenge therefore without any challenges on this data transport context there is no authentication.</p><p>The clear take away here is authentication requires a challenge, and one-time prior authentication and a resultant identity is significantly different to authentication performed in the context of each data access or transport - due to <em>inherent trust without verification</em>.</p><h2 id=\"network-microsegmentation\">Network Microsegmentation</h2><p>There are 4 types of identities used in network segmentation implementations;</p><p><strong>1. machine-based</strong><br>Similar to software licenses often being bound to unique hardware identifiers, identities used in network segmentation are often representing the lowest level operating system when there are virtual hosts or containers being used.<br>Common use case for this is network segmentation to assure data confidentiality in a multi-tenant environment where dedicated network infrastructure is infeasible or uneconomic.</p><p><strong>2. host-based</strong><br>When there are virtual network interfaces, visualised operating system guests, or containers being used the identity is typically generated by the host to uniquely differentiate the guests.<br>In most cases, but not all, the host shares the unique identifier with guests.<br>Even when a guest is self-aware the guest's unique identifier may not be unique on the network due to conflicts that may arise when other hosts generate matching guest identifiers. This is common in container environments like Kubernetes because the guest identifiers cannot be unique as they are not reproducible using the hardware based identifiers as the seed to the resultant cryptographicly verifiable hash. For Kubernetes the cluster generates the guest identifier using a pseudo-random seed regardless of the host.<br>This means the identifier is unique only to the cluster, not unique to the network (which often only consists of the Kubernetes cluster, but not limited to)</p><p><strong>3 application based</strong> (microsegmentation)<br>Very similar to the above scenario, there is a private certificate authority or public key infrastructure (PKI) that issue certificates to clients given the client provided unique identifier. Clients generally request their unique identifier from the host but in some environments like virtual machine guests and Kubernetes. But they often move to a pseudo-random value instead if they encounter any conflicts, sometimes even starting with this pseudo-random value if the administrator has encountered conflicts in their personal past experiences.<br>This is going to generate an identity for your application, but like the host-based approach it generally cannot be assumed it is a identity that remains unique to the network. It is possible, but we cannot assume it is fact.</p><p><strong>4. account based</strong><br>The least common method in context to network segmentation, but it is becoming more common in cloud native architectures.<br>The way this works is where a cloud service provider can assure each account is sufficiently segregated and you register multiple accounts and while you control each account you actually treat them as strong segregation control in workload architectures.<br>Commonly you have an account separate from workloads for audit logs, identity, and other highly sensitive purposes. You may also use multiple accounts to segregate each SDLC environment, i.e. production and any other non-production named account that will never need to be connected to production. <strong>Warning</strong>: these are semantically names only, do not think for a moment that your developer environment is exempt from the same security controls as production - <u>they are both an account in a public cloud</u> and you simply decided to semantically name them differently but that name does not change the risk in any way.</p><p>In the context of zero-trust and it's common referred to by network segmentation implementations, all 4 cases;</p><p>- uses identities for peers, but only some implementations will be sufficiently unique<br>- none are representation of an individual person<br>- none implement an authentication challenge per data access or transport<br>- all authentication is done one-time in the past<br>- the peers are always assumed/trusted to be previously authenticated<br>- identity is trusted to be that of the identity it represents, and not shared/copied/exposed</p><h1 id=\"conclusion\">Conclusion</h1><p>Zero-trust security means that no one is trusted by default from inside or outside the network, and verification is required from everyone trying to gain access to resources on the network. Each scenario is identity-based, and with that there is inherent trust and zero-challenges with assumed verification outside the data exchange.</p><p>We began this post stating zero-trust cannot exist, but the reality is much worse. All current implementations using only the identity do not even limit data access or decrease trust over any other system that uses actual authentication processes per data access and transmission.</p><p><strong>But that's OK</strong></p><p>Because we have a massive number of great authentication options, some that are session-based (app logins/time-based tokens), some that rely on pre-shared out-of-band secrets (OAuth/HMAC) that must be used per data access, and some that actually occur anytime data is accessed and expire the moment that access has ceased (database connections). The leading method for zero-trust (excluding micro-segmentation and M/TLS) is Multi-factor authentication (MFA) for the user performing actions (and we know the trust this requires from above), void of a person operator where we are in context of machine-to-machine there cannot be a MFA process, there must be some other machine friendly authentication method based on a secret <strong>which requires trust</strong> in how that machine obtains and stores the secret.</p><p>Our existing methods of authentication are already better security options for data access than zero-trust, network segmentation, and Mutual TLS can ever provide. Each has it's own use case, and offer a defence-in-depth layer critically important to use with authentication for data access. But do not be fooled that zero-trust exists or that anything identity-based can actually provide safe data access, safer than public, but less safe than real authentication provides.</p>","comment_id":"5f7960c6621b4004d83092b4","plaintext":"Where zero-trust might exist\n 1. Scenarios that have no data\n 2. Scenarios with data that are never connected to power\n\nWhy? \n\nBecause the moment data is accessible by a human, or a system with potential\nhuman access - you inherently trust that human.\n\nWhat about identity?\nDigital identity is the only identity. Digital identities can be bio-metric or\nsoftware-born authentication processes. Where bio-metrics are something you are,\nthere are also software-born processes based on the concept of something you\nknow or something you have challenges.\n\nAs we just explained, bio-metrics are authentication and not an identity. The\nresultant identity is a digital representation of a bio-metric authentication\nprocess that is usually unique to an individual person with some exceptions with\ntwins and situations where parents and children are similar enough for some\nfacial recognition not be able to differentiate them as seen with the iPhone.\n\nAnother reason bio-metric authentication can never be assurance that the person\nbehind a digital identity is the authorised person is because it cannot\neliminate coercion or physical force by other unauthorised people.\n\nRegardless of which person operates the authentication process, no digital\nidentity offers assurance that they are a true representation of a unique\nperson.\n\nIn all zero-trust scenarios there is a digital identity, thus we cannot be\nassured the identity is the unique person we authorise.\n\nEssentially, bio-metric and digital identity are synonymous for software\nprocesses, like zero-trust.\n\nWho are authorised people?\nWhen discussing identity or zero-trust, we always eventually need to use digital\nidentity of an authorised person. What makes the authorised identity \"trusted\"\nat all times? That is literally the definition of \"trust\" so there can never be\na zero-trust based on digital identity scenarios.\n\nSo how than can zero-trust provide any assurances that only authorised people\nand no other unauthorised person will? Zero-trust must trust the authorised\nhuman in an ironic twist of inherent conflicting design implementations;\nzero-trust uses identities and therefore assumes you have at some point\npreviously authorised the person using that identity - trusting your one-time\nauthentication for all future zero-trust actions using htat identity.\n\nCan anything identity-based assure protection form  unauthorised humans?\nShort answer, no.\n\nWhy? Let's look at the 2 most common identity-based systems; M/TLS and\nauto-segmentation.\n\nMutual TLS\nThere is no RFC for Mutual TLS.\n\nI have had people tell me that rfc8705 defines M/TLS, but each person who said\nthis must have simply done an internet search and saw Mutual-TLS in its very\nlong title;\n\n> rfc8705: OAuth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access\nTokens\nBut not actually read anything beyond the title..\n\nTo quote the description of the rfc; OAuth clients are provided a mechanism for\nauthentication to the authorization server using mutual TLS. This is plain and\nclear this OAuth 2.0 rfc simply happens to implement M/TLS for its Access Tokens\nbecause they are Certificate-Bound and they needed support for both self-signed\nand PKI.\n\nNotably, rfc8705 is a proposed standard and Mutual TLS has been around a lot \nlonger than February 2020 when rfc8705 was proposed.\nThe actual Mutual TLS rfc was none other than the rfc2246 for TLS 1.0 January\n1999 [https://tools.ietf.org/html/rfc2246#section-7.4.6], and slightly improved\nin rfc5246 August 2008 for TLS 1.2\n[https://tools.ietf.org/html/rfc5246#section-7.4.6] when the condition for\nDiffie-Hellman of TLS 1.0 no longer made sense in TLS 1.2 due to the supported\ncipher suites. \nAs you see, Mutual TLS has existed for more than 2 decades even if the name\nitself only started being used by zero-trust marketing slogans more recently.\n\nThe reason Mutual TLS is often associated to zero-trust is simply because both\nthe client and server identities are certificate based, the transport layer\nhandshake requires an authentication based on the validity of these\ncertificates. In normal TLS the client will establish a connection only after it\nvalidates the server provided certificate which assures the client that the\nserver identity is true based on a signing process done by the Certificate\nAuthority (CA), ergo there was a one-time authentication the server performed to\nobtain the certificate. To establish a M/TLS connection the normal TLS\nconnection is first established, then a mutual process is then required by the\nserver to validate the client certificate in the same way the client validated\nthe server. Ergo, each peer validates the identity mutually to then start\ntransmitting data using TLS.\n\nKey nuance to fully understand here;\n\n * The server assumes the client has at some point previously performed a\n   one-time authentication to obtain the certificate\n * The server does not know if the original client has the certificate with the\n   current client\n * The client assumes the server has at some point previously performed a\n   one-time authentication to obtain the certificate\n * The client does not know if the original server has the certificate with the\n   current server, unless the SNI extension is added and utilised correctly\n * These mutually validated certificates provide authentication in the TLS\n   handshake, they are unaware of the actually data being transmitted after\n   M/TLS is established\n\nDuring M/TLS data transmission, validated certificates provide a digital\nidentity that may be used for authorisation purposes only. M/TLS does not\nconfirm any authentication has occurred at the time of transport, the only\nauthentication that occurred was one-time for the server when obtaining a server\ncertificate and one-time for the client when obtaining the client certificate.\n\nFor authentication to occur during data transmission you can implement OAuth,\nHMAC, or some other additional process that provides authentication.\nWithout authentication per data transport, you \"trust\" it was done by this\nclient at some prior point because you are not performing any verification of\nyour own apart from this client having possession of the certificate. With SNI\nused the client is at-least verifying the server identity is associated with the\ndomain name they expected but there is equally no challenge therefore without\nany challenges on this data transport context there is no authentication.\n\nThe clear take away here is authentication requires a challenge, and one-time\nprior authentication and a resultant identity is significantly different to\nauthentication performed in the context of each data access or transport - due\nto inherent trust without verification.\n\nNetwork Microsegmentation\nThere are 4 types of identities used in network segmentation implementations;\n\n1. machine-based\nSimilar to software licenses often being bound to unique hardware identifiers,\nidentities used in network segmentation are often representing the lowest level\noperating system when there are virtual hosts or containers being used.\nCommon use case for this is network segmentation to assure data confidentiality\nin a multi-tenant environment where dedicated network infrastructure is\ninfeasible or uneconomic.\n\n2. host-based\nWhen there are virtual network interfaces, visualised operating system guests,\nor containers being used the identity is typically generated by the host to\nuniquely differentiate the guests.\nIn most cases, but not all, the host shares the unique identifier with guests.\nEven when a guest is self-aware the guest's unique identifier may not be unique\non the network due to conflicts that may arise when other hosts generate\nmatching guest identifiers. This is common in container environments like\nKubernetes because the guest identifiers cannot be unique as they are not\nreproducible using the hardware based identifiers as the seed to the resultant\ncryptographicly verifiable hash. For Kubernetes the cluster generates the guest\nidentifier using a pseudo-random seed regardless of the host.\nThis means the identifier is unique only to the cluster, not unique to the\nnetwork (which often only consists of the Kubernetes cluster, but not limited\nto)\n\n3 application based (microsegmentation)\nVery similar to the above scenario, there is a private certificate authority or\npublic key infrastructure (PKI) that issue certificates to clients given the\nclient provided unique identifier. Clients generally request their unique\nidentifier from the host but in some environments like virtual machine guests\nand Kubernetes. But they often move to a pseudo-random value instead if they\nencounter any conflicts, sometimes even starting with this pseudo-random value\nif the administrator has encountered conflicts in their personal past\nexperiences.\nThis is going to generate an identity for your application, but like the\nhost-based approach it generally cannot be assumed it is a identity that remains\nunique to the network. It is possible, but we cannot assume it is fact.\n\n4. account based\nThe least common method in context to network segmentation, but it is becoming\nmore common in cloud native architectures.\nThe way this works is where a cloud service provider can assure each account is\nsufficiently segregated and you register multiple accounts and while you control\neach account you actually treat them as strong segregation control in workload\narchitectures.\nCommonly you have an account separate from workloads for audit logs, identity,\nand other highly sensitive purposes. You may also use multiple accounts to\nsegregate each SDLC environment, i.e. production and any other non-production\nnamed account that will never need to be connected to production. Warning: these\nare semantically names only, do not think for a moment that your developer\nenvironment is exempt from the same security controls as production - they are\nboth an account in a public cloud and you simply decided to semantically name\nthem differently but that name does not change the risk in any way.\n\nIn the context of zero-trust and it's common referred to by network segmentation\nimplementations, all 4 cases;\n\n- uses identities for peers, but only some implementations will be sufficiently\nunique\n- none are representation of an individual person\n- none implement an authentication challenge per data access or transport\n- all authentication is done one-time in the past\n- the peers are always assumed/trusted to be previously authenticated\n- identity is trusted to be that of the identity it represents, and not\nshared/copied/exposed\n\nConclusion\nZero-trust security means that no one is trusted by default from inside or\noutside the network, and verification is required from everyone trying to gain\naccess to resources on the network. Each scenario is identity-based, and with\nthat there is inherent trust and zero-challenges with assumed verification\noutside the data exchange.\n\nWe began this post stating zero-trust cannot exist, but the reality is much\nworse. All current implementations using only the identity do not even limit\ndata access or decrease trust over any other system that uses actual\nauthentication processes per data access and transmission.\n\nBut that's OK\n\nBecause we have a massive number of great authentication options, some that are\nsession-based (app logins/time-based tokens), some that rely on pre-shared\nout-of-band secrets (OAuth/HMAC) that must be used per data access, and some\nthat actually occur anytime data is accessed and expire the moment that access\nhas ceased (database connections). The leading method for zero-trust (excluding\nmicro-segmentation and M/TLS) is Multi-factor authentication (MFA) for the user\nperforming actions (and we know the trust this requires from above), void of a\nperson operator where we are in context of machine-to-machine there cannot be a\nMFA process, there must be some other machine friendly authentication method\nbased on a secret which requires trust in how that machine obtains and stores\nthe secret.\n\nOur existing methods of authentication are already better security options for\ndata access than zero-trust, network segmentation, and Mutual TLS can ever\nprovide. Each has it's own use case, and offer a defence-in-depth layer\ncritically important to use with authentication for data access. But do not be\nfooled that zero-trust exists or that anything identity-based can actually\nprovide safe data access, safer than public, but less safe than real\nauthentication provides.","feature_image":"__GHOST_URL__/content/images/2020/10/zero-trust-model.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2020-10-04 05:42:30","created_by":"1","updated_at":"2021-03-31 13:58:36","updated_by":"1","published_at":"2020-10-04 05:59:17","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbccc","uuid":"a2053184-aeab-464b-b559-d7264a679580","title":"JWT and HMAC in the browser, safe?","slug":"jwt-and-hmac-in-the-browser-safe","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://www.linkedin.com/company/hackerone/\"]],[\"a\",[\"href\",\"https://www.linkedin.com/company/bugcrowd/\"]],[\"a\",[\"href\",\"https://www.linkedin.com/company/synack-red-team/\"]],[\"a\",[\"href\",\"https://www.linkedin.com/company/yes-we-hack/\"]],[\"strong\"],[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://gist.github.com/chrisdlangton/11c77772fc0409d4d6d724ea4a6c28bd\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Is using JWT and HMAC in the browser, safe?\"]]],[1,\"h2\",[[0,[],0,\"How could they be?\"]]],[1,\"p\",[[0,[],0,\"Don't they require a pre-shared secret? How can it be \\\"secret\\\" in the browser!?\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"Secrets can actually be \\\"secret\\\" in browsers and apps, but almost always isn't securely shared with the client..\"]]],[1,\"p\",[[0,[],0,\"If you participate in bug bounty programs like \"],[0,[0],1,\"HackerOne\"],[0,[],0,\", \"],[0,[1],1,\"Bugcrowd\"],[0,[],0,\", \"],[0,[2],1,\"Synack Red Team\"],[0,[],0,\", or \"],[0,[3],1,\"YesWeHack\"],[0,[],0,\" then you'll know that finding a HMAC secret in the browser of the logged in user will not pay a bounty, unless one of the following can be proven;\"],[1,[],0,2],[0,[],0,\"1) This is a pre-shared secret that allows actions other than the allowed user actions - \"],[0,[4],1,\"privesc\"],[1,[],0,3],[0,[],0,\"2) The pre-shared works when the user is deactivated - \"],[0,[4],1,\"persistence\"],[1,[],0,4],[0,[],0,\"3) The signed request itself works independent of: time skew, body, identity - \"],[0,[4],1,\"security bugs\"],[1,[],0,5],[0,[],0,\"4) You can observe any pre-shared secrets while not logged in as an active user - \"],[0,[4],1,\"information disclosure\"],[1,[],0,6],[1,[],0,7],[0,[],0,\"So if you make sure that the server only sends a pre-shared secret out-of-band or only after active users are logged in (they were actually challenged, not just an active session or cert or token) - the that secret is secure for a browser client, or any client including iOS Apps (which have the same security limitations as a browser, often embed a browser for auth, prove me wrong).\"]]],[1,\"h2\",[[0,[],0,\"Which should you use?\"]]],[1,\"p\",[[0,[],0,\"TL;DR Use HMAC\"]]],[1,\"p\",[[0,[],0,\"HMAC is not new, most clouds and SaaS APIs require clients to use it.\"],[1,[],0,8],[1,[],0,9],[0,[],0,\"HMAC is very simple for a client that has time awareness and the pre-shared secret. The secret acts fundamentally the same as the traditional APIKey in your application code, but a HMAC will not be disclosing the key on every request.\"]]],[1,\"blockquote\",[[0,[],0,\"HMAC is really the simplest possible way to secure APIs.\"]]],[1,\"h2\",[[0,[],0,\"Do I need an OAuth authentication server, or integrate with any third party?\"]]],[1,\"p\",[[0,[],0,\"No, If someone is bringing up OAuth when discussing HMAC or JWT, that tells you that they don't know much about HMAC or JWT yet.\"]]],[1,\"p\",[[0,[],0,\"To be clear, when the topic is about OAuth itself only, the goal is OAuth and not just HMAC or JWT, then you certainly can include HMAC or JWT as one of the signature options used for OAuth token verification. Because OAuth requires you to verify the identity of the user you need to chose and implement an authentication verification step if you run your own OAuth authentication server, and third parties typically support multiple methods of verification where M/TLS is the newest and HMAC is the oldest, and JWT is the hot/hyped overly complex option.\"]]],[1,\"p\",[[0,[],0,\"If you are not wanting or needing OAuth and just want to secure your API with HMAC or JWT, then forget all about OAuth. \"],[1,[],0,10],[0,[],0,\"As an analogy; talking about OAuth is similar to discussing public transport routes as a group and you were not listening to the question in the first place and blow in later with a \"],[0,[5],1,\"car route,\"],[0,[],0,\" after it was very clearly established there is no interest in using a car. Don't be that person..\"]]],[1,\"h2\",[[0,[],0,\"Python example\"]]],[1,\"p\",[[0,[],0,\"Before we look at a solution, can python maintainers please consider including time skew, optionally at least? Making all developers do it in a bespoke disparate way either means they don't bother, or do it poorly, either way, HMAC in python easier to pwn then the rfc indicates it can/should be.\"],[1,[],0,11],[1,[],0,12],[0,[],0,\"Also, while ranting; FastAPI/Django/Flask may one day get real HMAC support too. I mean \"],[0,[5],1,\"real support\"],[0,[],0,\" that doesn't involve developers hard-coding an \"],[0,[4],1,\"environment variable\"],[0,[],0,\" for the pre-shared secret/s. This is \"],[0,[4],1,\"dumb\"],[0,[],0,\". That typically means clients all share the same pre-shared secret, or, developers are forced to (again) make their own bespoke disparate HMAC system for the framework instead of using the limited and arguably insufficiently secure version.\"]]],[1,\"p\",[[0,[],0,\"Given the limitations, the following Flask route decorator demonstrated the following;\"]]],[3,\"ul\",[[[0,[],0,\"Time-based HMAC signature verification using SHA256, SHA512, SHA3-256, SHA3-384, SHA3-512, and BLAKE2\"]],[[0,[],0,\"Customisation of the \"],[0,[6],1,\"not_before\"],[0,[],0,\" or \"],[0,[6],1,\"expire_after\"],[0,[],0,\" time-skew per API route, good for allowing synchronisation API endpoints that need to work with offline and aeroplane mode applications and providing allowances for API testing while still blocking replay attacks\"]],[[0,[],0,\"Supports GET without request data, and other HTTP methods that include a body, both are supported in a single client signing procedure\"]],[[0,[],0,\"Identity-linked secret token support, no hard coding\"]]]],[1,\"p\",[[0,[],0,\"You can checkout this Gist; \"],[0,[7],1,\"https://gist.github.com/chrisdlangton/11c77772fc0409d4d6d724ea4a6c28bd\"]]],[1,\"h2\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Maintainers; let's try to get HMAC right.\"]]],[1,\"p\",[[0,[],0,\"If it remains incomplete or provided with insufficient consideration for applications use cases to be implemented securely, if HMAC fails to be ready for secure use, we'll all be forced to use that horrible disparate JWT method and third parties that managed to achieve a SDK that balances security with usability. Regardless of SDK it's undeniable that JWT client implementation is overly complex, and we all know complexity is the nemesis of security.\"]]],[1,\"p\",[[0,[],0,\"Stick to HMAC, it is typically safe and very easy to consume already. For examples like python it is fairly straight-forward to add application logic to implement the missing security features too.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>Is using JWT and HMAC in the browser, safe?</p><h2 id=\"how-could-they-be\">How could they be?</h2><p>Don't they require a pre-shared secret? How can it be \"secret\" in the browser!?<br><br>Secrets can actually be \"secret\" in browsers and apps, but almost always isn't securely shared with the client..</p><p>If you participate in bug bounty programs like <a href=\"https://www.linkedin.com/company/hackerone/\">HackerOne</a>, <a href=\"https://www.linkedin.com/company/bugcrowd/\">Bugcrowd</a>, <a href=\"https://www.linkedin.com/company/synack-red-team/\">Synack Red Team</a>, or <a href=\"https://www.linkedin.com/company/yes-we-hack/\">YesWeHack</a> then you'll know that finding a HMAC secret in the browser of the logged in user will not pay a bounty, unless one of the following can be proven;<br>1) This is a pre-shared secret that allows actions other than the allowed user actions - <strong>privesc</strong><br>2) The pre-shared works when the user is deactivated - <strong>persistence</strong><br>3) The signed request itself works independent of: time skew, body, identity - <strong>security bugs</strong><br>4) You can observe any pre-shared secrets while not logged in as an active user - <strong>information disclosure</strong><br><br>So if you make sure that the server only sends a pre-shared secret out-of-band or only after active users are logged in (they were actually challenged, not just an active session or cert or token) - the that secret is secure for a browser client, or any client including iOS Apps (which have the same security limitations as a browser, often embed a browser for auth, prove me wrong).</p><h2 id=\"which-should-you-use\">Which should you use?</h2><p>TL;DR Use HMAC</p><p>HMAC is not new, most clouds and SaaS APIs require clients to use it.<br><br>HMAC is very simple for a client that has time awareness and the pre-shared secret. The secret acts fundamentally the same as the traditional APIKey in your application code, but a HMAC will not be disclosing the key on every request.</p><blockquote>HMAC is really the simplest possible way to secure APIs.</blockquote><h2 id=\"do-i-need-an-oauth-authentication-server-or-integrate-with-any-third-party\">Do I need an OAuth authentication server, or integrate with any third party?</h2><p>No, If someone is bringing up OAuth when discussing HMAC or JWT, that tells you that they don't know much about HMAC or JWT yet.</p><p>To be clear, when the topic is about OAuth itself only, the goal is OAuth and not just HMAC or JWT, then you certainly can include HMAC or JWT as one of the signature options used for OAuth token verification. Because OAuth requires you to verify the identity of the user you need to chose and implement an authentication verification step if you run your own OAuth authentication server, and third parties typically support multiple methods of verification where M/TLS is the newest and HMAC is the oldest, and JWT is the hot/hyped overly complex option.</p><p>If you are not wanting or needing OAuth and just want to secure your API with HMAC or JWT, then forget all about OAuth. <br>As an analogy; talking about OAuth is similar to discussing public transport routes as a group and you were not listening to the question in the first place and blow in later with a <em>car route,</em> after it was very clearly established there is no interest in using a car. Don't be that person..</p><h2 id=\"python-example\">Python example</h2><p>Before we look at a solution, can python maintainers please consider including time skew, optionally at least? Making all developers do it in a bespoke disparate way either means they don't bother, or do it poorly, either way, HMAC in python easier to pwn then the rfc indicates it can/should be.<br><br>Also, while ranting; FastAPI/Django/Flask may one day get real HMAC support too. I mean <em>real support</em> that doesn't involve developers hard-coding an <strong>environment variable</strong> for the pre-shared secret/s. This is <strong>dumb</strong>. That typically means clients all share the same pre-shared secret, or, developers are forced to (again) make their own bespoke disparate HMAC system for the framework instead of using the limited and arguably insufficiently secure version.</p><p>Given the limitations, the following Flask route decorator demonstrated the following;</p><ul><li>Time-based HMAC signature verification using SHA256, SHA512, SHA3-256, SHA3-384, SHA3-512, and BLAKE2</li><li>Customisation of the <code>not_before</code> or <code>expire_after</code> time-skew per API route, good for allowing synchronisation API endpoints that need to work with offline and aeroplane mode applications and providing allowances for API testing while still blocking replay attacks</li><li>Supports GET without request data, and other HTTP methods that include a body, both are supported in a single client signing procedure</li><li>Identity-linked secret token support, no hard coding</li></ul><p>You can checkout this Gist; <a href=\"https://gist.github.com/chrisdlangton/11c77772fc0409d4d6d724ea4a6c28bd\">https://gist.github.com/chrisdlangton/11c77772fc0409d4d6d724ea4a6c28bd</a></p><h2 id=\"conclusion\">Conclusion</h2><p>Maintainers; let's try to get HMAC right.</p><p>If it remains incomplete or provided with insufficient consideration for applications use cases to be implemented securely, if HMAC fails to be ready for secure use, we'll all be forced to use that horrible disparate JWT method and third parties that managed to achieve a SDK that balances security with usability. Regardless of SDK it's undeniable that JWT client implementation is overly complex, and we all know complexity is the nemesis of security.</p><p>Stick to HMAC, it is typically safe and very easy to consume already. For examples like python it is fairly straight-forward to add application logic to implement the missing security features too.</p>","comment_id":"5fefd442621b4004d83096aa","plaintext":"Is using JWT and HMAC in the browser, safe?\n\nHow could they be?\nDon't they require a pre-shared secret? How can it be \"secret\" in the browser!?\n\nSecrets can actually be \"secret\" in browsers and apps, but almost always isn't\nsecurely shared with the client..\n\nIf you participate in bug bounty programs like HackerOne\n[https://www.linkedin.com/company/hackerone/], Bugcrowd\n[https://www.linkedin.com/company/bugcrowd/], Synack Red Team\n[https://www.linkedin.com/company/synack-red-team/], or YesWeHack\n[https://www.linkedin.com/company/yes-we-hack/] then you'll know that finding a\nHMAC secret in the browser of the logged in user will not pay a bounty, unless\none of the following can be proven;\n1) This is a pre-shared secret that allows actions other than the allowed user\nactions - privesc\n2) The pre-shared works when the user is deactivated - persistence\n3) The signed request itself works independent of: time skew, body, identity - \nsecurity bugs\n4) You can observe any pre-shared secrets while not logged in as an active user\n- information disclosure\n\nSo if you make sure that the server only sends a pre-shared secret out-of-band\nor only after active users are logged in (they were actually challenged, not\njust an active session or cert or token) - the that secret is secure for a\nbrowser client, or any client including iOS Apps (which have the same security\nlimitations as a browser, often embed a browser for auth, prove me wrong).\n\nWhich should you use?\nTL;DR Use HMAC\n\nHMAC is not new, most clouds and SaaS APIs require clients to use it.\n\nHMAC is very simple for a client that has time awareness and the pre-shared\nsecret. The secret acts fundamentally the same as the traditional APIKey in your\napplication code, but a HMAC will not be disclosing the key on every request.\n\n> HMAC is really the simplest possible way to secure APIs.\nDo I need an OAuth authentication server, or integrate with any third party?\nNo, If someone is bringing up OAuth when discussing HMAC or JWT, that tells you\nthat they don't know much about HMAC or JWT yet.\n\nTo be clear, when the topic is about OAuth itself only, the goal is OAuth and\nnot just HMAC or JWT, then you certainly can include HMAC or JWT as one of the\nsignature options used for OAuth token verification. Because OAuth requires you\nto verify the identity of the user you need to chose and implement an\nauthentication verification step if you run your own OAuth authentication\nserver, and third parties typically support multiple methods of verification\nwhere M/TLS is the newest and HMAC is the oldest, and JWT is the hot/hyped\noverly complex option.\n\nIf you are not wanting or needing OAuth and just want to secure your API with\nHMAC or JWT, then forget all about OAuth. \nAs an analogy; talking about OAuth is similar to discussing public transport\nroutes as a group and you were not listening to the question in the first place\nand blow in later with a car route, after it was very clearly established there\nis no interest in using a car. Don't be that person..\n\nPython example\nBefore we look at a solution, can python maintainers please consider including\ntime skew, optionally at least? Making all developers do it in a bespoke\ndisparate way either means they don't bother, or do it poorly, either way, HMAC\nin python easier to pwn then the rfc indicates it can/should be.\n\nAlso, while ranting; FastAPI/Django/Flask may one day get real HMAC support too.\nI mean real support that doesn't involve developers hard-coding an environment\nvariable for the pre-shared secret/s. This is dumb. That typically means clients\nall share the same pre-shared secret, or, developers are forced to (again) make\ntheir own bespoke disparate HMAC system for the framework instead of using the\nlimited and arguably insufficiently secure version.\n\nGiven the limitations, the following Flask route decorator demonstrated the\nfollowing;\n\n * Time-based HMAC signature verification using SHA256, SHA512, SHA3-256,\n   SHA3-384, SHA3-512, and BLAKE2\n * Customisation of the not_before or expire_after time-skew per API route, good\n   for allowing synchronisation API endpoints that need to work with offline and\n   aeroplane mode applications and providing allowances for API testing while\n   still blocking replay attacks\n * Supports GET without request data, and other HTTP methods that include a\n   body, both are supported in a single client signing procedure\n * Identity-linked secret token support, no hard coding\n\nYou can checkout this Gist; \nhttps://gist.github.com/chrisdlangton/11c77772fc0409d4d6d724ea4a6c28bd\n\nConclusion\nMaintainers; let's try to get HMAC right.\n\nIf it remains incomplete or provided with insufficient consideration for\napplications use cases to be implemented securely, if HMAC fails to be ready for\nsecure use, we'll all be forced to use that horrible disparate JWT method and\nthird parties that managed to achieve a SDK that balances security with\nusability. Regardless of SDK it's undeniable that JWT client implementation is\noverly complex, and we all know complexity is the nemesis of security.\n\nStick to HMAC, it is typically safe and very easy to consume already. For\nexamples like python it is fairly straight-forward to add application logic to\nimplement the missing security features too.","feature_image":"__GHOST_URL__/content/images/2021/01/HMAC.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-01-02 02:02:42","created_by":"1","updated_at":"2021-03-31 13:58:24","updated_by":"1","published_at":"2021-01-02 16:17:39","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"6064789220c4c500017fbccd","uuid":"551ff061-7188-47d5-87f6-c0fda2e61844","title":"WebStorage vs Cookies - Secure Session Management in 2021","slug":"webstorage-vs-cookies-in-2021","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-unauth-flow.jpg\",\"alt\":\"Unauthenticated Flow\",\"caption\":\"Unauthenticated Flow\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-basic-flow.jpg\",\"alt\":\"Basic flow using cookies\",\"caption\":\"Basic flow using cookies\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-mitm-flow.jpg\",\"alt\":\"man in the middle basic flow\",\"caption\":\"Session is not secure\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-signed-flow.jpg\",\"alt\":\"Signed cookie flow with a unique attribute prevents bad actors\",\"caption\":\"Signed cookies with a unique attribute\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-challenge-signed-flow.jpg\",\"alt\":\"signed cookie with IP address attribute\",\"caption\":\"signed cookie with IP address attribute\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-mixed-flow.jpg\",\"alt\":\"mixed human and machine challenge flow\",\"caption\":\"mixed human and machine challenge flow\"}]],\"markups\":[[\"a\",[\"href\",\"https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#cookies\"]],[\"u\"],[\"em\"],[\"a\",[\"href\",\"__GHOST_URL__/tls-secures-my-data-right/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Session management still occurs via cookies, but do not disclose secrets or authorisation in a cookie because the security of WebStorage is best practice.\"]]],[1,\"p\",[[0,[],0,\"I won't bore you with sections that cover \\\"what are Cookies\\\" or \\\"security of cookies\\\", \"],[0,[0],1,\"OWASP covers these very well here\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Secure Session Management\"]]],[1,\"p\",[[0,[],0,\"Before we tackle session management\"]]],[1,\"h3\",[[0,[],0,\"What is a secure session?\"]]],[1,\"p\",[[0,[],0,\"A secure session has 1 simple characteristic;\"]]],[1,\"blockquote\",[[0,[],0,\"Knowing the session value provides no information; it is not disclosing an identity, it's validity, it has no metadata, user data, secrets, or usable values of any type if read by anything other than the server that issued the value\"]]],[1,\"p\",[[0,[],0,\"A session does not represent identity, to have Identity you require assurance of Authentication because the challenge itself is the security characteristic of Authentication assurance.\"]]],[1,\"p\",[[0,[],0,\"The session is not Authentication, to have Authentication you require a challenge.\"]]],[1,\"p\",[[0,[],0,\"Knowing a session does not guarantee Authorisation, to have Authorisation you require assurance of Identity.\"]]],[1,\"p\",[[0,[],0,\"And so the circle is realised.\"]]],[1,\"p\",[[0,[],0,\"The question is; do you a) complete the circle and require a challenge. or; b) Challenge once, set a cookie, and hope everything works out. or; c) understand, plan, implement, and verify a secure session management solution?\"]]],[1,\"p\",[[0,[],0,\"Of course you chose option \\\"c\\\" so let's talk about when it is best-practice to use both cookies and WebStorage.\"]]],[1,\"h2\",[[0,[],0,\"When to use Cookies\"]]],[1,\"p\",[[0,[],0,\"A cookie should only be set \"],[0,[1],1,\"after\"],[0,[],0,\" the successful \"],[0,[2],1,\"server-side\"],[0,[],0,\" Authentication \"],[0,[1],1,\"challenge\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Never set a value in a cookie, the browser should never access the cookie. period; this is what WebStorage is for.\"]]],[1,\"p\",[[0,[],0,\"The only value a cookie should hold, is the server-side session management unique identifier. All other session management values (identity, expiry, authorisation, metadata, etc.) should never be included.\"]]],[1,\"h2\",[[0,[],0,\"When to use WebStorage\"]]],[1,\"p\",[[0,[],0,\"There are 2 types;\"]]],[1,\"h3\",[[0,[],0,\"Local Storage\"]]],[1,\"p\",[[0,[],0,\"Data stored in Local Storage is specific to the protocol of the page so that data stored by a script on a site accessed with HTTP is put in a different WebStorage object from the same site accessed with HTTPS. This applies to different origins too, all WebStorage is strictly same-origin. Basically an origin is the combination of domain and protocol, we covered protocol already and the domain is easy; example.com is a domain and www.example.com is a \"],[0,[2],1,\"different\"],[0,[],0,\" origin, it is a subdomain of example.com and is it's own origin.\"]]],[1,\"p\",[[0,[],0,\"Once set, data in Local Storage doesn't expire. Local Storage is never disclosed over HTTP, it is never accessible remotely or shared between devices or users on the same machine. All access to data in Local Storage occurs on the device from scripts which were loaded in the context of it's own origin.\"]]],[1,\"p\",[[0,[],0,\"Use the Local Storage object to persist a value used for session management that lasts beyond browser restarts, machine restarts, and even the existence of a cookie.\"]]],[1,\"h3\",[[0,[],0,\"Session Storage\"]]],[1,\"p\",[[0,[],0,\"This follows the same rules as Local Storage except one. Unlike Local Storage which doesn't expire, data in Session Storage is cleared when the \"],[0,[2],1,\"page session\"],[0,[],0,\" ends.\"]]],[1,\"p\",[[0,[],0,\"A Session Storage object is entirely distinct from a Session based on a cookie. You can have many Session Storage objects with unique values and while they are independent Session Storage objects, they may be confused as one 'Session' with inconsistent user values based on a cookie.\"]]],[1,\"p\",[[0,[],0,\"A page session is a particular tab, not an entire browser. If a tab closes the session is purged. If 2 tabs are opened in the same browser (which shares a cookie, and session) there can be unique values in Session Storage per tab for the same 'user session' defined by and stored in a cookie.\"]]],[1,\"p\",[[0,[],0,\"Do not Session Storage for purposes related to a server-side user session.\"]]],[1,\"p\",[[0,[],0,\"The word 'Session' in Session Storage \"],[0,[1],1,\"is not\"],[0,[],0,\" a traditional session. I have not encountered any use cases related to user session management.\"]]],[1,\"p\",[[0,[],0,\"Use the Session Storage for tab specific things, like tab naming, temporary styling, non-persistent form values (drafts), or when you require inter-communication between tabs but keep tabs distinct somehow, which can be useful for some types of games and chats.\"]]],[1,\"h2\",[[0,[],0,\"Secure Cookies are signed\"]]],[1,\"p\",[[0,[],0,\"Using signatures, ergo signed cookie, adds a characteristic Cookies alone never provided; Integrity. But wait, cookies introduced a 'Secure' flag to prevent cookie sniffing (when a cookie is disclosed to a middle actor, traditionally (wo)man-in-the-middle MitM).\"]]],[1,\"p\",[[0,[],0,\"The 'Secure' attribute simply tells browsers to never disclose the cookie over non-HTTPS requests and the server should (usually never) ignore the cookie if it is sent over non-HTTPS. The 'Secure; flag can never assure a cookie was never tampered, it only sets conditions for it being sent and reading teh cookie, tampering when sent, or inclusion of a cookie when it was never sent are all still possible. Signing is cryptographic assurance of integrity, and signatures require a server-side secret to verify the integrity preventing tampering entirely while the secret remains protected.\"]]],[1,\"p\",[[0,[],0,\"A secure cookie also sets the HttpOnly attribute, naturally.\"]]],[1,\"p\",[[0,[],0,\"And contrary to common understanding, \"],[0,[1],1,\"do not use\"],[0,[],0,\" the domain attribute because cookies will be sent to that domain and all its subdomains. This includes that subdomain used for tracking that you added a CNAME to delegate to a third party. Cookies do not understand DNS so a CNAME to another domain from your own subdomain actually get your user cookie.\"],[1,[],0,0],[0,[],0,\"So if you use the domain cookie attribute, immediately remove domain attribute to limit cookie to origin host only - which is very likely your original intention.\"],[1,[],0,1],[0,[],0,\"Use the domain cookie attribute in the very rare cases it is needed, and only if you fully understand why you are using this and not the alternative secure cookie method for each origin.\"]]],[1,\"p\",[[0,[],0,\"Signatures must be generated one-way, and reproducible using a server-side secret combined with user attributes that may be used to validate or invalidate the user session.\"],[1,[],0,2],[0,[],0,\"For example; combining the user's source IP address allows you to invalidate the session if the cookie is stolen and used by another IP address. This may not be ideal on IPv4 because most users are assigned dynamic IP addresses. This doesn't need to destroy a session, it may simply indicate that the session validation would require the successful completion of a challenge. If the user attribute signed is appropriate, this is a powerful and secure solution.\"]]],[1,\"h2\",[[0,[],0,\"Challenges are not always user actions\"]]],[1,\"p\",[[0,[],0,\"We've discussed challenges several times, but do they always require a human challenge? I like to argue that this is not necessary in every circumstance that requires a challenge given that at least 1 challenge was performed by a human and all subsequent challenges are performed by the machine strictly based on the result of the human challenge and with assurance the human identity has not changed.\"]]],[1,\"p\",[[0,[],0,\"Okay, let's unpack that one with some images\"]]],[10,0],[1,\"p\",[[0,[],0,\"Essentially all public pages are returned and protected pages are forbidden.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Upon the successful challenge is verified by the server, protected pages are returned.\"]]],[10,2],[1,\"p\",[[0,[],0,\"But the session is not secure, a bad actor may steal and replay the session or tamper with the session. This also allows bad actor to interact directly with a server freely.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Using a signed cookie where the server has validated a user unique attribute effectively prevents bad actors from leveraging a user session for malicious purposes, but all challenges are human actions and may be inappropriate with an some user attributes such as an IP Address;\"]]],[10,4],[1,\"p\",[[0,[],0,\"Which requires a user to have the challenge fulfilled whenever the IP address changes. This is similar to using the User-Agent as an attribute because it has been known to change when browser updates are applied. There are many options for user attributes to sign and I have not yet come across a sufficiently secure, reliable, and constant attribute - but this is not an issue if the challenge can be fulfilled by the machine directly when required by the server instead of interrupting the user;\"]]],[10,5],[1,\"p\",[[0,[],0,\"In the same way the bad actor was forbidden due to them not knowing the unique user attribute signed in the cookie when a machine challenge is used (such as HMAC) in combination with the signed cookie, the bad actor remains forbidden because the signed cookie is still in place.\"],[1,[],0,3],[0,[],0,\"When the user attribute changes or the bad actor makes an attempt, only the user continues to hold it's own unusable attribute value, the original challenge used to obtain the secret used for HMAC signing. The bad actor never fulfilled the original challenge so never obtain the secret used for HMAC signing and is unable to successfully perform the HMAC signing to make requests.\"]]],[1,\"p\",[[0,[],0,\"The user leverages the HMAC HTTP Signing on each request that fails cookie validation to maintain a valid session. A HMAC signed HTTP request is not capable of requesting and rendering HTML therefore the signed cookie still holds value in this context and cannot be abandoned upon establishing the HMAC. These solve separate problems and work together for best-practice secure session management.\"]]],[1,\"h2\",[[0,[],0,\"Key Considerations\"]]],[1,\"p\",[[0,[],0,\"Okay, I know I promised not to cover  \\\"what are Cookies\\\" or \\\"security of cookies\\\", so I'll simply highlight the grey areas that tend to confuse and missed by most people.\"]]],[1,\"p\",[[0,[],0,\"The cookie will disclose itself on every request the browser sends out, for example a website hosting a image, a 3rd part style or script include, or a interactive API. Some cookies can be restricted to only be sent over page loads and interactive API requests (referred to as HTTPOnly) so that servers hosting images cannot obtain user cookies. The fact is even though the request is encrypted using TLS the cookie cannot be secret or hidden because they are freely shared over HTTP and even images which are proven trivial to remotely obtain.\"]]],[1,\"p\",[[0,[],0,\"Cookies are not encrypted using TLS when the requester is the attacker that makes the TLS request, \"],[0,[3],1,\"read about how TLS was only ever intended to encrypt the transport, not protect the data from intended or unintended requesters\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Cookies have a lot known security risks, many are due to knowledge gaps in how to properly use cookies but this is not the biggest risk. The major issue with cookies is application developers almost never interact with the cookie directly, and therefore are not in control of the security of the cookie or the implementation of the code that manages the cookie. This leads to blind trust in the cookie, the developer and thus the business delegate the security controls to the third party code (likely unmaintained, and always unaware of your business risk posture).\"]]],[1,\"p\",[[0,[],0,\"WebStorage suffers few known issues, they can be summarised into the following;\"]]],[3,\"ul\",[[[0,[],0,\"Inappropriate access by 3rd party code: scripts included directly on the page from the same domain of the page. Same issue as cookies\"]],[[0,[],0,\"CNAME collusion: Where analytics.mydomain.com has a CNAME record to tracker.company.com meaning tracker can read WebStorage as though it was analytics.mydomain.com. Same issue as cookies\"]],[[0,[],0,\"Insecure Implementations: where values are set or read in business logic flow that introduces security issues unrelated to the WebStorage technology itself. Same issue as cookies\"]]]],[1,\"p\",[[0,[],0,\"Arguably we can claim that any risks shared between a cookie and WebStorage is extremely less of an inherent risk for WebStorage because cookies are exposed on every request and WebStorage is fully protected per origin.\"]]],[1,\"h2\",[]]],\"ghostVersion\":\"3.0\"}","html":"<p>Session management still occurs via cookies, but do not disclose secrets or authorisation in a cookie because the security of WebStorage is best practice.</p><p>I won't bore you with sections that cover \"what are Cookies\" or \"security of cookies\", <a href=\"https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#cookies\">OWASP covers these very well here</a>.</p><h2 id=\"secure-session-management\">Secure Session Management</h2><p>Before we tackle session management</p><h3 id=\"what-is-a-secure-session\">What is a secure session?</h3><p>A secure session has 1 simple characteristic;</p><blockquote>Knowing the session value provides no information; it is not disclosing an identity, it's validity, it has no metadata, user data, secrets, or usable values of any type if read by anything other than the server that issued the value</blockquote><p>A session does not represent identity, to have Identity you require assurance of Authentication because the challenge itself is the security characteristic of Authentication assurance.</p><p>The session is not Authentication, to have Authentication you require a challenge.</p><p>Knowing a session does not guarantee Authorisation, to have Authorisation you require assurance of Identity.</p><p>And so the circle is realised.</p><p>The question is; do you a) complete the circle and require a challenge. or; b) Challenge once, set a cookie, and hope everything works out. or; c) understand, plan, implement, and verify a secure session management solution?</p><p>Of course you chose option \"c\" so let's talk about when it is best-practice to use both cookies and WebStorage.</p><h2 id=\"when-to-use-cookies\">When to use Cookies</h2><p>A cookie should only be set <u>after</u> the successful <em>server-side</em> Authentication <u>challenge</u>.</p><p>Never set a value in a cookie, the browser should never access the cookie. period; this is what WebStorage is for.</p><p>The only value a cookie should hold, is the server-side session management unique identifier. All other session management values (identity, expiry, authorisation, metadata, etc.) should never be included.</p><h2 id=\"when-to-use-webstorage\">When to use WebStorage</h2><p>There are 2 types;</p><h3 id=\"local-storage\">Local Storage</h3><p>Data stored in Local Storage is specific to the protocol of the page so that data stored by a script on a site accessed with HTTP is put in a different WebStorage object from the same site accessed with HTTPS. This applies to different origins too, all WebStorage is strictly same-origin. Basically an origin is the combination of domain and protocol, we covered protocol already and the domain is easy; example.com is a domain and www.example.com is a <em>different</em> origin, it is a subdomain of example.com and is it's own origin.</p><p>Once set, data in Local Storage doesn't expire. Local Storage is never disclosed over HTTP, it is never accessible remotely or shared between devices or users on the same machine. All access to data in Local Storage occurs on the device from scripts which were loaded in the context of it's own origin.</p><p>Use the Local Storage object to persist a value used for session management that lasts beyond browser restarts, machine restarts, and even the existence of a cookie.</p><h3 id=\"session-storage\">Session Storage</h3><p>This follows the same rules as Local Storage except one. Unlike Local Storage which doesn't expire, data in Session Storage is cleared when the <em>page session</em> ends.</p><p>A Session Storage object is entirely distinct from a Session based on a cookie. You can have many Session Storage objects with unique values and while they are independent Session Storage objects, they may be confused as one 'Session' with inconsistent user values based on a cookie.</p><p>A page session is a particular tab, not an entire browser. If a tab closes the session is purged. If 2 tabs are opened in the same browser (which shares a cookie, and session) there can be unique values in Session Storage per tab for the same 'user session' defined by and stored in a cookie.</p><p>Do not Session Storage for purposes related to a server-side user session.</p><p>The word 'Session' in Session Storage <u>is not</u> a traditional session. I have not encountered any use cases related to user session management.</p><p>Use the Session Storage for tab specific things, like tab naming, temporary styling, non-persistent form values (drafts), or when you require inter-communication between tabs but keep tabs distinct somehow, which can be useful for some types of games and chats.</p><h2 id=\"secure-cookies-are-signed\">Secure Cookies are signed</h2><p>Using signatures, ergo signed cookie, adds a characteristic Cookies alone never provided; Integrity. But wait, cookies introduced a 'Secure' flag to prevent cookie sniffing (when a cookie is disclosed to a middle actor, traditionally (wo)man-in-the-middle MitM).</p><p>The 'Secure' attribute simply tells browsers to never disclose the cookie over non-HTTPS requests and the server should (usually never) ignore the cookie if it is sent over non-HTTPS. The 'Secure; flag can never assure a cookie was never tampered, it only sets conditions for it being sent and reading teh cookie, tampering when sent, or inclusion of a cookie when it was never sent are all still possible. Signing is cryptographic assurance of integrity, and signatures require a server-side secret to verify the integrity preventing tampering entirely while the secret remains protected.</p><p>A secure cookie also sets the HttpOnly attribute, naturally.</p><p>And contrary to common understanding, <u>do not use</u> the domain attribute because cookies will be sent to that domain and all its subdomains. This includes that subdomain used for tracking that you added a CNAME to delegate to a third party. Cookies do not understand DNS so a CNAME to another domain from your own subdomain actually get your user cookie.<br>So if you use the domain cookie attribute, immediately remove domain attribute to limit cookie to origin host only - which is very likely your original intention.<br>Use the domain cookie attribute in the very rare cases it is needed, and only if you fully understand why you are using this and not the alternative secure cookie method for each origin.</p><p>Signatures must be generated one-way, and reproducible using a server-side secret combined with user attributes that may be used to validate or invalidate the user session.<br>For example; combining the user's source IP address allows you to invalidate the session if the cookie is stolen and used by another IP address. This may not be ideal on IPv4 because most users are assigned dynamic IP addresses. This doesn't need to destroy a session, it may simply indicate that the session validation would require the successful completion of a challenge. If the user attribute signed is appropriate, this is a powerful and secure solution.</p><h2 id=\"challenges-are-not-always-user-actions\">Challenges are not always user actions</h2><p>We've discussed challenges several times, but do they always require a human challenge? I like to argue that this is not necessary in every circumstance that requires a challenge given that at least 1 challenge was performed by a human and all subsequent challenges are performed by the machine strictly based on the result of the human challenge and with assurance the human identity has not changed.</p><p>Okay, let's unpack that one with some images</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-unauth-flow.jpg\" class=\"kg-image\" alt=\"Unauthenticated Flow\" loading=\"lazy\"><figcaption>Unauthenticated Flow</figcaption></figure><p>Essentially all public pages are returned and protected pages are forbidden.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-basic-flow.jpg\" class=\"kg-image\" alt=\"Basic flow using cookies\" loading=\"lazy\"><figcaption>Basic flow using cookies</figcaption></figure><p>Upon the successful challenge is verified by the server, protected pages are returned.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-mitm-flow.jpg\" class=\"kg-image\" alt=\"man in the middle basic flow\" loading=\"lazy\"><figcaption>Session is not secure</figcaption></figure><p>But the session is not secure, a bad actor may steal and replay the session or tamper with the session. This also allows bad actor to interact directly with a server freely.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-signed-flow.jpg\" class=\"kg-image\" alt=\"Signed cookie flow with a unique attribute prevents bad actors\" loading=\"lazy\"><figcaption>Signed cookies with a unique attribute</figcaption></figure><p>Using a signed cookie where the server has validated a user unique attribute effectively prevents bad actors from leveraging a user session for malicious purposes, but all challenges are human actions and may be inappropriate with an some user attributes such as an IP Address;</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-challenge-signed-flow.jpg\" class=\"kg-image\" alt=\"signed cookie with IP address attribute\" loading=\"lazy\"><figcaption>signed cookie with IP address attribute</figcaption></figure><p>Which requires a user to have the challenge fulfilled whenever the IP address changes. This is similar to using the User-Agent as an attribute because it has been known to change when browser updates are applied. There are many options for user attributes to sign and I have not yet come across a sufficiently secure, reliable, and constant attribute - but this is not an issue if the challenge can be fulfilled by the machine directly when required by the server instead of interrupting the user;</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/03/human-machine-challenge-mixed-flow.jpg\" class=\"kg-image\" alt=\"mixed human and machine challenge flow\" loading=\"lazy\"><figcaption>mixed human and machine challenge flow</figcaption></figure><p>In the same way the bad actor was forbidden due to them not knowing the unique user attribute signed in the cookie when a machine challenge is used (such as HMAC) in combination with the signed cookie, the bad actor remains forbidden because the signed cookie is still in place.<br>When the user attribute changes or the bad actor makes an attempt, only the user continues to hold it's own unusable attribute value, the original challenge used to obtain the secret used for HMAC signing. The bad actor never fulfilled the original challenge so never obtain the secret used for HMAC signing and is unable to successfully perform the HMAC signing to make requests.</p><p>The user leverages the HMAC HTTP Signing on each request that fails cookie validation to maintain a valid session. A HMAC signed HTTP request is not capable of requesting and rendering HTML therefore the signed cookie still holds value in this context and cannot be abandoned upon establishing the HMAC. These solve separate problems and work together for best-practice secure session management.</p><h2 id=\"key-considerations\">Key Considerations</h2><p>Okay, I know I promised not to cover  \"what are Cookies\" or \"security of cookies\", so I'll simply highlight the grey areas that tend to confuse and missed by most people.</p><p>The cookie will disclose itself on every request the browser sends out, for example a website hosting a image, a 3rd part style or script include, or a interactive API. Some cookies can be restricted to only be sent over page loads and interactive API requests (referred to as HTTPOnly) so that servers hosting images cannot obtain user cookies. The fact is even though the request is encrypted using TLS the cookie cannot be secret or hidden because they are freely shared over HTTP and even images which are proven trivial to remotely obtain.</p><p>Cookies are not encrypted using TLS when the requester is the attacker that makes the TLS request, <a href=\"__GHOST_URL__/tls-secures-my-data-right/\">read about how TLS was only ever intended to encrypt the transport, not protect the data from intended or unintended requesters</a>.</p><p>Cookies have a lot known security risks, many are due to knowledge gaps in how to properly use cookies but this is not the biggest risk. The major issue with cookies is application developers almost never interact with the cookie directly, and therefore are not in control of the security of the cookie or the implementation of the code that manages the cookie. This leads to blind trust in the cookie, the developer and thus the business delegate the security controls to the third party code (likely unmaintained, and always unaware of your business risk posture).</p><p>WebStorage suffers few known issues, they can be summarised into the following;</p><ul><li>Inappropriate access by 3rd party code: scripts included directly on the page from the same domain of the page. Same issue as cookies</li><li>CNAME collusion: Where analytics.mydomain.com has a CNAME record to tracker.company.com meaning tracker can read WebStorage as though it was analytics.mydomain.com. Same issue as cookies</li><li>Insecure Implementations: where values are set or read in business logic flow that introduces security issues unrelated to the WebStorage technology itself. Same issue as cookies</li></ul><p>Arguably we can claim that any risks shared between a cookie and WebStorage is extremely less of an inherent risk for WebStorage because cookies are exposed on every request and WebStorage is fully protected per origin.</p><h2></h2>","comment_id":"605687989fee6e04cfd79a94","plaintext":"Session management still occurs via cookies, but do not disclose secrets or\nauthorisation in a cookie because the security of WebStorage is best practice.\n\nI won't bore you with sections that cover \"what are Cookies\" or \"security of\ncookies\", OWASP covers these very well here\n[https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#cookies]\n.\n\nSecure Session Management\nBefore we tackle session management\n\nWhat is a secure session?\nA secure session has 1 simple characteristic;\n\n> Knowing the session value provides no information; it is not disclosing an\nidentity, it's validity, it has no metadata, user data, secrets, or usable\nvalues of any type if read by anything other than the server that issued the\nvalue\nA session does not represent identity, to have Identity you require assurance of\nAuthentication because the challenge itself is the security characteristic of\nAuthentication assurance.\n\nThe session is not Authentication, to have Authentication you require a\nchallenge.\n\nKnowing a session does not guarantee Authorisation, to have Authorisation you\nrequire assurance of Identity.\n\nAnd so the circle is realised.\n\nThe question is; do you a) complete the circle and require a challenge. or; b)\nChallenge once, set a cookie, and hope everything works out. or; c) understand,\nplan, implement, and verify a secure session management solution?\n\nOf course you chose option \"c\" so let's talk about when it is best-practice to\nuse both cookies and WebStorage.\n\nWhen to use Cookies\nA cookie should only be set after the successful server-side Authentication \nchallenge.\n\nNever set a value in a cookie, the browser should never access the cookie.\nperiod; this is what WebStorage is for.\n\nThe only value a cookie should hold, is the server-side session management\nunique identifier. All other session management values (identity, expiry,\nauthorisation, metadata, etc.) should never be included.\n\nWhen to use WebStorage\nThere are 2 types;\n\nLocal Storage\nData stored in Local Storage is specific to the protocol of the page so that\ndata stored by a script on a site accessed with HTTP is put in a different\nWebStorage object from the same site accessed with HTTPS. This applies to\ndifferent origins too, all WebStorage is strictly same-origin. Basically an\norigin is the combination of domain and protocol, we covered protocol already\nand the domain is easy; example.com is a domain and www.example.com is a \ndifferent origin, it is a subdomain of example.com and is it's own origin.\n\nOnce set, data in Local Storage doesn't expire. Local Storage is never disclosed\nover HTTP, it is never accessible remotely or shared between devices or users on\nthe same machine. All access to data in Local Storage occurs on the device from\nscripts which were loaded in the context of it's own origin.\n\nUse the Local Storage object to persist a value used for session management that\nlasts beyond browser restarts, machine restarts, and even the existence of a\ncookie.\n\nSession Storage\nThis follows the same rules as Local Storage except one. Unlike Local Storage\nwhich doesn't expire, data in Session Storage is cleared when the page session \nends.\n\nA Session Storage object is entirely distinct from a Session based on a cookie.\nYou can have many Session Storage objects with unique values and while they are\nindependent Session Storage objects, they may be confused as one 'Session' with\ninconsistent user values based on a cookie.\n\nA page session is a particular tab, not an entire browser. If a tab closes the\nsession is purged. If 2 tabs are opened in the same browser (which shares a\ncookie, and session) there can be unique values in Session Storage per tab for\nthe same 'user session' defined by and stored in a cookie.\n\nDo not Session Storage for purposes related to a server-side user session.\n\nThe word 'Session' in Session Storage is not a traditional session. I have not\nencountered any use cases related to user session management.\n\nUse the Session Storage for tab specific things, like tab naming, temporary\nstyling, non-persistent form values (drafts), or when you require\ninter-communication between tabs but keep tabs distinct somehow, which can be\nuseful for some types of games and chats.\n\nSecure Cookies are signed\nUsing signatures, ergo signed cookie, adds a characteristic Cookies alone never\nprovided; Integrity. But wait, cookies introduced a 'Secure' flag to prevent\ncookie sniffing (when a cookie is disclosed to a middle actor, traditionally\n(wo)man-in-the-middle MitM).\n\nThe 'Secure' attribute simply tells browsers to never disclose the cookie over\nnon-HTTPS requests and the server should (usually never) ignore the cookie if it\nis sent over non-HTTPS. The 'Secure; flag can never assure a cookie was never\ntampered, it only sets conditions for it being sent and reading teh cookie,\ntampering when sent, or inclusion of a cookie when it was never sent are all\nstill possible. Signing is cryptographic assurance of integrity, and signatures\nrequire a server-side secret to verify the integrity preventing tampering\nentirely while the secret remains protected.\n\nA secure cookie also sets the HttpOnly attribute, naturally.\n\nAnd contrary to common understanding, do not use the domain attribute because\ncookies will be sent to that domain and all its subdomains. This includes that\nsubdomain used for tracking that you added a CNAME to delegate to a third party.\nCookies do not understand DNS so a CNAME to another domain from your own\nsubdomain actually get your user cookie.\nSo if you use the domain cookie attribute, immediately remove domain attribute\nto limit cookie to origin host only - which is very likely your original\nintention.\nUse the domain cookie attribute in the very rare cases it is needed, and only if\nyou fully understand why you are using this and not the alternative secure\ncookie method for each origin.\n\nSignatures must be generated one-way, and reproducible using a server-side\nsecret combined with user attributes that may be used to validate or invalidate\nthe user session.\nFor example; combining the user's source IP address allows you to invalidate the\nsession if the cookie is stolen and used by another IP address. This may not be\nideal on IPv4 because most users are assigned dynamic IP addresses. This doesn't\nneed to destroy a session, it may simply indicate that the session validation\nwould require the successful completion of a challenge. If the user attribute\nsigned is appropriate, this is a powerful and secure solution.\n\nChallenges are not always user actions\nWe've discussed challenges several times, but do they always require a human\nchallenge? I like to argue that this is not necessary in every circumstance that\nrequires a challenge given that at least 1 challenge was performed by a human\nand all subsequent challenges are performed by the machine strictly based on the\nresult of the human challenge and with assurance the human identity has not\nchanged.\n\nOkay, let's unpack that one with some images\n\nUnauthenticated FlowEssentially all public pages are returned and protected\npages are forbidden.\n\nBasic flow using cookiesUpon the successful challenge is verified by the server,\nprotected pages are returned.\n\nSession is not secureBut the session is not secure, a bad actor may steal and\nreplay the session or tamper with the session. This also allows bad actor to\ninteract directly with a server freely.\n\nSigned cookies with a unique attributeUsing a signed cookie where the server has\nvalidated a user unique attribute effectively prevents bad actors from\nleveraging a user session for malicious purposes, but all challenges are human\nactions and may be inappropriate with an some user attributes such as an IP\nAddress;\n\nsigned cookie with IP address attributeWhich requires a user to have the\nchallenge fulfilled whenever the IP address changes. This is similar to using\nthe User-Agent as an attribute because it has been known to change when browser\nupdates are applied. There are many options for user attributes to sign and I\nhave not yet come across a sufficiently secure, reliable, and constant attribute\n- but this is not an issue if the challenge can be fulfilled by the machine\ndirectly when required by the server instead of interrupting the user;\n\nmixed human and machine challenge flowIn the same way the bad actor was\nforbidden due to them not knowing the unique user attribute signed in the cookie\nwhen a machine challenge is used (such as HMAC) in combination with the signed\ncookie, the bad actor remains forbidden because the signed cookie is still in\nplace.\nWhen the user attribute changes or the bad actor makes an attempt, only the user\ncontinues to hold it's own unusable attribute value, the original challenge used\nto obtain the secret used for HMAC signing. The bad actor never fulfilled the\noriginal challenge so never obtain the secret used for HMAC signing and is\nunable to successfully perform the HMAC signing to make requests.\n\nThe user leverages the HMAC HTTP Signing on each request that fails cookie\nvalidation to maintain a valid session. A HMAC signed HTTP request is not\ncapable of requesting and rendering HTML therefore the signed cookie still holds\nvalue in this context and cannot be abandoned upon establishing the HMAC. These\nsolve separate problems and work together for best-practice secure session\nmanagement.\n\nKey Considerations\nOkay, I know I promised not to cover  \"what are Cookies\" or \"security of\ncookies\", so I'll simply highlight the grey areas that tend to confuse and\nmissed by most people.\n\nThe cookie will disclose itself on every request the browser sends out, for\nexample a website hosting a image, a 3rd part style or script include, or a\ninteractive API. Some cookies can be restricted to only be sent over page loads\nand interactive API requests (referred to as HTTPOnly) so that servers hosting\nimages cannot obtain user cookies. The fact is even though the request is\nencrypted using TLS the cookie cannot be secret or hidden because they are\nfreely shared over HTTP and even images which are proven trivial to remotely\nobtain.\n\nCookies are not encrypted using TLS when the requester is the attacker that\nmakes the TLS request, read about how TLS was only ever intended to encrypt the\ntransport, not protect the data from intended or unintended requesters\n[https://www.langton.cloud/tls-secures-my-data-right/].\n\nCookies have a lot known security risks, many are due to knowledge gaps in how\nto properly use cookies but this is not the biggest risk. The major issue with\ncookies is application developers almost never interact with the cookie\ndirectly, and therefore are not in control of the security of the cookie or the\nimplementation of the code that manages the cookie. This leads to blind trust in\nthe cookie, the developer and thus the business delegate the security controls\nto the third party code (likely unmaintained, and always unaware of your\nbusiness risk posture).\n\nWebStorage suffers few known issues, they can be summarised into the following;\n\n * Inappropriate access by 3rd party code: scripts included directly on the page\n   from the same domain of the page. Same issue as cookies\n * CNAME collusion: Where analytics.mydomain.com has a CNAME record to\n   tracker.company.com meaning tracker can read WebStorage as though it was\n   analytics.mydomain.com. Same issue as cookies\n * Insecure Implementations: where values are set or read in business logic flow\n   that introduces security issues unrelated to the WebStorage technology\n   itself. Same issue as cookies\n\nArguably we can claim that any risks shared between a cookie and WebStorage is\nextremely less of an inherent risk for WebStorage because cookies are exposed on\nevery request and WebStorage is fully protected per origin.","feature_image":"__GHOST_URL__/content/images/2021/03/which-one.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-03-20 23:39:04","created_by":"1","updated_at":"2021-03-31 13:58:04","updated_by":"1","published_at":"2021-03-21 00:39:00","published_by":"1","custom_excerpt":"Session management still occurs via cookies, but do not disclose secrets or authorisation in a cookie because the security of WebStorage is best practice.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null}],"posts_meta":[{"id":"6064789320c4c500017fbcd1","post_id":"6064789220c4c500017fbc8b","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Prototypes and Inheritance in JavaScript","meta_description":null,"email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcd4","post_id":"6064789220c4c500017fbc8c","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"DevOps Explained - Chris Langton Blog","meta_description":"DevOps; the combination of Development and Operations. Before looking at DevOps implementations first identify the key motivations.","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcd7","post_id":"6064789220c4c500017fbc8d","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"List of Software and Frameworks ready for PHP7","meta_description":"List of IDEs, Hosting, Software, and Frameworks that have support for PHP7 you can use now. Important before you get ready for your own PHP7 migration.\n","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcdb","post_id":"6064789220c4c500017fbc8e","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"It may not be immediately apparent why a callback executes twice. A common mistake made by early node developers is forgetting to return after a callback","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcdf","post_id":"6064789220c4c500017fbc8f","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Node.js Error Handling Patterns - Chris Langton Blog","meta_description":"There is always a exceptionally bad way of doing something in NodeJS (or any language) so I figured covering this will put perspective on the alternatives.","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbce3","post_id":"6064789220c4c500017fbc90","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"When you have blocks of Asynchronous code that execute and have a callback dependant on the result of all previous functions.","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbce6","post_id":"6064789220c4c500017fbc91","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Page Loads in JavaScript and Performance - the right way","meta_description":"This is a quick post to list the methods of initializing your JavaScript the right way in terms of Page Load Speed and in turn SEO rankings improvements.","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbceb","post_id":"6064789220c4c500017fbc93","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Some of you have heard about the advancements in offline capabilities using Service Workers, and may have skimmed on Web Workers, this is why you shouldn't","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcee","post_id":"6064789220c4c500017fbc94","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"You've been building an offline web app using Application Cache and you encounter \"Application Cache Error event: Manifest fetch failed (6)\"","email_subject":null,"frontmatter":null},{"id":"6064789320c4c500017fbcf2","post_id":"6064789220c4c500017fbc95","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Facebook need to waste less effort developing features that are obsolete - instead, direct that same energy to contributing to the standards themselves.","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbcfa","post_id":"6064789220c4c500017fbc97","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In an ecosystem where hardware is software developers must write IOPS considered code. Rearchitecture and hardware upgrades isn't scalability!","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd03","post_id":"6064789220c4c500017fbc9a","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"HTML: When is consistent an anti-pattern?","meta_description":"If consistency is the only why, then perhaps in this case consistency is an anti-pattern for HTML and you should be thinking about it's harmful outcomes.","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd07","post_id":"6064789220c4c500017fbc9b","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Demonstrating 3 ways to check for properties in JavaScript Objects and explanation how each can be useful, hasOwnProperty isn't always the correct option.","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd0f","post_id":"6064789220c4c500017fbc9e","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In my repo you'll find these data type structures and several building blocks needed to kick start your dive into Functional Programming with PHP","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd12","post_id":"6064789220c4c500017fbc9f","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Browsers have for a long time stored your passwords. The latest version of Chrome 51 (as of writing) supports the Credential Management API.","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd15","post_id":"6064789220c4c500017fbca0","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Finally Passive Event Listeners has landed on our devices but there are a few gotcha's as usual and i'll help you get up and scrolling asap. ","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd1d","post_id":"6064789220c4c500017fbca2","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"You simply cannot afford a bad cache strategy at scale. Having it before you have heavy load conditions can save you immeasurably when scale is necessary","email_subject":null,"frontmatter":null},{"id":"6064789420c4c500017fbd24","post_id":"6064789220c4c500017fbca6","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"About Christopher Langton","meta_description":"Founder of DevOps at Scale, a small Amazon Web Services consultancy who tackle the tough challenges of cost-effective cloud solutions.","email_subject":null,"frontmatter":null},{"id":"6064789520c4c500017fbd3f","post_id":"6064789220c4c500017fbcaa","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"When Math isn't accurate in code - data scientists beware","meta_description":"Arbitrary-precision arithmetic. The key is to understand your language of choice, and mentor team members that are not well informed.","email_subject":null,"frontmatter":null},{"id":"6064789820c4c500017fbd7c","post_id":"6064789220c4c500017fbcc6","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Private AWS S3 - How hard could that be?","meta_description":"Applying private routing to AWS management APIs is hard.\nAWS S3 has had some poor press coverage, but has always been subject to authentication by default.","email_subject":null,"frontmatter":null}],"users":[{"id":"1","name":"Christopher Langton","slug":"stof","password":"$2a$10$yXqYrvnnVCBgppWwW5jmVuYVDYitI3y73K3j29RCBh/XXoE5NLk5e","email":"chrislangton84@gmail.com","profile_image":"//www.gravatar.com/avatar/8c33fbd033afdc8b80d499109ce6fb8e?s=250&d=mm&r=x","cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"nightShift\":true,\"launchComplete\":true}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2021-03-31 13:31:51","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:33:04","updated_by":"1"},{"id":"6064789120c4c500017fbc6f","name":"Stof","slug":"chris","password":"$2a$10$erIowChze3FyMPR4upmkG.s0zQ7BjdSUaRHccpe/8tCHHKhAeUTEG","email":"chris@langton.cloud","profile_image":"__GHOST_URL__/content/images/2021/03/cartoon-me-2.jpg","cover_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/06/supply-chain-cybersecurity.jpg","bio":null,"website":null,"location":null,"facebook":null,"twitter":"@chrisdlangton","accessibility":"{\"nightShift\":true}","status":"locked","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":"[\"getting-started\",\"upload-a-theme\",\"using-the-editor\"]","last_seen":"2021-03-27 03:50:38","created_at":"2017-11-06 11:03:08","created_by":"1","updated_at":"2021-03-31 13:55:40","updated_by":"1"}],"posts_authors":[{"id":"6064789420c4c500017fbd22","post_id":"6064789220c4c500017fbca5","author_id":"6064789120c4c500017fbc6f","sort_order":0},{"id":"6064789420c4c500017fbd23","post_id":"6064789220c4c500017fbca6","author_id":"6064789120c4c500017fbc6f","sort_order":0},{"id":"60647f8920c4c500017fbd96","post_id":"6064789220c4c500017fbcc4","author_id":"1","sort_order":0},{"id":"60647f9c20c4c500017fbd99","post_id":"6064789220c4c500017fbcc2","author_id":"1","sort_order":0},{"id":"60647fa620c4c500017fbd9b","post_id":"6064789220c4c500017fbccd","author_id":"1","sort_order":0},{"id":"60647fff20c4c500017fbda1","post_id":"6064789220c4c500017fbccc","author_id":"1","sort_order":0},{"id":"6064800920c4c500017fbda4","post_id":"6064789220c4c500017fbccb","author_id":"1","sort_order":0},{"id":"6064801620c4c500017fbda7","post_id":"6064789220c4c500017fbcca","author_id":"1","sort_order":0},{"id":"6064801e20c4c500017fbdaa","post_id":"6064789220c4c500017fbcc6","author_id":"1","sort_order":0},{"id":"6064802920c4c500017fbdac","post_id":"6064789220c4c500017fbcc9","author_id":"1","sort_order":0},{"id":"6064803120c4c500017fbdae","post_id":"6064789220c4c500017fbcc5","author_id":"1","sort_order":0},{"id":"6064803820c4c500017fbdb0","post_id":"6064789220c4c500017fbcc7","author_id":"1","sort_order":0},{"id":"6064804820c4c500017fbdb2","post_id":"6064789220c4c500017fbcba","author_id":"1","sort_order":0},{"id":"6064805620c4c500017fbdb4","post_id":"6064789220c4c500017fbcc8","author_id":"1","sort_order":0},{"id":"6064806020c4c500017fbdb6","post_id":"6064789220c4c500017fbcc3","author_id":"1","sort_order":0},{"id":"6064806c20c4c500017fbdb8","post_id":"6064789220c4c500017fbcc1","author_id":"1","sort_order":0},{"id":"6064807720c4c500017fbdba","post_id":"6064789220c4c500017fbcc0","author_id":"1","sort_order":0},{"id":"6064808420c4c500017fbdbc","post_id":"6064789220c4c500017fbcbf","author_id":"1","sort_order":0},{"id":"6064808d20c4c500017fbdbe","post_id":"6064789220c4c500017fbcb6","author_id":"1","sort_order":0},{"id":"6064809d20c4c500017fbdc0","post_id":"6064789220c4c500017fbcbe","author_id":"1","sort_order":0},{"id":"606480a620c4c500017fbdc2","post_id":"6064789220c4c500017fbcbc","author_id":"1","sort_order":0},{"id":"606480b520c4c500017fbdc4","post_id":"6064789220c4c500017fbcbb","author_id":"1","sort_order":0},{"id":"606480d720c4c500017fbdc6","post_id":"6064789220c4c500017fbcbd","author_id":"1","sort_order":0},{"id":"606480e120c4c500017fbdc8","post_id":"6064789220c4c500017fbcb9","author_id":"1","sort_order":0},{"id":"606480eb20c4c500017fbdca","post_id":"6064789220c4c500017fbcb7","author_id":"1","sort_order":0},{"id":"6064811220c4c500017fbdd0","post_id":"6064789220c4c500017fbcb8","author_id":"1","sort_order":0},{"id":"606482e120c4c500017fbdd5","post_id":"6064789220c4c500017fbcb4","author_id":"1","sort_order":0},{"id":"606482ef20c4c500017fbdd8","post_id":"6064789220c4c500017fbcb5","author_id":"1","sort_order":0},{"id":"606482f920c4c500017fbdda","post_id":"6064789220c4c500017fbcb2","author_id":"1","sort_order":0},{"id":"6064830320c4c500017fbddc","post_id":"6064789220c4c500017fbcb3","author_id":"1","sort_order":0},{"id":"6064830f20c4c500017fbddf","post_id":"6064789220c4c500017fbcb1","author_id":"1","sort_order":0},{"id":"6064831c20c4c500017fbde1","post_id":"6064789220c4c500017fbc90","author_id":"1","sort_order":0},{"id":"6064832520c4c500017fbde3","post_id":"6064789220c4c500017fbcaa","author_id":"1","sort_order":0},{"id":"6064832f20c4c500017fbde6","post_id":"6064789220c4c500017fbcac","author_id":"1","sort_order":0},{"id":"6064833820c4c500017fbde8","post_id":"6064789220c4c500017fbcad","author_id":"1","sort_order":0},{"id":"6064834320c4c500017fbdea","post_id":"6064789220c4c500017fbcae","author_id":"1","sort_order":0},{"id":"6064834c20c4c500017fbdec","post_id":"6064789220c4c500017fbcb0","author_id":"1","sort_order":0},{"id":"6064835620c4c500017fbdee","post_id":"6064789220c4c500017fbcab","author_id":"1","sort_order":0},{"id":"6064836020c4c500017fbdf0","post_id":"6064789220c4c500017fbcaf","author_id":"1","sort_order":0},{"id":"6064837020c4c500017fbdf3","post_id":"6064789220c4c500017fbca9","author_id":"1","sort_order":0},{"id":"6064838520c4c500017fbdf5","post_id":"6064789220c4c500017fbca8","author_id":"1","sort_order":0},{"id":"6064838f20c4c500017fbdf7","post_id":"6064789220c4c500017fbca7","author_id":"1","sort_order":0},{"id":"6064839820c4c500017fbdf9","post_id":"6064789220c4c500017fbca2","author_id":"1","sort_order":0},{"id":"606483a220c4c500017fbdfb","post_id":"6064789220c4c500017fbca4","author_id":"1","sort_order":0},{"id":"606483ae20c4c500017fbdfe","post_id":"6064789220c4c500017fbca0","author_id":"1","sort_order":0},{"id":"606483ce20c4c500017fbe01","post_id":"6064789220c4c500017fbca3","author_id":"1","sort_order":0},{"id":"606483d720c4c500017fbe03","post_id":"6064789220c4c500017fbca1","author_id":"1","sort_order":0},{"id":"606483e420c4c500017fbe05","post_id":"6064789220c4c500017fbc9f","author_id":"1","sort_order":0},{"id":"606483ee20c4c500017fbe07","post_id":"6064789220c4c500017fbc9d","author_id":"1","sort_order":0},{"id":"606483fe20c4c500017fbe09","post_id":"6064789220c4c500017fbc9e","author_id":"1","sort_order":0},{"id":"6064840720c4c500017fbe0b","post_id":"6064789220c4c500017fbc9c","author_id":"1","sort_order":0},{"id":"6064841220c4c500017fbe0d","post_id":"6064789220c4c500017fbc9a","author_id":"1","sort_order":0},{"id":"6064841e20c4c500017fbe0f","post_id":"6064789220c4c500017fbc8f","author_id":"1","sort_order":0},{"id":"6064842920c4c500017fbe12","post_id":"6064789220c4c500017fbc97","author_id":"1","sort_order":0},{"id":"6064843120c4c500017fbe14","post_id":"6064789220c4c500017fbc9b","author_id":"1","sort_order":0},{"id":"6064843d20c4c500017fbe16","post_id":"6064789220c4c500017fbc99","author_id":"1","sort_order":0},{"id":"6064845020c4c500017fbe18","post_id":"6064789220c4c500017fbc98","author_id":"1","sort_order":0},{"id":"6064849e20c4c500017fbe1a","post_id":"6064789220c4c500017fbc93","author_id":"1","sort_order":0},{"id":"606484a920c4c500017fbe1c","post_id":"6064789220c4c500017fbc96","author_id":"1","sort_order":0},{"id":"606484b320c4c500017fbe1e","post_id":"6064789220c4c500017fbc95","author_id":"1","sort_order":0},{"id":"606484c120c4c500017fbe20","post_id":"6064789220c4c500017fbc94","author_id":"1","sort_order":0},{"id":"606484cd20c4c500017fbe22","post_id":"6064789220c4c500017fbc92","author_id":"1","sort_order":0},{"id":"606484d620c4c500017fbe24","post_id":"6064789220c4c500017fbc8b","author_id":"1","sort_order":0},{"id":"606484e520c4c500017fbe26","post_id":"6064789220c4c500017fbc8c","author_id":"1","sort_order":0},{"id":"606484ef20c4c500017fbe29","post_id":"6064789220c4c500017fbc8d","author_id":"1","sort_order":0},{"id":"606484f820c4c500017fbe2b","post_id":"6064789220c4c500017fbc91","author_id":"1","sort_order":0},{"id":"6064850120c4c500017fbe2d","post_id":"6064789220c4c500017fbc8e","author_id":"1","sort_order":0}],"roles":[{"id":"6064747b20c4c500017fb99f","name":"Administrator","description":"Administrators","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a0","name":"Editor","description":"Editors","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a1","name":"Author","description":"Authors","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a2","name":"Contributor","description":"Contributors","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a3","name":"Owner","description":"Blog Owner","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a4","name":"Admin Integration","description":"External Apps","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a5","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a6","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"}],"roles_users":[{"id":"6064747d20c4c500017fbaf6","role_id":"6064747b20c4c500017fb9a3","user_id":"1"},{"id":"6064789120c4c500017fbc70","role_id":"6064747b20c4c500017fb99f","user_id":"6064789120c4c500017fbc6f"}],"permissions":[{"id":"6064747b20c4c500017fb9a7","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a8","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9a9","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9aa","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ab","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ac","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ad","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ae","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9af","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b0","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b1","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b2","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b3","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b4","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b5","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b6","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b7","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b8","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9b9","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ba","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9bb","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9bc","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9bd","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9be","name":"Activate themes","object_type":"theme","action_type":"activate","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9bf","name":"Upload themes","object_type":"theme","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c0","name":"Download themes","object_type":"theme","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c1","name":"Delete themes","object_type":"theme","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c2","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c3","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c4","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c5","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c6","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c7","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c8","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9c9","name":"Browse invites","object_type":"invite","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ca","name":"Read invites","object_type":"invite","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9cb","name":"Edit invites","object_type":"invite","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9cc","name":"Add invites","object_type":"invite","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9cd","name":"Delete invites","object_type":"invite","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ce","name":"Download redirects","object_type":"redirect","action_type":"download","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9cf","name":"Upload redirects","object_type":"redirect","action_type":"upload","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d0","name":"Add webhooks","object_type":"webhook","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d1","name":"Edit webhooks","object_type":"webhook","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d2","name":"Delete webhooks","object_type":"webhook","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d3","name":"Browse integrations","object_type":"integration","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d4","name":"Read integrations","object_type":"integration","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d5","name":"Edit integrations","object_type":"integration","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d6","name":"Add integrations","object_type":"integration","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d7","name":"Delete integrations","object_type":"integration","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d8","name":"Browse API keys","object_type":"api_key","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9d9","name":"Read API keys","object_type":"api_key","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9da","name":"Edit API keys","object_type":"api_key","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9db","name":"Add API keys","object_type":"api_key","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9dc","name":"Delete API keys","object_type":"api_key","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9dd","name":"Browse Actions","object_type":"action","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9de","name":"Browse Members","object_type":"member","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9df","name":"Read Members","object_type":"member","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e0","name":"Edit Members","object_type":"member","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e1","name":"Add Members","object_type":"member","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e2","name":"Delete Members","object_type":"member","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e3","name":"Publish posts","object_type":"post","action_type":"publish","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e4","name":"Backup database","object_type":"db","action_type":"backupContent","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e5","name":"Email preview","object_type":"email_preview","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e6","name":"Send test email","object_type":"email_preview","action_type":"sendTestEmail","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e7","name":"Browse emails","object_type":"email","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e8","name":"Read emails","object_type":"email","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9e9","name":"Retry emails","object_type":"email","action_type":"retry","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ea","name":"Browse labels","object_type":"label","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9eb","name":"Read labels","object_type":"label","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ec","name":"Edit labels","object_type":"label","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ed","name":"Add labels","object_type":"label","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ee","name":"Delete labels","object_type":"label","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9ef","name":"Read member signin urls","object_type":"member_signin_url","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f0","name":"Read identities","object_type":"identity","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f1","name":"Auth Stripe Connect for Members","object_type":"members_stripe_connect","action_type":"auth","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f2","name":"Browse snippets","object_type":"snippet","action_type":"browse","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f3","name":"Read snippets","object_type":"snippet","action_type":"read","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f4","name":"Edit snippets","object_type":"snippet","action_type":"edit","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f5","name":"Add snippets","object_type":"snippet","action_type":"add","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"},{"id":"6064747b20c4c500017fb9f6","name":"Delete snippets","object_type":"snippet","action_type":"destroy","object_id":null,"created_at":"2021-03-31 13:09:15","created_by":"1","updated_at":"2021-03-31 13:09:15","updated_by":"1"}],"permissions_users":[],"permissions_roles":[{"id":"6064747c20c4c500017fba14","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9a7"},{"id":"6064747c20c4c500017fba15","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9a8"},{"id":"6064747c20c4c500017fba16","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9a9"},{"id":"6064747c20c4c500017fba17","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e4"},{"id":"6064747c20c4c500017fba18","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9aa"},{"id":"6064747c20c4c500017fba19","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ab"},{"id":"6064747c20c4c500017fba1a","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ac"},{"id":"6064747c20c4c500017fba1b","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ad"},{"id":"6064747c20c4c500017fba1c","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ae"},{"id":"6064747c20c4c500017fba1d","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9af"},{"id":"6064747c20c4c500017fba1e","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b0"},{"id":"6064747c20c4c500017fba1f","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b1"},{"id":"6064747c20c4c500017fba20","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b2"},{"id":"6064747c20c4c500017fba21","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e3"},{"id":"6064747c20c4c500017fba22","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b3"},{"id":"6064747c20c4c500017fba23","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b4"},{"id":"6064747c20c4c500017fba24","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b5"},{"id":"6064747c20c4c500017fba25","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b6"},{"id":"6064747c20c4c500017fba26","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b7"},{"id":"6064747c20c4c500017fba27","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b8"},{"id":"6064747c20c4c500017fba28","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9b9"},{"id":"6064747c20c4c500017fba29","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ba"},{"id":"6064747c20c4c500017fba2a","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9bb"},{"id":"6064747c20c4c500017fba2b","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9bc"},{"id":"6064747c20c4c500017fba2c","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9bd"},{"id":"6064747c20c4c500017fba2d","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9be"},{"id":"6064747c20c4c500017fba2e","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9bf"},{"id":"6064747c20c4c500017fba2f","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c0"},{"id":"6064747c20c4c500017fba30","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c1"},{"id":"6064747c20c4c500017fba31","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c2"},{"id":"6064747c20c4c500017fba32","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c3"},{"id":"6064747c20c4c500017fba33","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c4"},{"id":"6064747c20c4c500017fba34","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c5"},{"id":"6064747c20c4c500017fba35","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c6"},{"id":"6064747c20c4c500017fba36","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c7"},{"id":"6064747c20c4c500017fba37","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c8"},{"id":"6064747c20c4c500017fba38","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9c9"},{"id":"6064747c20c4c500017fba39","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ca"},{"id":"6064747c20c4c500017fba3a","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9cb"},{"id":"6064747c20c4c500017fba3b","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9cc"},{"id":"6064747c20c4c500017fba3c","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9cd"},{"id":"6064747c20c4c500017fba3d","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ce"},{"id":"6064747c20c4c500017fba3e","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9cf"},{"id":"6064747c20c4c500017fba3f","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d0"},{"id":"6064747c20c4c500017fba40","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d1"},{"id":"6064747c20c4c500017fba41","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d2"},{"id":"6064747c20c4c500017fba42","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d3"},{"id":"6064747c20c4c500017fba43","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d4"},{"id":"6064747c20c4c500017fba44","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d5"},{"id":"6064747c20c4c500017fba45","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d6"},{"id":"6064747c20c4c500017fba46","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d7"},{"id":"6064747c20c4c500017fba47","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d8"},{"id":"6064747c20c4c500017fba48","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9d9"},{"id":"6064747c20c4c500017fba49","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9da"},{"id":"6064747c20c4c500017fba4a","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9db"},{"id":"6064747c20c4c500017fba4b","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9dc"},{"id":"6064747c20c4c500017fba4c","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9dd"},{"id":"6064747c20c4c500017fba4d","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9de"},{"id":"6064747c20c4c500017fba4e","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9df"},{"id":"6064747c20c4c500017fba4f","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e0"},{"id":"6064747c20c4c500017fba50","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e1"},{"id":"6064747c20c4c500017fba51","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e2"},{"id":"6064747c20c4c500017fba52","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ea"},{"id":"6064747c20c4c500017fba53","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9eb"},{"id":"6064747c20c4c500017fba54","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ec"},{"id":"6064747c20c4c500017fba55","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ed"},{"id":"6064747c20c4c500017fba56","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ee"},{"id":"6064747c20c4c500017fba57","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e5"},{"id":"6064747c20c4c500017fba58","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e6"},{"id":"6064747c20c4c500017fba59","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e7"},{"id":"6064747c20c4c500017fba5a","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e8"},{"id":"6064747c20c4c500017fba5b","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9e9"},{"id":"6064747c20c4c500017fba5c","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9ef"},{"id":"6064747c20c4c500017fba5d","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9f2"},{"id":"6064747c20c4c500017fba5e","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9f3"},{"id":"6064747c20c4c500017fba5f","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9f4"},{"id":"6064747c20c4c500017fba60","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9f5"},{"id":"6064747c20c4c500017fba61","role_id":"6064747b20c4c500017fb99f","permission_id":"6064747b20c4c500017fb9f6"},{"id":"6064747c20c4c500017fba62","role_id":"6064747b20c4c500017fb9a5","permission_id":"6064747b20c4c500017fb9a7"},{"id":"6064747c20c4c500017fba63","role_id":"6064747b20c4c500017fb9a5","permission_id":"6064747b20c4c500017fb9a8"},{"id":"6064747c20c4c500017fba64","role_id":"6064747b20c4c500017fb9a5","permission_id":"6064747b20c4c500017fb9a9"},{"id":"6064747c20c4c500017fba65","role_id":"6064747b20c4c500017fb9a5","permission_id":"6064747b20c4c500017fb9e4"},{"id":"6064747c20c4c500017fba66","role_id":"6064747b20c4c500017fb9a6","permission_id":"6064747b20c4c500017fb9e3"},{"id":"6064747c20c4c500017fba67","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9aa"},{"id":"6064747c20c4c500017fba68","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ab"},{"id":"6064747c20c4c500017fba69","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ac"},{"id":"6064747c20c4c500017fba6a","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ad"},{"id":"6064747c20c4c500017fba6b","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ae"},{"id":"6064747c20c4c500017fba6c","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9af"},{"id":"6064747c20c4c500017fba6d","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b0"},{"id":"6064747c20c4c500017fba6e","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b1"},{"id":"6064747c20c4c500017fba6f","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b2"},{"id":"6064747c20c4c500017fba70","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e3"},{"id":"6064747c20c4c500017fba71","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b3"},{"id":"6064747c20c4c500017fba72","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b4"},{"id":"6064747c20c4c500017fba73","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b5"},{"id":"6064747c20c4c500017fba74","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b6"},{"id":"6064747c20c4c500017fba75","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b7"},{"id":"6064747c20c4c500017fba76","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b8"},{"id":"6064747c20c4c500017fba77","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9b9"},{"id":"6064747c20c4c500017fba78","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ba"},{"id":"6064747c20c4c500017fba79","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9bb"},{"id":"6064747c20c4c500017fba7a","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9bc"},{"id":"6064747c20c4c500017fba7b","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9bd"},{"id":"6064747c20c4c500017fba7c","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9be"},{"id":"6064747c20c4c500017fba7d","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9bf"},{"id":"6064747c20c4c500017fba7e","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c0"},{"id":"6064747c20c4c500017fba7f","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c1"},{"id":"6064747c20c4c500017fba80","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c2"},{"id":"6064747c20c4c500017fba81","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c3"},{"id":"6064747c20c4c500017fba82","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c4"},{"id":"6064747c20c4c500017fba83","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c5"},{"id":"6064747c20c4c500017fba84","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c6"},{"id":"6064747c20c4c500017fba85","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c7"},{"id":"6064747c20c4c500017fba86","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c8"},{"id":"6064747c20c4c500017fba87","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9c9"},{"id":"6064747c20c4c500017fba88","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ca"},{"id":"6064747c20c4c500017fba89","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9cb"},{"id":"6064747c20c4c500017fba8a","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9cc"},{"id":"6064747c20c4c500017fba8b","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9cd"},{"id":"6064747c20c4c500017fba8c","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ce"},{"id":"6064747c20c4c500017fba8d","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9cf"},{"id":"6064747c20c4c500017fba8e","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9d0"},{"id":"6064747c20c4c500017fba8f","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9d1"},{"id":"6064747c20c4c500017fba90","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9d2"},{"id":"6064747c20c4c500017fba91","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9dd"},{"id":"6064747c20c4c500017fba92","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9de"},{"id":"6064747c20c4c500017fba93","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9df"},{"id":"6064747c20c4c500017fba94","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e0"},{"id":"6064747c20c4c500017fba95","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e1"},{"id":"6064747c20c4c500017fba96","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e2"},{"id":"6064747c20c4c500017fba97","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ea"},{"id":"6064747c20c4c500017fba98","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9eb"},{"id":"6064747c20c4c500017fba99","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ec"},{"id":"6064747c20c4c500017fba9a","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ed"},{"id":"6064747c20c4c500017fba9b","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9ee"},{"id":"6064747c20c4c500017fba9c","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e5"},{"id":"6064747c20c4c500017fba9d","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e6"},{"id":"6064747c20c4c500017fba9e","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e7"},{"id":"6064747c20c4c500017fba9f","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e8"},{"id":"6064747c20c4c500017fbaa0","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9e9"},{"id":"6064747c20c4c500017fbaa1","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9f2"},{"id":"6064747c20c4c500017fbaa2","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9f3"},{"id":"6064747c20c4c500017fbaa3","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9f4"},{"id":"6064747c20c4c500017fbaa4","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9f5"},{"id":"6064747c20c4c500017fbaa5","role_id":"6064747b20c4c500017fb9a4","permission_id":"6064747b20c4c500017fb9f6"},{"id":"6064747c20c4c500017fbaa6","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ab"},{"id":"6064747c20c4c500017fbaa7","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ac"},{"id":"6064747c20c4c500017fbaa8","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ad"},{"id":"6064747c20c4c500017fbaa9","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ae"},{"id":"6064747c20c4c500017fbaaa","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9af"},{"id":"6064747c20c4c500017fbaab","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b0"},{"id":"6064747c20c4c500017fbaac","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b1"},{"id":"6064747c20c4c500017fbaad","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b2"},{"id":"6064747c20c4c500017fbaae","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e3"},{"id":"6064747c20c4c500017fbaaf","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b3"},{"id":"6064747c20c4c500017fbab0","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b4"},{"id":"6064747c20c4c500017fbab1","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b6"},{"id":"6064747c20c4c500017fbab2","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b7"},{"id":"6064747c20c4c500017fbab3","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b8"},{"id":"6064747c20c4c500017fbab4","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9b9"},{"id":"6064747c20c4c500017fbab5","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ba"},{"id":"6064747c20c4c500017fbab6","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9bb"},{"id":"6064747c20c4c500017fbab7","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c2"},{"id":"6064747c20c4c500017fbab8","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c3"},{"id":"6064747c20c4c500017fbab9","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c4"},{"id":"6064747c20c4c500017fbaba","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c5"},{"id":"6064747c20c4c500017fbabb","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c6"},{"id":"6064747c20c4c500017fbabc","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c7"},{"id":"6064747c20c4c500017fbabd","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c8"},{"id":"6064747c20c4c500017fbabe","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9c9"},{"id":"6064747c20c4c500017fbabf","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9ca"},{"id":"6064747c20c4c500017fbac0","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9cb"},{"id":"6064747c20c4c500017fbac1","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9cc"},{"id":"6064747c20c4c500017fbac2","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9cd"},{"id":"6064747c20c4c500017fbac3","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9bc"},{"id":"6064747c20c4c500017fbac4","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e5"},{"id":"6064747c20c4c500017fbac5","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e6"},{"id":"6064747c20c4c500017fbac6","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e7"},{"id":"6064747c20c4c500017fbac7","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e8"},{"id":"6064747c20c4c500017fbac8","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9e9"},{"id":"6064747c20c4c500017fbac9","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9f2"},{"id":"6064747c20c4c500017fbaca","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9f3"},{"id":"6064747c20c4c500017fbacb","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9f4"},{"id":"6064747c20c4c500017fbacc","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9f5"},{"id":"6064747c20c4c500017fbacd","role_id":"6064747b20c4c500017fb9a0","permission_id":"6064747b20c4c500017fb9f6"},{"id":"6064747c20c4c500017fbace","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9ae"},{"id":"6064747c20c4c500017fbacf","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9af"},{"id":"6064747c20c4c500017fbad0","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b1"},{"id":"6064747c20c4c500017fbad1","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b3"},{"id":"6064747c20c4c500017fbad2","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b4"},{"id":"6064747c20c4c500017fbad3","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b6"},{"id":"6064747c20c4c500017fbad4","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b7"},{"id":"6064747c20c4c500017fbad5","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9b8"},{"id":"6064747c20c4c500017fbad6","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9ba"},{"id":"6064747c20c4c500017fbad7","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9c2"},{"id":"6064747c20c4c500017fbad8","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9c3"},{"id":"6064747c20c4c500017fbad9","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9c8"},{"id":"6064747c20c4c500017fbada","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9bc"},{"id":"6064747c20c4c500017fbadb","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9e5"},{"id":"6064747c20c4c500017fbadc","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9e8"},{"id":"6064747c20c4c500017fbadd","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9f2"},{"id":"6064747c20c4c500017fbade","role_id":"6064747b20c4c500017fb9a1","permission_id":"6064747b20c4c500017fb9f3"},{"id":"6064747c20c4c500017fbadf","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9ae"},{"id":"6064747c20c4c500017fbae0","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9af"},{"id":"6064747c20c4c500017fbae1","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b1"},{"id":"6064747c20c4c500017fbae2","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b3"},{"id":"6064747c20c4c500017fbae3","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b4"},{"id":"6064747c20c4c500017fbae4","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b6"},{"id":"6064747c20c4c500017fbae5","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b7"},{"id":"6064747c20c4c500017fbae6","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9b8"},{"id":"6064747c20c4c500017fbae7","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9c2"},{"id":"6064747c20c4c500017fbae8","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9c3"},{"id":"6064747c20c4c500017fbae9","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9c8"},{"id":"6064747c20c4c500017fbaea","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9bc"},{"id":"6064747c20c4c500017fbaeb","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9e5"},{"id":"6064747c20c4c500017fbaec","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9e8"},{"id":"6064747c20c4c500017fbaed","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9f2"},{"id":"6064747c20c4c500017fbaee","role_id":"6064747b20c4c500017fb9a2","permission_id":"6064747b20c4c500017fb9f3"}],"settings":[{"id":"6064747d20c4c500017fbaf7","group":"core","key":"db_hash","value":"1b6663c7-515f-454d-bb43-95782e92b7ba","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747d20c4c500017fbaf8","group":"core","key":"routes_hash","value":"3d180d52c663d173a6be791ef411ed01","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:20","updated_by":"1"},{"id":"6064747d20c4c500017fbaf9","group":"core","key":"next_update_check","value":"1617282893","type":"number","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:14:53","updated_by":"1"},{"id":"6064747d20c4c500017fbafa","group":"core","key":"notifications","value":"[]","type":"array","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747d20c4c500017fbafb","group":"core","key":"session_secret","value":"b04dccdd37db5c26380c8d9bb0d7b0e1225d0bc18af3c674cef80cf694cb80d9","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747d20c4c500017fbafc","group":"core","key":"theme_session_secret","value":"2a256c4bb666c2d7932b05f4f79952f68cf872e5f957e8e3dca7b299b9808b4f","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747d20c4c500017fbafd","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJIU97yZfX7ul9de5BdmgMpIhtsZUJ8Uv4kDbG/mnKbRIfiFOQZf3323n+gRb7/K\ncRaQVxm6eDXVJuXVG//Ux1wxMzGfzT/kLm08FMrXxQOiFhL4BxPEgKvMpaQzWhibrM32AHT2\nDDcMJq3bLt41rzywid1NicRmD1VMgmwQXg8PAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747d20c4c500017fbafe","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQCSFPe8mX1+7pfXXuQXZoDKSIbbGVCfFL+JA2xv5pym0SH4hTkGX999t5/o\nEW+/ynEWkFcZung11Sbl1Rv/1MdcMTMxn80/5C5tPBTK18UDohYS+AcTxICrzKWkM1oYm6zN\n9gB09gw3DCat2y7eNa88sIndTYnEZg9VTIJsEF4PDwIDAQABAoGAA+4CeBVc9eQvO4D2+9Yr\nEsX+srgsngrt2B66dQwsHKS1mDzlOtGrbnNZHuIRMoScpDG/1hZZE4GnaTlhZnFv53WVvPaz\nARLXLNzcZFWkb+Kmw82kUitbhMOjnPRZqxVNgqK4G7YzRf1jzn63Oz3N10f/CfQbE9Gkexlh\nfWatkHECQQDJLfxq4Qdka6UTYjyQZx43v6MACvB9ptKDNaGT9P/p8f9XwiH7U1kY2N7uaYuZ\ngYfTkkWeo0fq9rhR9ik34G/HAkEAueNwrZgtxrshBTrgdgLrgFCQJUV+qJ7ghjl4rmfwCl1y\nqzQ+BldCoLXwPySVsMk1+dEetGno2JDh07Es5SD2eQJAFNGbc3wCBdRV6uGZnt/s8pv0REbd\niENBOa7NH8CU0yLl57EBTz7a04Vt3IIznhkXcz+J0NbY1GnfvSuaLFwfdQJAU9f6YtJZE2Ik\nKq+7vn7iqiWsXqNR1cyTiPczL7dB1esAZahSsZl6O/LUQibGPc8DwYJJTybWF5iBw3IOxepw\nQQJBAIMlQrS5/WQIXZ8WSxkN7L1SGeFm5mXHZzRNZzH/c7yFC0iahzquGiLoOzbh1ez0I3pw\naJvmiilUE32q9hzQc14=\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbaff","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAIrxWBF4HP/+ytzmTt3H5muu1mpYXLKkYAT4kPlydnXSw0I3qPNEsFJLeo9MXIgz\nLt71veuvF5B+9FjCslOdSplsI9uVNYlPSutJQHvQJeSqtCSMUFG0G0j/iKPoYv0LkxNioER8\nhGXWFRivN2oQh6EpJ502glAD6/JCPPYN5a+9AgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb00","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQCK8VgReBz//src5k7dx+ZrrtZqWFyypGAE+JD5cnZ10sNCN6jzRLBSS3qP\nTFyIMy7e9b3rrxeQfvRYwrJTnUqZbCPblTWJT0rrSUB70CXkqrQkjFBRtBtI/4ij6GL9C5MT\nYqBEfIRl1hUYrzdqEIehKSedNoJQA+vyQjz2DeWvvQIDAQABAoGAfo2XBspdt7ousoe/u/gw\n41IrkE+Nl1Vq8/wnqF1ekrD3EqA57eJZq4t+RXlrKs9b2M8CPuKLVxybmSE1Zj11TOV/MgAe\nv6tNztgGmPavql/oJwcvIsshFDYh6yE8m+v/hH70ixBArzcnMaYLzz13CpQBaShgKNoug06K\n2YLHGmECQQD6vkYMOfzW8bDzZMAwwzFtd03w99orarNKuUWXZICVXO+kekjij9VmStI8I5mo\nZe9SFrbUx2Mo2DXUNwbswMClAkEAjdsK8FA+0ObqEgpn6F//gWGjn1Q4pziP8h6T8vaQftwW\n7SbAGiJnNOj5Unb/9SuAp2GJt/s5LthOgzDrKgKvOQJAE/Dl6TsDXKTECddNYZNkbuw6Cpvr\ngeBuedbaLfwULczCUjZXbY06z3PL3HSIV9gjfEfKKong3PK09TebGWEK8QJAakIQwJArbYn6\n/caDrQHzvzOnP9rzkPcChEe5ZT6iYBR2q+/h5/GrQ2vcDpSLCFgsx7R9V648WEW5EfZbwR0/\nqQJBAPYDjXTyrIB1yk6m1nR0c2jaQzfGDA6C8EtM4BVMQ/PjtQE6yAf+84t/VckJk40Dvbth\n6O46vVV0CnbHQedi2cc=\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb01","group":"core","key":"members_email_auth_secret","value":"b70a408e2bdd8191ac6fbb50e6d8dd42132dc4e1b05a9413505c95f4249efe5f6a401de8441256674ff60b7905859ce772273a110bf58c8e2f7980db4f98c171","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb04","group":"site","key":"title","value":"Application Development and Security","type":"string","flags":"PUBLIC","created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2021-03-31 13:50:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb05","group":"site","key":"description","value":"Christopher Langton","type":"string","flags":"PUBLIC","created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2021-03-31 13:50:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb06","group":"site","key":"logo","value":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/C4D40304-966E-4D72-8EE0-5CB1EDA7DA0B.png","type":"string","flags":null,"created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2017-11-12 06:07:26","updated_by":"1"},{"id":"6064747e20c4c500017fbb07","group":"site","key":"cover_image","value":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2018/06/supply-chain-cybersecurity-11.jpg","type":"string","flags":null,"created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2018-06-24 04:50:08","updated_by":"1"},{"id":"6064747e20c4c500017fbb08","group":"site","key":"icon","value":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/11/C4D40304-966E-4D72-8EE0-5CB1EDA7DA0B.png","type":"string","flags":null,"created_at":"2017-11-06 11:03:18","created_by":"1","updated_at":"2017-11-12 12:05:08","updated_by":"1"},{"id":"6064747e20c4c500017fbb09","group":"site","key":"accent_color","value":"#1eb899","type":"string","flags":"PUBLIC","created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:32:51","updated_by":"1"},{"id":"6064747e20c4c500017fbb0a","group":"site","key":"lang","value":"en","type":"string","flags":null,"created_at":"2017-11-06 11:03:18","created_by":"1","updated_at":"2017-11-06 11:03:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb0b","group":"site","key":"timezone","value":"Australia/Sydney","type":"string","flags":null,"created_at":"2016-07-27 15:56:17","created_by":"1","updated_at":"2017-11-07 05:30:38","updated_by":"1"},{"id":"6064747e20c4c500017fbb0c","group":"site","key":"codeinjection_head","value":"<!-- Global site tag (gtag.js) - Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-75269882-1\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'UA-75269882-1');\n</script>\n<style>\n.post-full-content table td, .post-full-content table th {\n    white-space: pre-wrap;\n}\n</style>","type":"string","flags":null,"created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2019-09-08 02:39:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb0d","group":"site","key":"codeinjection_foot","value":"<figure class=\"op-tracker\">\n    <iframe>\n      <script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-75269882-1\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'UA-75269882-1');\n</script>\n    </iframe>\n</figure>","type":"string","flags":null,"created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2017-11-27 00:59:00","updated_by":"1"},{"id":"6064747e20c4c500017fbb0e","group":"site","key":"facebook","value":null,"type":"string","flags":null,"created_at":"2016-05-18 22:59:01","created_by":"1","updated_at":"2017-11-07 05:30:37","updated_by":"1"},{"id":"6064747e20c4c500017fbb0f","group":"site","key":"twitter","value":"@chrisdlangton","type":"string","flags":null,"created_at":"2016-05-18 22:59:01","created_by":"1","updated_at":"2017-11-07 05:30:37","updated_by":"1"},{"id":"6064747e20c4c500017fbb10","group":"site","key":"navigation","value":"[{\"label\":\"About\",\"url\":\"/about/\"}]","type":"array","flags":null,"created_at":"2016-03-16 20:52:16","created_by":"1","updated_at":"2017-11-07 05:30:37","updated_by":"1"},{"id":"6064747e20c4c500017fbb11","group":"site","key":"secondary_navigation","value":"[]","type":"array","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb12","group":"site","key":"meta_title","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2021-03-31 13:48:32","updated_by":"1"},{"id":"6064747e20c4c500017fbb13","group":"site","key":"meta_description","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2021-03-31 13:48:32","updated_by":"1"},{"id":"6064747e20c4c500017fbb14","group":"site","key":"og_image","value":"__GHOST_URL__/content/images/2021/03/supply-chain-cybersecurity-11-1.jpg","type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2021-03-31 13:48:32","updated_by":"1"},{"id":"6064747e20c4c500017fbb15","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb16","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb17","group":"site","key":"twitter_image","value":"__GHOST_URL__/content/images/2021/03/supply-chain-cybersecurity-11.jpg","type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2021-03-31 13:48:32","updated_by":"1"},{"id":"6064747e20c4c500017fbb18","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb19","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb1a","group":"theme","key":"active_theme","value":"attila-master","type":"string","flags":"RO","created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:38:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb1b","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb1c","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb1d","group":"private","key":"public_hash","value":"fbd8a11b3c1eaa581025aedc71af0a","type":"string","flags":null,"created_at":"2017-11-06 11:03:18","created_by":"1","updated_at":"2017-11-06 11:03:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb1e","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2020-02-21 01:33:58","created_by":"1","updated_at":"2020-02-21 01:33:58","updated_by":"1"},{"id":"6064747e20c4c500017fbb1f","group":"members","key":"members_allow_free_signup","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb20","group":"members","key":"members_from_address","value":"noreply","type":"string","flags":"RO","created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb21","group":"members","key":"members_support_address","value":"noreply","type":"string","flags":"PUBLIC,RO","created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb22","group":"members","key":"members_reply_address","value":"newsletter","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb23","group":"members","key":"members_free_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb24","group":"members","key":"members_paid_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb25","group":"members","key":"stripe_product_name","value":"Ghost Subscription","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb28","group":"members","key":"stripe_plans","value":"[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":500},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":5000}]","type":"array","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb2b","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb2c","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb2e","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb2f","group":"portal","key":"portal_button","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb30","group":"portal","key":"portal_plans","value":"[\"free\",\"monthly\",\"yearly\"]","type":"array","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:32:51","updated_by":"1"},{"id":"6064747e20c4c500017fbb31","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb32","group":"portal","key":"portal_button_icon","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb33","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb34","group":"email","key":"mailgun_domain","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb35","group":"email","key":"mailgun_api_key","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb36","group":"email","key":"mailgun_base_url","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb37","group":"email","key":"email_track_opens","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb38","group":"amp","key":"amp","value":"true","type":"boolean","flags":null,"created_at":"2017-01-12 21:41:25","created_by":"1","updated_at":"2017-11-07 05:30:38","updated_by":"1"},{"id":"6064747e20c4c500017fbb39","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb3a","group":"firstpromoter","key":"firstpromoter","value":"false","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb3b","group":"firstpromoter","key":"firstpromoter_id","value":null,"type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb3c","group":"slack","key":"slack_url","value":"","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:50:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb3d","group":"slack","key":"slack_username","value":"Ghost","type":"string","flags":null,"created_at":"2016-05-18 22:59:01","created_by":"1","updated_at":"2021-03-31 13:50:14","updated_by":"1"},{"id":"6064747e20c4c500017fbb3e","group":"unsplash","key":"unsplash","value":"false","type":"boolean","flags":null,"created_at":"2017-11-06 11:03:18","created_by":"1","updated_at":"2021-03-31 13:26:48","updated_by":"1"},{"id":"6064747e20c4c500017fbb3f","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb40","group":"newsletter","key":"newsletter_show_badge","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb41","group":"newsletter","key":"newsletter_show_header","value":"true","type":"boolean","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb42","group":"newsletter","key":"newsletter_body_font_category","value":"sans_serif","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"},{"id":"6064747e20c4c500017fbb43","group":"newsletter","key":"newsletter_footer_content","value":"","type":"string","flags":null,"created_at":"2021-03-31 13:09:18","created_by":"1","updated_at":"2021-03-31 13:09:18","updated_by":"1"}],"tags":[{"id":"6064789120c4c500017fbc79","name":"Getting Started","slug":"getting-started","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-21 01:33:56","created_by":"1","updated_at":"2020-02-21 01:33:56","updated_by":null},{"id":"6064789120c4c500017fbc7a","name":"Opinion","slug":"opinion","description":"Chris Langton's views and opinions are generally based on personal experience and should not be associated to any other person or organisation.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-15.jpeg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Opinions of Chris Langton","meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-18 02:00:50","created_by":"1","updated_at":"2019-01-09 23:49:56","updated_by":null},{"id":"6064789120c4c500017fbc7b","name":"JavaScript","slug":"javascript","description":"One of the most popular programming languages and one that I find so natural.\nHere i'll share with you the best and worst so you might learn as i have.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/6d88463f-4018-4b15-a2bb-c191bc4caacd_original.jpg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Chris Langton on JavaScript","meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-19 08:07:58","created_by":"1","updated_at":"2017-12-03 22:20:30","updated_by":null},{"id":"6064789120c4c500017fbc7c","name":"DevOps","slug":"devops","description":"DevOps is a software engineering practice that aims at unifying software development and software operation","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/devops-infinity@2x.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-19 10:43:56","created_by":"1","updated_at":"2017-12-03 22:35:36","updated_by":null},{"id":"6064789120c4c500017fbc7d","name":"PHP7","slug":"php7","description":"A revolution in the way we deliver applications that power everything from websites and mobile to enterprises and the cloud","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/php7_white_large-1.jpg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-19 11:02:37","created_by":"1","updated_at":"2017-12-03 22:29:09","updated_by":null},{"id":"6064789120c4c500017fbc7e","name":"Node.js","slug":"node-js","description":"Node.js is an open-source, cross-platform JavaScript run-time environment for executing JavaScript code server-side.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/1200px-Node.js_logo.svg.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-19 13:05:23","created_by":"1","updated_at":"2017-12-03 22:21:03","updated_by":null},{"id":"6064789120c4c500017fbc7f","name":"CSS","slug":"css","description":"Cascading Style Sheets is a style sheet language used for describing the presentation of a document written in a markup language.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/1-yVKDbwtvfoakj3RZ9g8ARQ.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-03-20 08:12:32","created_by":"1","updated_at":"2017-12-03 22:58:44","updated_by":null},{"id":"6064789120c4c500017fbc80","name":"MySQL","slug":"mysql","description":"MySQL is an open-source relational database management system.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/1200px-MySQL.svg.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-06-03 11:19:12","created_by":"1","updated_at":"2017-12-03 22:32:14","updated_by":null},{"id":"6064789120c4c500017fbc81","name":"AWS","slug":"aws","description":"Amazon Web Services provides on-demand cloud computing platforms to individuals, companies and governments, on a paid subscription basis with a free-tier option available for 12 months.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/aws_logo_smile_1200x630.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-07-01 14:05:08","created_by":"1","updated_at":"2017-12-03 22:23:30","updated_by":null},{"id":"6064789120c4c500017fbc82","name":"go","slug":"go","description":"Go is an open source programming language that makes it easy to build simple, reliable, and efficient software","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/golang-gopher.jpg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2016-11-27 11:30:42","created_by":"1","updated_at":"2017-12-03 22:31:26","updated_by":null},{"id":"6064789120c4c500017fbc83","name":"ElasticSearch","slug":"elasticsearch","description":"Elasticsearch is a search engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/elasticsearch-logo.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-04-21 04:12:11","created_by":"1","updated_at":"2017-12-03 22:36:47","updated_by":null},{"id":"6064789120c4c500017fbc84","name":"Docker","slug":"docker","description":"Docker is a software technology providing containers, an additional layer of abstraction and automation of operating-system-level virtualization","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/docker_twitter_share.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-04-21 04:12:11","created_by":"1","updated_at":"2017-12-03 22:22:07","updated_by":null},{"id":"6064789120c4c500017fbc85","name":"Bash","slug":"bash","description":"Bash is a Unix shell and command language for the GNU Project as a free software replacement for the Bourne shell.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/bash-logo-web.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-04-21 04:12:11","created_by":"1","updated_at":"2017-12-03 22:25:21","updated_by":null},{"id":"6064789120c4c500017fbc86","name":"Machine Learning","slug":"machine-learning","description":null,"feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-13.jpeg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-04-21 04:12:11","created_by":"1","updated_at":"2019-01-09 23:47:20","updated_by":null},{"id":"6064789120c4c500017fbc87","name":"Python","slug":"python","description":"Python is a widely used high-level programming language for general-purpose programming","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/python-logo.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-04-21 04:12:11","created_by":"1","updated_at":"2017-12-03 22:24:12","updated_by":null},{"id":"6064789120c4c500017fbc88","name":"Security","slug":"security","description":"The practice of preventing unauthorized access, use, disclosure, disruption, modification, inspection, recording or destruction of information.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/CIA.jpg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-11-09 11:13:27","created_by":"1","updated_at":"2017-12-03 22:28:01","updated_by":null},{"id":"6064789120c4c500017fbc89","name":"Blockchain","slug":"blockchain","description":"A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.","feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2017/12/Blockchain-Logo-Blue6.png","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-12-05 03:17:25","created_by":"1","updated_at":"2017-12-05 03:18:31","updated_by":null},{"id":"6064789120c4c500017fbc8a","name":"Strategy","slug":"strategy","description":null,"feature_image":"https://s3-ap-southeast-2.amazonaws.com/chrisdlangton/ghost/2019/01/placeholder-14.jpeg","parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2018-04-09 03:34:31","created_by":"1","updated_at":"2019-01-09 23:47:47","updated_by":null},{"id":"6064810720c4c500017fbdcc","name":"Linux","slug":"linux","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-03-31 14:02:47","created_by":"1","updated_at":"2021-03-31 14:02:47","updated_by":"1"}],"posts_tags":[{"id":"6064789320c4c500017fbcce","post_id":"6064789220c4c500017fbc8b","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789320c4c500017fbccf","post_id":"6064789220c4c500017fbc8b","tag_id":"6064789120c4c500017fbc7e","sort_order":1},{"id":"6064789320c4c500017fbcd2","post_id":"6064789220c4c500017fbc8c","tag_id":"6064789120c4c500017fbc7c","sort_order":0},{"id":"6064789320c4c500017fbcd5","post_id":"6064789220c4c500017fbc8d","tag_id":"6064789120c4c500017fbc7d","sort_order":0},{"id":"6064789320c4c500017fbcd8","post_id":"6064789220c4c500017fbc8e","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789320c4c500017fbcd9","post_id":"6064789220c4c500017fbc8e","tag_id":"6064789120c4c500017fbc7e","sort_order":1},{"id":"6064789320c4c500017fbcdc","post_id":"6064789220c4c500017fbc8f","tag_id":"6064789120c4c500017fbc7e","sort_order":0},{"id":"6064789320c4c500017fbcdd","post_id":"6064789220c4c500017fbc8f","tag_id":"6064789120c4c500017fbc7b","sort_order":1},{"id":"6064789320c4c500017fbce0","post_id":"6064789220c4c500017fbc90","tag_id":"6064789120c4c500017fbc7e","sort_order":0},{"id":"6064789320c4c500017fbce1","post_id":"6064789220c4c500017fbc90","tag_id":"6064789120c4c500017fbc7b","sort_order":1},{"id":"6064789320c4c500017fbce4","post_id":"6064789220c4c500017fbc91","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789320c4c500017fbce7","post_id":"6064789220c4c500017fbc92","tag_id":"6064789120c4c500017fbc7f","sort_order":0},{"id":"6064789320c4c500017fbce9","post_id":"6064789220c4c500017fbc93","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789320c4c500017fbcec","post_id":"6064789220c4c500017fbc94","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789320c4c500017fbcef","post_id":"6064789220c4c500017fbc95","tag_id":"6064789120c4c500017fbc7a","sort_order":0},{"id":"6064789320c4c500017fbcf0","post_id":"6064789220c4c500017fbc95","tag_id":"6064789120c4c500017fbc7b","sort_order":1},{"id":"6064789420c4c500017fbcf3","post_id":"6064789220c4c500017fbc96","tag_id":"6064789120c4c500017fbc7e","sort_order":0},{"id":"6064789420c4c500017fbcf4","post_id":"6064789220c4c500017fbc96","tag_id":"6064789120c4c500017fbc7b","sort_order":1},{"id":"6064789420c4c500017fbcf6","post_id":"6064789220c4c500017fbc97","tag_id":"6064789120c4c500017fbc80","sort_order":0},{"id":"6064789420c4c500017fbcf7","post_id":"6064789220c4c500017fbc97","tag_id":"6064789120c4c500017fbc7d","sort_order":1},{"id":"6064789420c4c500017fbcf8","post_id":"6064789220c4c500017fbc97","tag_id":"6064789120c4c500017fbc7a","sort_order":2},{"id":"6064789420c4c500017fbcfb","post_id":"6064789220c4c500017fbc98","tag_id":"6064789120c4c500017fbc7f","sort_order":0},{"id":"6064789420c4c500017fbcfd","post_id":"6064789220c4c500017fbc99","tag_id":"6064789120c4c500017fbc7e","sort_order":0},{"id":"6064789420c4c500017fbcfe","post_id":"6064789220c4c500017fbc99","tag_id":"6064789120c4c500017fbc7d","sort_order":1},{"id":"6064789420c4c500017fbd00","post_id":"6064789220c4c500017fbc9a","tag_id":"6064789120c4c500017fbc7f","sort_order":0},{"id":"6064789420c4c500017fbd01","post_id":"6064789220c4c500017fbc9a","tag_id":"6064789120c4c500017fbc7a","sort_order":1},{"id":"6064789420c4c500017fbd04","post_id":"6064789220c4c500017fbc9b","tag_id":"6064789120c4c500017fbc7e","sort_order":0},{"id":"6064789420c4c500017fbd05","post_id":"6064789220c4c500017fbc9b","tag_id":"6064789120c4c500017fbc7b","sort_order":1},{"id":"6064789420c4c500017fbd08","post_id":"6064789220c4c500017fbc9c","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789420c4c500017fbd09","post_id":"6064789220c4c500017fbc9c","tag_id":"6064789120c4c500017fbc7a","sort_order":1},{"id":"6064789420c4c500017fbd0b","post_id":"6064789220c4c500017fbc9d","tag_id":"6064789120c4c500017fbc7d","sort_order":0},{"id":"6064789420c4c500017fbd0d","post_id":"6064789220c4c500017fbc9e","tag_id":"6064789120c4c500017fbc7d","sort_order":0},{"id":"6064789420c4c500017fbd10","post_id":"6064789220c4c500017fbc9f","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789420c4c500017fbd13","post_id":"6064789220c4c500017fbca0","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789420c4c500017fbd16","post_id":"6064789220c4c500017fbca1","tag_id":"6064789120c4c500017fbc81","sort_order":0},{"id":"6064789420c4c500017fbd18","post_id":"6064789220c4c500017fbca2","tag_id":"6064789120c4c500017fbc7a","sort_order":0},{"id":"6064789420c4c500017fbd19","post_id":"6064789220c4c500017fbca2","tag_id":"6064789120c4c500017fbc7d","sort_order":1},{"id":"6064789420c4c500017fbd1a","post_id":"6064789220c4c500017fbca2","tag_id":"6064789120c4c500017fbc7e","sort_order":2},{"id":"6064789420c4c500017fbd1b","post_id":"6064789220c4c500017fbca2","tag_id":"6064789120c4c500017fbc80","sort_order":3},{"id":"6064789420c4c500017fbd1e","post_id":"6064789220c4c500017fbca3","tag_id":"6064789120c4c500017fbc82","sort_order":0},{"id":"6064789420c4c500017fbd20","post_id":"6064789220c4c500017fbca4","tag_id":"6064789120c4c500017fbc81","sort_order":0},{"id":"6064789520c4c500017fbd25","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc86","sort_order":0},{"id":"6064789520c4c500017fbd26","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc80","sort_order":1},{"id":"6064789520c4c500017fbd27","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc83","sort_order":2},{"id":"6064789520c4c500017fbd28","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc84","sort_order":3},{"id":"6064789520c4c500017fbd29","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc85","sort_order":4},{"id":"6064789520c4c500017fbd2a","post_id":"6064789220c4c500017fbca7","tag_id":"6064789120c4c500017fbc87","sort_order":5},{"id":"6064789520c4c500017fbd2c","post_id":"6064789220c4c500017fbca8","tag_id":"6064789120c4c500017fbc83","sort_order":0},{"id":"6064789520c4c500017fbd2d","post_id":"6064789220c4c500017fbca8","tag_id":"6064789120c4c500017fbc86","sort_order":1},{"id":"6064789520c4c500017fbd2e","post_id":"6064789220c4c500017fbca8","tag_id":"6064789120c4c500017fbc81","sort_order":2},{"id":"6064789520c4c500017fbd30","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc7a","sort_order":0},{"id":"6064789520c4c500017fbd31","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc7d","sort_order":1},{"id":"6064789520c4c500017fbd32","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc7b","sort_order":2},{"id":"6064789520c4c500017fbd33","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc7e","sort_order":3},{"id":"6064789520c4c500017fbd34","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc87","sort_order":4},{"id":"6064789520c4c500017fbd35","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc82","sort_order":5},{"id":"6064789520c4c500017fbd36","post_id":"6064789220c4c500017fbca9","tag_id":"6064789120c4c500017fbc85","sort_order":6},{"id":"6064789520c4c500017fbd38","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc86","sort_order":0},{"id":"6064789520c4c500017fbd39","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc7d","sort_order":1},{"id":"6064789520c4c500017fbd3a","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc7b","sort_order":2},{"id":"6064789520c4c500017fbd3b","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc7e","sort_order":3},{"id":"6064789520c4c500017fbd3c","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc82","sort_order":4},{"id":"6064789520c4c500017fbd3d","post_id":"6064789220c4c500017fbcaa","tag_id":"6064789120c4c500017fbc87","sort_order":5},{"id":"6064789520c4c500017fbd40","post_id":"6064789220c4c500017fbcab","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789520c4c500017fbd42","post_id":"6064789220c4c500017fbcac","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789520c4c500017fbd44","post_id":"6064789220c4c500017fbcad","tag_id":"6064789120c4c500017fbc7b","sort_order":0},{"id":"6064789520c4c500017fbd46","post_id":"6064789220c4c500017fbcae","tag_id":"6064789120c4c500017fbc84","sort_order":0},{"id":"6064789520c4c500017fbd47","post_id":"6064789220c4c500017fbcae","tag_id":"6064789120c4c500017fbc88","sort_order":1},{"id":"6064789520c4c500017fbd49","post_id":"6064789220c4c500017fbcaf","tag_id":"6064789120c4c500017fbc81","sort_order":0},{"id":"6064789520c4c500017fbd4b","post_id":"6064789220c4c500017fbcb0","tag_id":"6064789120c4c500017fbc87","sort_order":0},{"id":"6064789520c4c500017fbd4c","post_id":"6064789220c4c500017fbcb0","tag_id":"6064789120c4c500017fbc85","sort_order":1},{"id":"6064789520c4c500017fbd4d","post_id":"6064789220c4c500017fbcb0","tag_id":"6064789120c4c500017fbc88","sort_order":2},{"id":"6064789620c4c500017fbd4f","post_id":"6064789220c4c500017fbcb1","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789620c4c500017fbd51","post_id":"6064789220c4c500017fbcb2","tag_id":"6064789120c4c500017fbc89","sort_order":0},{"id":"6064789620c4c500017fbd53","post_id":"6064789220c4c500017fbcb3","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789620c4c500017fbd55","post_id":"6064789220c4c500017fbcb4","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd57","post_id":"6064789220c4c500017fbcb5","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd58","post_id":"6064789220c4c500017fbcb5","tag_id":"6064789120c4c500017fbc7c","sort_order":1},{"id":"6064789720c4c500017fbd5a","post_id":"6064789220c4c500017fbcb6","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd5d","post_id":"6064789220c4c500017fbcb8","tag_id":"6064789120c4c500017fbc8a","sort_order":0},{"id":"6064789720c4c500017fbd5f","post_id":"6064789220c4c500017fbcb9","tag_id":"6064789120c4c500017fbc7c","sort_order":0},{"id":"6064789720c4c500017fbd60","post_id":"6064789220c4c500017fbcb9","tag_id":"6064789120c4c500017fbc81","sort_order":1},{"id":"6064789720c4c500017fbd62","post_id":"6064789220c4c500017fbcba","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd63","post_id":"6064789220c4c500017fbcba","tag_id":"6064789120c4c500017fbc81","sort_order":1},{"id":"6064789720c4c500017fbd65","post_id":"6064789220c4c500017fbcbb","tag_id":"6064789120c4c500017fbc86","sort_order":0},{"id":"6064789720c4c500017fbd67","post_id":"6064789220c4c500017fbcbc","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd69","post_id":"6064789220c4c500017fbcbd","tag_id":"6064789120c4c500017fbc89","sort_order":0},{"id":"6064789720c4c500017fbd6b","post_id":"6064789220c4c500017fbcbe","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd6d","post_id":"6064789220c4c500017fbcbf","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd6f","post_id":"6064789220c4c500017fbcc0","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd71","post_id":"6064789220c4c500017fbcc1","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789720c4c500017fbd74","post_id":"6064789220c4c500017fbcc3","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd77","post_id":"6064789220c4c500017fbcc5","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd79","post_id":"6064789220c4c500017fbcc6","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd7a","post_id":"6064789220c4c500017fbcc6","tag_id":"6064789120c4c500017fbc81","sort_order":1},{"id":"6064789820c4c500017fbd7d","post_id":"6064789220c4c500017fbcc7","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd7e","post_id":"6064789220c4c500017fbcc7","tag_id":"6064789120c4c500017fbc81","sort_order":1},{"id":"6064789820c4c500017fbd80","post_id":"6064789220c4c500017fbcc8","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd82","post_id":"6064789220c4c500017fbcc9","tag_id":"6064789120c4c500017fbc85","sort_order":0},{"id":"6064789820c4c500017fbd83","post_id":"6064789220c4c500017fbcc9","tag_id":"6064789120c4c500017fbc87","sort_order":1},{"id":"6064789820c4c500017fbd85","post_id":"6064789220c4c500017fbcca","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd87","post_id":"6064789220c4c500017fbccb","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064789820c4c500017fbd89","post_id":"6064789220c4c500017fbccc","tag_id":"6064789120c4c500017fbc87","sort_order":0},{"id":"6064789820c4c500017fbd8a","post_id":"6064789220c4c500017fbccc","tag_id":"6064789120c4c500017fbc88","sort_order":1},{"id":"6064789820c4c500017fbd8c","post_id":"6064789220c4c500017fbccd","tag_id":"6064789120c4c500017fbc88","sort_order":0},{"id":"6064810720c4c500017fbdcd","post_id":"6064789220c4c500017fbcb7","tag_id":"6064810720c4c500017fbdcc","sort_order":0}],"invites":[],"brute":[{"key":"1W5moZuMlmSkT0bTLB4FOz79mthkQhemD7Mhf6R9IJk=","firstRequest":1617196691202,"lastRequest":1617197511655,"lifetime":1617201111656,"count":4},{"key":"t6wibdpKCMzVP2VeHr+HgwVAfkLQY8+FLVHZhAmzP+c=","firstRequest":1617197330142,"lastRequest":1617197378280,"lifetime":1629898178280,"count":2},{"key":"Zcm3am3w4KCEjTgDRUPF5abzVjTvH5jL41/mJ1l3VeI=","firstRequest":1617197449626,"lastRequest":1617197468545,"lifetime":1617201068546,"count":2},{"key":"xKVpPNOU2iEs5ZC8prQkZ4f4JjpPRItfCGu8G6FNJZo=","firstRequest":1617197449631,"lastRequest":1617197468549,"lifetime":1617201068549,"count":2}],"integrations":[{"id":"6064747c20c4c500017fba0e","type":"builtin","name":"Zapier","slug":"zapier","icon_image":null,"description":"Built-in Zapier integration","created_at":"2021-03-31 13:09:16","created_by":"1","updated_at":"2021-03-31 13:09:16","updated_by":"1"},{"id":"6064747c20c4c500017fba10","type":"internal","name":"Ghost Backup","slug":"ghost-backup","icon_image":null,"description":"Internal DB Backup integration","created_at":"2021-03-31 13:09:16","created_by":"1","updated_at":"2021-03-31 13:09:16","updated_by":"1"},{"id":"6064747c20c4c500017fba12","type":"internal","name":"Ghost Scheduler","slug":"ghost-scheduler","icon_image":null,"description":"Internal Scheduler integration","created_at":"2021-03-31 13:09:16","created_by":"1","updated_at":"2021-03-31 13:09:16","updated_by":"1"}],"webhooks":[],"api_keys":[{"id":"6064747c20c4c500017fba0f","type":"admin","secret":"82b65f440f164e268de77a7ad8b27031ed3021cb04d59e92f092e471fc1c881b","role_id":"6064747b20c4c500017fb9a4","integration_id":"6064747c20c4c500017fba0e","user_id":null,"last_seen_at":null,"last_seen_version":null,"created_at":1617196156544,"created_by":"1","updated_at":1617196156544,"updated_by":"1"},{"id":"6064747c20c4c500017fba11","type":"admin","secret":"28dcaeeeeae27aa58fc4d21d20e42a23efe49e4079d24b4aed55bb69a2dae79e","role_id":"6064747b20c4c500017fb9a5","integration_id":"6064747c20c4c500017fba10","user_id":null,"last_seen_at":null,"last_seen_version":null,"created_at":1617196156557,"created_by":"1","updated_at":1617196156557,"updated_by":"1"},{"id":"6064747c20c4c500017fba13","type":"admin","secret":"de44097362e75e07cbd2eb03030ed3db17515de6a50c18eb30397a12e48cc3a2","role_id":"6064747b20c4c500017fb9a6","integration_id":"6064747c20c4c500017fba12","user_id":null,"last_seen_at":null,"last_seen_version":null,"created_at":1617196156565,"created_by":"1","updated_at":1617196156565,"updated_by":"1"},{"id":"6064782e20c4c500017fbc6d","type":"admin","secret":"18dd5034c1b7fe6e1f107f6d4577fb58a1f32240892d0b56289a3adb62fec8b5","role_id":"6064747b20c4c500017fb9a4","integration_id":null,"user_id":"1","last_seen_at":null,"last_seen_version":null,"created_at":1617197102949,"created_by":"6064782e20c4c500017fbc6d","updated_at":1617197102949,"updated_by":"6064782e20c4c500017fbc6d"}],"members":[],"labels":[],"members_labels":[],"members_stripe_customers":[],"members_stripe_customers_subscriptions":[],"actions":[{"id":"606476c120c4c500017fbb45","resource_id":"6064747c20c4c500017fba04","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:18:57"},{"id":"606476c920c4c500017fbb46","resource_id":"6064747c20c4c500017fb9f8","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:05"},{"id":"606476cf20c4c500017fbb47","resource_id":"6064747c20c4c500017fba02","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:11"},{"id":"606476d620c4c500017fbb48","resource_id":"6064747c20c4c500017fba00","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:18"},{"id":"606476dc20c4c500017fbb49","resource_id":"6064747c20c4c500017fb9fe","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:24"},{"id":"606476e320c4c500017fbb4a","resource_id":"6064747c20c4c500017fb9fc","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:31"},{"id":"606476e820c4c500017fbb4b","resource_id":"6064747c20c4c500017fb9fa","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:19:36"},{"id":"6064782820c4c500017fbc6c","resource_id":"5951f5fca366002ebd5dbef7","resource_type":"user","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2021-03-31 13:24:56"},{"id":"6064785420c4c500017fbc6e","resource_id":"1","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:25:40"},{"id":"606478af20c4c500017fbd90","resource_id":"1","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:27:11"},{"id":"60647a1020c4c500017fbd92","resource_id":"1","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:33:04"},{"id":"60647e5020c4c500017fbd94","resource_id":"6064789120c4c500017fbc6f","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:51:12"},{"id":"60647f5c20c4c500017fbd95","resource_id":"6064789120c4c500017fbc6f","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:55:40"},{"id":"60647f8920c4c500017fbd97","resource_id":"6064789220c4c500017fbcc4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:56:25"},{"id":"60647f8a20c4c500017fbd98","resource_id":"6064789220c4c500017fbcc4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:56:26"},{"id":"60647f9c20c4c500017fbd9a","resource_id":"6064789220c4c500017fbcc2","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:56:44"},{"id":"60647fa620c4c500017fbd9c","resource_id":"6064789220c4c500017fbccd","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:56:54"},{"id":"60647fa720c4c500017fbd9d","resource_id":"6064789220c4c500017fbccd","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:56:55"},{"id":"60647fec20c4c500017fbda0","resource_id":"6064789220c4c500017fbccd","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:04"},{"id":"60647fff20c4c500017fbda2","resource_id":"6064789220c4c500017fbccc","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:23"},{"id":"6064800020c4c500017fbda3","resource_id":"6064789220c4c500017fbccc","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:24"},{"id":"6064800920c4c500017fbda5","resource_id":"6064789220c4c500017fbccb","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:33"},{"id":"6064800c20c4c500017fbda6","resource_id":"6064789220c4c500017fbccb","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:36"},{"id":"6064801620c4c500017fbda8","resource_id":"6064789220c4c500017fbcca","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:46"},{"id":"6064801720c4c500017fbda9","resource_id":"6064789220c4c500017fbcca","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:47"},{"id":"6064801e20c4c500017fbdab","resource_id":"6064789220c4c500017fbcc6","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:58:54"},{"id":"6064802920c4c500017fbdad","resource_id":"6064789220c4c500017fbcc9","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:59:05"},{"id":"6064803120c4c500017fbdaf","resource_id":"6064789220c4c500017fbcc5","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:59:13"},{"id":"6064803820c4c500017fbdb1","resource_id":"6064789220c4c500017fbcc7","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:59:20"},{"id":"6064804920c4c500017fbdb3","resource_id":"6064789220c4c500017fbcba","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:59:37"},{"id":"6064805620c4c500017fbdb5","resource_id":"6064789220c4c500017fbcc8","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 13:59:50"},{"id":"6064806020c4c500017fbdb7","resource_id":"6064789220c4c500017fbcc3","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:00:00"},{"id":"6064806c20c4c500017fbdb9","resource_id":"6064789220c4c500017fbcc1","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:00:12"},{"id":"6064807720c4c500017fbdbb","resource_id":"6064789220c4c500017fbcc0","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:00:23"},{"id":"6064808420c4c500017fbdbd","resource_id":"6064789220c4c500017fbcbf","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:00:36"},{"id":"6064808d20c4c500017fbdbf","resource_id":"6064789220c4c500017fbcb6","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:00:45"},{"id":"6064809d20c4c500017fbdc1","resource_id":"6064789220c4c500017fbcbe","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:01:01"},{"id":"606480a620c4c500017fbdc3","resource_id":"6064789220c4c500017fbcbc","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:01:10"},{"id":"606480b520c4c500017fbdc5","resource_id":"6064789220c4c500017fbcbb","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:01:25"},{"id":"606480d820c4c500017fbdc7","resource_id":"6064789220c4c500017fbcbd","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:02:00"},{"id":"606480e220c4c500017fbdc9","resource_id":"6064789220c4c500017fbcb9","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:02:10"},{"id":"606480eb20c4c500017fbdcb","resource_id":"6064789220c4c500017fbcb7","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:02:19"},{"id":"6064810720c4c500017fbdce","resource_id":"6064810720c4c500017fbdcc","resource_type":"tag","actor_id":"1","actor_type":"user","event":"added","context":null,"created_at":"2021-03-31 14:02:47"},{"id":"6064810720c4c500017fbdcf","resource_id":"6064789220c4c500017fbcb7","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:02:47"},{"id":"6064811220c4c500017fbdd1","resource_id":"6064789220c4c500017fbcb8","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:02:58"},{"id":"6064817120c4c500017fbdd4","resource_id":"6064789220c4c500017fbcb8","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:04:33"},{"id":"606482e120c4c500017fbdd6","resource_id":"6064789220c4c500017fbcb4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:10:41"},{"id":"606482e220c4c500017fbdd7","resource_id":"6064789220c4c500017fbcb4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:10:42"},{"id":"606482ef20c4c500017fbdd9","resource_id":"6064789220c4c500017fbcb5","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:10:55"},{"id":"606482f920c4c500017fbddb","resource_id":"6064789220c4c500017fbcb2","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:05"},{"id":"6064830320c4c500017fbddd","resource_id":"6064789220c4c500017fbcb3","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:15"},{"id":"6064830420c4c500017fbdde","resource_id":"6064789220c4c500017fbcb3","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:16"},{"id":"6064831020c4c500017fbde0","resource_id":"6064789220c4c500017fbcb1","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:28"},{"id":"6064831c20c4c500017fbde2","resource_id":"6064789220c4c500017fbc90","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:40"},{"id":"6064832520c4c500017fbde4","resource_id":"6064789220c4c500017fbcaa","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:49"},{"id":"6064832620c4c500017fbde5","resource_id":"6064789220c4c500017fbcaa","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:50"},{"id":"6064832f20c4c500017fbde7","resource_id":"6064789220c4c500017fbcac","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:11:59"},{"id":"6064833820c4c500017fbde9","resource_id":"6064789220c4c500017fbcad","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:08"},{"id":"6064834320c4c500017fbdeb","resource_id":"6064789220c4c500017fbcae","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:19"},{"id":"6064834c20c4c500017fbded","resource_id":"6064789220c4c500017fbcb0","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:28"},{"id":"6064835620c4c500017fbdef","resource_id":"6064789220c4c500017fbcab","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:38"},{"id":"6064836020c4c500017fbdf1","resource_id":"6064789220c4c500017fbcaf","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:48"},{"id":"6064836120c4c500017fbdf2","resource_id":"6064789220c4c500017fbcaf","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:12:49"},{"id":"6064837020c4c500017fbdf4","resource_id":"6064789220c4c500017fbca9","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:04"},{"id":"6064838620c4c500017fbdf6","resource_id":"6064789220c4c500017fbca8","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:26"},{"id":"6064838f20c4c500017fbdf8","resource_id":"6064789220c4c500017fbca7","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:35"},{"id":"6064839820c4c500017fbdfa","resource_id":"6064789220c4c500017fbca2","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:44"},{"id":"606483a220c4c500017fbdfc","resource_id":"6064789220c4c500017fbca4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:54"},{"id":"606483a320c4c500017fbdfd","resource_id":"6064789220c4c500017fbca4","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:13:55"},{"id":"606483ae20c4c500017fbdff","resource_id":"6064789220c4c500017fbca0","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:14:06"},{"id":"606483b020c4c500017fbe00","resource_id":"6064789220c4c500017fbca0","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:14:08"},{"id":"606483ce20c4c500017fbe02","resource_id":"6064789220c4c500017fbca3","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:14:38"},{"id":"606483d720c4c500017fbe04","resource_id":"6064789220c4c500017fbca1","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:14:47"},{"id":"606483e420c4c500017fbe06","resource_id":"6064789220c4c500017fbc9f","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:00"},{"id":"606483ee20c4c500017fbe08","resource_id":"6064789220c4c500017fbc9d","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:10"},{"id":"606483fe20c4c500017fbe0a","resource_id":"6064789220c4c500017fbc9e","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:26"},{"id":"6064840720c4c500017fbe0c","resource_id":"6064789220c4c500017fbc9c","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:35"},{"id":"6064841220c4c500017fbe0e","resource_id":"6064789220c4c500017fbc9a","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:46"},{"id":"6064841f20c4c500017fbe10","resource_id":"6064789220c4c500017fbc8f","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:15:59"},{"id":"6064842020c4c500017fbe11","resource_id":"6064789220c4c500017fbc8f","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:16:00"},{"id":"6064842920c4c500017fbe13","resource_id":"6064789220c4c500017fbc97","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:16:09"},{"id":"6064843120c4c500017fbe15","resource_id":"6064789220c4c500017fbc9b","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:16:17"},{"id":"6064843d20c4c500017fbe17","resource_id":"6064789220c4c500017fbc99","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:16:29"},{"id":"6064845020c4c500017fbe19","resource_id":"6064789220c4c500017fbc98","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:16:48"},{"id":"6064849e20c4c500017fbe1b","resource_id":"6064789220c4c500017fbc93","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:18:06"},{"id":"606484a920c4c500017fbe1d","resource_id":"6064789220c4c500017fbc96","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:18:17"},{"id":"606484b320c4c500017fbe1f","resource_id":"6064789220c4c500017fbc95","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:18:27"},{"id":"606484c120c4c500017fbe21","resource_id":"6064789220c4c500017fbc94","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:18:41"},{"id":"606484cd20c4c500017fbe23","resource_id":"6064789220c4c500017fbc92","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:18:53"},{"id":"606484d620c4c500017fbe25","resource_id":"6064789220c4c500017fbc8b","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:02"},{"id":"606484e520c4c500017fbe27","resource_id":"6064789220c4c500017fbc8c","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:17"},{"id":"606484e620c4c500017fbe28","resource_id":"6064789220c4c500017fbc8c","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:18"},{"id":"606484ef20c4c500017fbe2a","resource_id":"6064789220c4c500017fbc8d","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:27"},{"id":"606484f820c4c500017fbe2c","resource_id":"6064789220c4c500017fbc91","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:36"},{"id":"6064850120c4c500017fbe2e","resource_id":"6064789220c4c500017fbc8e","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:19:45"},{"id":"6064851020c4c500017fbe31","resource_id":"6064789220c4c500017fbcc2","resource_type":"post","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2021-03-31 14:20:00"}],"emails":[],"tokens":[],"snippets":[]}}