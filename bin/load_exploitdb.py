import re
import json
import logging
import pathlib
import glob
from datetime import datetime, timedelta
from random import randint, shuffle
from time import sleep
import requests
from requests.exceptions import ConnectTimeout, ReadTimeout
from bs4 import BeautifulSoup as bs
from retry.api import retry
from trivialsec.helpers.config import config
from trivialsec.models.cve import CVE
from trivialsec.models.cve_exploit import Exploit


session = requests.Session()
logger = logging.getLogger(__name__)
logging.basicConfig(
    format='%(asctime)s - %(name)s - [%(levelname)s] %(message)s',
    level=logging.INFO
)
BASE_URL = 'https://www.exploit-db.com/'
DATAFILE_DIR = 'datafiles/exploit-db/submissions'
RAWFILE_DIR = 'datafiles/exploit-db/raw'
CVE_PATTERN = r"(CVE\-\d{4}\-\d*)"
PROXIES = None
if config.http_proxy or config.https_proxy:
    PROXIES = {
        'http': f'http://{config.http_proxy}',
        'https': f'https://{config.https_proxy}'
    }

def process_file(filename :str):
    edb_file = pathlib.Path(filename)
    if edb_file.is_file():
        raw_text = edb_file.read_text()
        edb_data = json.loads(raw_text)
        for cve_ref in edb_data.get('cve', []):
            try:
                Exploit(
                    cve_id=cve_ref,
                    source='exploit-db',
                    source_id=edb_data['edb_id'],
                    source_url=edb_data['url'],
                    title=edb_data['title'],
                    published_at=edb_data['published'],
                    author=edb_data['author'],
                    author_url=edb_data['author_url'],
                    verified=edb_data.get('verified', False),
                ).persist()
            except Exception as ex:
                logger.exception(ex)
                logger.error(f'cve ref {cve_ref}')
            cve = CVE(cve_id=cve_ref)
            cve.hydrate()
            if cve.published_at is None:
                logger.warning(f'Official {cve_ref} missing from our Database')
                cve.assigner = 'Unknown'
                cve.title = edb_data['title']
                cve.description = f"Exploit submitted by {edb_data['author']}"
                cve.reported_at = edb_data['published']
                cve.published_at = edb_data['published']
                cve.cvss_version = '3.1'
                cve.vector = 'CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:N'

            if cve.cvss_version in ['2.0']:
                vec = CVE.vector_to_dict(cve.vector, 2)
                vec['E'] = 'POC'
                vec['RC'] = 'UC'
                if edb_data['exploit_exists'] is True:
                    vec['E'] = 'F'
                    vec['RC'] = 'UR'
                if edb_data.get('verified', False):
                    vec['E'] = 'H'
                    vec['RC'] = 'C'
                cve.vector = CVE.dict_to_vector(vec, 2)
            if cve.cvss_version in ['3.0', '3.1']:
                vec = CVE.vector_to_dict(cve.vector, 3)
                vec['E'] = 'P'
                vec['RC'] = 'U'
                if edb_data['exploit_exists'] is True:
                    vec['E'] = 'F'
                    vec['RC'] = 'R'
                if edb_data.get('verified', False):
                    vec['E'] = 'H'
                    vec['RC'] = 'C'
                cve.vector = CVE.dict_to_vector(vec, 3)
            cve.persist()

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_raw(ref_id :int):
    api_url = f'{BASE_URL}raw/{ref_id}'
    logger.info(api_url)
    resp = requests.get(
        api_url,
        proxies=PROXIES,
        headers={
            'user-agent': config.user_agent,
            'referer': f'{BASE_URL}exploits/{ref_id}'
        },
        timeout=3
    )
    if resp.status_code != 200:
        logger.error(f'status_code {resp.status_code}')
        return None

    return resp.text

def html_to_dict(html_content :str):
    meta_title = None
    keywords = None
    cve_id = None
    platform = None
    edb_type = None
    meta_author = None
    meta_author_url = None
    meta_published_time = None
    soup = bs(html_content, 'html.parser')
    
    meta_tag = soup.find(property='og:title')
    if meta_tag:
        meta_title = meta_tag.get("content")
    meta_tag = [item['content'] for item in soup.select('meta[name=keywords]')]
    if meta_tag:
        meta_keywords = meta_tag[0]
        if ',' in meta_keywords:
            keywords_arr = meta_keywords.split(',')
            if len(keywords_arr) == 2:
                platform, edb_type = keywords_arr
            if len(keywords_arr) == 3:
                platform, edb_type, keywords = keywords_arr
            if len(keywords_arr) > 3:
                platform, edb_type, *keywords = keywords_arr
                keywords = ','.join(keywords)
        else:
            keywords = meta_keywords

        cve_id = None
        matches = re.search(CVE_PATTERN, keywords)
        if matches is not None:
            cve_id = matches.group(1)
    meta_tag = soup.find(property='article:author')
    if meta_tag:
        meta_author = meta_tag.get("content")
    meta_tag = soup.find(property='article:authorUrl')
    if meta_tag:
        meta_author_url = meta_tag.get("content")
    meta_tag = soup.find(property='article:published_time')
    if meta_tag:
        meta_published_time = meta_tag.get("content")

    return {
        'cve': cve_id,
        'published': meta_published_time,
        'title': meta_title,
        'author': meta_author,
        'author_url': meta_author_url,
        'platform': platform,
        'type': edb_type,
        'keywords': keywords if cve_id is None else keywords.replace(cve_id, '').strip(),
    }

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_bulk(start :int, length :int = 15):
    timestamp = int((datetime.utcnow() - datetime(1970, 1, 1)) / timedelta(seconds=1)*1000)
    edb_url = f'{BASE_URL}?draw=2&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start={start}&length={length}&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_={timestamp}'
    logger.info(f'start={start} length={length} timestamp={timestamp}')
    try:
        resp = session.get(
            edb_url,
            proxies=PROXIES,
            headers={
                'x-requested-with': 'XMLHttpRequest',
                'user-agent': config.user_agent,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    if code != 200:
        logger.warning(f'{code} {edb_url}')
        return None

    return json.loads(resp.text)

@retry((ConnectTimeout, ReadTimeout), tries=10, delay=30, backoff=5)
def query_single(ref_id :int, original_data :dict):
    if not isinstance(original_data, dict):
        original_data = {}
    edb_url = f'{BASE_URL}exploits/{ref_id}'
    logger.info(f'query_single data ref_id={ref_id}')
    try:
        resp = session.head(
            edb_url,
            proxies=PROXIES,
            headers={
                'user-agent': config.user_agent,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=3
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    ret = {}
    if code == 200:
        logger.info(f'query_single raw file ref_id={ref_id}')
        resp = session.get(
            edb_url,
            proxies=PROXIES,
            headers={
                'user-agent': config.user_agent,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
        ret = html_to_dict(resp.text)

    else:
        logger.warning(f'{code} {edb_url}')

    ret['edb_id'] = ref_id
    ret['url'] = edb_url
    ret['http_code'] = code
    ret['created_at'] = original_data.get('created_at') if 'created_at' in original_data else datetime.utcnow().isoformat()
    ret['download_url'] = f'{BASE_URL}download/{ref_id}'
    raw_file = pathlib.Path(f'{RAWFILE_DIR}/{ref_id}')
    ret['exploit_exists'] = raw_file.is_file()
    if not ret['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(ref_id)
        if raw is not None and raw:
            raw_file.write_text(raw)
            ret['exploit_exists'] = True

    return ret

def main():
    next_id = 1
    last_id = 50200
    ids = list(range(next_id, last_id))
    shuffle(ids)
    for check_id in ids:
        not_today = datetime.utcnow() - timedelta(seconds=86400)
        submission_filepath = f'{DATAFILE_DIR}/{check_id}.json'
        submission_file = pathlib.Path(submission_filepath)
        json_data = {}
        if submission_file.is_file():
            json_data = json.loads(submission_file.read_text())
            last_modified = datetime.fromtimestamp(submission_file.stat().st_mtime)
            if last_modified > not_today:
                raw_file = pathlib.Path(f'{RAWFILE_DIR}/{check_id}')
                json_data['exploit_exists'] = raw_file.is_file()
                if not json_data['exploit_exists']:
                    sleep(randint(3,6))
                    raw = query_raw(check_id)
                    if raw is not None and raw:
                        raw_file.write_text(raw)
                        json_data['exploit_exists'] = True
                continue

        sleep(randint(3,6))
        data = query_single(check_id, json_data)
        if data is not None:
            submission_file.write_text(json.dumps(data, default=str, sort_keys=True))
            process_file(submission_filepath)

def save_edb_format(data :dict):
    del data['download']
    cve = []
    if data['code']:
        for code in data['code']:
            # find $DATAFILE_DIR -name '*.json' -exec jq -r '._raw.code[].code_type' {} \; 2>/dev/null
            if code['code_type'].lower() == 'cve':
                cve.append(f"CVE-{code['code'].upper().replace('CVE‑', '').replace('VE‑', '').replace('CVE ', '').replace('‑', '-').replace('–', '-')}")

    submission_data = {}
    submission_filepath = f"{DATAFILE_DIR}/{data['id']}.json"
    submission_file = pathlib.Path(submission_filepath)
    if submission_file.is_file():
        submission_data = json.loads(submission_file.read_text())

    submission_data |= {
        "_raw": data,
        "edb_id": int(data['id']),
        "verified": data['verified'] == 1,
        "url": f"{BASE_URL}exploits/{data['id']}",
        "cve": cve,
        "published": data['date_published'],
        "title": data['description'][1],
        "author": data['author']['name'],
        "author_url": f"{BASE_URL}?author={data['author']['id']}",
        "platform": data['platform']['platform'],
        "type": data['type']['display'],
        "download_url": f"{BASE_URL}download/{data['id']}",
        "created_at": submission_data['created_at'] if 'created_at' in submission_data else datetime.utcnow().isoformat(),
    }
    raw_file = pathlib.Path(f'{RAWFILE_DIR}/{data["id"]}')
    submission_data['exploit_exists'] = raw_file.is_file()
    if not submission_data['exploit_exists']:
        sleep(randint(3,6))
        raw = query_raw(data['id'])
        if raw is not None and raw:
            raw_file.write_text(raw)
            submission_data['exploit_exists'] = True
    if 'last_checked' in submission_data:
        del submission_data['last_checked']
    submission_file.write_text(json.dumps(submission_data, default=str, sort_keys=True))
    process_file(submission_filepath)

def do_bulk():
    length = 1000
    response = query_bulk(0, length)
    if response is None:
        return
    data = response.get('data', [])
    if not data:
        return
    for edb in data:
        save_edb_format(edb)

    break_on = int(data[0]['id']) + length
    start = length
    while start <= break_on:
        sleep(randint(4,8))
        response = query_bulk(start, length)
        if response is None:
            continue
        data = response.get('data', [])
        if not data:
            start += length
            continue
        for edb in data:
            save_edb_format(edb)
        start += length

def do_latest():
    length = 1000
    response = query_bulk(0, length)
    if response is None:
        return
    data = response.get('data', [])
    if not data:
        return
    for edb in data:
        save_edb_format(edb)

def read_file(file_path :str):
    bulk_file = pathlib.Path(file_path)
    if bulk_file.is_file():
        for item in json.loads(bulk_file.read_text()).get('data', []):
            datafile = f"{DATAFILE_DIR}/{item['id']}.json"
            logger.info(datafile)
            edb_file = pathlib.Path(datafile)
            if edb_file.is_file():
                original_data = json.loads(edb_file.read_text())
                original_data |= item
                save_edb_format(original_data)
                continue
            save_edb_format(item)

if __name__ == "__main__":
    # read_file("exploitdb-response.json")
    # do_bulk()
    do_latest()
